{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anemll/qwen3_apple_style_2bit_qat_lora/blob/main/notebooks/Qwen3_QAT_KD_LoRA-per-layer-2bit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rRgC0uK43c2v",
      "metadata": {
        "id": "rRgC0uK43c2v"
      },
      "source": [
        "# Qwen3-0.6B — Apple-style QAT (2-bit / 4-bit) + KD + LoRA recovery\n",
        "\n",
        "This notebook mirrors the structure of common “phone deployment” notebooks, but uses **this repo’s** pipeline:\n",
        "\n",
        "- **Stage A (recommended default):** KD-QAT on plain text (C4 streaming) or KD-cache QAT\n",
        "- **Stage B:** LoRA recovery (either SFT or cached KD-LoRA)\n",
        "- Plot `loss.csv`\n",
        "- Run inference sanity checks\n",
        "\n",
        "Notes:\n",
        "- Qwen3 requires `transformers>=4.51.0`.\n",
        "- For disk usage: C4 is huge; prefer `--streaming` unless you explicitly want to download.\n",
        "- Bitwidth: use `-q 2` (default) or `-q 4` (less aggressive). Checkpoints persist the bitwidth per layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i6Pb9Kki3c2w",
      "metadata": {
        "id": "i6Pb9Kki3c2w"
      },
      "source": [
        "## 0) Setup (Colab / local)\n",
        "\n",
        "If you’re in Colab, clone the repo. If you’re already in the repo directory locally, you can skip this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "UXlLPdtkGM35",
      "metadata": {
        "id": "UXlLPdtkGM35"
      },
      "outputs": [],
      "source": [
        "# ---- Config (edit these) ----\n",
        "#MODEL_NAME = 'Qwen/Qwen3-4B-Thinking-2507'\n",
        "MODEL_NAME = 'Qwen/Qwen3-0.6B'\n",
        "TEACHER_NAME = MODEL_NAME\n",
        "QUANT_BITS = 2  # 2 or 4\n",
        "DEVICE = 'auto'\n",
        "AMP_DTYPE = 'auto'\n",
        "PARAM_DTYPE = 'auto'\n",
        "DTYPE = 'auto'\n",
        "\n",
        "# Cache dirs\n",
        "CACHE_DIR_CHAT = 'caches/alpaca_chat_think_both_L128_K32_R256'\n",
        "CACHE_DIR_TEXT = 'caches/c4_qwen3_L64_K32_R256'\n",
        "#CACHE_DIR_CHAT = 'caches/Q4B_alpaca_chat_think_L128_K32_R256'\n",
        "#CACHE_DIR_TEXT = 'caches/Q4B_c4_qwen3_L64_K32_R256'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "u2vonfu23c2w",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u2vonfu23c2w",
        "outputId": "09702797-f647-4f43-b8e2-606ee7fec85f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'qwen3_apple_style_2bit_qat_lora'...\n",
            "remote: Enumerating objects: 228, done.\u001b[K\n",
            "remote: Counting objects: 100% (228/228), done.\u001b[K\n",
            "remote: Compressing objects: 100% (162/162), done.\u001b[K\n",
            "remote: Total 228 (delta 131), reused 144 (delta 63), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (228/228), 281.62 KiB | 9.08 MiB/s, done.\n",
            "Resolving deltas: 100% (131/131), done.\n",
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "Already up to date.\n",
            "HEAD is now at 69a5e58 Update layer-by-layer for 4 point checkpoint\n"
          ]
        }
      ],
      "source": [
        "# Colab-only:\n",
        "%cd /content/\n",
        "!git clone https://github.com/Anemll/qwen3_apple_style_2bit_qat_lora\n",
        "%cd qwen3_apple_style_2bit_qat_lora\n",
        "!git fetch\n",
        "!git pull\n",
        "!git reset --hard HEAD\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZUmiISSL3c2w",
      "metadata": {
        "id": "ZUmiISSL3c2w"
      },
      "source": [
        "## 1) Install dependencies (uv)\n",
        "\n",
        "This repo is set up to work with `uv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ACRjVuVa3c2x",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACRjVuVa3c2x",
        "outputId": "697288a1-37f4-4f67-b8a7-cfbb8ab7f1ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m123.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m7 packages\u001b[0m \u001b[2min 133ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m62 packages\u001b[0m \u001b[2min 419ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 869ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mqat-lora\u001b[0m\u001b[2m==0.0.0 (from file:///content/qwen3_apple_style_2bit_qat_lora)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip -q install uv\n",
        "!uv pip install -r requirements.txt\n",
        "!uv pip install -e .\n",
        "# plotting\n",
        "!uv pip install -q matplotlib\n",
        "!uv pip install -q plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yfoLPbBk3c2x",
      "metadata": {
        "id": "yfoLPbBk3c2x"
      },
      "source": [
        "## 2) Optional: Hugging Face login\n",
        "\n",
        "If you hit gated model/dataset errors, log in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wLMWX7E23c2x",
      "metadata": {
        "id": "wLMWX7E23c2x"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()  # paste token when prompted\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fA37ilf33c2x",
      "metadata": {
        "id": "fA37ilf33c2x"
      },
      "source": [
        "## 3) Quick environment check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "VUCXehkU3c2x",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUCXehkU3c2x",
        "outputId": "eecbc948-5402-4b26-f301-8151dadcb868"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch 2.9.0+cu126\n",
            "transformers 4.57.3\n",
            "cuda True\n",
            "mps False\n"
          ]
        }
      ],
      "source": [
        "import torch, transformers\n",
        "print('torch', torch.__version__)\n",
        "print('transformers', transformers.__version__)\n",
        "print('cuda', torch.cuda.is_available())\n",
        "print('mps', torch.backends.mps.is_available())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GCXUYtVC3c2x",
      "metadata": {
        "id": "GCXUYtVC3c2x"
      },
      "source": [
        "## 4) Stage A ((Optional)): KD-QAT on streaming C4\n",
        "\n",
        "This preserves the base model’s behavior under low-bit fake-quant weights.\n",
        "\n",
        "Tips:\n",
        "- Start with a small run (`--max_steps 50`) to validate the pipeline.\n",
        "- Use `-q 4` if 2-bit is too unstable; 4-bit is less aggressive.\n",
        "- On MPS, prefer `--ema_decay 0` for KD-QAT.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jERGktjwjz29",
      "metadata": {
        "id": "jERGktjwjz29"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TpT02cskrs6D",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TpT02cskrs6D",
        "outputId": "5e5648b6-a34d-4ada-85c9-2292e31b316f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[device] cuda | dtype=torch.bfloat16\n",
            "tokenizer_config.json: 9.73kB [00:00, 39.0MB/s]\n",
            "vocab.json: 2.78MB [00:00, 65.7MB/s]\n",
            "merges.txt: 1.67MB [00:00, 132MB/s]\n",
            "tokenizer.json: 100% 11.4M/11.4M [00:00<00:00, 20.4MB/s]\n",
            "config.json: 100% 726/726 [00:00<00:00, 6.09MB/s]\n",
            "2025-12-24 01:22:39.394668: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-24 01:22:39.417464: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766539359.443242    8009 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766539359.449385    8009 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766539359.463433    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766539359.463456    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766539359.463459    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766539359.463461    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 01:22:39.467717: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors: 100% 1.50G/1.50G [00:01<00:00, 935MB/s] \n",
            "generation_config.json: 100% 239/239 [00:00<00:00, 2.03MB/s]\n",
            "README.md: 7.47kB [00:00, 30.6MB/s]\n",
            "data/train-00000-of-00001-a09b74b3ef9c3b(…): 100% 24.2M/24.2M [00:00<00:00, 34.7MB/s]\n",
            "Generating train split: 100% 52002/52002 [00:00<00:00, 336912.65 examples/s]\n",
            "[write] shard_00000.pt | N=512\n",
            "[write] shard_00001.pt | N=512\n",
            "[write] shard_00002.pt | N=512\n",
            "[write] shard_00003.pt | N=512\n",
            "[write] shard_00004.pt | N=512\n",
            "[write] shard_00005.pt | N=512\n",
            "[write] shard_00006.pt | N=512\n",
            "[write] shard_00007.pt | N=512\n",
            "[write] shard_00008.pt | N=512\n",
            "[write] shard_00009.pt | N=512\n",
            "[write] shard_00010.pt | N=512\n",
            "[write] shard_00011.pt | N=512\n",
            "[write] shard_00012.pt | N=512\n",
            "[write] shard_00013.pt | N=512\n",
            "[write] shard_00014.pt | N=512\n",
            "[write] shard_00015.pt | N=512\n",
            "[write] shard_00016.pt | N=512\n",
            "[write] shard_00017.pt | N=512\n",
            "[write] shard_00018.pt | N=512\n",
            "[write] shard_00019.pt | N=512\n",
            "[write] shard_00020.pt | N=512\n",
            "[write] shard_00021.pt | N=512\n",
            "[write] shard_00022.pt | N=512\n",
            "[write] shard_00023.pt | N=512\n",
            "[write] shard_00024.pt | N=512\n",
            "[write] shard_00025.pt | N=512\n",
            "[write] shard_00026.pt | N=512\n",
            "[write] shard_00027.pt | N=512\n",
            "[write] shard_00028.pt | N=512\n",
            "[write] shard_00029.pt | N=512\n",
            "[write] shard_00030.pt | N=512\n",
            "[write] shard_00031.pt | N=512\n",
            "[write] shard_00032.pt | N=512\n",
            "[write] shard_00033.pt | N=512\n",
            "[write] shard_00034.pt | N=512\n",
            "[write] shard_00035.pt | N=512\n",
            "[write] shard_00036.pt | N=512\n",
            "[write] shard_00037.pt | N=512\n",
            "[write] shard_00038.pt | N=512\n",
            "[write] shard_00039.pt | N=32\n",
            "Done. Cached sequences=20000 | out_dir=caches/alpaca_chat_think_L128_K32_R256\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# GENERATE THINKING DATASET (Alpaca chat format)\n",
        "# ============================================================\n",
        "# SKIP THIS CELL if you already have the cache on Google Drive!\n",
        "# Use the \"LOAD FROM GOOGLE DRIVE\" cell instead.\n",
        "\n",
        "!python scripts/precompute_teacher_topk.py \\\n",
        "  --teacher_model_name_or_path {MODEL_NAME} \\\n",
        "  --dataset_name tatsu-lab/alpaca \\\n",
        "  --dataset_split train \\\n",
        "  --dataset_format alpaca_chat \\\n",
        "  --enable_thinking true \\\n",
        "  --max_length 128 \\\n",
        "  --topk 32 \\\n",
        "  --rand_neg 256 \\\n",
        "  --num_sequences 20000 \\\n",
        "  --batch_size 1 \\\n",
        "  --shard_size 512 \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  --output_dir {CACHE_DIR_CHAT}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rlxJ3X8a3c2x",
      "metadata": {
        "id": "rlxJ3X8a3c2x"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# GENERATE TEXT DATASET (C4 streaming)\n",
        "# ============================================================\n",
        "# SKIP THIS CELL if you already have the cache on Google Drive!\n",
        "# Use the \"LOAD FROM GOOGLE DRIVE\" cell instead.\n",
        "\n",
        "import os\n",
        "\n",
        "CACHE_DIR = CACHE_DIR_TEXT\n",
        "\n",
        "if not os.path.isdir(CACHE_DIR):\n",
        "    print(f\"[cache] {CACHE_DIR} not found -> generating cache...\")\n",
        "\n",
        "    !python scripts/precompute_teacher_topk.py \\\n",
        "      --teacher_model_name_or_path {MODEL_NAME} \\\n",
        "      --dataset_name allenai/c4 \\\n",
        "      --dataset_config_name en \\\n",
        "      --dataset_split train \\\n",
        "      --dataset_text_field text \\\n",
        "      --streaming \\\n",
        "      --shuffle_buffer 10000 \\\n",
        "      --max_length 64 \\\n",
        "      --topk 32 \\\n",
        "      --rand_neg 256 \\\n",
        "      --num_sequences 2000 \\\n",
        "      --batch_size 1 \\\n",
        "      --shard_size 512 \\\n",
        "      --device {DEVICE} \\\n",
        "      --dtype {DTYPE} \\\n",
        "      --output_dir {CACHE_DIR}\n",
        "\n",
        "else:\n",
        "    print(f\"[cache] {CACHE_DIR} already exists -> skipping generation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Nt45qTnlKO7a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt45qTnlKO7a",
        "outputId": "be3ace25-b2da-42ec-df47-30b79fc479a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[gzip] Compressing caches/alpaca_chat_think_L128_K32_R256...\n",
            "caches/alpaca_chat_think_L128_K32_R256/\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00018.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00011.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00033.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00031.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00036.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00038.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00030.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00000.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00032.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00010.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00039.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00008.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00005.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00025.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/meta.json\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00015.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00028.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00016.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00009.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00021.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00037.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00001.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00035.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00029.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00003.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00006.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00014.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00007.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00002.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00013.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00024.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00034.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00022.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00020.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00004.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00012.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00019.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00027.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00026.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00017.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00023.pt\n",
            "[gzip] Done: 2.84 GB\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# COMPRESS CHAT CACHE (for Google Drive upload)\n",
        "# ============================================================\n",
        "# SKIP if cache is already compressed or loaded from Google Drive\n",
        "\n",
        "import os\n",
        "\n",
        "if os.path.isdir(CACHE_DIR_CHAT):\n",
        "    print(f\"[gzip] Compressing {CACHE_DIR_CHAT}...\")\n",
        "    !tar -zcvf {CACHE_DIR_CHAT}.tgz {CACHE_DIR_CHAT}\n",
        "    compressed_size = os.path.getsize(f\"{CACHE_DIR_CHAT}.tgz\")\n",
        "    print(f\"[gzip] Done: {compressed_size / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(f\"[gzip] Directory {CACHE_DIR_CHAT} not found. Skipping.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IIIWEkllwGEA",
      "metadata": {
        "collapsed": true,
        "id": "IIIWEkllwGEA"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SAVE CACHED KD DATA TO GOOGLE DRIVE (run after generating cache)\n",
        "# ============================================================\n",
        "# This saves the generated cache to Google Drive for future sessions\n",
        "# Only run this AFTER you've generated the cache with precompute_teacher_topk.py\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create destination directory\n",
        "!mkdir -p /content/drive/MyDrive/qwen3_caches\n",
        "\n",
        "# Choose which cache to save (should match what you generated)\n",
        "CACHE_NAME = \"alpaca_chat_think_both_L128_K32_R256\"\n",
        "\n",
        "import os\n",
        "\n",
        "# Check if cache exists - copy folder directly (no compression needed)\n",
        "if os.path.isdir(f\"caches/{CACHE_NAME}\"):\n",
        "    # Copy folder to Google Drive\n",
        "    print(f\"[save] Copying {CACHE_NAME} to Google Drive...\")\n",
        "    !rsync -ah --info=progress2 caches/{CACHE_NAME}/ /content/drive/MyDrive/qwen3_caches/{CACHE_NAME}/\n",
        "\n",
        "    # Verify\n",
        "    num_shards = len([f for f in os.listdir(f\"/content/drive/MyDrive/qwen3_caches/{CACHE_NAME}\") if f.startswith(\"shard_\")])\n",
        "    print(f\"[save] Saved to Google Drive: {num_shards} shards\")\n",
        "else:\n",
        "    print(f\"[save] ERROR: Cache directory caches/{CACHE_NAME} not found\")\n",
        "    print(\"[save] Run precompute_teacher_topk.py first to generate the cache\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n4snxtz2q6q",
      "metadata": {
        "id": "n4snxtz2q6q"
      },
      "source": [
        "## 4.5) Google Drive Cache Management\n",
        "\n",
        "**Workflow for KD Cache:**\n",
        "\n",
        "1. **First time setup** (slow):\n",
        "   - Run `precompute_teacher_topk.py` to generate cache\n",
        "   - Run \"SAVE TO GOOGLE DRIVE\" cell to persist\n",
        "   \n",
        "2. **Subsequent sessions** (fast):\n",
        "   - Run \"LOAD FROM GOOGLE DRIVE\" cell to restore cache\n",
        "   - Skip cache generation step\n",
        "\n",
        "The cached KD data (~2-3 GB compressed) contains precomputed teacher logits for knowledge distillation training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7745eb2",
      "metadata": {
        "id": "d7745eb2"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# COMPRESS TEXT CACHE (for Google Drive upload)\n",
        "# ============================================================\n",
        "# SKIP if cache is already compressed or loaded from Google Drive\n",
        "\n",
        "import os\n",
        "\n",
        "if os.path.isdir(CACHE_DIR_TEXT):\n",
        "    print(f\"[gzip] Compressing {CACHE_DIR_TEXT}...\")\n",
        "    !tar -zcvf {CACHE_DIR_TEXT}.tgz {CACHE_DIR_TEXT}\n",
        "    compressed_size = os.path.getsize(f\"{CACHE_DIR_TEXT}.tgz\")\n",
        "    print(f\"[gzip] Done: {compressed_size / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(f\"[gzip] Directory {CACHE_DIR_TEXT} not found. Skipping.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (!!) LOAD CACHED KD DATA FROM GOOGLE DRIVE\n"
      ],
      "metadata": {
        "id": "JhOpTWn8hthM"
      },
      "id": "JhOpTWn8hthM"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "vlhuS4N9GbN4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlhuS4N9GbN4",
        "outputId": "0993ebd7-b313-4bcd-9656-8b45239e1249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[cache] Copying alpaca_chat_think_both_L128_K32_R256 from Google Drive...\n",
            "          4.40G 100%   25.73MB/s    0:02:43 (xfr#41, to-chk=0/42)\n",
            "[cache] Successfully loaded alpaca_chat_think_both_L128_K32_R256 with 40 shards\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# LOAD CACHED KD DATA FROM GOOGLE DRIVE (run this cell first!)\n",
        "# ============================================================\n",
        "# Mount Google Drive and copy cached KD data back to local storage\n",
        "# This avoids regenerating the cache every session\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create local cache directory\n",
        "!mkdir -p caches\n",
        "\n",
        "# Cache folder to load (copy folder directly, no .tgz)\n",
        "CACHE_NAME = \"alpaca_chat_think_both_L128_K32_R256\"\n",
        "\n",
        "# Copy folder directly from Google Drive\n",
        "SRC_PATH = f\"/content/drive/MyDrive/qwen3_caches/{CACHE_NAME}\"\n",
        "DST_PATH = f\"caches/{CACHE_NAME}\"\n",
        "\n",
        "print(f\"[cache] Copying {CACHE_NAME} from Google Drive...\")\n",
        "!rsync -ah --info=progress2 {SRC_PATH}/ {DST_PATH}/\n",
        "\n",
        "# Verify copy\n",
        "import os\n",
        "if os.path.isdir(DST_PATH):\n",
        "    num_shards = len([f for f in os.listdir(DST_PATH) if f.startswith(\"shard_\")])\n",
        "    print(f\"[cache] Successfully loaded {CACHE_NAME} with {num_shards} shards\")\n",
        "else:\n",
        "    print(f\"[cache] ERROR: Failed to copy {CACHE_NAME}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "NKhv4eaYs4qR",
      "metadata": {
        "id": "NKhv4eaYs4qR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a28b3d7b-eb60-4800-88c2-8459cec2acbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[checkpoint] Copying qwen3_kdqat_cache_q2_4.tgz from Google Drive...\n",
            "\r              0   0%    0.00kB/s    0:00:00 (xfr#0, to-chk=0/1)\n",
            "[checkpoint] Checking tarball structure...\n",
            "qwen3_kdqat_cache_q2_4/\n",
            "qwen3_kdqat_cache_q2_4/special_tokens_map.json\n",
            "qwen3_kdqat_cache_q2_4/loss.csv\n",
            "qwen3_kdqat_cache_q2_4/added_tokens.json\n",
            "qwen3_kdqat_cache_q2_4/tokenizer_config.json\n",
            "[checkpoint] Extracting qwen3_kdqat_cache_q2_4.tgz...\n",
            "[checkpoint] Successfully loaded qwen3_kdqat_cache_q2_4 with 12 files:\n",
            "  - added_tokens.json\n",
            "  - chat_template.jinja\n",
            "  - final_state_dict.pt\n",
            "  - loss.csv\n",
            "  - merges.txt\n",
            "  ... and 7 more\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# LOAD 4-BIT CHECKPOINT FROM GOOGLE DRIVE (for 2-bit initialization)\n",
        "# ============================================================\n",
        "# Copy the 4-bit trained checkpoint to use as starting point for 2-bit training\n",
        "\n",
        "import os\n",
        "\n",
        "# Create runs directory\n",
        "!mkdir -p runs\n",
        "\n",
        "# 4-bit checkpoint to load (best result from 4-bit training)\n",
        "CHECKPOINT_NAME = \"qwen3_kdqat_cache_q2_4\"\n",
        "SRC_PATH = f\"/content/drive/MyDrive/qwen3_runs/{CHECKPOINT_NAME}.tgz\"\n",
        "DST_PATH = f\"runs/{CHECKPOINT_NAME}.tgz\"\n",
        "\n",
        "# Copy from Google Drive\n",
        "print(f\"[checkpoint] Copying {CHECKPOINT_NAME}.tgz from Google Drive...\")\n",
        "!rsync -ah --info=progress2 {SRC_PATH} {DST_PATH}\n",
        "\n",
        "# Check tarball structure first\n",
        "print(f\"[checkpoint] Checking tarball structure...\")\n",
        "!tar -tzf {DST_PATH} | head -5\n",
        "\n",
        "# Extract to runs/ directory (tarball contains folder without runs/ prefix)\n",
        "print(f\"[checkpoint] Extracting {CHECKPOINT_NAME}.tgz...\")\n",
        "!tar -xzf {DST_PATH} -C runs/\n",
        "\n",
        "# Verify extraction\n",
        "if os.path.isdir(f\"runs/{CHECKPOINT_NAME}\"):\n",
        "    files = os.listdir(f\"runs/{CHECKPOINT_NAME}\")\n",
        "    print(f\"[checkpoint] Successfully loaded {CHECKPOINT_NAME} with {len(files)} files:\")\n",
        "    for f in sorted(files)[:5]:\n",
        "        print(f\"  - {f}\")\n",
        "    if len(files) > 5:\n",
        "        print(f\"  ... and {len(files)-5} more\")\n",
        "else:\n",
        "    # Try to find where it extracted\n",
        "    print(f\"[checkpoint] Checking runs/ directory...\")\n",
        "    !ls -la runs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AHSsmdhzsBTo",
      "metadata": {
        "id": "AHSsmdhzsBTo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A7UPbdfK3c2x",
      "metadata": {
        "id": "A7UPbdfK3c2x"
      },
      "outputs": [],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_stream_q2\"\n",
        "\n",
        "# DISABLED --- NOTE used! see #5 for first QAT step\n",
        "# Construct the command string in Python to ensure variable interpolation\n",
        "command_str = f\"\"\"python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --teacher_model_name_or_path {MODEL_NAME} \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --dataset_name allenai/c4 \\\n",
        "  --dataset_config_name en \\\n",
        "  --dataset_split train \\\n",
        "  --dataset_format text \\\n",
        "  --dataset_text_field text \\\n",
        "  --streaming \\\n",
        "  --shuffle_buffer 10000 \\\n",
        "  --output_dir {RUN_DIR} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 1 \\\n",
        "  --gradient_accumulation_steps 16 \\\n",
        "  --learning_rate 5e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 50 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --logging_steps 10 \\\n",
        "  --save_steps 50\"\"\"\n",
        "\n",
        "# Execute the constructed command string\n",
        "!{command_str}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UmgbC-RI3c2x",
      "metadata": {
        "id": "UmgbC-RI3c2x"
      },
      "source": [
        "### (Optional) Resume\n",
        "\n",
        "`--resume_from_checkpoint auto` resolves to `checkpoint_last.pt` if it exists in the output directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mJqdmXSA3c2x",
      "metadata": {
        "id": "mJqdmXSA3c2x"
      },
      "outputs": [],
      "source": [
        "# !python scripts/train_qat.py ... --output_dir {RUN_DIR} --max_steps 500 --resume_from_checkpoint auto\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R7UZBhau3c2x",
      "metadata": {
        "id": "R7UZBhau3c2x"
      },
      "source": [
        "## 5)  KD-cache: precompute teacher top-k + negatives\n",
        "\n",
        "Cache mode is MPS-friendly:\n",
        "- no teacher model during training\n",
        "- no full-vocab logits\n",
        "\n",
        "If you see good KD loss but bad greedy decoding, increase negative coverage (`--rand_neg`) and/or add hard top-1 terms:\n",
        "- `--hard-top1-weight 0.05`\n",
        "- `--hard-full-top1-weight 0.02`–`0.05`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g4s_PzQW3c2x",
      "metadata": {
        "id": "g4s_PzQW3c2x"
      },
      "source": [
        "### KD-cache QAT training\n",
        "\n",
        "This uses cached teacher signals + candidate softmax."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m7q7F2_jgN6Z"
      },
      "id": "m7q7F2_jgN6Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1Li8Aysa3c2x",
      "metadata": {
        "id": "1Li8Aysa3c2x"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STAGE 1: KD-QAT (Conservative - freeze MLP/Attention)\n",
        "# ============================================================\n",
        "# First training stage with frozen output layers for stability\n",
        "\n",
        "%pwd\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "CACHE_DIR = CACHE_DIR_CHAT\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2\"\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --output_dir {RUN_DIR_CACHE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 1 \\\n",
        "  --gradient_accumulation_steps 4 \\\n",
        "  --learning_rate 5e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 1000 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 5 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.05 \\\n",
        "  --hard-full-top1-weight 0.03 \\\n",
        "  --ov-freeze \\\n",
        "  --freeze-last-mlp \\\n",
        "  --freeze-last-mlp-layers 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UKgDA-7OuA8m",
      "metadata": {
        "id": "UKgDA-7OuA8m"
      },
      "outputs": [],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --prompt \"What capital city of France is?\" \\\n",
        "  --do_sample true \\\n",
        "  --max_new_tokens 64\n",
        "\n",
        "#  --prompt \"What is Capital of france?\" \\\n",
        "#   --prompt \"What is Apple Neural Engine?\" \\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sexJNkyJyMgv",
      "metadata": {
        "id": "sexJNkyJyMgv"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb19fb17",
      "metadata": {
        "id": "eb19fb17"
      },
      "outputs": [],
      "source": [
        "# Define source and destination paths\n",
        "SOURCE_FILE = \"runs/qwen3_kdqat_cache_q2_2/qat_state_dict.pt\"\n",
        "DEST_DIR_GD = \"/content/drive/MyDrive/runs/Q4B/q2_2/\"\n",
        "\n",
        "# Ensure the destination directory exists on Google Drive\n",
        "!mkdir -p {DEST_DIR_GD}\n",
        "\n",
        "# Copy the file to Google Drive\n",
        "!cp -v {SOURCE_FILE} {DEST_DIR_GD}\n",
        "print(f\"Copied {SOURCE_FILE} to {DEST_DIR_GD}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hbfDaYP5yN6-",
      "metadata": {
        "id": "hbfDaYP5yN6-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JUvQIIDeRUF6",
      "metadata": {
        "id": "JUvQIIDeRUF6"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STAGE 2: KD-QAT (Unfrozen layers, resume from Stage 1)\n",
        "# ============================================================\n",
        "# Continue training with all layers unfrozen\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "CACHE_DIR = CACHE_DIR_CHAT\n",
        "INIT_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2\"\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_DIR_CACHE}/qat_state_dict.pt \\\n",
        "  --output_dir {RUN_DIR_CACHE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 1 \\\n",
        "  --gradient_accumulation_steps 4 \\\n",
        "  --learning_rate 5e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 1000 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --ov-freeze \\\n",
        "  --hard-top1-weight 0.02 \\\n",
        "  --hard-full-top1-weight 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (!!!!)Stage 3 resume KD-QAT with\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "GJ7WaTFCgRHN"
      },
      "id": "GJ7WaTFCgRHN"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "KXQUTgEJfzl4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXQUTgEJfzl4",
        "outputId": "afa02860-d593-411a-ce05-c6c2eb2ecd57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-24 23:29:24.155008: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-24 23:29:24.171198: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766618964.192763   82536 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766618964.199201   82536 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766618964.215547   82536 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766618964.215573   82536 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766618964.215576   82536 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766618964.215579   82536 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 23:29:24.220382: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[device] cuda | amp_dtype=torch.bfloat16 | param_dtype=torch.bfloat16\n",
            "[qat] weight_bits=2\n",
            "[init] loading model state from runs/progressive_qat_q2_v1/qat_state_dict.pt\n",
            "[init] model weights loaded.\n",
            "[kd-cache] cache topk=32\n",
            "[kd-cache] dir=caches/alpaca_chat_think_both_L128_K32_R256 | weight=1.0 | T=2.0 | hard_top1=0.0 | hard_full_top1=0.0005\n",
            "opt_step: 100% 3000/3000 [53:54<00:00,  1.08s/step, loss=1.1946, lr=0.00e+00]\n",
            "Done. QAT checkpoint saved to: runs/qwen3_kdqat_cache_q2_3/qat_state_dict.pt\n"
          ]
        }
      ],
      "source": [
        "# Stage 3 resume KD-QAT with unfrozen attention and relaxed hard-top/full!\n",
        "\n",
        "CACHE_DIR = CACHE_DIR_CHAT  # Use config variable\n",
        "#INIT_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "INIT_DIR_CACHE =  \"runs/progressive_qat_q2_v1\"\n",
        "\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_3\"\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_DIR_CACHE}/qat_state_dict.pt \\\n",
        "  --output_dir {RUN_DIR_CACHE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 128 \\\n",
        "  --gradient_accumulation_steps 1 \\\n",
        "  --learning_rate 5e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 3000 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.00 \\\n",
        "  --hard-full-top1-weight 0.0005\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#save run\n"
      ],
      "metadata": {
        "id": "ESr5L_EStT6y"
      },
      "id": "ESr5L_EStT6y"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# SAVE PROGRESSIVE QAT CHECKPOINT TO GOOGLE DRIVE\n",
        "# ============================================================\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive if not already mounted\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Source directory (matches RUN_DIR_PROGRESSIVE from config)\n",
        "RUN_NAME = \"qwen3_kdqat_cache_q2_3\"\n",
        "RUN_DIR = f\"runs/{RUN_NAME}\"\n",
        "\n",
        "# Destination on Google Drive\n",
        "DEST_DIR_GD = \"/content/drive/MyDrive/qwen3_runs/\"\n",
        "!mkdir -p {DEST_DIR_GD}\n",
        "\n",
        "# Check if run directory exists and has content\n",
        "if os.path.isdir(RUN_DIR) and os.listdir(RUN_DIR):\n",
        "    # Compress the run directory\n",
        "    print(f\"[archive] Compressing {RUN_DIR}...\")\n",
        "    !tar -zcvf {RUN_NAME}.tgz -C runs {RUN_NAME}\n",
        "\n",
        "    # Copy to Google Drive\n",
        "    print(f\"[save] Copying {RUN_NAME}.tgz to Google Drive...\")\n",
        "    !rsync -ah --info=progress2 {RUN_NAME}.tgz {DEST_DIR_GD}\n",
        "\n",
        "    # Verify\n",
        "    gd_size = os.path.getsize(f\"{DEST_DIR_GD}/{RUN_NAME}.tgz\")\n",
        "    print(f\"[save] Saved to Google Drive: {gd_size / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(f\"[save] ERROR: {RUN_DIR} is empty or doesn't exist\")\n",
        "\n",
        "# got to \"Stage 3 resume\" to continue distill treaining"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPk8pkD3tUeu",
        "outputId": "6e0433c4-8709-4eda-8aec-d27443c92fc6"
      },
      "id": "RPk8pkD3tUeu",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[archive] Compressing runs/qwen3_kdqat_cache_q2_3...\n",
            "qwen3_kdqat_cache_q2_3/\n",
            "qwen3_kdqat_cache_q2_3/special_tokens_map.json\n",
            "qwen3_kdqat_cache_q2_3/loss.csv\n",
            "qwen3_kdqat_cache_q2_3/added_tokens.json\n",
            "qwen3_kdqat_cache_q2_3/tokenizer_config.json\n",
            "qwen3_kdqat_cache_q2_3/run_state.json\n",
            "qwen3_kdqat_cache_q2_3/merges.txt\n",
            "qwen3_kdqat_cache_q2_3/training_args.json\n",
            "qwen3_kdqat_cache_q2_3/chat_template.jinja\n",
            "qwen3_kdqat_cache_q2_3/vocab.json\n",
            "qwen3_kdqat_cache_q2_3/final_state_dict.pt\n",
            "qwen3_kdqat_cache_q2_3/qat_state_dict.pt\n",
            "qwen3_kdqat_cache_q2_3/checkpoint_step3000.pt\n",
            "qwen3_kdqat_cache_q2_3/checkpoint_last.pt\n",
            "qwen3_kdqat_cache_q2_3/tokenizer.json\n",
            "[save] Copying qwen3_kdqat_cache_q2_3.tgz to Google Drive...\n",
            "          7.46G 100%  332.14MB/s    0:00:21 (xfr#1, to-chk=0/1)\n",
            "[save] Saved to Google Drive: 6.95 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "U5THSkU3oo2V"
      },
      "id": "U5THSkU3oo2V"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28cb12c7"
      },
      "source": [
        "#### Pull and Unzip Progressive QAT Checkpoint"
      ],
      "id": "28cb12c7"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the checkpoint name to pull\n",
        "RUN_NAME = \"qwen3_kdqat_cache_q2_3\"\n",
        "\n",
        "# Source path on Google Drive\n",
        "SRC_PATH_GD = f\"/content/drive/MyDrive/qwen3_runs/{RUN_NAME}.tgz\"\n",
        "# Destination path locally\n",
        "DST_PATH_LOCAL = f\"{RUN_NAME}.tgz\"\n",
        "\n",
        "# Create runs directory if it doesn't exist\n",
        "!mkdir -p runs\n",
        "\n",
        "print(f\"[pull] Copying {RUN_NAME}.tgz from Google Drive...\")\n",
        "!rsync -ah --info=progress2 {SRC_PATH_GD} {DST_PATH_LOCAL}\n",
        "\n",
        "# Check if the tarball was copied successfully\n",
        "if os.path.exists(DST_PATH_LOCAL):\n",
        "    print(f\"[pull] Extracting {RUN_NAME}.tgz...\")\n",
        "    !tar -xzf {DST_PATH_LOCAL} -C runs/\n",
        "    print(f\"[pull] Successfully extracted to runs/{RUN_NAME}\")\n",
        "\n",
        "    # Optionally, remove the tarball after extraction to save space\n",
        "    # !rm {DST_PATH_LOCAL}\n",
        "else:\n",
        "    print(f\"[pull] ERROR: {RUN_NAME}.tgz not found on Google Drive. Make sure it was saved correctly.\")\n",
        "\n",
        "# Use the path where the checkpoint was unzipped to the inference check code cell\n",
        "#(4coakmebsik). This comment clarifies that the TEST_RUN variable should point to the d\n",
        "#irectory where the QAT checkpoint was extracted for inference."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVx16qzAodl2",
        "outputId": "ae861164-dfef-41b1-aec9-24e6d1005dcd"
      },
      "id": "uVx16qzAodl2",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[pull] Copying qwen3_kdqat_cache_q2_3.tgz from Google Drive...\n",
            "          7.46G 100%  136.13MB/s    0:00:52 (xfr#1, to-chk=0/1)\n",
            "[pull] Extracting qwen3_kdqat_cache_q2_3.tgz...\n",
            "[pull] Successfully extracted to runs/qwen3_kdqat_cache_q2_3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q1Zjl_WnHZ4O",
      "metadata": {
        "id": "Q1Zjl_WnHZ4O"
      },
      "outputs": [],
      "source": [
        "# Stage 4 resume KD-QAT with unfrozen attention and relaxed hard-top/full!\n",
        "#   --hard-full-top1-weight 0.0000\n",
        "#   learning_rate 2e-6\n",
        "CACHE_DIR = CACHE_DIR_CHAT  # Use config variable\n",
        "INIT_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_3\"\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_4\"\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_DIR_CACHE}/qat_state_dict.pt \\\n",
        "  --output_dir {RUN_DIR_CACHE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 160 \\\n",
        "  --gradient_accumulation_steps 1 \\\n",
        "  --learning_rate 2e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 500 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 5 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.00 \\\n",
        "  --hard-full-top1-weight 0.0000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acak6adahpf",
      "metadata": {
        "id": "acak6adahpf"
      },
      "source": [
        "## 5.5) Progressive Layer-by-Layer QAT (Experimental)\n",
        "\n",
        "This approach trains one layer at a time with:\n",
        "- **Local reconstruction loss**: MSE between quantized and fp MLP outputs\n",
        "- **Global KD loss**: Cached teacher logits\n",
        "- **Prefix quantized / suffix fp**: Earlier layers stay quantized, later layers use full precision\n",
        "\n",
        "### Recommended Training Order (most stable first):\n",
        "\n",
        "1. **E2E f-only** (Option 1): Train ALL f parameters at once\n",
        "   - Skip progressive passes, just run Pass 4\n",
        "   - Most stable, fastest validation\n",
        "   \n",
        "2. **Progressive f-only** (Option 2): Layer-by-layer f-param training\n",
        "   - Uses `--train_f_only` flag\n",
        "   - Disable local loss with `--local_weight 0.0`\n",
        "   \n",
        "3. **Full progressive** (Option 3): Train weights + f per layer\n",
        "   - Most aggressive, may show instability at later layers\n",
        "\n",
        "### GPU Configuration:\n",
        "\n",
        "| GPU | Recommended batch_size |\n",
        "|-----|------------------------|\n",
        "| T4 (15GB) | 2-4 |\n",
        "| V100 (32GB) | 4-8 |\n",
        "| A100 (40GB) | 8-16 |\n",
        "| A100 (80GB) / H100 | 16-32 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "scxwylalctd",
      "metadata": {
        "id": "scxwylalctd"
      },
      "outputs": [],
      "source": [
        "# ---- Progressive QAT Config (2-bit from 4-bit checkpoint) ----\n",
        "# Starting from 4-bit trained checkpoint for better 2-bit initialization\n",
        "\n",
        "# 4-bit checkpoint as initialization (loaded from Google Drive)\n",
        "INIT_CHECKPOINT = \"runs/qwen3_kdqat_cache_q2_4/qat_state_dict.pt\"\n",
        "\n",
        "# Adjust batch_size for your GPU (A100: 8-16, V100: 4-8, T4: 2-4)\n",
        "BATCH_SIZE = 96                # Increase for faster instances (A100/H100)\n",
        "STEPS_PER_LAYER_MLP = 100      # Steps per MLP layer (Pass 1 + Pass 3)\n",
        "STEPS_PER_LAYER_ATTN = 30      # Steps per attention layer (Pass 2)\n",
        "E2E_STEPS = 500                # E2E quantizer tuning steps (Pass 4)\n",
        "LOCAL_WEIGHT = 0.3             # Local reconstruction loss weight\n",
        "GLOBAL_WEIGHT = 1.0            # Global KD loss weight\n",
        "LOCAL_TOKEN_SAMPLES = 128      # Tokens to sample for local loss\n",
        "MAX_GRAD_NORM = 1.0            # Gradient clipping (important for 2-bit)\n",
        "\n",
        "# Learning rates (lower for 2-bit stability)\n",
        "LR_PROGRESSIVE = 2e-6          # Learning rate for progressive passes\n",
        "LR_E2E = 5e-5                  # Learning rate for E2E f-only tuning\n",
        "\n",
        "# Output directories (2-bit versions)\n",
        "RUN_DIR_E2E_FONLY = \"runs/e2e_f_only_q2\"\n",
        "RUN_DIR_PROGRESSIVE_FONLY = \"runs/progressive_f_only_q2\"\n",
        "RUN_DIR_PROGRESSIVE = \"runs/progressive_qat_q2_v1\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q9f32hx0qo",
      "metadata": {
        "id": "q9f32hx0qo"
      },
      "source": [
        "### Option 1: E2E f-only Training (Recommended First)\n",
        "\n",
        "**Most stable approach** - trains ALL `_f_param` (quantization scales) simultaneously.\n",
        "Skip all progressive layer-by-layer passes and go straight to Pass 4.\n",
        "\n",
        "This is recommended when:\n",
        "- Progressive layer-by-layer shows instability (local loss hitting 10.0)\n",
        "- You want to validate the infrastructure works before trying progressive\n",
        "- You have limited time and want the fastest path to a working checkpoint\n",
        "\n",
        "The `f` parameter is the learnable quantization scale from Apple-style quantization:\n",
        "- Actual scale `s = softplus(f)` ensures positivity\n",
        "- Training only `f` keeps weights frozen - more stable for ultra-low-bit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "vh3iifkgpj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vh3iifkgpj",
        "outputId": "c0ebe461-dfaa-44e1-a38e-6c5fdb2c9a87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "[device] cuda | amp_dtype=torch.bfloat16 | param_dtype=torch.bfloat16\n",
            "tokenizer_config.json: 9.73kB [00:00, 38.5MB/s]\n",
            "vocab.json: 2.78MB [00:00, 52.2MB/s]\n",
            "merges.txt: 1.67MB [00:00, 139MB/s]\n",
            "tokenizer.json: 100% 11.4M/11.4M [00:01<00:00, 10.2MB/s]\n",
            "[model] Loading Qwen/Qwen3-0.6B\n",
            "config.json: 100% 726/726 [00:00<00:00, 8.23MB/s]\n",
            "2025-12-24 18:47:22.719539: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-24 18:47:22.740144: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766602042.765073    9837 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766602042.770583    9837 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766602042.784760    9837 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766602042.784783    9837 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766602042.784786    9837 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766602042.784789    9837 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 18:47:22.789000: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors: 100% 1.50G/1.50G [00:02<00:00, 570MB/s]\n",
            "generation_config.json: 100% 239/239 [00:00<00:00, 2.04MB/s]\n",
            "[init] Loading model state from runs/qwen3_kdqat_cache_q2_4/qat_state_dict.pt\n",
            "[qat] weight_bits=2\n",
            "[kd-cache] Loading from caches/alpaca_chat_think_both_L128_K32_R256\n",
            "[model] 28 transformer layers\n",
            "[training] batch_size=96 steps_per_mlp=50 e2e_steps=500\n",
            "\n",
            "============================================================\n",
            "PASS 4: E2E Quantizer Tuning (f-only)\n",
            "============================================================\n",
            "  Trainable f parameters: 196\n",
            "  step 0: loss=12.1407\n",
            "  step 10: loss=11.5678\n",
            "  step 20: loss=11.8305\n",
            "  step 30: loss=11.9946\n",
            "  step 40: loss=11.7304\n",
            "  step 50: loss=11.8770\n",
            "  step 60: loss=12.0282\n",
            "  step 70: loss=11.6242\n",
            "  step 80: loss=11.5261\n",
            "  step 90: loss=11.6720\n",
            "  step 100: loss=12.3927\n",
            "  step 110: loss=11.5418\n",
            "  step 120: loss=12.0243\n",
            "  step 130: loss=11.7296\n",
            "  step 140: loss=12.1050\n",
            "  step 150: loss=11.6462\n",
            "  step 160: loss=11.9153\n",
            "  step 170: loss=11.9587\n",
            "  step 180: loss=11.9907\n",
            "  step 190: loss=11.9216\n",
            "  step 200: loss=12.2080\n",
            "  step 210: loss=12.0106\n",
            "  step 220: loss=11.6917\n",
            "  step 230: loss=12.0309\n",
            "  step 240: loss=11.9478\n",
            "  step 250: loss=11.7766\n",
            "  step 260: loss=11.9367\n",
            "  step 270: loss=11.8300\n",
            "  step 280: loss=12.2159\n",
            "  step 290: loss=11.9723\n",
            "  step 300: loss=11.9617\n",
            "  step 310: loss=11.8662\n",
            "  step 320: loss=11.9585\n",
            "  step 330: loss=11.6636\n",
            "  step 340: loss=11.9637\n",
            "  step 350: loss=11.8636\n",
            "  step 360: loss=11.8626\n",
            "  step 370: loss=12.0558\n",
            "  step 380: loss=11.7337\n",
            "  step 390: loss=11.9441\n",
            "  step 400: loss=11.9119\n",
            "  step 410: loss=12.0105\n",
            "  step 420: loss=11.6374\n",
            "  step 430: loss=11.8217\n",
            "  step 440: loss=11.6547\n",
            "  step 450: loss=11.7303\n",
            "  step 460: loss=11.8293\n",
            "  step 470: loss=11.6669\n",
            "  step 480: loss=11.8896\n",
            "  step 490: loss=11.7591\n",
            "\n",
            "============================================================\n",
            "Saving outputs\n",
            "============================================================\n",
            "  Model saved to: runs/e2e_f_only_q2/qat_state_dict.pt\n",
            "  Loss log saved to: runs/e2e_f_only_q2/loss_per_layer.csv\n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# E2E f-only: Skip ALL progressive passes, train all f parameters at once\n",
        "# This is the simplest and most stable approach\n",
        "# Starting from 4-bit checkpoint for 2-bit training\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/train_qat_progressive.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_CHECKPOINT} \\\n",
        "  --output_dir {RUN_DIR_E2E_FONLY} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --skip_mlp_pass \\\n",
        "  --skip_attention_pass \\\n",
        "  --skip_mlp_refinement \\\n",
        "  --e2e_steps {E2E_STEPS} \\\n",
        "  --e2e_learning_rate {LR_E2E} \\\n",
        "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vab9x1zlqhs",
      "metadata": {
        "id": "vab9x1zlqhs"
      },
      "source": [
        "### Option 2: Progressive f-only Training\n",
        "\n",
        "Layer-by-layer training but only trains `_f_param` (quantization scales), not weights.\n",
        "More stable than full progressive training, but may still see instability at later layers.\n",
        "\n",
        "Use `--train_f_only` flag to freeze weights and only train quantization scales per layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oqxrx3c2fe",
      "metadata": {
        "id": "oqxrx3c2fe"
      },
      "outputs": [],
      "source": [
        "# Progressive f-only: Layer-by-layer, but only train quantization scales\n",
        "# Use --train_f_only for more stable training\n",
        "# Starting from 4-bit checkpoint for 2-bit training\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/train_qat_progressive.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_CHECKPOINT} \\\n",
        "  --output_dir {RUN_DIR_PROGRESSIVE_FONLY} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --train_f_only \\\n",
        "  --steps_per_layer_mlp {STEPS_PER_LAYER_MLP} \\\n",
        "  --steps_per_layer_attn {STEPS_PER_LAYER_ATTN} \\\n",
        "  --e2e_steps {E2E_STEPS} \\\n",
        "  --local_weight 0.0 \\\n",
        "  --global_weight {GLOBAL_WEIGHT} \\\n",
        "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
        "  --learning_rate {LR_PROGRESSIVE} \\\n",
        "  --e2e_learning_rate {LR_E2E} \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --skip_mlp_refinement"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e0gsxkcc4",
      "metadata": {
        "id": "8e0gsxkcc4"
      },
      "source": [
        "### (!!!) Option 3: Full Progressive Training (weights + f)\n",
        "\n",
        "Full layer-by-layer training with weights and quantization scales.\n",
        "Most aggressive but potentially unstable for ultra-low-bit (2-bit).\n",
        "\n",
        "**Training Order (3-pass v3):**\n",
        "1. **Pass 1**: Train MLP layers (local reconstruction + global KD)\n",
        "2. **Pass 2**: Train attention layers (global KD only)\n",
        "3. **Pass 3**: MLP refinement (addresses MLP-attention coupling)\n",
        "4. **Pass 4**: E2E quantizer-only tuning (f-param only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7qzyw0rd18e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qzyw0rd18e",
        "outputId": "c8417f9f-9c6b-43e6-d8b8-55e2cf2200af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  step 20: local=0.4505 global=1.4807\n",
            "  step 30: local=0.4532 global=1.3654\n",
            "  step 40: local=0.4528 global=1.4021\n",
            "  step 50: local=0.4493 global=1.4020\n",
            "  step 60: local=0.4494 global=1.4319\n",
            "  step 70: local=0.4599 global=1.4422\n",
            "  step 80: local=0.4542 global=1.4362\n",
            "  step 90: local=0.4558 global=1.3706\n",
            "  Layer 10 not converged (global=1.4458 > 0.4), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4655 global=1.3227\n",
            "  step 10: local=0.4546 global=1.3391\n",
            "  step 20: local=0.4520 global=1.3870\n",
            "  step 30: local=0.4616 global=1.4243\n",
            "  step 40: local=0.4457 global=1.3548\n",
            "  step 50: local=0.4567 global=1.4170\n",
            "  step 60: local=0.4602 global=1.4128\n",
            "  step 70: local=0.4500 global=1.4022\n",
            "  step 80: local=0.4606 global=1.3591\n",
            "  step 90: local=0.4484 global=1.4510\n",
            "  Layer 10 not converged (global=1.4140 > 0.4), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4488 global=1.3872\n",
            "  step 10: local=0.4540 global=1.4040\n",
            "  step 20: local=0.4580 global=1.4163\n",
            "  step 30: local=0.4628 global=1.4200\n",
            "  step 40: local=0.4506 global=1.4086\n",
            "  step 50: local=0.4486 global=1.3784\n",
            "  step 60: local=0.4524 global=1.3568\n",
            "  step 70: local=0.4452 global=1.3783\n",
            "  step 80: local=0.4427 global=1.3245\n",
            "  step 90: local=0.4419 global=1.3779\n",
            "  Layer 10 not converged (global=1.3962 > 0.4), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4401 global=1.3410\n",
            "  step 10: local=0.4519 global=1.3544\n",
            "  step 20: local=0.4486 global=1.4078\n",
            "  step 30: local=0.4454 global=1.3511\n",
            "  step 40: local=0.4350 global=1.3462\n",
            "  step 50: local=0.4507 global=1.3645\n",
            "  step 60: local=0.4543 global=1.3503\n",
            "  step 70: local=0.4439 global=1.4072\n",
            "  step 80: local=0.4486 global=1.4933\n",
            "  step 90: local=0.4484 global=1.3860\n",
            "  Layer 10 not converged (global=1.3484 > 0.4), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4462 global=1.3836\n",
            "  step 10: local=0.4503 global=1.4519\n",
            "  step 20: local=0.4364 global=1.4320\n",
            "  step 30: local=0.4442 global=1.4021\n",
            "  step 40: local=0.4411 global=1.4144\n",
            "  step 50: local=0.4504 global=1.3765\n",
            "  step 60: local=0.4296 global=1.3890\n",
            "  step 70: local=0.4447 global=1.3392\n",
            "  step 80: local=0.4525 global=1.4010\n",
            "  step 90: local=0.4469 global=1.4156\n",
            "  Layer 10 not converged (global=1.4209 > 0.4), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4417 global=1.3193\n",
            "  step 10: local=0.4546 global=1.3441\n",
            "  step 20: local=0.4541 global=1.3312\n",
            "  step 30: local=0.4326 global=1.3583\n",
            "  step 40: local=0.4509 global=1.3258\n",
            "  step 50: local=0.4505 global=1.3131\n",
            "  step 60: local=0.4564 global=1.3433\n",
            "  step 70: local=0.4377 global=1.4227\n",
            "  step 80: local=0.4402 global=1.3026\n",
            "  step 90: local=0.4513 global=1.3446\n",
            "  Layer 10 not converged (global=1.3129 > 0.4), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4429 global=1.3284\n",
            "  step 10: local=0.4448 global=1.5445\n",
            "  step 20: local=0.4444 global=1.3690\n",
            "  step 30: local=0.4462 global=1.3905\n",
            "  step 40: local=0.4478 global=1.3805\n",
            "  step 50: local=0.4425 global=1.3819\n",
            "  step 60: local=0.4412 global=1.3901\n",
            "  step 70: local=0.4396 global=1.4644\n",
            "  step 80: local=0.4490 global=1.4032\n",
            "  step 90: local=0.4387 global=1.3608\n",
            "  Layer 10 not converged (global=1.3519 > 0.4), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4357 global=1.4549\n",
            "  step 10: local=0.4381 global=1.4012\n",
            "  step 20: local=0.4391 global=1.2756\n",
            "  step 30: local=0.4394 global=1.3569\n",
            "  step 40: local=0.4445 global=1.3854\n",
            "  step 50: local=0.4473 global=1.4437\n",
            "  step 60: local=0.4394 global=1.4423\n",
            "  step 70: local=0.4323 global=1.2718\n",
            "  step 80: local=0.4396 global=1.3483\n",
            "  step 90: local=0.4541 global=1.4473\n",
            "  Layer 10 not converged (global=1.4335 > 0.4), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4475 global=1.4531\n",
            "  step 10: local=0.4367 global=1.3050\n",
            "  step 20: local=0.4442 global=1.3485\n",
            "  step 30: local=0.4484 global=1.3592\n",
            "  step 40: local=0.4393 global=1.3463\n",
            "  step 50: local=0.4491 global=1.3730\n",
            "  step 60: local=0.4479 global=1.3320\n",
            "  step 70: local=0.4316 global=1.4407\n",
            "  step 80: local=0.4327 global=1.3493\n",
            "  step 90: local=0.4418 global=1.3835\n",
            "  Layer 10 not converged (global=1.3952 > 0.4), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4432 global=1.3525\n",
            "  step 10: local=0.4400 global=1.4292\n",
            "  step 20: local=0.4399 global=1.3438\n",
            "  step 30: local=0.4492 global=1.4049\n",
            "  step 40: local=0.4422 global=1.4216\n",
            "  step 50: local=0.4359 global=1.3289\n",
            "  step 60: local=0.4476 global=1.3425\n",
            "  step 70: local=0.4438 global=1.3442\n",
            "  step 80: local=0.4404 global=1.4268\n",
            "  step 90: local=0.4327 global=1.3349\n",
            "  Layer 10 not converged (global=1.3569 > 0.4), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4425 global=1.3638\n",
            "  step 10: local=0.4387 global=1.3556\n",
            "  step 20: local=0.4400 global=1.4046\n",
            "  step 30: local=0.4401 global=1.4297\n",
            "  step 40: local=0.4322 global=1.4568\n",
            "  step 50: local=0.4354 global=1.4463\n",
            "  step 60: local=0.4427 global=1.4734\n",
            "  step 70: local=0.4427 global=1.3581\n",
            "  step 80: local=0.4328 global=1.3924\n",
            "  step 90: local=0.4386 global=1.3922\n",
            "  Layer 10 not converged (global=1.2822 > 0.4), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4303 global=1.4211\n",
            "  step 10: local=0.4401 global=1.4327\n",
            "  step 20: local=0.4322 global=1.4278\n",
            "  step 30: local=0.4403 global=1.3627\n",
            "  step 40: local=0.4437 global=1.3148\n",
            "  step 50: local=0.4384 global=1.3312\n",
            "  step 60: local=0.4365 global=1.3801\n",
            "  step 70: local=0.4381 global=1.4149\n",
            "  step 80: local=0.4253 global=1.3483\n",
            "  step 90: local=0.4391 global=1.4074\n",
            "  Layer 10 not converged (global=1.3726 > 0.4), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4468 global=1.4048\n",
            "  step 10: local=0.4276 global=1.3957\n",
            "  step 20: local=0.4430 global=1.3519\n",
            "  step 30: local=0.4491 global=1.4456\n",
            "  step 40: local=0.4420 global=1.3799\n",
            "  step 50: local=0.4383 global=1.3976\n",
            "  step 60: local=0.4364 global=1.4083\n",
            "  step 70: local=0.4388 global=1.4134\n",
            "  step 80: local=0.4489 global=1.4010\n",
            "  step 90: local=0.4398 global=1.3703\n",
            "  [WARN] Layer 10 did not converge after 20 repeats (global=1.3753)\n",
            "\n",
            "--- Layer 11/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4902 global=1.4227\n",
            "  step 10: local=0.4863 global=1.4268\n",
            "  step 20: local=0.4708 global=1.3757\n",
            "  step 30: local=0.4829 global=1.4330\n",
            "  step 40: local=0.4565 global=1.3805\n",
            "  step 50: local=0.4600 global=1.4041\n",
            "  step 60: local=0.4609 global=1.4502\n",
            "  step 70: local=0.4690 global=1.3936\n",
            "  step 80: local=0.4522 global=1.3807\n",
            "  step 90: local=0.4644 global=1.4016\n",
            "  Layer 11 not converged (global=1.3467 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4505 global=1.3824\n",
            "  step 10: local=0.4469 global=1.4403\n",
            "  step 20: local=0.4451 global=1.5327\n",
            "  step 30: local=0.4388 global=1.4180\n",
            "  step 40: local=0.4486 global=1.4094\n",
            "  step 50: local=0.4414 global=1.4855\n",
            "  step 60: local=0.4412 global=1.4605\n",
            "  step 70: local=0.4372 global=1.4261\n",
            "  step 80: local=0.4357 global=1.4428\n",
            "  step 90: local=0.4330 global=1.3948\n",
            "  Layer 11 not converged (global=1.4214 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4320 global=1.4096\n",
            "  step 10: local=0.4287 global=1.3644\n",
            "  step 20: local=0.4292 global=1.4244\n",
            "  step 30: local=0.4221 global=1.4336\n",
            "  step 40: local=0.4310 global=1.3327\n",
            "  step 50: local=0.4205 global=1.3577\n",
            "  step 60: local=0.4187 global=1.3525\n",
            "  step 70: local=0.4227 global=1.3732\n",
            "  step 80: local=0.4189 global=1.3444\n",
            "  step 90: local=0.4120 global=1.3279\n",
            "  Layer 11 not converged (global=1.4034 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4161 global=1.3573\n",
            "  step 10: local=0.4073 global=1.4386\n",
            "  step 20: local=0.4108 global=1.3059\n",
            "  step 30: local=0.4116 global=1.3576\n",
            "  step 40: local=0.4160 global=1.3442\n",
            "  step 50: local=0.4105 global=1.5613\n",
            "  step 60: local=0.4056 global=1.3747\n",
            "  step 70: local=0.4204 global=1.4042\n",
            "  step 80: local=0.4108 global=1.3870\n",
            "  step 90: local=0.4085 global=1.3905\n",
            "  Layer 11 not converged (global=1.3917 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4115 global=1.3997\n",
            "  step 10: local=0.4040 global=1.4696\n",
            "  step 20: local=0.4056 global=1.4134\n",
            "  step 30: local=0.4084 global=1.3699\n",
            "  step 40: local=0.4011 global=1.4628\n",
            "  step 50: local=0.4033 global=1.4080\n",
            "  step 60: local=0.4024 global=1.2826\n",
            "  step 70: local=0.4072 global=1.3651\n",
            "  step 80: local=0.4075 global=1.3880\n",
            "  step 90: local=0.4057 global=1.4495\n",
            "  Layer 11 not converged (global=1.4147 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4120 global=1.4440\n",
            "  step 10: local=0.4049 global=1.2784\n",
            "  step 20: local=0.4046 global=1.3539\n",
            "  step 30: local=0.4060 global=1.4521\n",
            "  step 40: local=0.4147 global=1.4613\n",
            "  step 50: local=0.4120 global=1.3111\n",
            "  step 60: local=0.4049 global=1.3538\n",
            "  step 70: local=0.4029 global=1.3673\n",
            "  step 80: local=0.3952 global=1.3601\n",
            "  step 90: local=0.4118 global=1.3827\n",
            "  Layer 11 not converged (global=1.3910 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4008 global=1.3426\n",
            "  step 10: local=0.4029 global=1.4364\n",
            "  step 20: local=0.3962 global=1.3493\n",
            "  step 30: local=0.4037 global=1.3877\n",
            "  step 40: local=0.4005 global=1.3573\n",
            "  step 50: local=0.3940 global=1.4308\n",
            "  step 60: local=0.3951 global=1.3468\n",
            "  step 70: local=0.4066 global=1.4116\n",
            "  step 80: local=0.3981 global=1.4254\n",
            "  step 90: local=0.4014 global=1.3352\n",
            "  Layer 11 not converged (global=1.4087 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4026 global=1.3507\n",
            "  step 10: local=0.4023 global=1.3436\n",
            "  step 20: local=0.3926 global=1.4286\n",
            "  step 30: local=0.3909 global=1.3326\n",
            "  step 40: local=0.4109 global=1.3696\n",
            "  step 50: local=0.4012 global=1.3594\n",
            "  step 60: local=0.3967 global=1.4024\n",
            "  step 70: local=0.3929 global=1.4292\n",
            "  step 80: local=0.4072 global=1.4633\n",
            "  step 90: local=0.3964 global=1.4446\n",
            "  Layer 11 not converged (global=1.4213 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3993 global=1.4762\n",
            "  step 10: local=0.4041 global=1.3602\n",
            "  step 20: local=0.3993 global=1.3983\n",
            "  step 30: local=0.4054 global=1.3953\n",
            "  step 40: local=0.3958 global=1.4220\n",
            "  step 50: local=0.4021 global=1.4327\n",
            "  step 60: local=0.3981 global=1.4355\n",
            "  step 70: local=0.3916 global=1.3662\n",
            "  step 80: local=0.3965 global=1.3170\n",
            "  step 90: local=0.3982 global=1.3370\n",
            "  Layer 11 not converged (global=1.3655 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3987 global=1.3774\n",
            "  step 10: local=0.3916 global=1.4171\n",
            "  step 20: local=0.3960 global=1.3427\n",
            "  step 30: local=0.4020 global=1.4121\n",
            "  step 40: local=0.3940 global=1.4078\n",
            "  step 50: local=0.3913 global=1.3970\n",
            "  step 60: local=0.4014 global=1.3495\n",
            "  step 70: local=0.3859 global=1.4506\n",
            "  step 80: local=0.3906 global=1.3863\n",
            "  step 90: local=0.3960 global=1.4027\n",
            "  Layer 11 not converged (global=1.3866 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3982 global=1.4185\n",
            "  step 10: local=0.3969 global=1.4129\n",
            "  step 20: local=0.3913 global=1.4024\n",
            "  step 30: local=0.3915 global=1.3692\n",
            "  step 40: local=0.3989 global=1.3558\n",
            "  step 50: local=0.3978 global=1.3677\n",
            "  step 60: local=0.3977 global=1.3202\n",
            "  step 70: local=0.3935 global=1.3761\n",
            "  step 80: local=0.3946 global=1.3327\n",
            "  step 90: local=0.3901 global=1.3549\n",
            "  Layer 11 not converged (global=1.4052 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3896 global=1.4022\n",
            "  step 10: local=0.3933 global=1.3490\n",
            "  step 20: local=0.3852 global=1.3352\n",
            "  step 30: local=0.3946 global=1.3577\n",
            "  step 40: local=0.3995 global=1.3416\n",
            "  step 50: local=0.3927 global=1.3979\n",
            "  step 60: local=0.3956 global=1.4914\n",
            "  step 70: local=0.3909 global=1.3826\n",
            "  step 80: local=0.3969 global=1.3755\n",
            "  step 90: local=0.3958 global=1.4515\n",
            "  Layer 11 not converged (global=1.3340 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4022 global=1.4268\n",
            "  step 10: local=0.3907 global=1.3941\n",
            "  step 20: local=0.3969 global=1.4112\n",
            "  step 30: local=0.3929 global=1.3655\n",
            "  step 40: local=0.3903 global=1.3797\n",
            "  step 50: local=0.3943 global=1.3394\n",
            "  step 60: local=0.3957 global=1.3972\n",
            "  step 70: local=0.3982 global=1.4082\n",
            "  step 80: local=0.3896 global=1.3101\n",
            "  step 90: local=0.3973 global=1.3362\n",
            "  Layer 11 not converged (global=1.4349 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3946 global=1.3289\n",
            "  step 10: local=0.3934 global=1.3525\n",
            "  step 20: local=0.4007 global=1.3232\n",
            "  step 30: local=0.3833 global=1.3072\n",
            "  step 40: local=0.4012 global=1.3369\n",
            "  step 50: local=0.3891 global=1.4174\n",
            "  step 60: local=0.3872 global=1.2892\n",
            "  step 70: local=0.3890 global=1.3385\n",
            "  step 80: local=0.3973 global=1.3251\n",
            "  step 90: local=0.3919 global=1.5435\n",
            "  Layer 11 not converged (global=1.3213 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3897 global=1.3577\n",
            "  step 10: local=0.3905 global=1.3887\n",
            "  step 20: local=0.3841 global=1.3737\n",
            "  step 30: local=0.3924 global=1.3762\n",
            "  step 40: local=0.3901 global=1.3832\n",
            "  step 50: local=0.3908 global=1.4568\n",
            "  step 60: local=0.3936 global=1.3988\n",
            "  step 70: local=0.3928 global=1.3551\n",
            "  step 80: local=0.3863 global=1.4499\n",
            "  step 90: local=0.3875 global=1.3946\n",
            "  Layer 11 not converged (global=1.4165 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3863 global=1.2706\n",
            "  step 10: local=0.3861 global=1.3525\n",
            "  step 20: local=0.3921 global=1.3746\n",
            "  step 30: local=0.3997 global=1.4383\n",
            "  step 40: local=0.3795 global=1.4311\n",
            "  step 50: local=0.3890 global=1.2665\n",
            "  step 60: local=0.3923 global=1.3407\n",
            "  step 70: local=0.3870 global=1.4421\n",
            "  step 80: local=0.3961 global=1.4504\n",
            "  step 90: local=0.3908 global=1.2989\n",
            "  Layer 11 not converged (global=1.4009 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3889 global=1.3424\n",
            "  step 10: local=0.3844 global=1.3557\n",
            "  step 20: local=0.3855 global=1.3504\n",
            "  step 30: local=0.3982 global=1.3722\n",
            "  step 40: local=0.3835 global=1.3329\n",
            "  step 50: local=0.3863 global=1.4263\n",
            "  step 60: local=0.3853 global=1.3385\n",
            "  step 70: local=0.3859 global=1.3770\n",
            "  step 80: local=0.3847 global=1.3477\n",
            "  step 90: local=0.3817 global=1.4216\n",
            "  Layer 11 not converged (global=1.4497 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3967 global=1.3377\n",
            "  step 10: local=0.3971 global=1.4005\n",
            "  step 20: local=0.3892 global=1.4158\n",
            "  step 30: local=0.3885 global=1.3256\n",
            "  step 40: local=0.3797 global=1.3419\n",
            "  step 50: local=0.3922 global=1.3359\n",
            "  step 60: local=0.3897 global=1.4194\n",
            "  step 70: local=0.3782 global=1.3232\n",
            "  step 80: local=0.3981 global=1.3610\n",
            "  step 90: local=0.3845 global=1.3509\n",
            "  Layer 11 not converged (global=1.3371 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3832 global=1.3946\n",
            "  step 10: local=0.3906 global=1.4206\n",
            "  step 20: local=0.3861 global=1.4525\n",
            "  step 30: local=0.3849 global=1.4357\n",
            "  step 40: local=0.3829 global=1.4668\n",
            "  step 50: local=0.3951 global=1.3507\n",
            "  step 60: local=0.3764 global=1.3887\n",
            "  step 70: local=0.3860 global=1.3877\n",
            "  step 80: local=0.3783 global=1.4149\n",
            "  step 90: local=0.3838 global=1.4253\n",
            "  Layer 11 not converged (global=1.3930 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3926 global=1.4282\n",
            "  step 10: local=0.3793 global=1.3584\n",
            "  step 20: local=0.3829 global=1.3097\n",
            "  step 30: local=0.4021 global=1.3307\n",
            "  step 40: local=0.3953 global=1.3694\n",
            "  step 50: local=0.3820 global=1.4096\n",
            "  step 60: local=0.3885 global=1.3352\n",
            "  step 70: local=0.3848 global=1.4054\n",
            "  step 80: local=0.3817 global=1.4013\n",
            "  step 90: local=0.3863 global=1.3906\n",
            "  [WARN] Layer 11 did not converge after 20 repeats (global=1.3391)\n",
            "\n",
            "--- Layer 12/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5216 global=1.4098\n",
            "  step 10: local=0.5092 global=1.4969\n",
            "  step 20: local=0.4967 global=1.4294\n",
            "  step 30: local=0.4824 global=1.4437\n",
            "  step 40: local=0.4763 global=1.4525\n",
            "  step 50: local=0.4682 global=1.4578\n",
            "  step 60: local=0.4622 global=1.4328\n",
            "  step 70: local=0.4705 global=1.3986\n",
            "  step 80: local=0.4553 global=1.3805\n",
            "  step 90: local=0.4605 global=1.3917\n",
            "  Layer 12 not converged (global=1.3667 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4488 global=1.3416\n",
            "  step 10: local=0.4490 global=1.3990\n",
            "  step 20: local=0.4426 global=1.3540\n",
            "  step 30: local=0.4489 global=1.3796\n",
            "  step 40: local=0.4381 global=1.4165\n",
            "  step 50: local=0.4256 global=1.3695\n",
            "  step 60: local=0.4249 global=1.3514\n",
            "  step 70: local=0.4345 global=1.3740\n",
            "  step 80: local=0.4281 global=1.3613\n",
            "  step 90: local=0.4200 global=1.4163\n",
            "  Layer 12 not converged (global=1.4294 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4288 global=1.5040\n",
            "  step 10: local=0.4147 global=1.3907\n",
            "  step 20: local=0.4165 global=1.3867\n",
            "  step 30: local=0.4183 global=1.4581\n",
            "  step 40: local=0.4111 global=1.4367\n",
            "  step 50: local=0.4152 global=1.4107\n",
            "  step 60: local=0.4142 global=1.4136\n",
            "  step 70: local=0.4073 global=1.3777\n",
            "  step 80: local=0.4074 global=1.3858\n",
            "  step 90: local=0.4078 global=1.3519\n",
            "  Layer 12 not converged (global=1.3579 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4066 global=1.4078\n",
            "  step 10: local=0.4046 global=1.4096\n",
            "  step 20: local=0.3957 global=1.3205\n",
            "  step 30: local=0.4105 global=1.3440\n",
            "  step 40: local=0.4080 global=1.3286\n",
            "  step 50: local=0.4047 global=1.3540\n",
            "  step 60: local=0.3969 global=1.3260\n",
            "  step 70: local=0.3994 global=1.3060\n",
            "  step 80: local=0.4017 global=1.3361\n",
            "  step 90: local=0.4049 global=1.4186\n",
            "  Layer 12 not converged (global=1.3361 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3929 global=1.2888\n",
            "  step 10: local=0.4010 global=1.3395\n",
            "  step 20: local=0.3996 global=1.3239\n",
            "  step 30: local=0.3945 global=1.5395\n",
            "  step 40: local=0.3921 global=1.3571\n",
            "  step 50: local=0.4008 global=1.3867\n",
            "  step 60: local=0.3879 global=1.3790\n",
            "  step 70: local=0.3849 global=1.3803\n",
            "  step 80: local=0.3942 global=1.3765\n",
            "  step 90: local=0.3928 global=1.4544\n",
            "  Layer 12 not converged (global=1.4004 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3894 global=1.4012\n",
            "  step 10: local=0.3909 global=1.3505\n",
            "  step 20: local=0.3836 global=1.4487\n",
            "  step 30: local=0.3837 global=1.3873\n",
            "  step 40: local=0.3857 global=1.2663\n",
            "  step 50: local=0.3899 global=1.3502\n",
            "  step 60: local=0.3897 global=1.3692\n",
            "  step 70: local=0.3930 global=1.4383\n",
            "  step 80: local=0.3798 global=1.4287\n",
            "  step 90: local=0.3836 global=1.2652\n",
            "  Layer 12 not converged (global=1.3723 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3864 global=1.3346\n",
            "  step 10: local=0.3906 global=1.4385\n",
            "  step 20: local=0.3867 global=1.4412\n",
            "  step 30: local=0.3821 global=1.2963\n",
            "  step 40: local=0.3788 global=1.3412\n",
            "  step 50: local=0.3863 global=1.3539\n",
            "  step 60: local=0.3896 global=1.3399\n",
            "  step 70: local=0.4005 global=1.3717\n",
            "  step 80: local=0.3839 global=1.3257\n",
            "  step 90: local=0.3828 global=1.4231\n",
            "  Layer 12 not converged (global=1.4076 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3787 global=1.3343\n",
            "  step 10: local=0.3898 global=1.3792\n",
            "  step 20: local=0.3881 global=1.3424\n",
            "  step 30: local=0.3820 global=1.4186\n",
            "  step 40: local=0.3867 global=1.3360\n",
            "  step 50: local=0.3981 global=1.3968\n",
            "  step 60: local=0.3901 global=1.4098\n",
            "  step 70: local=0.3893 global=1.3208\n",
            "  step 80: local=0.3834 global=1.3429\n",
            "  step 90: local=0.3866 global=1.3317\n",
            "  Layer 12 not converged (global=1.3912 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3823 global=1.4173\n",
            "  step 10: local=0.3806 global=1.3190\n",
            "  step 20: local=0.3829 global=1.3517\n",
            "  step 30: local=0.3906 global=1.3461\n",
            "  step 40: local=0.3864 global=1.3936\n",
            "  step 50: local=0.3775 global=1.4118\n",
            "  step 60: local=0.3833 global=1.4533\n",
            "  step 70: local=0.3727 global=1.4284\n",
            "  step 80: local=0.3780 global=1.4569\n",
            "  step 90: local=0.3906 global=1.3481\n",
            "  Layer 12 not converged (global=1.3796 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3867 global=1.3785\n",
            "  step 10: local=0.3813 global=1.3795\n",
            "  step 20: local=0.3705 global=1.4026\n",
            "  step 30: local=0.3748 global=1.4233\n",
            "  step 40: local=0.3843 global=1.4187\n",
            "  step 50: local=0.3785 global=1.3497\n",
            "  step 60: local=0.3787 global=1.3080\n",
            "  step 70: local=0.3806 global=1.3282\n",
            "  step 80: local=0.3774 global=1.3704\n",
            "  step 90: local=0.3814 global=1.4080\n",
            "  Layer 12 not converged (global=1.3032 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3714 global=1.3287\n",
            "  step 10: local=0.3819 global=1.4005\n",
            "  step 20: local=0.3770 global=1.3932\n",
            "  step 30: local=0.3755 global=1.3848\n",
            "  step 40: local=0.3884 global=1.3370\n",
            "  step 50: local=0.3795 global=1.4366\n",
            "  step 60: local=0.3759 global=1.3696\n",
            "  step 70: local=0.3766 global=1.3907\n",
            "  step 80: local=0.3804 global=1.3998\n",
            "  step 90: local=0.3833 global=1.4040\n",
            "  Layer 12 not converged (global=1.3881 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3749 global=1.3885\n",
            "  step 10: local=0.3854 global=1.3520\n",
            "  step 20: local=0.3753 global=1.3381\n",
            "  step 30: local=0.3747 global=1.3529\n",
            "  step 40: local=0.3743 global=1.3049\n",
            "  step 50: local=0.3735 global=1.3643\n",
            "  step 60: local=0.3738 global=1.3212\n",
            "  step 70: local=0.3877 global=1.3441\n",
            "  step 80: local=0.3757 global=1.3825\n",
            "  step 90: local=0.3875 global=1.3374\n",
            "  Layer 12 not converged (global=1.3980 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3712 global=1.3193\n",
            "  step 10: local=0.3797 global=1.3443\n",
            "  step 20: local=0.3837 global=1.3342\n",
            "  step 30: local=0.3802 global=1.3866\n",
            "  step 40: local=0.3764 global=1.4766\n",
            "  step 50: local=0.3744 global=1.3671\n",
            "  step 60: local=0.3807 global=1.3634\n",
            "  step 70: local=0.3755 global=1.4353\n",
            "  step 80: local=0.3817 global=1.4152\n",
            "  step 90: local=0.3760 global=1.3893\n",
            "  Layer 12 not converged (global=1.4383 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3792 global=1.3908\n",
            "  step 10: local=0.3738 global=1.3576\n",
            "  step 20: local=0.3730 global=1.3664\n",
            "  step 30: local=0.3879 global=1.3327\n",
            "  step 40: local=0.3741 global=1.3864\n",
            "  step 50: local=0.3779 global=1.3926\n",
            "  step 60: local=0.3704 global=1.3034\n",
            "  step 70: local=0.3774 global=1.3291\n",
            "  step 80: local=0.3758 global=1.3118\n",
            "  step 90: local=0.3747 global=1.3394\n",
            "  Layer 12 not converged (global=1.2723 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3738 global=1.3112\n",
            "  step 10: local=0.3784 global=1.2909\n",
            "  step 20: local=0.3799 global=1.3200\n",
            "  step 30: local=0.3800 global=1.4039\n",
            "  step 40: local=0.3689 global=1.2746\n",
            "  step 50: local=0.3861 global=1.3244\n",
            "  step 60: local=0.3714 global=1.3103\n",
            "  step 70: local=0.3754 global=1.5280\n",
            "  step 80: local=0.3746 global=1.3461\n",
            "  step 90: local=0.3834 global=1.3756\n",
            "  Layer 12 not converged (global=1.3394 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3672 global=1.3683\n",
            "  step 10: local=0.3781 global=1.3698\n",
            "  step 20: local=0.3769 global=1.3677\n",
            "  step 30: local=0.3729 global=1.4437\n",
            "  step 40: local=0.3760 global=1.3901\n",
            "  step 50: local=0.3665 global=1.3403\n",
            "  step 60: local=0.3697 global=1.4405\n",
            "  step 70: local=0.3687 global=1.3780\n",
            "  step 80: local=0.3870 global=1.2591\n",
            "  step 90: local=0.3786 global=1.3417\n",
            "  Layer 12 not converged (global=1.3305 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3833 global=1.3605\n",
            "  step 10: local=0.3832 global=1.4277\n",
            "  step 20: local=0.3767 global=1.4192\n",
            "  step 30: local=0.3709 global=1.2559\n",
            "  step 40: local=0.3785 global=1.3269\n",
            "  step 50: local=0.3842 global=1.4291\n",
            "  step 60: local=0.3749 global=1.4321\n",
            "  step 70: local=0.3767 global=1.2872\n",
            "  step 80: local=0.3720 global=1.3345\n",
            "  step 90: local=0.3802 global=1.3474\n",
            "  Layer 12 not converged (global=1.3046 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3772 global=1.3323\n",
            "  step 10: local=0.3793 global=1.3638\n",
            "  step 20: local=0.3712 global=1.3173\n",
            "  step 30: local=0.3746 global=1.4153\n",
            "  step 40: local=0.3761 global=1.3284\n",
            "  step 50: local=0.3796 global=1.3711\n",
            "  step 60: local=0.3714 global=1.3337\n",
            "  step 70: local=0.3680 global=1.4122\n",
            "  step 80: local=0.3764 global=1.3286\n",
            "  step 90: local=0.3832 global=1.3891\n",
            "  Layer 12 not converged (global=1.3561 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3697 global=1.4025\n",
            "  step 10: local=0.3714 global=1.3137\n",
            "  step 20: local=0.3809 global=1.3369\n",
            "  step 30: local=0.3693 global=1.3251\n",
            "  step 40: local=0.3670 global=1.4087\n",
            "  step 50: local=0.3762 global=1.3126\n",
            "  step 60: local=0.3779 global=1.3452\n",
            "  step 70: local=0.3650 global=1.3391\n",
            "  step 80: local=0.3695 global=1.3881\n",
            "  step 90: local=0.3788 global=1.4039\n",
            "  Layer 12 not converged (global=1.3383 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3735 global=1.4460\n",
            "  step 10: local=0.3698 global=1.4218\n",
            "  step 20: local=0.3729 global=1.4510\n",
            "  step 30: local=0.3745 global=1.3415\n",
            "  step 40: local=0.3804 global=1.3723\n",
            "  step 50: local=0.3721 global=1.3721\n",
            "  step 60: local=0.3640 global=1.3970\n",
            "  step 70: local=0.3690 global=1.4155\n",
            "  step 80: local=0.3719 global=1.4126\n",
            "  step 90: local=0.3647 global=1.3428\n",
            "  [WARN] Layer 12 did not converge after 20 repeats (global=1.4198)\n",
            "\n",
            "--- Layer 13/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5175 global=1.3551\n",
            "  step 10: local=0.5048 global=1.3703\n",
            "  step 20: local=0.4942 global=1.4094\n",
            "  step 30: local=0.4993 global=1.4487\n",
            "  step 40: local=0.4904 global=1.3531\n",
            "  step 50: local=0.4802 global=1.4303\n",
            "  step 60: local=0.4660 global=1.4166\n",
            "  step 70: local=0.4713 global=1.4153\n",
            "  step 80: local=0.4732 global=1.3629\n",
            "  step 90: local=0.4619 global=1.4626\n",
            "  Layer 13 not converged (global=1.4410 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4604 global=1.3848\n",
            "  step 10: local=0.4585 global=1.4161\n",
            "  step 20: local=0.4452 global=1.4187\n",
            "  step 30: local=0.4389 global=1.4213\n",
            "  step 40: local=0.4456 global=1.4106\n",
            "  step 50: local=0.4329 global=1.3681\n",
            "  step 60: local=0.4341 global=1.3505\n",
            "  step 70: local=0.4342 global=1.3633\n",
            "  step 80: local=0.4381 global=1.3185\n",
            "  step 90: local=0.4304 global=1.3726\n",
            "  Layer 13 not converged (global=1.3906 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4227 global=1.3312\n",
            "  step 10: local=0.4291 global=1.3565\n",
            "  step 20: local=0.4206 global=1.3987\n",
            "  step 30: local=0.4253 global=1.3438\n",
            "  step 40: local=0.4142 global=1.3225\n",
            "  step 50: local=0.4241 global=1.3536\n",
            "  step 60: local=0.4175 global=1.3415\n",
            "  step 70: local=0.4285 global=1.3962\n",
            "  step 80: local=0.4169 global=1.4838\n",
            "  step 90: local=0.4248 global=1.3727\n",
            "  Layer 13 not converged (global=1.3417 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4184 global=1.3701\n",
            "  step 10: local=0.4097 global=1.4399\n",
            "  step 20: local=0.4076 global=1.4191\n",
            "  step 30: local=0.4126 global=1.3907\n",
            "  step 40: local=0.4054 global=1.3985\n",
            "  step 50: local=0.4065 global=1.3564\n",
            "  step 60: local=0.3944 global=1.3617\n",
            "  step 70: local=0.4004 global=1.3299\n",
            "  step 80: local=0.4104 global=1.3885\n",
            "  step 90: local=0.4053 global=1.3907\n",
            "  Layer 13 not converged (global=1.4119 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4022 global=1.3084\n",
            "  step 10: local=0.4000 global=1.3290\n",
            "  step 20: local=0.4011 global=1.3119\n",
            "  step 30: local=0.3969 global=1.3426\n",
            "  step 40: local=0.3902 global=1.3148\n",
            "  step 50: local=0.3965 global=1.2885\n",
            "  step 60: local=0.3930 global=1.3252\n",
            "  step 70: local=0.3897 global=1.3981\n",
            "  step 80: local=0.3934 global=1.2697\n",
            "  step 90: local=0.3976 global=1.3196\n",
            "  Layer 13 not converged (global=1.2939 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4022 global=1.3087\n",
            "  step 10: local=0.3853 global=1.5248\n",
            "  step 20: local=0.4011 global=1.3413\n",
            "  step 30: local=0.4007 global=1.3733\n",
            "  step 40: local=0.4001 global=1.3622\n",
            "  step 50: local=0.3956 global=1.3656\n",
            "  step 60: local=0.3856 global=1.3622\n",
            "  step 70: local=0.3902 global=1.4422\n",
            "  step 80: local=0.3940 global=1.3934\n",
            "  step 90: local=0.3862 global=1.3391\n",
            "  Layer 13 not converged (global=1.3249 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3883 global=1.4415\n",
            "  step 10: local=0.3911 global=1.3676\n",
            "  step 20: local=0.3924 global=1.2551\n",
            "  step 30: local=0.3932 global=1.3398\n",
            "  step 40: local=0.3972 global=1.3496\n",
            "  step 50: local=0.3947 global=1.4212\n",
            "  step 60: local=0.3861 global=1.4162\n",
            "  step 70: local=0.3936 global=1.2548\n",
            "  step 80: local=0.3902 global=1.3201\n",
            "  step 90: local=0.3827 global=1.4238\n",
            "  Layer 13 not converged (global=1.4190 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3927 global=1.4284\n",
            "  step 10: local=0.3966 global=1.2856\n",
            "  step 20: local=0.3848 global=1.3267\n",
            "  step 30: local=0.3885 global=1.3449\n",
            "  step 40: local=0.3933 global=1.3277\n",
            "  step 50: local=0.4009 global=1.3592\n",
            "  step 60: local=0.3879 global=1.3101\n",
            "  step 70: local=0.3807 global=1.4095\n",
            "  step 80: local=0.3896 global=1.3226\n",
            "  step 90: local=0.3931 global=1.3694\n",
            "  Layer 13 not converged (global=1.3722 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3943 global=1.3288\n",
            "  step 10: local=0.3877 global=1.4034\n",
            "  step 20: local=0.3871 global=1.3202\n",
            "  step 30: local=0.3885 global=1.3802\n",
            "  step 40: local=0.3918 global=1.3967\n",
            "  step 50: local=0.3825 global=1.3124\n",
            "  step 60: local=0.3897 global=1.3329\n",
            "  step 70: local=0.3938 global=1.3204\n",
            "  step 80: local=0.3773 global=1.4051\n",
            "  step 90: local=0.3862 global=1.3065\n",
            "  Layer 13 not converged (global=1.3411 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3807 global=1.3429\n",
            "  step 10: local=0.3782 global=1.3361\n",
            "  step 20: local=0.3923 global=1.3845\n",
            "  step 30: local=0.3834 global=1.3968\n",
            "  step 40: local=0.3929 global=1.4374\n",
            "  step 50: local=0.3784 global=1.4147\n",
            "  step 60: local=0.3819 global=1.4428\n",
            "  step 70: local=0.3854 global=1.3332\n",
            "  step 80: local=0.3842 global=1.3684\n",
            "  step 90: local=0.3815 global=1.3653\n",
            "  Layer 13 not converged (global=1.2682 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3898 global=1.3892\n",
            "  step 10: local=0.3939 global=1.4180\n",
            "  step 20: local=0.3928 global=1.4066\n",
            "  step 30: local=0.3852 global=1.3422\n",
            "  step 40: local=0.3920 global=1.2932\n",
            "  step 50: local=0.3850 global=1.3153\n",
            "  step 60: local=0.3766 global=1.3581\n",
            "  step 70: local=0.3779 global=1.4002\n",
            "  step 80: local=0.3934 global=1.3140\n",
            "  step 90: local=0.3823 global=1.3863\n",
            "  Layer 13 not converged (global=1.3477 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3880 global=1.3783\n",
            "  step 10: local=0.3802 global=1.3762\n",
            "  step 20: local=0.3915 global=1.3260\n",
            "  step 30: local=0.3939 global=1.4243\n",
            "  step 40: local=0.3919 global=1.3535\n",
            "  step 50: local=0.3895 global=1.3828\n",
            "  step 60: local=0.3939 global=1.3879\n",
            "  step 70: local=0.3852 global=1.3899\n",
            "  step 80: local=0.3804 global=1.3809\n",
            "  step 90: local=0.3867 global=1.3415\n",
            "  Layer 13 not converged (global=1.3482 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3863 global=1.3249\n",
            "  step 10: local=0.3779 global=1.3384\n",
            "  step 20: local=0.3808 global=1.2955\n",
            "  step 30: local=0.3901 global=1.3472\n",
            "  step 40: local=0.3898 global=1.3096\n",
            "  step 50: local=0.3900 global=1.3317\n",
            "  step 60: local=0.3920 global=1.3770\n",
            "  step 70: local=0.3800 global=1.3242\n",
            "  step 80: local=0.3836 global=1.3030\n",
            "  step 90: local=0.3830 global=1.3339\n",
            "  Layer 13 not converged (global=1.2825 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3888 global=1.3236\n",
            "  step 10: local=0.3812 global=1.3772\n",
            "  step 20: local=0.3970 global=1.4669\n",
            "  step 30: local=0.3898 global=1.3576\n",
            "  step 40: local=0.3792 global=1.3538\n",
            "  step 50: local=0.3743 global=1.4270\n",
            "  step 60: local=0.3996 global=1.4025\n",
            "  step 70: local=0.3861 global=1.3774\n",
            "  step 80: local=0.3802 global=1.3844\n",
            "  step 90: local=0.3820 global=1.3441\n",
            "  Layer 13 not converged (global=1.3689 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3839 global=1.3484\n",
            "  step 10: local=0.3872 global=1.3170\n",
            "  step 20: local=0.3915 global=1.3758\n",
            "  step 30: local=0.3901 global=1.3799\n",
            "  step 40: local=0.3801 global=1.2953\n",
            "  step 50: local=0.3885 global=1.3188\n",
            "  step 60: local=0.3822 global=1.3022\n",
            "  step 70: local=0.3780 global=1.3308\n",
            "  step 80: local=0.3806 global=1.3035\n",
            "  step 90: local=0.3824 global=1.2787\n",
            "  Layer 13 not converged (global=1.3450 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3845 global=1.3158\n",
            "  step 10: local=0.3874 global=1.3875\n",
            "  step 20: local=0.3712 global=1.2612\n",
            "  step 30: local=0.3872 global=1.3118\n",
            "  step 40: local=0.3745 global=1.2996\n",
            "  step 50: local=0.3894 global=1.5139\n",
            "  step 60: local=0.3851 global=1.3333\n",
            "  step 70: local=0.3902 global=1.3654\n",
            "  step 80: local=0.3871 global=1.3538\n",
            "  step 90: local=0.3878 global=1.3582\n",
            "  Layer 13 not converged (global=1.3520 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3853 global=1.3542\n",
            "  step 10: local=0.3833 global=1.4331\n",
            "  step 20: local=0.3850 global=1.3861\n",
            "  step 30: local=0.3786 global=1.3306\n",
            "  step 40: local=0.3859 global=1.4336\n",
            "  step 50: local=0.3791 global=1.3617\n",
            "  step 60: local=0.3791 global=1.2475\n",
            "  step 70: local=0.3827 global=1.3322\n",
            "  step 80: local=0.3807 global=1.3423\n",
            "  step 90: local=0.3830 global=1.4151\n",
            "  Layer 13 not converged (global=1.3834 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3890 global=1.4095\n",
            "  step 10: local=0.3758 global=1.2491\n",
            "  step 20: local=0.3857 global=1.3131\n",
            "  step 30: local=0.3850 global=1.4171\n",
            "  step 40: local=0.3851 global=1.4201\n",
            "  step 50: local=0.3774 global=1.2780\n",
            "  step 60: local=0.3807 global=1.3192\n",
            "  step 70: local=0.3795 global=1.3378\n",
            "  step 80: local=0.3776 global=1.3214\n",
            "  step 90: local=0.3863 global=1.3539\n",
            "  Layer 13 not converged (global=1.3655 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3876 global=1.3056\n",
            "  step 10: local=0.3749 global=1.4026\n",
            "  step 20: local=0.3708 global=1.3175\n",
            "  step 30: local=0.3853 global=1.3623\n",
            "  step 40: local=0.3792 global=1.3238\n",
            "  step 50: local=0.3869 global=1.3984\n",
            "  step 60: local=0.3832 global=1.3155\n",
            "  step 70: local=0.3861 global=1.3741\n",
            "  step 80: local=0.3737 global=1.3912\n",
            "  step 90: local=0.3765 global=1.3072\n",
            "  Layer 13 not converged (global=1.3919 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3711 global=1.3274\n",
            "  step 10: local=0.3839 global=1.3154\n",
            "  step 20: local=0.3801 global=1.4004\n",
            "  step 30: local=0.3828 global=1.3024\n",
            "  step 40: local=0.3844 global=1.3371\n",
            "  step 50: local=0.3747 global=1.3302\n",
            "  step 60: local=0.3875 global=1.3793\n",
            "  step 70: local=0.3786 global=1.3912\n",
            "  step 80: local=0.3804 global=1.4317\n",
            "  step 90: local=0.3783 global=1.4093\n",
            "  [WARN] Layer 13 did not converge after 20 repeats (global=1.3871)\n",
            "\n",
            "--- Layer 14/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4815 global=1.5210\n",
            "  step 10: local=0.4617 global=1.3977\n",
            "  step 20: local=0.4587 global=1.4316\n",
            "  step 30: local=0.4422 global=1.4274\n",
            "  step 40: local=0.4490 global=1.4422\n",
            "  step 50: local=0.4383 global=1.4716\n",
            "  step 60: local=0.4388 global=1.4592\n",
            "  step 70: local=0.4366 global=1.3842\n",
            "  step 80: local=0.4306 global=1.3451\n",
            "  step 90: local=0.4348 global=1.3555\n",
            "  Layer 14 not converged (global=1.3773 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4369 global=1.3986\n",
            "  step 10: local=0.4296 global=1.4429\n",
            "  step 20: local=0.4153 global=1.3426\n",
            "  step 30: local=0.4128 global=1.4201\n",
            "  step 40: local=0.4176 global=1.4079\n",
            "  step 50: local=0.4087 global=1.4075\n",
            "  step 60: local=0.4106 global=1.3542\n",
            "  step 70: local=0.3942 global=1.4584\n",
            "  step 80: local=0.4119 global=1.3752\n",
            "  step 90: local=0.3976 global=1.4017\n",
            "  Layer 14 not converged (global=1.3780 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3986 global=1.4094\n",
            "  step 10: local=0.3981 global=1.4106\n",
            "  step 20: local=0.4007 global=1.4075\n",
            "  step 30: local=0.4032 global=1.3659\n",
            "  step 40: local=0.3853 global=1.3416\n",
            "  step 50: local=0.3909 global=1.3533\n",
            "  step 60: local=0.3928 global=1.3157\n",
            "  step 70: local=0.3868 global=1.3634\n",
            "  step 80: local=0.3847 global=1.3274\n",
            "  step 90: local=0.3842 global=1.3461\n",
            "  Layer 14 not converged (global=1.3879 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3787 global=1.3948\n",
            "  step 10: local=0.3820 global=1.3369\n",
            "  step 20: local=0.3928 global=1.3221\n",
            "  step 30: local=0.3841 global=1.3437\n",
            "  step 40: local=0.3807 global=1.3396\n",
            "  step 50: local=0.3796 global=1.3914\n",
            "  step 60: local=0.3717 global=1.4737\n",
            "  step 70: local=0.3732 global=1.3650\n",
            "  step 80: local=0.3760 global=1.3624\n",
            "  step 90: local=0.3616 global=1.4411\n",
            "  Layer 14 not converged (global=1.3131 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3696 global=1.4081\n",
            "  step 10: local=0.3806 global=1.3843\n",
            "  step 20: local=0.3674 global=1.3951\n",
            "  step 30: local=0.3723 global=1.3515\n",
            "  step 40: local=0.3686 global=1.3561\n",
            "  step 50: local=0.3697 global=1.3278\n",
            "  step 60: local=0.3703 global=1.3843\n",
            "  step 70: local=0.3794 global=1.3831\n",
            "  step 80: local=0.3643 global=1.3067\n",
            "  step 90: local=0.3753 global=1.3285\n",
            "  Layer 14 not converged (global=1.4196 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3618 global=1.3016\n",
            "  step 10: local=0.3655 global=1.3373\n",
            "  step 20: local=0.3723 global=1.3113\n",
            "  step 30: local=0.3643 global=1.2877\n",
            "  step 40: local=0.3691 global=1.3259\n",
            "  step 50: local=0.3586 global=1.3880\n",
            "  step 60: local=0.3637 global=1.2644\n",
            "  step 70: local=0.3687 global=1.3198\n",
            "  step 80: local=0.3729 global=1.3013\n",
            "  step 90: local=0.3636 global=1.5225\n",
            "  Layer 14 not converged (global=1.3055 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3648 global=1.3372\n",
            "  step 10: local=0.3730 global=1.3682\n",
            "  step 20: local=0.3592 global=1.3618\n",
            "  step 30: local=0.3615 global=1.3640\n",
            "  step 40: local=0.3664 global=1.3596\n",
            "  step 50: local=0.3659 global=1.4372\n",
            "  step 60: local=0.3633 global=1.3905\n",
            "  step 70: local=0.3641 global=1.3333\n",
            "  step 80: local=0.3541 global=1.4425\n",
            "  step 90: local=0.3595 global=1.3665\n",
            "  Layer 14 not converged (global=1.3961 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3671 global=1.2499\n",
            "  step 10: local=0.3628 global=1.3353\n",
            "  step 20: local=0.3511 global=1.3484\n",
            "  step 30: local=0.3652 global=1.4213\n",
            "  step 40: local=0.3518 global=1.4066\n",
            "  step 50: local=0.3584 global=1.2540\n",
            "  step 60: local=0.3582 global=1.3190\n",
            "  step 70: local=0.3556 global=1.4135\n",
            "  step 80: local=0.3595 global=1.4188\n",
            "  step 90: local=0.3686 global=1.2813\n",
            "  Layer 14 not converged (global=1.3776 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3652 global=1.3257\n",
            "  step 10: local=0.3554 global=1.3386\n",
            "  step 20: local=0.3672 global=1.3227\n",
            "  step 30: local=0.3673 global=1.3576\n",
            "  step 40: local=0.3646 global=1.3120\n",
            "  step 50: local=0.3587 global=1.3999\n",
            "  step 60: local=0.3549 global=1.3224\n",
            "  step 70: local=0.3523 global=1.3708\n",
            "  step 80: local=0.3614 global=1.3260\n",
            "  step 90: local=0.3500 global=1.4002\n",
            "  Layer 14 not converged (global=1.4256 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3651 global=1.3153\n",
            "  step 10: local=0.3654 global=1.3761\n",
            "  step 20: local=0.3610 global=1.3937\n",
            "  step 30: local=0.3483 global=1.3066\n",
            "  step 40: local=0.3582 global=1.3293\n",
            "  step 50: local=0.3588 global=1.3127\n",
            "  step 60: local=0.3448 global=1.4025\n",
            "  step 70: local=0.3512 global=1.3065\n",
            "  step 80: local=0.3637 global=1.3308\n",
            "  step 90: local=0.3527 global=1.3339\n",
            "  Layer 14 not converged (global=1.3168 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3493 global=1.3802\n",
            "  step 10: local=0.3615 global=1.3888\n",
            "  step 20: local=0.3536 global=1.4332\n",
            "  step 30: local=0.3513 global=1.4086\n",
            "  step 40: local=0.3526 global=1.4383\n",
            "  step 50: local=0.3498 global=1.3298\n",
            "  step 60: local=0.3556 global=1.3651\n",
            "  step 70: local=0.3541 global=1.3636\n",
            "  step 80: local=0.3507 global=1.3828\n",
            "  step 90: local=0.3529 global=1.4170\n",
            "  Layer 14 not converged (global=1.3721 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3535 global=1.4060\n",
            "  step 10: local=0.3542 global=1.3346\n",
            "  step 20: local=0.3445 global=1.2971\n",
            "  step 30: local=0.3553 global=1.3132\n",
            "  step 40: local=0.3484 global=1.3547\n",
            "  step 50: local=0.3484 global=1.3981\n",
            "  step 60: local=0.3505 global=1.3057\n",
            "  step 70: local=0.3516 global=1.3826\n",
            "  step 80: local=0.3575 global=1.3718\n",
            "  step 90: local=0.3536 global=1.3777\n",
            "  Layer 14 not converged (global=1.3158 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3572 global=1.3213\n",
            "  step 10: local=0.3601 global=1.4258\n",
            "  step 20: local=0.3570 global=1.3473\n",
            "  step 30: local=0.3452 global=1.3732\n",
            "  step 40: local=0.3498 global=1.3832\n",
            "  step 50: local=0.3558 global=1.3830\n",
            "  step 60: local=0.3440 global=1.3810\n",
            "  step 70: local=0.3535 global=1.3442\n",
            "  step 80: local=0.3533 global=1.3218\n",
            "  step 90: local=0.3519 global=1.3323\n",
            "  Layer 14 not converged (global=1.3073 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3424 global=1.2950\n",
            "  step 10: local=0.3466 global=1.3428\n",
            "  step 20: local=0.3529 global=1.3103\n",
            "  step 30: local=0.3512 global=1.3285\n",
            "  step 40: local=0.3572 global=1.3741\n",
            "  step 50: local=0.3637 global=1.3191\n",
            "  step 60: local=0.3501 global=1.3031\n",
            "  step 70: local=0.3467 global=1.3262\n",
            "  step 80: local=0.3436 global=1.3222\n",
            "  step 90: local=0.3479 global=1.3754\n",
            "  Layer 14 not converged (global=1.3878 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3502 global=1.4593\n",
            "  step 10: local=0.3494 global=1.3494\n",
            "  step 20: local=0.3491 global=1.3475\n",
            "  step 30: local=0.3491 global=1.4278\n",
            "  step 40: local=0.3566 global=1.3941\n",
            "  step 50: local=0.3487 global=1.3721\n",
            "  step 60: local=0.3483 global=1.3814\n",
            "  step 70: local=0.3643 global=1.3413\n",
            "  step 80: local=0.3516 global=1.3447\n",
            "  step 90: local=0.3437 global=1.3142\n",
            "  Layer 14 not converged (global=1.3262 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3549 global=1.3735\n",
            "  step 10: local=0.3607 global=1.3725\n",
            "  step 20: local=0.3565 global=1.2958\n",
            "  step 30: local=0.3591 global=1.3184\n",
            "  step 40: local=0.3385 global=1.2922\n",
            "  step 50: local=0.3553 global=1.3262\n",
            "  step 60: local=0.3547 global=1.3020\n",
            "  step 70: local=0.3553 global=1.2775\n",
            "  step 80: local=0.3421 global=1.3159\n",
            "  step 90: local=0.3434 global=1.3785\n",
            "  Layer 14 not converged (global=1.3103 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3447 global=1.2567\n",
            "  step 10: local=0.3532 global=1.3128\n",
            "  step 20: local=0.3503 global=1.2916\n",
            "  step 30: local=0.3474 global=1.5125\n",
            "  step 40: local=0.3590 global=1.3276\n",
            "  step 50: local=0.3562 global=1.3604\n",
            "  step 60: local=0.3556 global=1.3546\n",
            "  step 70: local=0.3521 global=1.3575\n",
            "  step 80: local=0.3519 global=1.3505\n",
            "  step 90: local=0.3506 global=1.4295\n",
            "  Layer 14 not converged (global=1.3799 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3511 global=1.3817\n",
            "  step 10: local=0.3585 global=1.3262\n",
            "  step 20: local=0.3400 global=1.4337\n",
            "  step 30: local=0.3467 global=1.3589\n",
            "  step 40: local=0.3488 global=1.2426\n",
            "  step 50: local=0.3466 global=1.3300\n",
            "  step 60: local=0.3417 global=1.3415\n",
            "  step 70: local=0.3470 global=1.4131\n",
            "  step 80: local=0.3457 global=1.3997\n",
            "  step 90: local=0.3492 global=1.2482\n",
            "  Layer 14 not converged (global=1.3531 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3433 global=1.3112\n",
            "  step 10: local=0.3506 global=1.4054\n",
            "  step 20: local=0.3434 global=1.4117\n",
            "  step 30: local=0.3601 global=1.2738\n",
            "  step 40: local=0.3578 global=1.3199\n",
            "  step 50: local=0.3471 global=1.3334\n",
            "  step 60: local=0.3550 global=1.3170\n",
            "  step 70: local=0.3461 global=1.3517\n",
            "  step 80: local=0.3445 global=1.3066\n",
            "  step 90: local=0.3444 global=1.3928\n",
            "  Layer 14 not converged (global=1.3819 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3434 global=1.3165\n",
            "  step 10: local=0.3543 global=1.3624\n",
            "  step 20: local=0.3551 global=1.3213\n",
            "  step 30: local=0.3406 global=1.3918\n",
            "  step 40: local=0.3391 global=1.3109\n",
            "  step 50: local=0.3479 global=1.3705\n",
            "  step 60: local=0.3572 global=1.3868\n",
            "  step 70: local=0.3389 global=1.3010\n",
            "  step 80: local=0.3491 global=1.3238\n",
            "  step 90: local=0.3485 global=1.3061\n",
            "  [WARN] Layer 14 did not converge after 20 repeats (global=1.3707)\n",
            "\n",
            "--- Layer 15/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5062 global=1.4893\n",
            "  step 10: local=0.4967 global=1.3829\n",
            "  step 20: local=0.4848 global=1.3976\n",
            "  step 30: local=0.4945 global=1.3913\n",
            "  step 40: local=0.4776 global=1.4423\n",
            "  step 50: local=0.4625 global=1.4399\n",
            "  step 60: local=0.4612 global=1.4862\n",
            "  step 70: local=0.4484 global=1.4603\n",
            "  step 80: local=0.4520 global=1.4866\n",
            "  step 90: local=0.4391 global=1.3723\n",
            "  Layer 15 not converged (global=1.3968 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4335 global=1.4146\n",
            "  step 10: local=0.4458 global=1.4105\n",
            "  step 20: local=0.4230 global=1.4216\n",
            "  step 30: local=0.4295 global=1.4607\n",
            "  step 40: local=0.4156 global=1.4501\n",
            "  step 50: local=0.4106 global=1.3727\n",
            "  step 60: local=0.4204 global=1.3316\n",
            "  step 70: local=0.4164 global=1.3436\n",
            "  step 80: local=0.4227 global=1.3845\n",
            "  step 90: local=0.4095 global=1.4321\n",
            "  Layer 15 not converged (global=1.3209 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4026 global=1.3272\n",
            "  step 10: local=0.4135 global=1.4089\n",
            "  step 20: local=0.4038 global=1.4034\n",
            "  step 30: local=0.4011 global=1.4000\n",
            "  step 40: local=0.3961 global=1.3424\n",
            "  step 50: local=0.3859 global=1.4640\n",
            "  step 60: local=0.3936 global=1.3717\n",
            "  step 70: local=0.3950 global=1.3927\n",
            "  step 80: local=0.4005 global=1.4075\n",
            "  step 90: local=0.3929 global=1.4039\n",
            "  Layer 15 not converged (global=1.3900 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3890 global=1.4103\n",
            "  step 10: local=0.3873 global=1.3666\n",
            "  step 20: local=0.3835 global=1.3452\n",
            "  step 30: local=0.3757 global=1.3514\n",
            "  step 40: local=0.3812 global=1.3093\n",
            "  step 50: local=0.3858 global=1.3561\n",
            "  step 60: local=0.3735 global=1.3264\n",
            "  step 70: local=0.3861 global=1.3402\n",
            "  step 80: local=0.3818 global=1.3839\n",
            "  step 90: local=0.3811 global=1.3352\n",
            "  Layer 15 not converged (global=1.4001 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3628 global=1.3150\n",
            "  step 10: local=0.3828 global=1.3391\n",
            "  step 20: local=0.3790 global=1.3327\n",
            "  step 30: local=0.3739 global=1.3873\n",
            "  step 40: local=0.3752 global=1.4692\n",
            "  step 50: local=0.3712 global=1.3640\n",
            "  step 60: local=0.3750 global=1.3627\n",
            "  step 70: local=0.3725 global=1.4374\n",
            "  step 80: local=0.3785 global=1.3993\n",
            "  step 90: local=0.3690 global=1.3833\n",
            "  Layer 15 not converged (global=1.4365 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3670 global=1.3940\n",
            "  step 10: local=0.3656 global=1.3555\n",
            "  step 20: local=0.3687 global=1.3524\n",
            "  step 30: local=0.3728 global=1.3332\n",
            "  step 40: local=0.3648 global=1.3830\n",
            "  step 50: local=0.3723 global=1.3815\n",
            "  step 60: local=0.3742 global=1.3121\n",
            "  step 70: local=0.3689 global=1.3286\n",
            "  step 80: local=0.3661 global=1.3013\n",
            "  step 90: local=0.3616 global=1.3336\n",
            "  Layer 15 not converged (global=1.2729 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3762 global=1.3177\n",
            "  step 10: local=0.3603 global=1.2921\n",
            "  step 20: local=0.3748 global=1.3272\n",
            "  step 30: local=0.3667 global=1.3833\n",
            "  step 40: local=0.3591 global=1.2605\n",
            "  step 50: local=0.3686 global=1.3198\n",
            "  step 60: local=0.3711 global=1.2946\n",
            "  step 70: local=0.3600 global=1.5192\n",
            "  step 80: local=0.3631 global=1.3380\n",
            "  step 90: local=0.3649 global=1.3687\n",
            "  Layer 15 not converged (global=1.3357 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3554 global=1.3593\n",
            "  step 10: local=0.3617 global=1.3637\n",
            "  step 20: local=0.3613 global=1.3599\n",
            "  step 30: local=0.3615 global=1.4381\n",
            "  step 40: local=0.3654 global=1.3872\n",
            "  step 50: local=0.3658 global=1.3374\n",
            "  step 60: local=0.3581 global=1.4435\n",
            "  step 70: local=0.3538 global=1.3610\n",
            "  step 80: local=0.3618 global=1.2568\n",
            "  step 90: local=0.3587 global=1.3368\n",
            "  Layer 15 not converged (global=1.3234 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3605 global=1.3461\n",
            "  step 10: local=0.3580 global=1.4238\n",
            "  step 20: local=0.3626 global=1.4036\n",
            "  step 30: local=0.3641 global=1.2542\n",
            "  step 40: local=0.3582 global=1.3193\n",
            "  step 50: local=0.3656 global=1.4060\n",
            "  step 60: local=0.3627 global=1.4225\n",
            "  step 70: local=0.3627 global=1.2867\n",
            "  step 80: local=0.3650 global=1.3245\n",
            "  step 90: local=0.3645 global=1.3403\n",
            "  Layer 15 not converged (global=1.3026 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3603 global=1.3266\n",
            "  step 10: local=0.3621 global=1.3593\n",
            "  step 20: local=0.3551 global=1.3128\n",
            "  step 30: local=0.3584 global=1.4002\n",
            "  step 40: local=0.3617 global=1.3244\n",
            "  step 50: local=0.3582 global=1.3747\n",
            "  step 60: local=0.3594 global=1.3301\n",
            "  step 70: local=0.3583 global=1.3999\n",
            "  step 80: local=0.3507 global=1.3150\n",
            "  step 90: local=0.3642 global=1.3731\n",
            "  Layer 15 not converged (global=1.3517 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3581 global=1.3855\n",
            "  step 10: local=0.3521 global=1.3044\n",
            "  step 20: local=0.3674 global=1.3281\n",
            "  step 30: local=0.3550 global=1.3119\n",
            "  step 40: local=0.3517 global=1.4052\n",
            "  step 50: local=0.3557 global=1.3071\n",
            "  step 60: local=0.3608 global=1.3338\n",
            "  step 70: local=0.3517 global=1.3347\n",
            "  step 80: local=0.3586 global=1.3827\n",
            "  step 90: local=0.3653 global=1.3862\n",
            "  Layer 15 not converged (global=1.3392 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3580 global=1.4294\n",
            "  step 10: local=0.3522 global=1.4097\n",
            "  step 20: local=0.3538 global=1.4368\n",
            "  step 30: local=0.3650 global=1.3287\n",
            "  step 40: local=0.3490 global=1.3710\n",
            "  step 50: local=0.3532 global=1.3691\n",
            "  step 60: local=0.3674 global=1.3820\n",
            "  step 70: local=0.3539 global=1.4205\n",
            "  step 80: local=0.3554 global=1.4120\n",
            "  step 90: local=0.3465 global=1.3368\n",
            "  Layer 15 not converged (global=1.4081 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3577 global=1.2986\n",
            "  step 10: local=0.3669 global=1.3115\n",
            "  step 20: local=0.3601 global=1.3528\n",
            "  step 30: local=0.3505 global=1.4021\n",
            "  step 40: local=0.3489 global=1.3004\n",
            "  step 50: local=0.3634 global=1.3821\n",
            "  step 60: local=0.3588 global=1.3772\n",
            "  step 70: local=0.3555 global=1.3757\n",
            "  step 80: local=0.3577 global=1.3200\n",
            "  step 90: local=0.3520 global=1.4394\n",
            "  Layer 15 not converged (global=1.4045 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3569 global=1.3516\n",
            "  step 10: local=0.3551 global=1.3729\n",
            "  step 20: local=0.3593 global=1.3868\n",
            "  step 30: local=0.3570 global=1.3840\n",
            "  step 40: local=0.3561 global=1.3889\n",
            "  step 50: local=0.3591 global=1.3482\n",
            "  step 60: local=0.3543 global=1.3265\n",
            "  step 70: local=0.3469 global=1.3355\n",
            "  step 80: local=0.3505 global=1.2948\n",
            "  step 90: local=0.3470 global=1.3391\n",
            "  Layer 15 not converged (global=1.3663 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3478 global=1.3104\n",
            "  step 10: local=0.3553 global=1.3280\n",
            "  step 20: local=0.3572 global=1.3710\n",
            "  step 30: local=0.3590 global=1.3224\n",
            "  step 40: local=0.3441 global=1.3021\n",
            "  step 50: local=0.3631 global=1.3262\n",
            "  step 60: local=0.3559 global=1.3212\n",
            "  step 70: local=0.3583 global=1.3766\n",
            "  step 80: local=0.3529 global=1.4585\n",
            "  step 90: local=0.3572 global=1.3523\n",
            "  Layer 15 not converged (global=1.3123 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3505 global=1.3503\n",
            "  step 10: local=0.3535 global=1.4284\n",
            "  step 20: local=0.3500 global=1.3891\n",
            "  step 30: local=0.3456 global=1.3727\n",
            "  step 40: local=0.3546 global=1.3825\n",
            "  step 50: local=0.3498 global=1.3463\n",
            "  step 60: local=0.3517 global=1.3434\n",
            "  step 70: local=0.3536 global=1.3220\n",
            "  step 80: local=0.3371 global=1.3741\n",
            "  step 90: local=0.3539 global=1.3724\n",
            "  Layer 15 not converged (global=1.3884 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3560 global=1.3044\n",
            "  step 10: local=0.3650 global=1.3215\n",
            "  step 20: local=0.3527 global=1.2925\n",
            "  step 30: local=0.3502 global=1.3266\n",
            "  step 40: local=0.3508 global=1.3089\n",
            "  step 50: local=0.3475 global=1.2849\n",
            "  step 60: local=0.3531 global=1.3188\n",
            "  step 70: local=0.3560 global=1.3751\n",
            "  step 80: local=0.3440 global=1.2548\n",
            "  step 90: local=0.3506 global=1.3118\n",
            "  Layer 15 not converged (global=1.2836 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3579 global=1.2891\n",
            "  step 10: local=0.3427 global=1.5114\n",
            "  step 20: local=0.3525 global=1.3289\n",
            "  step 30: local=0.3535 global=1.3625\n",
            "  step 40: local=0.3464 global=1.3513\n",
            "  step 50: local=0.3551 global=1.3546\n",
            "  step 60: local=0.3579 global=1.3539\n",
            "  step 70: local=0.3458 global=1.4298\n",
            "  step 80: local=0.3524 global=1.3803\n",
            "  step 90: local=0.3538 global=1.3299\n",
            "  Layer 15 not converged (global=1.3160 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3624 global=1.4383\n",
            "  step 10: local=0.3494 global=1.3551\n",
            "  step 20: local=0.3527 global=1.2504\n",
            "  step 30: local=0.3572 global=1.3313\n",
            "  step 40: local=0.3481 global=1.3400\n",
            "  step 50: local=0.3560 global=1.4178\n",
            "  step 60: local=0.3483 global=1.3960\n",
            "  step 70: local=0.3484 global=1.2477\n",
            "  step 80: local=0.3518 global=1.3149\n",
            "  step 90: local=0.3439 global=1.3998\n",
            "  Layer 15 not converged (global=1.4148 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3523 global=1.4144\n",
            "  step 10: local=0.3547 global=1.2817\n",
            "  step 20: local=0.3465 global=1.3197\n",
            "  step 30: local=0.3513 global=1.3369\n",
            "  step 40: local=0.3448 global=1.3213\n",
            "  step 50: local=0.3588 global=1.3540\n",
            "  step 60: local=0.3531 global=1.3066\n",
            "  step 70: local=0.3405 global=1.3935\n",
            "  step 80: local=0.3469 global=1.3179\n",
            "  step 90: local=0.3600 global=1.3679\n",
            "  [WARN] Layer 15 did not converge after 20 repeats (global=1.3616)\n",
            "\n",
            "--- Layer 16/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.6815 global=1.5730\n",
            "  step 10: local=0.6533 global=1.6237\n",
            "  step 20: local=0.6318 global=1.4997\n",
            "  step 30: local=0.6408 global=1.5470\n",
            "  step 40: local=0.6300 global=1.5503\n",
            "  step 50: local=0.6143 global=1.4558\n",
            "  step 60: local=0.6117 global=1.4814\n",
            "  step 70: local=0.6113 global=1.4576\n",
            "  step 80: local=0.6105 global=1.5533\n",
            "  step 90: local=0.5782 global=1.4567\n",
            "  Layer 16 not converged (global=1.4793 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5837 global=1.4648\n",
            "  step 10: local=0.5759 global=1.4677\n",
            "  step 20: local=0.5759 global=1.5111\n",
            "  step 30: local=0.5594 global=1.5118\n",
            "  step 40: local=0.5558 global=1.5601\n",
            "  step 50: local=0.5725 global=1.5406\n",
            "  step 60: local=0.5644 global=1.5881\n",
            "  step 70: local=0.5701 global=1.4509\n",
            "  step 80: local=0.5405 global=1.4964\n",
            "  step 90: local=0.5503 global=1.5018\n",
            "  Layer 16 not converged (global=1.3790 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5556 global=1.5026\n",
            "  step 10: local=0.5607 global=1.5382\n",
            "  step 20: local=0.5531 global=1.5272\n",
            "  step 30: local=0.5300 global=1.4516\n",
            "  step 40: local=0.5441 global=1.3926\n",
            "  step 50: local=0.5338 global=1.4224\n",
            "  step 60: local=0.5474 global=1.4541\n",
            "  step 70: local=0.5279 global=1.5154\n",
            "  step 80: local=0.5270 global=1.3958\n",
            "  step 90: local=0.5282 global=1.4886\n",
            "  Layer 16 not converged (global=1.4481 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5302 global=1.4846\n",
            "  step 10: local=0.5183 global=1.4682\n",
            "  step 20: local=0.5283 global=1.4093\n",
            "  step 30: local=0.5225 global=1.5366\n",
            "  step 40: local=0.5335 global=1.4495\n",
            "  step 50: local=0.5313 global=1.4777\n",
            "  step 60: local=0.5355 global=1.4834\n",
            "  step 70: local=0.5222 global=1.4853\n",
            "  step 80: local=0.5112 global=1.4925\n",
            "  step 90: local=0.5079 global=1.4464\n",
            "  Layer 16 not converged (global=1.4529 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5168 global=1.4220\n",
            "  step 10: local=0.5230 global=1.4268\n",
            "  step 20: local=0.5139 global=1.3825\n",
            "  step 30: local=0.5074 global=1.4258\n",
            "  step 40: local=0.5192 global=1.4009\n",
            "  step 50: local=0.5016 global=1.4080\n",
            "  step 60: local=0.5045 global=1.4644\n",
            "  step 70: local=0.5117 global=1.4017\n",
            "  step 80: local=0.5005 global=1.3949\n",
            "  step 90: local=0.5132 global=1.4171\n",
            "  Layer 16 not converged (global=1.3646 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4978 global=1.4045\n",
            "  step 10: local=0.5075 global=1.4768\n",
            "  step 20: local=0.5037 global=1.5483\n",
            "  step 30: local=0.5038 global=1.4381\n",
            "  step 40: local=0.5196 global=1.4346\n",
            "  step 50: local=0.5066 global=1.5077\n",
            "  step 60: local=0.4977 global=1.4789\n",
            "  step 70: local=0.5004 global=1.4514\n",
            "  step 80: local=0.5084 global=1.4777\n",
            "  step 90: local=0.5177 global=1.4399\n",
            "  Layer 16 not converged (global=1.4571 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5133 global=1.4375\n",
            "  step 10: local=0.5021 global=1.4076\n",
            "  step 20: local=0.5075 global=1.4706\n",
            "  step 30: local=0.5225 global=1.4617\n",
            "  step 40: local=0.5056 global=1.3862\n",
            "  step 50: local=0.5128 global=1.3938\n",
            "  step 60: local=0.5138 global=1.3702\n",
            "  step 70: local=0.4825 global=1.4075\n",
            "  step 80: local=0.5117 global=1.3906\n",
            "  step 90: local=0.5026 global=1.3580\n",
            "  Layer 16 not converged (global=1.4212 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5137 global=1.4035\n",
            "  step 10: local=0.5006 global=1.4706\n",
            "  step 20: local=0.4769 global=1.3290\n",
            "  step 30: local=0.4994 global=1.3962\n",
            "  step 40: local=0.5153 global=1.3723\n",
            "  step 50: local=0.5081 global=1.5927\n",
            "  step 60: local=0.5135 global=1.4078\n",
            "  step 70: local=0.4924 global=1.4548\n",
            "  step 80: local=0.5013 global=1.4359\n",
            "  step 90: local=0.4902 global=1.4360\n",
            "  Layer 16 not converged (global=1.4328 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4779 global=1.4333\n",
            "  step 10: local=0.4995 global=1.5225\n",
            "  step 20: local=0.4882 global=1.4563\n",
            "  step 30: local=0.4962 global=1.4071\n",
            "  step 40: local=0.5024 global=1.5327\n",
            "  step 50: local=0.4835 global=1.4373\n",
            "  step 60: local=0.4972 global=1.3297\n",
            "  step 70: local=0.5014 global=1.4120\n",
            "  step 80: local=0.5128 global=1.4270\n",
            "  step 90: local=0.5027 global=1.5053\n",
            "  Layer 16 not converged (global=1.4741 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4998 global=1.4760\n",
            "  step 10: local=0.4985 global=1.3214\n",
            "  step 20: local=0.4996 global=1.3949\n",
            "  step 30: local=0.4979 global=1.4793\n",
            "  step 40: local=0.4850 global=1.5049\n",
            "  step 50: local=0.5023 global=1.3602\n",
            "  step 60: local=0.5021 global=1.3861\n",
            "  step 70: local=0.4976 global=1.4092\n",
            "  step 80: local=0.4929 global=1.4063\n",
            "  step 90: local=0.4991 global=1.4394\n",
            "  Layer 16 not converged (global=1.4475 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4969 global=1.3859\n",
            "  step 10: local=0.4845 global=1.4763\n",
            "  step 20: local=0.4932 global=1.4007\n",
            "  step 30: local=0.5018 global=1.4564\n",
            "  step 40: local=0.5015 global=1.4010\n",
            "  step 50: local=0.4830 global=1.4773\n",
            "  step 60: local=0.5065 global=1.3900\n",
            "  step 70: local=0.5051 global=1.4485\n",
            "  step 80: local=0.4908 global=1.4592\n",
            "  step 90: local=0.4941 global=1.3706\n",
            "  Layer 16 not converged (global=1.4670 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5090 global=1.4064\n",
            "  step 10: local=0.4888 global=1.3817\n",
            "  step 20: local=0.4939 global=1.4809\n",
            "  step 30: local=0.4932 global=1.3876\n",
            "  step 40: local=0.5117 global=1.4032\n",
            "  step 50: local=0.4886 global=1.4098\n",
            "  step 60: local=0.5098 global=1.4556\n",
            "  step 70: local=0.5037 global=1.4591\n",
            "  step 80: local=0.4966 global=1.4996\n",
            "  step 90: local=0.4961 global=1.4889\n",
            "  Layer 16 not converged (global=1.4617 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5180 global=1.5290\n",
            "  step 10: local=0.4950 global=1.4019\n",
            "  step 20: local=0.4963 global=1.4493\n",
            "  step 30: local=0.4931 global=1.4566\n",
            "  step 40: local=0.5001 global=1.4609\n",
            "  step 50: local=0.4945 global=1.5022\n",
            "  step 60: local=0.5005 global=1.4914\n",
            "  step 70: local=0.4922 global=1.4171\n",
            "  step 80: local=0.4922 global=1.3623\n",
            "  step 90: local=0.4885 global=1.3912\n",
            "  Layer 16 not converged (global=1.4029 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4890 global=1.4246\n",
            "  step 10: local=0.4839 global=1.4870\n",
            "  step 20: local=0.4940 global=1.3746\n",
            "  step 30: local=0.4995 global=1.4627\n",
            "  step 40: local=0.4946 global=1.4552\n",
            "  step 50: local=0.4995 global=1.4413\n",
            "  step 60: local=0.4850 global=1.3883\n",
            "  step 70: local=0.4870 global=1.5130\n",
            "  step 80: local=0.4866 global=1.4278\n",
            "  step 90: local=0.4871 global=1.4599\n",
            "  Layer 16 not converged (global=1.4208 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4876 global=1.4596\n",
            "  step 10: local=0.5067 global=1.4665\n",
            "  step 20: local=0.4991 global=1.4697\n",
            "  step 30: local=0.4989 global=1.4267\n",
            "  step 40: local=0.4913 global=1.4072\n",
            "  step 50: local=0.5076 global=1.4072\n",
            "  step 60: local=0.4748 global=1.3648\n",
            "  step 70: local=0.5083 global=1.4145\n",
            "  step 80: local=0.4942 global=1.3830\n",
            "  step 90: local=0.4927 global=1.3959\n",
            "  Layer 16 not converged (global=1.4416 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4940 global=1.4492\n",
            "  step 10: local=0.4980 global=1.3926\n",
            "  step 20: local=0.4989 global=1.3843\n",
            "  step 30: local=0.4956 global=1.4027\n",
            "  step 40: local=0.5163 global=1.3969\n",
            "  step 50: local=0.4829 global=1.4616\n",
            "  step 60: local=0.5009 global=1.5400\n",
            "  step 70: local=0.4955 global=1.4282\n",
            "  step 80: local=0.5086 global=1.4230\n",
            "  step 90: local=0.4902 global=1.4999\n",
            "  Layer 16 not converged (global=1.3898 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4849 global=1.4680\n",
            "  step 10: local=0.5122 global=1.4396\n",
            "  step 20: local=0.4870 global=1.4667\n",
            "  step 30: local=0.5025 global=1.4297\n",
            "  step 40: local=0.5103 global=1.4269\n",
            "  step 50: local=0.4869 global=1.3969\n",
            "  step 60: local=0.4984 global=1.4611\n",
            "  step 70: local=0.5026 global=1.4533\n",
            "  step 80: local=0.4971 global=1.3762\n",
            "  step 90: local=0.5032 global=1.3866\n",
            "  Layer 16 not converged (global=1.4947 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4903 global=1.3628\n",
            "  step 10: local=0.4850 global=1.4007\n",
            "  step 20: local=0.4950 global=1.3836\n",
            "  step 30: local=0.5009 global=1.3531\n",
            "  step 40: local=0.4916 global=1.3960\n",
            "  step 50: local=0.5003 global=1.4615\n",
            "  step 60: local=0.4887 global=1.3238\n",
            "  step 70: local=0.5001 global=1.3896\n",
            "  step 80: local=0.4874 global=1.3653\n",
            "  step 90: local=0.4936 global=1.5885\n",
            "  Layer 16 not converged (global=1.3826 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4921 global=1.4022\n",
            "  step 10: local=0.4965 global=1.4412\n",
            "  step 20: local=0.4882 global=1.4331\n",
            "  step 30: local=0.4945 global=1.4292\n",
            "  step 40: local=0.4835 global=1.4302\n",
            "  step 50: local=0.5015 global=1.5132\n",
            "  step 60: local=0.4939 global=1.4498\n",
            "  step 70: local=0.5061 global=1.3997\n",
            "  step 80: local=0.4978 global=1.5271\n",
            "  step 90: local=0.4848 global=1.4350\n",
            "  Layer 16 not converged (global=1.4646 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4802 global=1.3231\n",
            "  step 10: local=0.5032 global=1.4096\n",
            "  step 20: local=0.4966 global=1.4175\n",
            "  step 30: local=0.4954 global=1.5051\n",
            "  step 40: local=0.4988 global=1.4683\n",
            "  step 50: local=0.5050 global=1.3190\n",
            "  step 60: local=0.5053 global=1.3919\n",
            "  step 70: local=0.4968 global=1.4725\n",
            "  step 80: local=0.4919 global=1.4992\n",
            "  step 90: local=0.4998 global=1.3521\n",
            "  [WARN] Layer 16 did not converge after 20 repeats (global=1.4567)\n",
            "\n",
            "--- Layer 17/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5398 global=1.5182\n",
            "  step 10: local=0.5250 global=1.5318\n",
            "  step 20: local=0.5132 global=1.5175\n",
            "  step 30: local=0.5161 global=1.5490\n",
            "  step 40: local=0.4986 global=1.4884\n",
            "  step 50: local=0.4993 global=1.5784\n",
            "  step 60: local=0.4947 global=1.4992\n",
            "  step 70: local=0.5032 global=1.5618\n",
            "  step 80: local=0.4890 global=1.4972\n",
            "  step 90: local=0.4809 global=1.5681\n",
            "  Layer 17 not converged (global=1.6133 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4821 global=1.4769\n",
            "  step 10: local=0.4930 global=1.5367\n",
            "  step 20: local=0.4751 global=1.5442\n",
            "  step 30: local=0.4794 global=1.4525\n",
            "  step 40: local=0.4751 global=1.4807\n",
            "  step 50: local=0.4778 global=1.4598\n",
            "  step 60: local=0.4712 global=1.5575\n",
            "  step 70: local=0.4494 global=1.4586\n",
            "  step 80: local=0.4617 global=1.4797\n",
            "  step 90: local=0.4526 global=1.4761\n",
            "  Layer 17 not converged (global=1.4509 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4495 global=1.5177\n",
            "  step 10: local=0.4474 global=1.5207\n",
            "  step 20: local=0.4585 global=1.5657\n",
            "  step 30: local=0.4494 global=1.5531\n",
            "  step 40: local=0.4405 global=1.5961\n",
            "  step 50: local=0.4451 global=1.4555\n",
            "  step 60: local=0.4357 global=1.5204\n",
            "  step 70: local=0.4382 global=1.5171\n",
            "  step 80: local=0.4374 global=1.5222\n",
            "  step 90: local=0.4390 global=1.5673\n",
            "  Layer 17 not converged (global=1.5154 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4486 global=1.5474\n",
            "  step 10: local=0.4287 global=1.4773\n",
            "  step 20: local=0.4277 global=1.4189\n",
            "  step 30: local=0.4346 global=1.4493\n",
            "  step 40: local=0.4297 global=1.4792\n",
            "  step 50: local=0.4417 global=1.5382\n",
            "  step 60: local=0.4210 global=1.4275\n",
            "  step 70: local=0.4320 global=1.5197\n",
            "  step 80: local=0.4300 global=1.5110\n",
            "  step 90: local=0.4205 global=1.4923\n",
            "  Layer 17 not converged (global=1.4354 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4293 global=1.4392\n",
            "  step 10: local=0.4284 global=1.5562\n",
            "  step 20: local=0.4232 global=1.4721\n",
            "  step 30: local=0.4259 global=1.5041\n",
            "  step 40: local=0.4194 global=1.5154\n",
            "  step 50: local=0.4315 global=1.5128\n",
            "  step 60: local=0.4239 global=1.5318\n",
            "  step 70: local=0.4140 global=1.4745\n",
            "  step 80: local=0.4157 global=1.4500\n",
            "  step 90: local=0.4162 global=1.4574\n",
            "  Layer 17 not converged (global=1.4153 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4097 global=1.4055\n",
            "  step 10: local=0.4220 global=1.4572\n",
            "  step 20: local=0.4150 global=1.4251\n",
            "  step 30: local=0.4234 global=1.4350\n",
            "  step 40: local=0.4146 global=1.4891\n",
            "  step 50: local=0.4304 global=1.4285\n",
            "  step 60: local=0.4074 global=1.4252\n",
            "  step 70: local=0.4217 global=1.4503\n",
            "  step 80: local=0.4310 global=1.4368\n",
            "  step 90: local=0.4074 global=1.5158\n",
            "  Layer 17 not converged (global=1.4944 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4225 global=1.5826\n",
            "  step 10: local=0.4168 global=1.4724\n",
            "  step 20: local=0.4267 global=1.4638\n",
            "  step 30: local=0.4152 global=1.5395\n",
            "  step 40: local=0.4087 global=1.5101\n",
            "  step 50: local=0.4271 global=1.4895\n",
            "  step 60: local=0.4138 global=1.5032\n",
            "  step 70: local=0.4274 global=1.4739\n",
            "  step 80: local=0.4166 global=1.4637\n",
            "  step 90: local=0.4154 global=1.4374\n",
            "  Layer 17 not converged (global=1.4302 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4214 global=1.5022\n",
            "  step 10: local=0.4161 global=1.5026\n",
            "  step 20: local=0.4077 global=1.4173\n",
            "  step 30: local=0.4215 global=1.4265\n",
            "  step 40: local=0.4123 global=1.4036\n",
            "  step 50: local=0.4135 global=1.4421\n",
            "  step 60: local=0.4172 global=1.4248\n",
            "  step 70: local=0.4070 global=1.3977\n",
            "  step 80: local=0.4279 global=1.4438\n",
            "  step 90: local=0.4093 global=1.5016\n",
            "  Layer 17 not converged (global=1.4340 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4208 global=1.3525\n",
            "  step 10: local=0.4310 global=1.4379\n",
            "  step 20: local=0.4060 global=1.4028\n",
            "  step 30: local=0.4134 global=1.6286\n",
            "  step 40: local=0.4149 global=1.4419\n",
            "  step 50: local=0.4122 global=1.4823\n",
            "  step 60: local=0.4191 global=1.4675\n",
            "  step 70: local=0.4163 global=1.4728\n",
            "  step 80: local=0.4110 global=1.4731\n",
            "  step 90: local=0.4145 global=1.5580\n",
            "  Layer 17 not converged (global=1.4965 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4239 global=1.4887\n",
            "  step 10: local=0.4150 global=1.4390\n",
            "  step 20: local=0.4066 global=1.5656\n",
            "  step 30: local=0.4115 global=1.4733\n",
            "  step 40: local=0.4231 global=1.3565\n",
            "  step 50: local=0.4088 global=1.4501\n",
            "  step 60: local=0.4093 global=1.4545\n",
            "  step 70: local=0.4134 global=1.5459\n",
            "  step 80: local=0.4043 global=1.5125\n",
            "  step 90: local=0.4024 global=1.3593\n",
            "  Layer 17 not converged (global=1.4649 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4132 global=1.4256\n",
            "  step 10: local=0.4060 global=1.5144\n",
            "  step 20: local=0.4081 global=1.5328\n",
            "  step 30: local=0.4082 global=1.3924\n",
            "  step 40: local=0.4159 global=1.4207\n",
            "  step 50: local=0.4099 global=1.4412\n",
            "  step 60: local=0.4050 global=1.4368\n",
            "  step 70: local=0.4149 global=1.4749\n",
            "  step 80: local=0.4044 global=1.4176\n",
            "  step 90: local=0.4089 global=1.5080\n",
            "  Layer 17 not converged (global=1.4914 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4064 global=1.4377\n",
            "  step 10: local=0.4020 global=1.4961\n",
            "  step 20: local=0.4002 global=1.4346\n",
            "  step 30: local=0.3986 global=1.5096\n",
            "  step 40: local=0.4050 global=1.4216\n",
            "  step 50: local=0.4189 global=1.4867\n",
            "  step 60: local=0.4065 global=1.4940\n",
            "  step 70: local=0.4055 global=1.4041\n",
            "  step 80: local=0.3921 global=1.4361\n",
            "  step 90: local=0.3995 global=1.4134\n",
            "  Layer 17 not converged (global=1.4726 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3924 global=1.5139\n",
            "  step 10: local=0.3987 global=1.4189\n",
            "  step 20: local=0.4125 global=1.4425\n",
            "  step 30: local=0.4062 global=1.4374\n",
            "  step 40: local=0.4078 global=1.4828\n",
            "  step 50: local=0.4132 global=1.4910\n",
            "  step 60: local=0.4081 global=1.5343\n",
            "  step 70: local=0.4159 global=1.5221\n",
            "  step 80: local=0.4116 global=1.5603\n",
            "  step 90: local=0.4132 global=1.4255\n",
            "  Layer 17 not converged (global=1.4702 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4009 global=1.4901\n",
            "  step 10: local=0.4103 global=1.4892\n",
            "  step 20: local=0.4122 global=1.4961\n",
            "  step 30: local=0.4057 global=1.5409\n",
            "  step 40: local=0.4019 global=1.5248\n",
            "  step 50: local=0.4024 global=1.4532\n",
            "  step 60: local=0.4295 global=1.3971\n",
            "  step 70: local=0.4120 global=1.4274\n",
            "  step 80: local=0.4063 global=1.4583\n",
            "  step 90: local=0.4075 global=1.5189\n",
            "  Layer 17 not converged (global=1.4013 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4084 global=1.4072\n",
            "  step 10: local=0.4108 global=1.5013\n",
            "  step 20: local=0.4146 global=1.4926\n",
            "  step 30: local=0.4126 global=1.4741\n",
            "  step 40: local=0.4130 global=1.4187\n",
            "  step 50: local=0.4036 global=1.5398\n",
            "  step 60: local=0.3982 global=1.4541\n",
            "  step 70: local=0.4121 global=1.4892\n",
            "  step 80: local=0.4088 global=1.4993\n",
            "  step 90: local=0.4041 global=1.4967\n",
            "  Layer 17 not converged (global=1.4865 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4017 global=1.5145\n",
            "  step 10: local=0.4093 global=1.4609\n",
            "  step 20: local=0.3982 global=1.4365\n",
            "  step 30: local=0.3957 global=1.4466\n",
            "  step 40: local=0.3989 global=1.3911\n",
            "  step 50: local=0.4067 global=1.4431\n",
            "  step 60: local=0.4083 global=1.4135\n",
            "  step 70: local=0.4086 global=1.4225\n",
            "  step 80: local=0.4106 global=1.4765\n",
            "  step 90: local=0.4108 global=1.4187\n",
            "  Layer 17 not converged (global=1.4960 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4075 global=1.4140\n",
            "  step 10: local=0.4169 global=1.4387\n",
            "  step 20: local=0.4076 global=1.4243\n",
            "  step 30: local=0.4027 global=1.5034\n",
            "  step 40: local=0.4217 global=1.5698\n",
            "  step 50: local=0.4067 global=1.4624\n",
            "  step 60: local=0.4144 global=1.4536\n",
            "  step 70: local=0.4010 global=1.5311\n",
            "  step 80: local=0.4186 global=1.5039\n",
            "  step 90: local=0.4029 global=1.4786\n",
            "  Layer 17 not converged (global=1.5501 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4066 global=1.4947\n",
            "  step 10: local=0.4088 global=1.4633\n",
            "  step 20: local=0.4064 global=1.4550\n",
            "  step 30: local=0.4137 global=1.4283\n",
            "  step 40: local=0.3977 global=1.4932\n",
            "  step 50: local=0.4069 global=1.4943\n",
            "  step 60: local=0.3991 global=1.4096\n",
            "  step 70: local=0.4060 global=1.4185\n",
            "  step 80: local=0.4107 global=1.3965\n",
            "  step 90: local=0.3988 global=1.4388\n",
            "  Layer 17 not converged (global=1.3647 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4134 global=1.4172\n",
            "  step 10: local=0.4026 global=1.3886\n",
            "  step 20: local=0.4092 global=1.4363\n",
            "  step 30: local=0.4029 global=1.4920\n",
            "  step 40: local=0.4003 global=1.3439\n",
            "  step 50: local=0.3945 global=1.4287\n",
            "  step 60: local=0.3954 global=1.3931\n",
            "  step 70: local=0.4001 global=1.6189\n",
            "  step 80: local=0.3897 global=1.4341\n",
            "  step 90: local=0.4095 global=1.4713\n",
            "  Layer 17 not converged (global=1.4457 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4078 global=1.4599\n",
            "  step 10: local=0.3965 global=1.4615\n",
            "  step 20: local=0.3929 global=1.4659\n",
            "  step 30: local=0.3985 global=1.5510\n",
            "  step 40: local=0.3970 global=1.4792\n",
            "  step 50: local=0.4145 global=1.4321\n",
            "  step 60: local=0.4035 global=1.5578\n",
            "  step 70: local=0.4034 global=1.4655\n",
            "  step 80: local=0.4040 global=1.3487\n",
            "  step 90: local=0.4037 global=1.4405\n",
            "  [WARN] Layer 17 did not converge after 20 repeats (global=1.4251)\n",
            "\n",
            "--- Layer 18/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5513 global=1.6073\n",
            "  step 10: local=0.5329 global=1.7058\n",
            "  step 20: local=0.5244 global=1.6488\n",
            "  step 30: local=0.5348 global=1.4811\n",
            "  step 40: local=0.5087 global=1.5435\n",
            "  step 50: local=0.5078 global=1.6256\n",
            "  step 60: local=0.5049 global=1.6513\n",
            "  step 70: local=0.4881 global=1.5048\n",
            "  step 80: local=0.4957 global=1.5277\n",
            "  step 90: local=0.4851 global=1.5529\n",
            "  Layer 18 not converged (global=1.5177 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4864 global=1.5479\n",
            "  step 10: local=0.4813 global=1.5779\n",
            "  step 20: local=0.4855 global=1.5043\n",
            "  step 30: local=0.4798 global=1.6267\n",
            "  step 40: local=0.4701 global=1.5391\n",
            "  step 50: local=0.4809 global=1.5982\n",
            "  step 60: local=0.4801 global=1.5314\n",
            "  step 70: local=0.4550 global=1.6099\n",
            "  step 80: local=0.4593 global=1.5005\n",
            "  step 90: local=0.4598 global=1.5718\n",
            "  Layer 18 not converged (global=1.5448 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4515 global=1.5829\n",
            "  step 10: local=0.4573 global=1.4956\n",
            "  step 20: local=0.4617 global=1.5185\n",
            "  step 30: local=0.4696 global=1.5031\n",
            "  step 40: local=0.4582 global=1.6080\n",
            "  step 50: local=0.4447 global=1.5045\n",
            "  step 60: local=0.4636 global=1.5279\n",
            "  step 70: local=0.4447 global=1.5161\n",
            "  step 80: local=0.4605 global=1.5663\n",
            "  step 90: local=0.4583 global=1.5716\n",
            "  Layer 18 not converged (global=1.5228 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4627 global=1.6249\n",
            "  step 10: local=0.4426 global=1.6001\n",
            "  step 20: local=0.4513 global=1.6415\n",
            "  step 30: local=0.4556 global=1.5060\n",
            "  step 40: local=0.4510 global=1.5712\n",
            "  step 50: local=0.4367 global=1.5651\n",
            "  step 60: local=0.4348 global=1.5701\n",
            "  step 70: local=0.4296 global=1.6222\n",
            "  step 80: local=0.4387 global=1.5889\n",
            "  step 90: local=0.4341 global=1.5254\n",
            "  Layer 18 not converged (global=1.5992 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4365 global=1.4601\n",
            "  step 10: local=0.4347 global=1.4838\n",
            "  step 20: local=0.4329 global=1.5259\n",
            "  step 30: local=0.4345 global=1.5853\n",
            "  step 40: local=0.4310 global=1.4663\n",
            "  step 50: local=0.4297 global=1.5736\n",
            "  step 60: local=0.4318 global=1.5688\n",
            "  step 70: local=0.4391 global=1.5467\n",
            "  step 80: local=0.4331 global=1.4787\n",
            "  step 90: local=0.4428 global=1.6518\n",
            "  Layer 18 not converged (global=1.5965 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4242 global=1.5144\n",
            "  step 10: local=0.4366 global=1.5577\n",
            "  step 20: local=0.4289 global=1.5696\n",
            "  step 30: local=0.4155 global=1.5668\n",
            "  step 40: local=0.4291 global=1.5897\n",
            "  step 50: local=0.4411 global=1.5228\n",
            "  step 60: local=0.4228 global=1.5019\n",
            "  step 70: local=0.4284 global=1.5289\n",
            "  step 80: local=0.4263 global=1.4473\n",
            "  step 90: local=0.4310 global=1.5025\n",
            "  Layer 18 not converged (global=1.5369 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4323 global=1.4772\n",
            "  step 10: local=0.4311 global=1.4842\n",
            "  step 20: local=0.4264 global=1.5383\n",
            "  step 30: local=0.4204 global=1.4675\n",
            "  step 40: local=0.4347 global=1.4730\n",
            "  step 50: local=0.4336 global=1.4964\n",
            "  step 60: local=0.4263 global=1.4810\n",
            "  step 70: local=0.4221 global=1.5791\n",
            "  step 80: local=0.4266 global=1.6299\n",
            "  step 90: local=0.4343 global=1.5219\n",
            "  Layer 18 not converged (global=1.4751 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4158 global=1.5087\n",
            "  step 10: local=0.4299 global=1.5854\n",
            "  step 20: local=0.4261 global=1.5668\n",
            "  step 30: local=0.4202 global=1.5422\n",
            "  step 40: local=0.4370 global=1.5563\n",
            "  step 50: local=0.4172 global=1.5280\n",
            "  step 60: local=0.4269 global=1.5132\n",
            "  step 70: local=0.4412 global=1.4930\n",
            "  step 80: local=0.4290 global=1.5685\n",
            "  step 90: local=0.4357 global=1.5561\n",
            "  Layer 18 not converged (global=1.6296 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4275 global=1.4604\n",
            "  step 10: local=0.4323 global=1.4769\n",
            "  step 20: local=0.4279 global=1.4548\n",
            "  step 30: local=0.4239 global=1.4936\n",
            "  step 40: local=0.4255 global=1.4795\n",
            "  step 50: local=0.4217 global=1.4410\n",
            "  step 60: local=0.4247 global=1.4911\n",
            "  step 70: local=0.4315 global=1.5458\n",
            "  step 80: local=0.4170 global=1.3966\n",
            "  step 90: local=0.4282 global=1.4794\n",
            "  Layer 18 not converged (global=1.4295 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4233 global=1.4434\n",
            "  step 10: local=0.4251 global=1.6872\n",
            "  step 20: local=0.4142 global=1.4890\n",
            "  step 30: local=0.4170 global=1.5309\n",
            "  step 40: local=0.4188 global=1.5150\n",
            "  step 50: local=0.4142 global=1.5309\n",
            "  step 60: local=0.4093 global=1.5302\n",
            "  step 70: local=0.4200 global=1.6155\n",
            "  step 80: local=0.4320 global=1.5438\n",
            "  step 90: local=0.4237 global=1.4788\n",
            "  Layer 18 not converged (global=1.4767 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4291 global=1.6177\n",
            "  step 10: local=0.4185 global=1.5191\n",
            "  step 20: local=0.4243 global=1.4031\n",
            "  step 30: local=0.4234 global=1.5018\n",
            "  step 40: local=0.4181 global=1.4953\n",
            "  step 50: local=0.4234 global=1.6062\n",
            "  step 60: local=0.4243 global=1.5592\n",
            "  step 70: local=0.4183 global=1.4038\n",
            "  step 80: local=0.4090 global=1.4702\n",
            "  step 90: local=0.4173 global=1.5560\n",
            "  Layer 18 not converged (global=1.5724 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4250 global=1.5865\n",
            "  step 10: local=0.4143 global=1.4415\n",
            "  step 20: local=0.4205 global=1.4675\n",
            "  step 30: local=0.4078 global=1.4934\n",
            "  step 40: local=0.4191 global=1.4879\n",
            "  step 50: local=0.4283 global=1.5218\n",
            "  step 60: local=0.4189 global=1.4544\n",
            "  step 70: local=0.4111 global=1.5715\n",
            "  step 80: local=0.4202 global=1.4878\n",
            "  step 90: local=0.4272 global=1.5502\n",
            "  Layer 18 not converged (global=1.5225 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4219 global=1.4834\n",
            "  step 10: local=0.4194 global=1.5645\n",
            "  step 20: local=0.4253 global=1.4614\n",
            "  step 30: local=0.4050 global=1.5317\n",
            "  step 40: local=0.4214 global=1.5433\n",
            "  step 50: local=0.4102 global=1.4551\n",
            "  step 60: local=0.4157 global=1.4829\n",
            "  step 70: local=0.4097 global=1.4703\n",
            "  step 80: local=0.4235 global=1.5720\n",
            "  step 90: local=0.4267 global=1.4719\n",
            "  Layer 18 not converged (global=1.5005 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4162 global=1.4961\n",
            "  step 10: local=0.4361 global=1.4865\n",
            "  step 20: local=0.4164 global=1.5378\n",
            "  step 30: local=0.4253 global=1.5432\n",
            "  step 40: local=0.4198 global=1.5942\n",
            "  step 50: local=0.4219 global=1.5769\n",
            "  step 60: local=0.4124 global=1.6175\n",
            "  step 70: local=0.4207 global=1.4853\n",
            "  step 80: local=0.4125 global=1.5446\n",
            "  step 90: local=0.4322 global=1.5420\n",
            "  Layer 18 not converged (global=1.4721 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4080 global=1.5511\n",
            "  step 10: local=0.4236 global=1.6009\n",
            "  step 20: local=0.4094 global=1.5706\n",
            "  step 30: local=0.4194 global=1.5053\n",
            "  step 40: local=0.4228 global=1.4430\n",
            "  step 50: local=0.4249 global=1.4652\n",
            "  step 60: local=0.4180 global=1.5107\n",
            "  step 70: local=0.4303 global=1.5666\n",
            "  step 80: local=0.4228 global=1.4512\n",
            "  step 90: local=0.4080 global=1.5590\n",
            "  Layer 18 not converged (global=1.4977 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4266 global=1.5534\n",
            "  step 10: local=0.4099 global=1.5322\n",
            "  step 20: local=0.4017 global=1.4647\n",
            "  step 30: local=0.4252 global=1.6355\n",
            "  step 40: local=0.4188 global=1.4997\n",
            "  step 50: local=0.4237 global=1.5457\n",
            "  step 60: local=0.4187 global=1.5568\n",
            "  step 70: local=0.4130 global=1.5548\n",
            "  step 80: local=0.4298 global=1.5784\n",
            "  step 90: local=0.4245 global=1.5111\n",
            "  Layer 18 not converged (global=1.5113 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4181 global=1.4876\n",
            "  step 10: local=0.4119 global=1.5170\n",
            "  step 20: local=0.4108 global=1.4374\n",
            "  step 30: local=0.4078 global=1.4934\n",
            "  step 40: local=0.4200 global=1.4649\n",
            "  step 50: local=0.4124 global=1.4755\n",
            "  step 60: local=0.4119 global=1.5306\n",
            "  step 70: local=0.4181 global=1.4564\n",
            "  step 80: local=0.4157 global=1.4654\n",
            "  step 90: local=0.4125 global=1.4871\n",
            "  Layer 18 not converged (global=1.4232 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4251 global=1.4715\n",
            "  step 10: local=0.4282 global=1.5675\n",
            "  step 20: local=0.4159 global=1.6195\n",
            "  step 30: local=0.4084 global=1.5121\n",
            "  step 40: local=0.4213 global=1.4975\n",
            "  step 50: local=0.4226 global=1.5766\n",
            "  step 60: local=0.4143 global=1.5589\n",
            "  step 70: local=0.4085 global=1.5344\n",
            "  step 80: local=0.4072 global=1.5468\n",
            "  step 90: local=0.4113 global=1.5194\n",
            "  Layer 18 not converged (global=1.5181 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4193 global=1.5058\n",
            "  step 10: local=0.4149 global=1.4868\n",
            "  step 20: local=0.4091 global=1.5571\n",
            "  step 30: local=0.4111 global=1.5480\n",
            "  step 40: local=0.4037 global=1.4515\n",
            "  step 50: local=0.4272 global=1.4697\n",
            "  step 60: local=0.4183 global=1.4501\n",
            "  step 70: local=0.4118 global=1.4879\n",
            "  step 80: local=0.4119 global=1.4695\n",
            "  step 90: local=0.4156 global=1.4353\n",
            "  Layer 18 not converged (global=1.4917 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4047 global=1.4837\n",
            "  step 10: local=0.4167 global=1.5378\n",
            "  step 20: local=0.4037 global=1.3884\n",
            "  step 30: local=0.4232 global=1.4716\n",
            "  step 40: local=0.4072 global=1.4362\n",
            "  step 50: local=0.4097 global=1.6795\n",
            "  step 60: local=0.4165 global=1.4797\n",
            "  step 70: local=0.4182 global=1.5245\n",
            "  step 80: local=0.4044 global=1.5084\n",
            "  step 90: local=0.4203 global=1.5242\n",
            "  [WARN] Layer 18 did not converge after 20 repeats (global=1.4990)\n",
            "\n",
            "--- Layer 19/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5545 global=1.8034\n",
            "  step 10: local=0.5664 global=1.8938\n",
            "  step 20: local=0.5449 global=1.7701\n",
            "  step 30: local=0.5387 global=1.7037\n",
            "  step 40: local=0.5378 global=1.8632\n",
            "  step 50: local=0.5313 global=1.7480\n",
            "  step 60: local=0.5201 global=1.6060\n",
            "  step 70: local=0.5302 global=1.7244\n",
            "  step 80: local=0.5313 global=1.6993\n",
            "  step 90: local=0.5221 global=1.8047\n",
            "  Layer 19 not converged (global=1.7807 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5319 global=1.7516\n",
            "  step 10: local=0.5190 global=1.5787\n",
            "  step 20: local=0.5148 global=1.6598\n",
            "  step 30: local=0.5128 global=1.7469\n",
            "  step 40: local=0.4975 global=1.8099\n",
            "  step 50: local=0.4908 global=1.6134\n",
            "  step 60: local=0.5048 global=1.6393\n",
            "  step 70: local=0.5012 global=1.6573\n",
            "  step 80: local=0.5065 global=1.6549\n",
            "  step 90: local=0.4978 global=1.7018\n",
            "  Layer 19 not converged (global=1.7126 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4927 global=1.6146\n",
            "  step 10: local=0.5074 global=1.7630\n",
            "  step 20: local=0.4880 global=1.6618\n",
            "  step 30: local=0.4985 global=1.7302\n",
            "  step 40: local=0.4767 global=1.6592\n",
            "  step 50: local=0.4914 global=1.7315\n",
            "  step 60: local=0.4614 global=1.6062\n",
            "  step 70: local=0.4759 global=1.6778\n",
            "  step 80: local=0.4823 global=1.6890\n",
            "  step 90: local=0.4748 global=1.6039\n",
            "  Layer 19 not converged (global=1.7203 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4723 global=1.6332\n",
            "  step 10: local=0.4838 global=1.6151\n",
            "  step 20: local=0.4774 global=1.7242\n",
            "  step 30: local=0.4605 global=1.6378\n",
            "  step 40: local=0.4758 global=1.6502\n",
            "  step 50: local=0.4735 global=1.6425\n",
            "  step 60: local=0.4717 global=1.6764\n",
            "  step 70: local=0.4849 global=1.6904\n",
            "  step 80: local=0.4748 global=1.7468\n",
            "  step 90: local=0.4743 global=1.7309\n",
            "  Layer 19 not converged (global=1.7019 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4873 global=1.7658\n",
            "  step 10: local=0.4687 global=1.6218\n",
            "  step 20: local=0.4691 global=1.6915\n",
            "  step 30: local=0.4813 global=1.7063\n",
            "  step 40: local=0.4845 global=1.6863\n",
            "  step 50: local=0.4399 global=1.7482\n",
            "  step 60: local=0.4556 global=1.7163\n",
            "  step 70: local=0.4621 global=1.6631\n",
            "  step 80: local=0.4581 global=1.5894\n",
            "  step 90: local=0.4590 global=1.6097\n",
            "  Layer 19 not converged (global=1.6066 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4497 global=1.6521\n",
            "  step 10: local=0.4614 global=1.7099\n",
            "  step 20: local=0.4670 global=1.5860\n",
            "  step 30: local=0.4719 global=1.7021\n",
            "  step 40: local=0.4492 global=1.7108\n",
            "  step 50: local=0.4638 global=1.6693\n",
            "  step 60: local=0.4605 global=1.5888\n",
            "  step 70: local=0.4719 global=1.8018\n",
            "  step 80: local=0.4752 global=1.6434\n",
            "  step 90: local=0.4496 global=1.6814\n",
            "  Layer 19 not converged (global=1.6205 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4563 global=1.6844\n",
            "  step 10: local=0.4585 global=1.7014\n",
            "  step 20: local=0.4592 global=1.7054\n",
            "  step 30: local=0.4597 global=1.6490\n",
            "  step 40: local=0.4509 global=1.6270\n",
            "  step 50: local=0.4528 global=1.6666\n",
            "  step 60: local=0.4428 global=1.5639\n",
            "  step 70: local=0.4440 global=1.6288\n",
            "  step 80: local=0.4505 global=1.5958\n",
            "  step 90: local=0.4669 global=1.6011\n",
            "  Layer 19 not converged (global=1.6410 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4536 global=1.6578\n",
            "  step 10: local=0.4649 global=1.5748\n",
            "  step 20: local=0.4546 global=1.6055\n",
            "  step 30: local=0.4368 global=1.6275\n",
            "  step 40: local=0.4563 global=1.6055\n",
            "  step 50: local=0.4659 global=1.7173\n",
            "  step 60: local=0.4650 global=1.7571\n",
            "  step 70: local=0.4695 global=1.6371\n",
            "  step 80: local=0.4526 global=1.6382\n",
            "  step 90: local=0.4522 global=1.7076\n",
            "  Layer 19 not converged (global=1.6130 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4591 global=1.6843\n",
            "  step 10: local=0.4612 global=1.6596\n",
            "  step 20: local=0.4597 global=1.6864\n",
            "  step 30: local=0.4601 global=1.6643\n",
            "  step 40: local=0.4546 global=1.6398\n",
            "  step 50: local=0.4605 global=1.6089\n",
            "  step 60: local=0.4504 global=1.6901\n",
            "  step 70: local=0.4470 global=1.6819\n",
            "  step 80: local=0.4723 global=1.5676\n",
            "  step 90: local=0.4600 global=1.5896\n",
            "  Layer 19 not converged (global=1.6906 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4402 global=1.5840\n",
            "  step 10: local=0.4454 global=1.6186\n",
            "  step 20: local=0.4486 global=1.6023\n",
            "  step 30: local=0.4345 global=1.5643\n",
            "  step 40: local=0.4475 global=1.6060\n",
            "  step 50: local=0.4527 global=1.6742\n",
            "  step 60: local=0.4497 global=1.5089\n",
            "  step 70: local=0.4578 global=1.5936\n",
            "  step 80: local=0.4339 global=1.5701\n",
            "  step 90: local=0.4578 global=1.8174\n",
            "  Layer 19 not converged (global=1.5843 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4417 global=1.5992\n",
            "  step 10: local=0.4734 global=1.6593\n",
            "  step 20: local=0.4436 global=1.6240\n",
            "  step 30: local=0.4652 global=1.6554\n",
            "  step 40: local=0.4454 global=1.6586\n",
            "  step 50: local=0.4599 global=1.7556\n",
            "  step 60: local=0.4401 global=1.6598\n",
            "  step 70: local=0.4550 global=1.5947\n",
            "  step 80: local=0.4670 global=1.7494\n",
            "  step 90: local=0.4516 global=1.6422\n",
            "  Layer 19 not converged (global=1.6743 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4580 global=1.5176\n",
            "  step 10: local=0.4485 global=1.6293\n",
            "  step 20: local=0.4455 global=1.6094\n",
            "  step 30: local=0.4447 global=1.7244\n",
            "  step 40: local=0.4549 global=1.6754\n",
            "  step 50: local=0.4529 global=1.5047\n",
            "  step 60: local=0.4555 global=1.5878\n",
            "  step 70: local=0.4437 global=1.6838\n",
            "  step 80: local=0.4501 global=1.7357\n",
            "  step 90: local=0.4618 global=1.5514\n",
            "  Layer 19 not converged (global=1.6614 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4429 global=1.5810\n",
            "  step 10: local=0.4410 global=1.6024\n",
            "  step 20: local=0.4402 global=1.5972\n",
            "  step 30: local=0.4571 global=1.6495\n",
            "  step 40: local=0.4510 global=1.5614\n",
            "  step 50: local=0.4420 global=1.7085\n",
            "  step 60: local=0.4245 global=1.6125\n",
            "  step 70: local=0.4581 global=1.6819\n",
            "  step 80: local=0.4455 global=1.6126\n",
            "  step 90: local=0.4347 global=1.6873\n",
            "  Layer 19 not converged (global=1.7260 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4493 global=1.5669\n",
            "  step 10: local=0.4568 global=1.6445\n",
            "  step 20: local=0.4289 global=1.6522\n",
            "  step 30: local=0.4499 global=1.5657\n",
            "  step 40: local=0.4569 global=1.5998\n",
            "  step 50: local=0.4378 global=1.5786\n",
            "  step 60: local=0.4560 global=1.6926\n",
            "  step 70: local=0.4442 global=1.6050\n",
            "  step 80: local=0.4497 global=1.6195\n",
            "  step 90: local=0.4500 global=1.6100\n",
            "  Layer 19 not converged (global=1.5807 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4475 global=1.6491\n",
            "  step 10: local=0.4594 global=1.6634\n",
            "  step 20: local=0.4378 global=1.7196\n",
            "  step 30: local=0.4559 global=1.7057\n",
            "  step 40: local=0.4426 global=1.7398\n",
            "  step 50: local=0.4514 global=1.5988\n",
            "  step 60: local=0.4622 global=1.6650\n",
            "  step 70: local=0.4513 global=1.6841\n",
            "  step 80: local=0.4319 global=1.6661\n",
            "  step 90: local=0.4441 global=1.7253\n",
            "  Layer 19 not converged (global=1.6642 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4645 global=1.6962\n",
            "  step 10: local=0.4437 global=1.6408\n",
            "  step 20: local=0.4429 global=1.5703\n",
            "  step 30: local=0.4411 global=1.5912\n",
            "  step 40: local=0.4493 global=1.6326\n",
            "  step 50: local=0.4505 global=1.6912\n",
            "  step 60: local=0.4501 global=1.5707\n",
            "  step 70: local=0.4419 global=1.6855\n",
            "  step 80: local=0.4372 global=1.6908\n",
            "  step 90: local=0.4531 global=1.6548\n",
            "  Layer 19 not converged (global=1.5855 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4461 global=1.5726\n",
            "  step 10: local=0.4565 global=1.7823\n",
            "  step 20: local=0.4557 global=1.6271\n",
            "  step 30: local=0.4591 global=1.6663\n",
            "  step 40: local=0.4578 global=1.6709\n",
            "  step 50: local=0.4416 global=1.6878\n",
            "  step 60: local=0.4522 global=1.6907\n",
            "  step 70: local=0.4542 global=1.6348\n",
            "  step 80: local=0.4599 global=1.6136\n",
            "  step 90: local=0.4536 global=1.6528\n",
            "  Layer 19 not converged (global=1.5528 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4348 global=1.5512\n",
            "  step 10: local=0.4390 global=1.6162\n",
            "  step 20: local=0.4431 global=1.5816\n",
            "  step 30: local=0.4449 global=1.5875\n",
            "  step 40: local=0.4740 global=1.6464\n",
            "  step 50: local=0.4498 global=1.5613\n",
            "  step 60: local=0.4463 global=1.5948\n",
            "  step 70: local=0.4375 global=1.6143\n",
            "  step 80: local=0.4421 global=1.5930\n",
            "  step 90: local=0.4517 global=1.7039\n",
            "  Layer 19 not converged (global=1.6458 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4425 global=1.7444\n",
            "  step 10: local=0.4333 global=1.6252\n",
            "  step 20: local=0.4471 global=1.6260\n",
            "  step 30: local=0.4567 global=1.6990\n",
            "  step 40: local=0.4316 global=1.6743\n",
            "  step 50: local=0.4616 global=1.6488\n",
            "  step 60: local=0.4538 global=1.6788\n",
            "  step 70: local=0.4478 global=1.6534\n",
            "  step 80: local=0.4335 global=1.6277\n",
            "  step 90: local=0.4452 global=1.5988\n",
            "  Layer 19 not converged (global=1.5663 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4514 global=1.6799\n",
            "  step 10: local=0.4455 global=1.6711\n",
            "  step 20: local=0.4250 global=1.5552\n",
            "  step 30: local=0.4461 global=1.5798\n",
            "  step 40: local=0.4447 global=1.5721\n",
            "  step 50: local=0.4509 global=1.6074\n",
            "  step 60: local=0.4506 global=1.5927\n",
            "  step 70: local=0.4336 global=1.5539\n",
            "  step 80: local=0.4426 global=1.5984\n",
            "  step 90: local=0.4354 global=1.6660\n",
            "  [WARN] Layer 19 did not converge after 20 repeats (global=1.5951)\n",
            "\n",
            "--- Layer 20/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5278 global=1.6531\n",
            "  step 10: local=0.5170 global=1.7207\n",
            "  step 20: local=0.5126 global=1.6925\n",
            "  step 30: local=0.5062 global=1.9442\n",
            "  step 40: local=0.4903 global=1.7185\n",
            "  step 50: local=0.5006 global=1.7795\n",
            "  step 60: local=0.4773 global=1.7431\n",
            "  step 70: local=0.4778 global=1.7693\n",
            "  step 80: local=0.4755 global=1.7691\n",
            "  step 90: local=0.4699 global=1.8941\n",
            "  Layer 20 not converged (global=1.7960 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4692 global=1.7691\n",
            "  step 10: local=0.4818 global=1.6947\n",
            "  step 20: local=0.4708 global=1.8655\n",
            "  step 30: local=0.4664 global=1.7514\n",
            "  step 40: local=0.4712 global=1.6079\n",
            "  step 50: local=0.4772 global=1.7347\n",
            "  step 60: local=0.4708 global=1.6940\n",
            "  step 70: local=0.4614 global=1.8391\n",
            "  step 80: local=0.4595 global=1.7598\n",
            "  step 90: local=0.4596 global=1.5796\n",
            "  Layer 20 not converged (global=1.7187 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4599 global=1.6783\n",
            "  step 10: local=0.4582 global=1.7631\n",
            "  step 20: local=0.4649 global=1.8341\n",
            "  step 30: local=0.4518 global=1.6285\n",
            "  step 40: local=0.4544 global=1.6632\n",
            "  step 50: local=0.4484 global=1.6783\n",
            "  step 60: local=0.4472 global=1.6809\n",
            "  step 70: local=0.4617 global=1.7309\n",
            "  step 80: local=0.4415 global=1.6383\n",
            "  step 90: local=0.4511 global=1.8105\n",
            "  Layer 20 not converged (global=1.7314 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4412 global=1.6947\n",
            "  step 10: local=0.4452 global=1.7611\n",
            "  step 20: local=0.4583 global=1.6996\n",
            "  step 30: local=0.4485 global=1.7753\n",
            "  step 40: local=0.4451 global=1.6347\n",
            "  step 50: local=0.4351 global=1.7111\n",
            "  step 60: local=0.4397 global=1.7341\n",
            "  step 70: local=0.4433 global=1.6365\n",
            "  step 80: local=0.4343 global=1.6675\n",
            "  step 90: local=0.4408 global=1.6533\n",
            "  Layer 20 not converged (global=1.7221 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4393 global=1.7788\n",
            "  step 10: local=0.4386 global=1.6775\n",
            "  step 20: local=0.4326 global=1.6944\n",
            "  step 30: local=0.4407 global=1.6842\n",
            "  step 40: local=0.4324 global=1.7180\n",
            "  step 50: local=0.4396 global=1.7440\n",
            "  step 60: local=0.4515 global=1.8056\n",
            "  step 70: local=0.4243 global=1.7766\n",
            "  step 80: local=0.4297 global=1.8154\n",
            "  step 90: local=0.4285 global=1.6706\n",
            "  Layer 20 not converged (global=1.7343 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4368 global=1.7562\n",
            "  step 10: local=0.4393 global=1.7580\n",
            "  step 20: local=0.4310 global=1.7470\n",
            "  step 30: local=0.4326 global=1.7969\n",
            "  step 40: local=0.4203 global=1.7689\n",
            "  step 50: local=0.4341 global=1.7174\n",
            "  step 60: local=0.4472 global=1.6349\n",
            "  step 70: local=0.4200 global=1.6611\n",
            "  step 80: local=0.4307 global=1.6988\n",
            "  step 90: local=0.4401 global=1.7760\n",
            "  Layer 20 not converged (global=1.6362 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4283 global=1.6331\n",
            "  step 10: local=0.4298 global=1.7513\n",
            "  step 20: local=0.4456 global=1.7691\n",
            "  step 30: local=0.4401 global=1.7095\n",
            "  step 40: local=0.4368 global=1.6307\n",
            "  step 50: local=0.4268 global=1.8578\n",
            "  step 60: local=0.4230 global=1.6906\n",
            "  step 70: local=0.4321 global=1.7303\n",
            "  step 80: local=0.4276 global=1.7341\n",
            "  step 90: local=0.4340 global=1.7668\n",
            "  Layer 20 not converged (global=1.7138 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4302 global=1.7584\n",
            "  step 10: local=0.4312 global=1.6996\n",
            "  step 20: local=0.4273 global=1.6813\n",
            "  step 30: local=0.4343 global=1.7190\n",
            "  step 40: local=0.4250 global=1.6236\n",
            "  step 50: local=0.4311 global=1.6753\n",
            "  step 60: local=0.4293 global=1.6440\n",
            "  step 70: local=0.4154 global=1.6530\n",
            "  step 80: local=0.4320 global=1.7023\n",
            "  step 90: local=0.4274 global=1.6289\n",
            "  Layer 20 not converged (global=1.7442 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4307 global=1.6637\n",
            "  step 10: local=0.4305 global=1.6734\n",
            "  step 20: local=0.4285 global=1.6508\n",
            "  step 30: local=0.4323 global=1.7734\n",
            "  step 40: local=0.4264 global=1.8162\n",
            "  step 50: local=0.4182 global=1.6827\n",
            "  step 60: local=0.4275 global=1.6891\n",
            "  step 70: local=0.4246 global=1.7619\n",
            "  step 80: local=0.4174 global=1.7441\n",
            "  step 90: local=0.4162 global=1.7053\n",
            "  Layer 20 not converged (global=1.8167 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4265 global=1.7363\n",
            "  step 10: local=0.4273 global=1.7293\n",
            "  step 20: local=0.4302 global=1.6910\n",
            "  step 30: local=0.4229 global=1.6738\n",
            "  step 40: local=0.4269 global=1.7603\n",
            "  step 50: local=0.4304 global=1.7320\n",
            "  step 60: local=0.4217 global=1.6104\n",
            "  step 70: local=0.4304 global=1.6480\n",
            "  step 80: local=0.4283 global=1.6435\n",
            "  step 90: local=0.4215 global=1.6861\n",
            "  Layer 20 not converged (global=1.5913 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4249 global=1.6581\n",
            "  step 10: local=0.4270 global=1.6115\n",
            "  step 20: local=0.4262 global=1.6613\n",
            "  step 30: local=0.4210 global=1.7317\n",
            "  step 40: local=0.4200 global=1.5603\n",
            "  step 50: local=0.4374 global=1.6387\n",
            "  step 60: local=0.4187 global=1.6194\n",
            "  step 70: local=0.4369 global=1.8761\n",
            "  step 80: local=0.4180 global=1.6534\n",
            "  step 90: local=0.4239 global=1.7141\n",
            "  Layer 20 not converged (global=1.6942 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4245 global=1.6828\n",
            "  step 10: local=0.4223 global=1.7109\n",
            "  step 20: local=0.4230 global=1.7156\n",
            "  step 30: local=0.4196 global=1.8321\n",
            "  step 40: local=0.4244 global=1.7189\n",
            "  step 50: local=0.4178 global=1.6422\n",
            "  step 60: local=0.4326 global=1.8115\n",
            "  step 70: local=0.4256 global=1.6990\n",
            "  step 80: local=0.4210 global=1.5636\n",
            "  step 90: local=0.4209 global=1.6836\n",
            "  Layer 20 not converged (global=1.6676 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4155 global=1.6487\n",
            "  step 10: local=0.4200 global=1.7916\n",
            "  step 20: local=0.4236 global=1.7231\n",
            "  step 30: local=0.4112 global=1.5434\n",
            "  step 40: local=0.4202 global=1.6417\n",
            "  step 50: local=0.4248 global=1.7284\n",
            "  step 60: local=0.4276 global=1.7947\n",
            "  step 70: local=0.4049 global=1.5984\n",
            "  step 80: local=0.4245 global=1.6321\n",
            "  step 90: local=0.4273 global=1.6466\n",
            "  Layer 20 not converged (global=1.6191 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4158 global=1.6527\n",
            "  step 10: local=0.4250 global=1.7013\n",
            "  step 20: local=0.4157 global=1.6125\n",
            "  step 30: local=0.4209 global=1.7808\n",
            "  step 40: local=0.4054 global=1.6703\n",
            "  step 50: local=0.4282 global=1.7339\n",
            "  step 60: local=0.4132 global=1.6770\n",
            "  step 70: local=0.4240 global=1.7504\n",
            "  step 80: local=0.4238 global=1.6138\n",
            "  step 90: local=0.4178 global=1.6907\n",
            "  Layer 20 not converged (global=1.6692 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4089 global=1.7126\n",
            "  step 10: local=0.4026 global=1.6174\n",
            "  step 20: local=0.4202 global=1.6486\n",
            "  step 30: local=0.4261 global=1.6344\n",
            "  step 40: local=0.4141 global=1.7584\n",
            "  step 50: local=0.4121 global=1.6586\n",
            "  step 60: local=0.4214 global=1.6766\n",
            "  step 70: local=0.4146 global=1.6692\n",
            "  step 80: local=0.4268 global=1.7050\n",
            "  step 90: local=0.4243 global=1.7279\n",
            "  Layer 20 not converged (global=1.6608 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4180 global=1.7877\n",
            "  step 10: local=0.4162 global=1.7624\n",
            "  step 20: local=0.4297 global=1.7963\n",
            "  step 30: local=0.4187 global=1.6553\n",
            "  step 40: local=0.4211 global=1.7414\n",
            "  step 50: local=0.4257 global=1.7429\n",
            "  step 60: local=0.4089 global=1.7320\n",
            "  step 70: local=0.4239 global=1.7832\n",
            "  step 80: local=0.4129 global=1.7538\n",
            "  step 90: local=0.4121 global=1.7017\n",
            "  Layer 20 not converged (global=1.7802 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4159 global=1.6209\n",
            "  step 10: local=0.4303 global=1.6485\n",
            "  step 20: local=0.4157 global=1.6846\n",
            "  step 30: local=0.4310 global=1.7631\n",
            "  step 40: local=0.4180 global=1.6204\n",
            "  step 50: local=0.4136 global=1.7391\n",
            "  step 60: local=0.4144 global=1.7570\n",
            "  step 70: local=0.4210 global=1.7002\n",
            "  step 80: local=0.4119 global=1.6192\n",
            "  step 90: local=0.4276 global=1.8412\n",
            "  Layer 20 not converged (global=1.7642 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4192 global=1.6807\n",
            "  step 10: local=0.4256 global=1.7198\n",
            "  step 20: local=0.4223 global=1.7216\n",
            "  step 30: local=0.4143 global=1.7559\n",
            "  step 40: local=0.4291 global=1.7463\n",
            "  step 50: local=0.4223 global=1.6910\n",
            "  step 60: local=0.4170 global=1.6693\n",
            "  step 70: local=0.4054 global=1.7034\n",
            "  step 80: local=0.4093 global=1.6146\n",
            "  step 90: local=0.4174 global=1.6665\n",
            "  Layer 20 not converged (global=1.6971 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4265 global=1.6327\n",
            "  step 10: local=0.4169 global=1.6434\n",
            "  step 20: local=0.4186 global=1.6926\n",
            "  step 30: local=0.4224 global=1.6195\n",
            "  step 40: local=0.4157 global=1.6522\n",
            "  step 50: local=0.4198 global=1.6612\n",
            "  step 60: local=0.4064 global=1.6437\n",
            "  step 70: local=0.4222 global=1.7614\n",
            "  step 80: local=0.4109 global=1.8046\n",
            "  step 90: local=0.4172 global=1.6729\n",
            "  Layer 20 not converged (global=1.6421 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4214 global=1.6814\n",
            "  step 10: local=0.4135 global=1.7528\n",
            "  step 20: local=0.4050 global=1.7347\n",
            "  step 30: local=0.4201 global=1.6944\n",
            "  step 40: local=0.4149 global=1.7274\n",
            "  step 50: local=0.4305 global=1.7213\n",
            "  step 60: local=0.4149 global=1.6800\n",
            "  step 70: local=0.4144 global=1.6660\n",
            "  step 80: local=0.4299 global=1.7483\n",
            "  step 90: local=0.4156 global=1.7256\n",
            "  [WARN] Layer 20 did not converge after 20 repeats (global=1.8237)\n",
            "\n",
            "--- Layer 21/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4373 global=1.7071\n",
            "  step 10: local=0.4376 global=1.7392\n",
            "  step 20: local=0.4351 global=1.7204\n",
            "  step 30: local=0.4133 global=1.7656\n",
            "  step 40: local=0.4184 global=1.7324\n",
            "  step 50: local=0.4109 global=1.6931\n",
            "  step 60: local=0.4277 global=1.7263\n",
            "  step 70: local=0.4034 global=1.8185\n",
            "  step 80: local=0.4059 global=1.6260\n",
            "  step 90: local=0.4132 global=1.6986\n",
            "  Layer 21 not converged (global=1.6680 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4094 global=1.6950\n",
            "  step 10: local=0.4071 global=1.9643\n",
            "  step 20: local=0.4157 global=1.7126\n",
            "  step 30: local=0.4103 global=1.7787\n",
            "  step 40: local=0.4073 global=1.7413\n",
            "  step 50: local=0.3997 global=1.7762\n",
            "  step 60: local=0.3891 global=1.7931\n",
            "  step 70: local=0.3949 global=1.8802\n",
            "  step 80: local=0.3859 global=1.7758\n",
            "  step 90: local=0.3883 global=1.6953\n",
            "  Layer 21 not converged (global=1.6987 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3865 global=1.8724\n",
            "  step 10: local=0.3896 global=1.7537\n",
            "  step 20: local=0.3915 global=1.6199\n",
            "  step 30: local=0.3865 global=1.7395\n",
            "  step 40: local=0.3893 global=1.7038\n",
            "  step 50: local=0.3811 global=1.8531\n",
            "  step 60: local=0.3864 global=1.7693\n",
            "  step 70: local=0.3881 global=1.5887\n",
            "  step 80: local=0.3873 global=1.6921\n",
            "  step 90: local=0.3875 global=1.7804\n",
            "  Layer 21 not converged (global=1.7851 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3815 global=1.8448\n",
            "  step 10: local=0.3826 global=1.6367\n",
            "  step 20: local=0.3780 global=1.6878\n",
            "  step 30: local=0.3805 global=1.6911\n",
            "  step 40: local=0.3852 global=1.7024\n",
            "  step 50: local=0.3831 global=1.7401\n",
            "  step 60: local=0.3783 global=1.6577\n",
            "  step 70: local=0.3838 global=1.8370\n",
            "  step 80: local=0.3729 global=1.7116\n",
            "  step 90: local=0.3835 global=1.7732\n",
            "  Layer 21 not converged (global=1.7311 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3889 global=1.7216\n",
            "  step 10: local=0.3689 global=1.7964\n",
            "  step 20: local=0.3733 global=1.6477\n",
            "  step 30: local=0.3747 global=1.7277\n",
            "  step 40: local=0.3767 global=1.7504\n",
            "  step 50: local=0.3695 global=1.6582\n",
            "  step 60: local=0.3747 global=1.6869\n",
            "  step 70: local=0.3679 global=1.6742\n",
            "  step 80: local=0.3827 global=1.8053\n",
            "  step 90: local=0.3690 global=1.7031\n",
            "  Layer 21 not converged (global=1.7245 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3780 global=1.7146\n",
            "  step 10: local=0.3696 global=1.7056\n",
            "  step 20: local=0.3628 global=1.7484\n",
            "  step 30: local=0.3658 global=1.7716\n",
            "  step 40: local=0.3724 global=1.8201\n",
            "  step 50: local=0.3734 global=1.8079\n",
            "  step 60: local=0.3720 global=1.8329\n",
            "  step 70: local=0.3737 global=1.6973\n",
            "  step 80: local=0.3658 global=1.7818\n",
            "  step 90: local=0.3706 global=1.7737\n",
            "  Layer 21 not converged (global=1.6860 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3630 global=1.7675\n",
            "  step 10: local=0.3727 global=1.8269\n",
            "  step 20: local=0.3709 global=1.7881\n",
            "  step 30: local=0.3698 global=1.7246\n",
            "  step 40: local=0.3725 global=1.6506\n",
            "  step 50: local=0.3735 global=1.6853\n",
            "  step 60: local=0.3697 global=1.7277\n",
            "  step 70: local=0.3688 global=1.7954\n",
            "  step 80: local=0.3721 global=1.6554\n",
            "  step 90: local=0.3844 global=1.7768\n",
            "  Layer 21 not converged (global=1.7144 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3850 global=1.8000\n",
            "  step 10: local=0.3738 global=1.7404\n",
            "  step 20: local=0.3612 global=1.6628\n",
            "  step 30: local=0.3758 global=1.8707\n",
            "  step 40: local=0.3623 global=1.7176\n",
            "  step 50: local=0.3642 global=1.7547\n",
            "  step 60: local=0.3709 global=1.7536\n",
            "  step 70: local=0.3621 global=1.7917\n",
            "  step 80: local=0.3792 global=1.7825\n",
            "  step 90: local=0.3558 global=1.7269\n",
            "  Layer 21 not converged (global=1.7304 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3702 global=1.7042\n",
            "  step 10: local=0.3627 global=1.7419\n",
            "  step 20: local=0.3550 global=1.6511\n",
            "  step 30: local=0.3817 global=1.6976\n",
            "  step 40: local=0.3639 global=1.6625\n",
            "  step 50: local=0.3666 global=1.6808\n",
            "  step 60: local=0.3666 global=1.7256\n",
            "  step 70: local=0.3579 global=1.6561\n",
            "  step 80: local=0.3688 global=1.6889\n",
            "  step 90: local=0.3655 global=1.6986\n",
            "  Layer 21 not converged (global=1.6172 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3656 global=1.6802\n",
            "  step 10: local=0.3611 global=1.7996\n",
            "  step 20: local=0.3726 global=1.8428\n",
            "  step 30: local=0.3622 global=1.7042\n",
            "  step 40: local=0.3752 global=1.7135\n",
            "  step 50: local=0.3655 global=1.7894\n",
            "  step 60: local=0.3670 global=1.7735\n",
            "  step 70: local=0.3695 global=1.7272\n",
            "  step 80: local=0.3660 global=1.7628\n",
            "  step 90: local=0.3614 global=1.7545\n",
            "  Layer 21 not converged (global=1.7334 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3635 global=1.7099\n",
            "  step 10: local=0.3615 global=1.6970\n",
            "  step 20: local=0.3664 global=1.7749\n",
            "  step 30: local=0.3785 global=1.7552\n",
            "  step 40: local=0.3584 global=1.6418\n",
            "  step 50: local=0.3620 global=1.6782\n",
            "  step 60: local=0.3719 global=1.6592\n",
            "  step 70: local=0.3698 global=1.7088\n",
            "  step 80: local=0.3722 global=1.6784\n",
            "  step 90: local=0.3674 global=1.6403\n",
            "  Layer 21 not converged (global=1.7011 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3750 global=1.6790\n",
            "  step 10: local=0.3573 global=1.7636\n",
            "  step 20: local=0.3555 global=1.5790\n",
            "  step 30: local=0.3583 global=1.6543\n",
            "  step 40: local=0.3650 global=1.6504\n",
            "  step 50: local=0.3620 global=1.9186\n",
            "  step 60: local=0.3643 global=1.6731\n",
            "  step 70: local=0.3632 global=1.7413\n",
            "  step 80: local=0.3630 global=1.7049\n",
            "  step 90: local=0.3640 global=1.7407\n",
            "  Layer 21 not converged (global=1.6930 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3654 global=1.7569\n",
            "  step 10: local=0.3565 global=1.8428\n",
            "  step 20: local=0.3598 global=1.7446\n",
            "  step 30: local=0.3603 global=1.6663\n",
            "  step 40: local=0.3624 global=1.8381\n",
            "  step 50: local=0.3561 global=1.7230\n",
            "  step 60: local=0.3556 global=1.5914\n",
            "  step 70: local=0.3623 global=1.7127\n",
            "  step 80: local=0.3595 global=1.6762\n",
            "  step 90: local=0.3655 global=1.8270\n",
            "  Layer 21 not converged (global=1.7767 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3660 global=1.7460\n",
            "  step 10: local=0.3574 global=1.5666\n",
            "  step 20: local=0.3717 global=1.6673\n",
            "  step 30: local=0.3577 global=1.7563\n",
            "  step 40: local=0.3621 global=1.8197\n",
            "  step 50: local=0.3592 global=1.6163\n",
            "  step 60: local=0.3628 global=1.6659\n",
            "  step 70: local=0.3684 global=1.6691\n",
            "  step 80: local=0.3646 global=1.6842\n",
            "  step 90: local=0.3618 global=1.7238\n",
            "  Layer 21 not converged (global=1.7352 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3600 global=1.6406\n",
            "  step 10: local=0.3622 global=1.8173\n",
            "  step 20: local=0.3622 global=1.6932\n",
            "  step 30: local=0.3614 global=1.7565\n",
            "  step 40: local=0.3682 global=1.7045\n",
            "  step 50: local=0.3569 global=1.7798\n",
            "  step 60: local=0.3602 global=1.6346\n",
            "  step 70: local=0.3697 global=1.7129\n",
            "  step 80: local=0.3620 global=1.7376\n",
            "  step 90: local=0.3660 global=1.6423\n",
            "  Layer 21 not converged (global=1.7882 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3635 global=1.6743\n",
            "  step 10: local=0.3638 global=1.6607\n",
            "  step 20: local=0.3616 global=1.7899\n",
            "  step 30: local=0.3491 global=1.6906\n",
            "  step 40: local=0.3688 global=1.7005\n",
            "  step 50: local=0.3692 global=1.6931\n",
            "  step 60: local=0.3612 global=1.7345\n",
            "  step 70: local=0.3558 global=1.7602\n",
            "  step 80: local=0.3608 global=1.8069\n",
            "  step 90: local=0.3624 global=1.7974\n",
            "  Layer 21 not converged (global=1.7743 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3619 global=1.8205\n",
            "  step 10: local=0.3640 global=1.6853\n",
            "  step 20: local=0.3599 global=1.7671\n",
            "  step 30: local=0.3643 global=1.7605\n",
            "  step 40: local=0.3507 global=1.7565\n",
            "  step 50: local=0.3533 global=1.8154\n",
            "  step 60: local=0.3697 global=1.7767\n",
            "  step 70: local=0.3517 global=1.7152\n",
            "  step 80: local=0.3625 global=1.6416\n",
            "  step 90: local=0.3628 global=1.6767\n",
            "  Layer 21 not converged (global=1.6586 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3590 global=1.7154\n",
            "  step 10: local=0.3670 global=1.7852\n",
            "  step 20: local=0.3597 global=1.6466\n",
            "  step 30: local=0.3654 global=1.7665\n",
            "  step 40: local=0.3669 global=1.7895\n",
            "  step 50: local=0.3562 global=1.7302\n",
            "  step 60: local=0.3562 global=1.6539\n",
            "  step 70: local=0.3523 global=1.8576\n",
            "  step 80: local=0.3562 global=1.7078\n",
            "  step 90: local=0.3622 global=1.7437\n",
            "  Layer 21 not converged (global=1.6755 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3584 global=1.7466\n",
            "  step 10: local=0.3651 global=1.7837\n",
            "  step 20: local=0.3662 global=1.7757\n",
            "  step 30: local=0.3653 global=1.7193\n",
            "  step 40: local=0.3621 global=1.6950\n",
            "  step 50: local=0.3610 global=1.7319\n",
            "  step 60: local=0.3494 global=1.6423\n",
            "  step 70: local=0.3724 global=1.6892\n",
            "  step 80: local=0.3665 global=1.6545\n",
            "  step 90: local=0.3665 global=1.6736\n",
            "  Layer 21 not converged (global=1.7090 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3589 global=1.7173\n",
            "  step 10: local=0.3598 global=1.6485\n",
            "  step 20: local=0.3517 global=1.6809\n",
            "  step 30: local=0.3550 global=1.6911\n",
            "  step 40: local=0.3660 global=1.6717\n",
            "  step 50: local=0.3712 global=1.7904\n",
            "  step 60: local=0.3643 global=1.8324\n",
            "  step 70: local=0.3495 global=1.6983\n",
            "  step 80: local=0.3537 global=1.7064\n",
            "  step 90: local=0.3582 global=1.7806\n",
            "  [WARN] Layer 21 did not converge after 20 repeats (global=1.6916)\n",
            "\n",
            "--- Layer 22/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4140 global=1.8694\n",
            "  step 10: local=0.4121 global=1.8167\n",
            "  step 20: local=0.4137 global=1.8485\n",
            "  step 30: local=0.4093 global=1.8313\n",
            "  step 40: local=0.3963 global=1.7784\n",
            "  step 50: local=0.4008 global=1.7803\n",
            "  step 60: local=0.3994 global=1.8556\n",
            "  step 70: local=0.3965 global=1.8370\n",
            "  step 80: local=0.4011 global=1.7076\n",
            "  step 90: local=0.3920 global=1.7436\n",
            "  Layer 22 not converged (global=1.8354 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3907 global=1.7281\n",
            "  step 10: local=0.3891 global=1.7824\n",
            "  step 20: local=0.3942 global=1.7467\n",
            "  step 30: local=0.3917 global=1.7020\n",
            "  step 40: local=0.3891 global=1.7397\n",
            "  step 50: local=0.3766 global=1.8405\n",
            "  step 60: local=0.3800 global=1.6370\n",
            "  step 70: local=0.3807 global=1.7080\n",
            "  step 80: local=0.3778 global=1.7092\n",
            "  step 90: local=0.3643 global=1.9771\n",
            "  Layer 22 not converged (global=1.7123 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3874 global=1.7296\n",
            "  step 10: local=0.3784 global=1.8006\n",
            "  step 20: local=0.3733 global=1.7572\n",
            "  step 30: local=0.3685 global=1.7938\n",
            "  step 40: local=0.3743 global=1.8120\n",
            "  step 50: local=0.3652 global=1.9025\n",
            "  step 60: local=0.3699 global=1.7947\n",
            "  step 70: local=0.3746 global=1.7112\n",
            "  step 80: local=0.3610 global=1.8924\n",
            "  step 90: local=0.3714 global=1.7693\n",
            "  Layer 22 not converged (global=1.7958 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3690 global=1.6477\n",
            "  step 10: local=0.3577 global=1.7638\n",
            "  step 20: local=0.3617 global=1.7267\n",
            "  step 30: local=0.3724 global=1.8775\n",
            "  step 40: local=0.3535 global=1.7896\n",
            "  step 50: local=0.3619 global=1.6163\n",
            "  step 60: local=0.3600 global=1.7164\n",
            "  step 70: local=0.3602 global=1.8024\n",
            "  step 80: local=0.3645 global=1.8606\n",
            "  step 90: local=0.3580 global=1.6598\n",
            "  Layer 22 not converged (global=1.7904 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3631 global=1.7199\n",
            "  step 10: local=0.3654 global=1.7088\n",
            "  step 20: local=0.3678 global=1.7280\n",
            "  step 30: local=0.3620 global=1.7682\n",
            "  step 40: local=0.3601 global=1.6809\n",
            "  step 50: local=0.3654 global=1.8629\n",
            "  step 60: local=0.3483 global=1.7346\n",
            "  step 70: local=0.3576 global=1.8061\n",
            "  step 80: local=0.3579 global=1.7418\n",
            "  step 90: local=0.3504 global=1.8210\n",
            "  Layer 22 not converged (global=1.8575 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3591 global=1.6724\n",
            "  step 10: local=0.3536 global=1.7591\n",
            "  step 20: local=0.3573 global=1.7865\n",
            "  step 30: local=0.3464 global=1.6812\n",
            "  step 40: local=0.3563 global=1.7091\n",
            "  step 50: local=0.3547 global=1.7089\n",
            "  step 60: local=0.3583 global=1.8353\n",
            "  step 70: local=0.3603 global=1.7396\n",
            "  step 80: local=0.3548 global=1.7413\n",
            "  step 90: local=0.3588 global=1.7400\n",
            "  Layer 22 not converged (global=1.7024 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3541 global=1.7812\n",
            "  step 10: local=0.3511 global=1.8020\n",
            "  step 20: local=0.3578 global=1.8515\n",
            "  step 30: local=0.3519 global=1.8371\n",
            "  step 40: local=0.3567 global=1.8620\n",
            "  step 50: local=0.3521 global=1.7315\n",
            "  step 60: local=0.3473 global=1.8032\n",
            "  step 70: local=0.3593 global=1.8035\n",
            "  step 80: local=0.3495 global=1.8002\n",
            "  step 90: local=0.3558 global=1.8570\n",
            "  Layer 22 not converged (global=1.7978 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3613 global=1.8232\n",
            "  step 10: local=0.3511 global=1.7591\n",
            "  step 20: local=0.3522 global=1.6843\n",
            "  step 30: local=0.3552 global=1.7211\n",
            "  step 40: local=0.3540 global=1.7535\n",
            "  step 50: local=0.3575 global=1.8227\n",
            "  step 60: local=0.3488 global=1.6833\n",
            "  step 70: local=0.3614 global=1.8080\n",
            "  step 80: local=0.3444 global=1.8371\n",
            "  step 90: local=0.3444 global=1.7709\n",
            "  Layer 22 not converged (global=1.7033 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3556 global=1.6937\n",
            "  step 10: local=0.3545 global=1.8982\n",
            "  step 20: local=0.3626 global=1.7485\n",
            "  step 30: local=0.3526 global=1.7867\n",
            "  step 40: local=0.3532 global=1.7743\n",
            "  step 50: local=0.3490 global=1.8249\n",
            "  step 60: local=0.3585 global=1.8060\n",
            "  step 70: local=0.3469 global=1.7534\n",
            "  step 80: local=0.3595 global=1.7287\n",
            "  step 90: local=0.3450 global=1.7709\n",
            "  Layer 22 not converged (global=1.6686 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3564 global=1.6840\n",
            "  step 10: local=0.3462 global=1.7350\n",
            "  step 20: local=0.3468 global=1.6941\n",
            "  step 30: local=0.3423 global=1.7117\n",
            "  step 40: local=0.3552 global=1.7526\n",
            "  step 50: local=0.3419 global=1.6890\n",
            "  step 60: local=0.3464 global=1.7197\n",
            "  step 70: local=0.3528 global=1.7323\n",
            "  step 80: local=0.3405 global=1.7118\n",
            "  step 90: local=0.3599 global=1.8360\n",
            "  Layer 22 not converged (global=1.7778 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3459 global=1.8841\n",
            "  step 10: local=0.3492 global=1.7374\n",
            "  step 20: local=0.3520 global=1.7406\n",
            "  step 30: local=0.3446 global=1.8146\n",
            "  step 40: local=0.3450 global=1.8113\n",
            "  step 50: local=0.3478 global=1.7580\n",
            "  step 60: local=0.3498 global=1.7940\n",
            "  step 70: local=0.3600 global=1.7823\n",
            "  step 80: local=0.3529 global=1.7359\n",
            "  step 90: local=0.3421 global=1.7323\n",
            "  Layer 22 not converged (global=1.6956 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3469 global=1.8122\n",
            "  step 10: local=0.3564 global=1.7901\n",
            "  step 20: local=0.3394 global=1.6703\n",
            "  step 30: local=0.3445 global=1.7062\n",
            "  step 40: local=0.3530 global=1.6901\n",
            "  step 50: local=0.3526 global=1.7484\n",
            "  step 60: local=0.3493 global=1.7104\n",
            "  step 70: local=0.3459 global=1.6685\n",
            "  step 80: local=0.3575 global=1.7118\n",
            "  step 90: local=0.3434 global=1.8059\n",
            "  Layer 22 not converged (global=1.7170 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3451 global=1.6092\n",
            "  step 10: local=0.3504 global=1.6805\n",
            "  step 20: local=0.3530 global=1.6797\n",
            "  step 30: local=0.3561 global=1.9509\n",
            "  step 40: local=0.3354 global=1.7023\n",
            "  step 50: local=0.3549 global=1.7744\n",
            "  step 60: local=0.3494 global=1.7342\n",
            "  step 70: local=0.3515 global=1.7693\n",
            "  step 80: local=0.3494 global=1.7907\n",
            "  step 90: local=0.3438 global=1.8803\n",
            "  Layer 22 not converged (global=1.7855 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3466 global=1.7739\n",
            "  step 10: local=0.3398 global=1.6922\n",
            "  step 20: local=0.3425 global=1.8733\n",
            "  step 30: local=0.3423 global=1.7505\n",
            "  step 40: local=0.3453 global=1.6297\n",
            "  step 50: local=0.3439 global=1.7462\n",
            "  step 60: local=0.3468 global=1.7103\n",
            "  step 70: local=0.3454 global=1.8601\n",
            "  step 80: local=0.3459 global=1.7738\n",
            "  step 90: local=0.3488 global=1.6029\n",
            "  Layer 22 not converged (global=1.7452 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3510 global=1.7007\n",
            "  step 10: local=0.3495 global=1.7864\n",
            "  step 20: local=0.3494 global=1.8444\n",
            "  step 30: local=0.3395 global=1.6468\n",
            "  step 40: local=0.3460 global=1.7056\n",
            "  step 50: local=0.3366 global=1.6954\n",
            "  step 60: local=0.3513 global=1.7139\n",
            "  step 70: local=0.3510 global=1.7563\n",
            "  step 80: local=0.3468 global=1.6684\n",
            "  step 90: local=0.3490 global=1.8489\n",
            "  Layer 22 not converged (global=1.7646 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3447 global=1.7227\n",
            "  step 10: local=0.3467 global=1.7938\n",
            "  step 20: local=0.3447 global=1.7306\n",
            "  step 30: local=0.3443 global=1.8097\n",
            "  step 40: local=0.3437 global=1.6619\n",
            "  step 50: local=0.3414 global=1.7498\n",
            "  step 60: local=0.3463 global=1.7742\n",
            "  step 70: local=0.3453 global=1.6710\n",
            "  step 80: local=0.3427 global=1.6991\n",
            "  step 90: local=0.3366 global=1.7005\n",
            "  Layer 22 not converged (global=1.7457 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3431 global=1.8236\n",
            "  step 10: local=0.3447 global=1.7279\n",
            "  step 20: local=0.3388 global=1.7304\n",
            "  step 30: local=0.3586 global=1.7291\n",
            "  step 40: local=0.3561 global=1.7728\n",
            "  step 50: local=0.3549 global=1.7920\n",
            "  step 60: local=0.3479 global=1.8419\n",
            "  step 70: local=0.3374 global=1.8281\n",
            "  step 80: local=0.3469 global=1.8546\n",
            "  step 90: local=0.3443 global=1.7226\n",
            "  Layer 22 not converged (global=1.7559 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3422 global=1.7949\n",
            "  step 10: local=0.3409 global=1.7940\n",
            "  step 20: local=0.3518 global=1.7914\n",
            "  step 30: local=0.3543 global=1.8471\n",
            "  step 40: local=0.3440 global=1.8125\n",
            "  step 50: local=0.3410 global=1.7502\n",
            "  step 60: local=0.3462 global=1.6762\n",
            "  step 70: local=0.3519 global=1.7125\n",
            "  step 80: local=0.3405 global=1.7451\n",
            "  step 90: local=0.3437 global=1.8150\n",
            "  Layer 22 not converged (global=1.6711 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3407 global=1.6770\n",
            "  step 10: local=0.3459 global=1.7996\n",
            "  step 20: local=0.3460 global=1.8300\n",
            "  step 30: local=0.3454 global=1.7639\n",
            "  step 40: local=0.3528 global=1.6865\n",
            "  step 50: local=0.3457 global=1.8915\n",
            "  step 60: local=0.3416 global=1.7401\n",
            "  step 70: local=0.3488 global=1.7790\n",
            "  step 80: local=0.3501 global=1.7671\n",
            "  step 90: local=0.3329 global=1.8169\n",
            "  Layer 22 not converged (global=1.7536 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3488 global=1.8009\n",
            "  step 10: local=0.3479 global=1.7458\n",
            "  step 20: local=0.3435 global=1.7210\n",
            "  step 30: local=0.3396 global=1.7642\n",
            "  step 40: local=0.3468 global=1.6766\n",
            "  step 50: local=0.3406 global=1.7272\n",
            "  step 60: local=0.3496 global=1.6873\n",
            "  step 70: local=0.3437 global=1.7056\n",
            "  step 80: local=0.3483 global=1.7461\n",
            "  step 90: local=0.3453 global=1.6827\n",
            "  [WARN] Layer 22 did not converge after 20 repeats (global=1.8078)\n",
            "\n",
            "--- Layer 23/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4147 global=1.7650\n",
            "  step 10: local=0.4106 global=1.7788\n",
            "  step 20: local=0.4019 global=1.7536\n",
            "  step 30: local=0.4058 global=1.8834\n",
            "  step 40: local=0.4005 global=1.9277\n",
            "  step 50: local=0.3962 global=1.7785\n",
            "  step 60: local=0.3972 global=1.7803\n",
            "  step 70: local=0.3874 global=1.8474\n",
            "  step 80: local=0.3871 global=1.8547\n",
            "  step 90: local=0.3873 global=1.7884\n",
            "  Layer 23 not converged (global=1.9082 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3947 global=1.8303\n",
            "  step 10: local=0.3951 global=1.8142\n",
            "  step 20: local=0.3704 global=1.7627\n",
            "  step 30: local=0.3816 global=1.7598\n",
            "  step 40: local=0.3886 global=1.8445\n",
            "  step 50: local=0.3809 global=1.8215\n",
            "  step 60: local=0.3802 global=1.6982\n",
            "  step 70: local=0.3757 global=1.7338\n",
            "  step 80: local=0.3773 global=1.7193\n",
            "  step 90: local=0.3751 global=1.7737\n",
            "  Layer 23 not converged (global=1.6641 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3698 global=1.7356\n",
            "  step 10: local=0.3726 global=1.6910\n",
            "  step 20: local=0.3716 global=1.7350\n",
            "  step 30: local=0.3665 global=1.8385\n",
            "  step 40: local=0.3652 global=1.6375\n",
            "  step 50: local=0.3709 global=1.6988\n",
            "  step 60: local=0.3591 global=1.7029\n",
            "  step 70: local=0.3681 global=1.9697\n",
            "  step 80: local=0.3602 global=1.7254\n",
            "  step 90: local=0.3683 global=1.7930\n",
            "  Layer 23 not converged (global=1.7653 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3683 global=1.7617\n",
            "  step 10: local=0.3506 global=1.7872\n",
            "  step 20: local=0.3770 global=1.8171\n",
            "  step 30: local=0.3629 global=1.9033\n",
            "  step 40: local=0.3562 global=1.7968\n",
            "  step 50: local=0.3600 global=1.7133\n",
            "  step 60: local=0.3610 global=1.8935\n",
            "  step 70: local=0.3648 global=1.7653\n",
            "  step 80: local=0.3556 global=1.6496\n",
            "  step 90: local=0.3682 global=1.7660\n",
            "  Layer 23 not converged (global=1.7346 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3534 global=1.7240\n",
            "  step 10: local=0.3649 global=1.8746\n",
            "  step 20: local=0.3654 global=1.7947\n",
            "  step 30: local=0.3619 global=1.6147\n",
            "  step 40: local=0.3479 global=1.7126\n",
            "  step 50: local=0.3420 global=1.8044\n",
            "  step 60: local=0.3609 global=1.8650\n",
            "  step 70: local=0.3493 global=1.6603\n",
            "  step 80: local=0.3476 global=1.7186\n",
            "  step 90: local=0.3481 global=1.7110\n",
            "  Layer 23 not converged (global=1.6874 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3578 global=1.7284\n",
            "  step 10: local=0.3522 global=1.7777\n",
            "  step 20: local=0.3539 global=1.6846\n",
            "  step 30: local=0.3535 global=1.8734\n",
            "  step 40: local=0.3499 global=1.7402\n",
            "  step 50: local=0.3613 global=1.8099\n",
            "  step 60: local=0.3769 global=1.7494\n",
            "  step 70: local=0.3564 global=1.8141\n",
            "  step 80: local=0.3477 global=1.6755\n",
            "  step 90: local=0.3495 global=1.7672\n",
            "  Layer 23 not converged (global=1.7508 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3527 global=1.7940\n",
            "  step 10: local=0.3489 global=1.6858\n",
            "  step 20: local=0.3474 global=1.7195\n",
            "  step 30: local=0.3515 global=1.7224\n",
            "  step 40: local=0.3420 global=1.8365\n",
            "  step 50: local=0.3530 global=1.7473\n",
            "  step 60: local=0.3586 global=1.7466\n",
            "  step 70: local=0.3472 global=1.7514\n",
            "  step 80: local=0.3533 global=1.7873\n",
            "  step 90: local=0.3551 global=1.8049\n",
            "  Layer 23 not converged (global=1.7407 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3547 global=1.8645\n",
            "  step 10: local=0.3576 global=1.8453\n",
            "  step 20: local=0.3466 global=1.8703\n",
            "  step 30: local=0.3510 global=1.7399\n",
            "  step 40: local=0.3533 global=1.8078\n",
            "  step 50: local=0.3587 global=1.8110\n",
            "  step 60: local=0.3513 global=1.8041\n",
            "  step 70: local=0.3457 global=1.8643\n",
            "  step 80: local=0.3554 global=1.8328\n",
            "  step 90: local=0.3433 global=1.7635\n",
            "  Layer 23 not converged (global=1.8504 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3497 global=1.6929\n",
            "  step 10: local=0.3502 global=1.7244\n",
            "  step 20: local=0.3497 global=1.7587\n",
            "  step 30: local=0.3494 global=1.8336\n",
            "  step 40: local=0.3520 global=1.6928\n",
            "  step 50: local=0.3503 global=1.8139\n",
            "  step 60: local=0.3446 global=1.8410\n",
            "  step 70: local=0.3428 global=1.7760\n",
            "  step 80: local=0.3532 global=1.7015\n",
            "  step 90: local=0.3488 global=1.9010\n",
            "  Layer 23 not converged (global=1.8295 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3632 global=1.7537\n",
            "  step 10: local=0.3489 global=1.7977\n",
            "  step 20: local=0.3399 global=1.7799\n",
            "  step 30: local=0.3455 global=1.8308\n",
            "  step 40: local=0.3553 global=1.8189\n",
            "  step 50: local=0.3579 global=1.7629\n",
            "  step 60: local=0.3493 global=1.7311\n",
            "  step 70: local=0.3504 global=1.7849\n",
            "  step 80: local=0.3516 global=1.6900\n",
            "  step 90: local=0.3427 global=1.7436\n",
            "  Layer 23 not converged (global=1.7655 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3488 global=1.6988\n",
            "  step 10: local=0.3441 global=1.7173\n",
            "  step 20: local=0.3505 global=1.7595\n",
            "  step 30: local=0.3432 global=1.6954\n",
            "  step 40: local=0.3457 global=1.7240\n",
            "  step 50: local=0.3399 global=1.7383\n",
            "  step 60: local=0.3481 global=1.7139\n",
            "  step 70: local=0.3509 global=1.8448\n",
            "  step 80: local=0.3495 global=1.8925\n",
            "  step 90: local=0.3515 global=1.7479\n",
            "  Layer 23 not converged (global=1.7173 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3426 global=1.7499\n",
            "  step 10: local=0.3470 global=1.8148\n",
            "  step 20: local=0.3369 global=1.8240\n",
            "  step 30: local=0.3475 global=1.7598\n",
            "  step 40: local=0.3448 global=1.7987\n",
            "  step 50: local=0.3484 global=1.7894\n",
            "  step 60: local=0.3373 global=1.7367\n",
            "  step 70: local=0.3464 global=1.7332\n",
            "  step 80: local=0.3490 global=1.8204\n",
            "  step 90: local=0.3422 global=1.7967\n",
            "  Layer 23 not converged (global=1.9167 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3453 global=1.6783\n",
            "  step 10: local=0.3414 global=1.7120\n",
            "  step 20: local=0.3448 global=1.6988\n",
            "  step 30: local=0.3401 global=1.7552\n",
            "  step 40: local=0.3443 global=1.7162\n",
            "  step 50: local=0.3506 global=1.6720\n",
            "  step 60: local=0.3438 global=1.7180\n",
            "  step 70: local=0.3430 global=1.8191\n",
            "  step 80: local=0.3411 global=1.6225\n",
            "  step 90: local=0.3497 global=1.6868\n",
            "  Layer 23 not converged (global=1.6665 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3435 global=1.6871\n",
            "  step 10: local=0.3502 global=1.9534\n",
            "  step 20: local=0.3404 global=1.7097\n",
            "  step 30: local=0.3471 global=1.7787\n",
            "  step 40: local=0.3472 global=1.7485\n",
            "  step 50: local=0.3439 global=1.7738\n",
            "  step 60: local=0.3552 global=1.8058\n",
            "  step 70: local=0.3498 global=1.8885\n",
            "  step 80: local=0.3505 global=1.7844\n",
            "  step 90: local=0.3352 global=1.7030\n",
            "  Layer 23 not converged (global=1.7010 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3519 global=1.8814\n",
            "  step 10: local=0.3311 global=1.7550\n",
            "  step 20: local=0.3430 global=1.6386\n",
            "  step 30: local=0.3571 global=1.7551\n",
            "  step 40: local=0.3393 global=1.7130\n",
            "  step 50: local=0.3477 global=1.8648\n",
            "  step 60: local=0.3508 global=1.7852\n",
            "  step 70: local=0.3399 global=1.6048\n",
            "  step 80: local=0.3397 global=1.7037\n",
            "  step 90: local=0.3403 global=1.7941\n",
            "  Layer 23 not converged (global=1.7959 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3510 global=1.8542\n",
            "  step 10: local=0.3379 global=1.6510\n",
            "  step 20: local=0.3350 global=1.7102\n",
            "  step 30: local=0.3418 global=1.7028\n",
            "  step 40: local=0.3399 global=1.7206\n",
            "  step 50: local=0.3355 global=1.7690\n",
            "  step 60: local=0.3466 global=1.6769\n",
            "  step 70: local=0.3383 global=1.8637\n",
            "  step 80: local=0.3425 global=1.7312\n",
            "  step 90: local=0.3395 global=1.8018\n",
            "  Layer 23 not converged (global=1.7546 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3448 global=1.7407\n",
            "  step 10: local=0.3374 global=1.8082\n",
            "  step 20: local=0.3353 global=1.6682\n",
            "  step 30: local=0.3442 global=1.7585\n",
            "  step 40: local=0.3345 global=1.7868\n",
            "  step 50: local=0.3387 global=1.6798\n",
            "  step 60: local=0.3349 global=1.7117\n",
            "  step 70: local=0.3382 global=1.7163\n",
            "  step 80: local=0.3452 global=1.8281\n",
            "  step 90: local=0.3342 global=1.7406\n",
            "  Layer 23 not converged (global=1.7486 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3497 global=1.7401\n",
            "  step 10: local=0.3424 global=1.7444\n",
            "  step 20: local=0.3479 global=1.7807\n",
            "  step 30: local=0.3461 global=1.7985\n",
            "  step 40: local=0.3445 global=1.8571\n",
            "  step 50: local=0.3483 global=1.8384\n",
            "  step 60: local=0.3395 global=1.8632\n",
            "  step 70: local=0.3382 global=1.7346\n",
            "  step 80: local=0.3471 global=1.8029\n",
            "  step 90: local=0.3490 global=1.8052\n",
            "  Layer 23 not converged (global=1.7262 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3428 global=1.7991\n",
            "  step 10: local=0.3359 global=1.8573\n",
            "  step 20: local=0.3419 global=1.8263\n",
            "  step 30: local=0.3487 global=1.7592\n",
            "  step 40: local=0.3467 global=1.6873\n",
            "  step 50: local=0.3516 global=1.7200\n",
            "  step 60: local=0.3431 global=1.7525\n",
            "  step 70: local=0.3417 global=1.8278\n",
            "  step 80: local=0.3512 global=1.6865\n",
            "  step 90: local=0.3407 global=1.8099\n",
            "  Layer 23 not converged (global=1.7475 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3462 global=1.8360\n",
            "  step 10: local=0.3305 global=1.7706\n",
            "  step 20: local=0.3438 global=1.6964\n",
            "  step 30: local=0.3440 global=1.8958\n",
            "  step 40: local=0.3423 global=1.7493\n",
            "  step 50: local=0.3478 global=1.7926\n",
            "  step 60: local=0.3452 global=1.7750\n",
            "  step 70: local=0.3473 global=1.8255\n",
            "  step 80: local=0.3444 global=1.8146\n",
            "  step 90: local=0.3414 global=1.7579\n",
            "  [WARN] Layer 23 did not converge after 20 repeats (global=1.7585)\n",
            "\n",
            "--- Layer 24/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4030 global=1.7779\n",
            "  step 10: local=0.3918 global=1.8280\n",
            "  step 20: local=0.3880 global=1.7365\n",
            "  step 30: local=0.3890 global=1.7867\n",
            "  step 40: local=0.3901 global=1.7416\n",
            "  step 50: local=0.3791 global=1.7517\n",
            "  step 60: local=0.3889 global=1.7964\n",
            "  step 70: local=0.3698 global=1.7304\n",
            "  step 80: local=0.3789 global=1.7632\n",
            "  step 90: local=0.3776 global=1.7750\n",
            "  Layer 24 not converged (global=1.6864 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3810 global=1.7530\n",
            "  step 10: local=0.3793 global=1.8855\n",
            "  step 20: local=0.3721 global=1.9275\n",
            "  step 30: local=0.3597 global=1.7708\n",
            "  step 40: local=0.3622 global=1.7747\n",
            "  step 50: local=0.3586 global=1.8461\n",
            "  step 60: local=0.3580 global=1.8571\n",
            "  step 70: local=0.3626 global=1.7878\n",
            "  step 80: local=0.3665 global=1.8284\n",
            "  step 90: local=0.3685 global=1.8237\n",
            "  Layer 24 not converged (global=1.7986 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3551 global=1.7594\n",
            "  step 10: local=0.3502 global=1.7682\n",
            "  step 20: local=0.3587 global=1.8526\n",
            "  step 30: local=0.3573 global=1.8293\n",
            "  step 40: local=0.3502 global=1.7062\n",
            "  step 50: local=0.3445 global=1.7384\n",
            "  step 60: local=0.3455 global=1.7267\n",
            "  step 70: local=0.3438 global=1.7832\n",
            "  step 80: local=0.3519 global=1.7417\n",
            "  step 90: local=0.3425 global=1.6948\n",
            "  Layer 24 not converged (global=1.7655 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3495 global=1.7433\n",
            "  step 10: local=0.3507 global=1.8445\n",
            "  step 20: local=0.3473 global=1.6457\n",
            "  step 30: local=0.3469 global=1.7093\n",
            "  step 40: local=0.3399 global=1.7079\n",
            "  step 50: local=0.3378 global=1.9770\n",
            "  step 60: local=0.3379 global=1.7283\n",
            "  step 70: local=0.3473 global=1.8018\n",
            "  step 80: local=0.3357 global=1.7735\n",
            "  step 90: local=0.3443 global=1.8008\n",
            "  Layer 24 not converged (global=1.7542 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3359 global=1.8211\n",
            "  step 10: local=0.3434 global=1.9091\n",
            "  step 20: local=0.3357 global=1.8041\n",
            "  step 30: local=0.3331 global=1.7202\n",
            "  step 40: local=0.3450 global=1.9022\n",
            "  step 50: local=0.3357 global=1.7775\n",
            "  step 60: local=0.3395 global=1.6603\n",
            "  step 70: local=0.3378 global=1.7760\n",
            "  step 80: local=0.3311 global=1.7394\n",
            "  step 90: local=0.3402 global=1.8865\n",
            "  Layer 24 not converged (global=1.8426 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3278 global=1.8047\n",
            "  step 10: local=0.3283 global=1.6266\n",
            "  step 20: local=0.3326 global=1.7304\n",
            "  step 30: local=0.3357 global=1.8104\n",
            "  step 40: local=0.3387 global=1.8697\n",
            "  step 50: local=0.3330 global=1.6715\n",
            "  step 60: local=0.3429 global=1.7299\n",
            "  step 70: local=0.3326 global=1.7230\n",
            "  step 80: local=0.3348 global=1.7359\n",
            "  step 90: local=0.3296 global=1.7841\n",
            "  Layer 24 not converged (global=1.7892 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3275 global=1.6969\n",
            "  step 10: local=0.3315 global=1.8821\n",
            "  step 20: local=0.3365 global=1.7520\n",
            "  step 30: local=0.3293 global=1.8189\n",
            "  step 40: local=0.3319 global=1.7554\n",
            "  step 50: local=0.3284 global=1.8183\n",
            "  step 60: local=0.3283 global=1.6857\n",
            "  step 70: local=0.3371 global=1.7749\n",
            "  step 80: local=0.3338 global=1.8037\n",
            "  step 90: local=0.3309 global=1.6994\n",
            "  Layer 24 not converged (global=1.8444 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3280 global=1.7320\n",
            "  step 10: local=0.3301 global=1.7345\n",
            "  step 20: local=0.3247 global=1.8464\n",
            "  step 30: local=0.3334 global=1.7632\n",
            "  step 40: local=0.3359 global=1.7561\n",
            "  step 50: local=0.3311 global=1.7639\n",
            "  step 60: local=0.3295 global=1.8095\n",
            "  step 70: local=0.3336 global=1.8174\n",
            "  step 80: local=0.3308 global=1.8739\n",
            "  step 90: local=0.3287 global=1.8583\n",
            "  Layer 24 not converged (global=1.8261 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3262 global=1.8828\n",
            "  step 10: local=0.3332 global=1.7529\n",
            "  step 20: local=0.3314 global=1.8207\n",
            "  step 30: local=0.3341 global=1.8224\n",
            "  step 40: local=0.3301 global=1.8124\n",
            "  step 50: local=0.3237 global=1.8755\n",
            "  step 60: local=0.3279 global=1.8459\n",
            "  step 70: local=0.3272 global=1.7701\n",
            "  step 80: local=0.3303 global=1.7093\n",
            "  step 90: local=0.3267 global=1.7403\n",
            "  Layer 24 not converged (global=1.7100 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3310 global=1.7632\n",
            "  step 10: local=0.3328 global=1.8536\n",
            "  step 20: local=0.3277 global=1.6962\n",
            "  step 30: local=0.3229 global=1.8225\n",
            "  step 40: local=0.3253 global=1.8531\n",
            "  step 50: local=0.3301 global=1.7914\n",
            "  step 60: local=0.3252 global=1.7110\n",
            "  step 70: local=0.3307 global=1.9116\n",
            "  step 80: local=0.3219 global=1.7561\n",
            "  step 90: local=0.3250 global=1.8064\n",
            "  Layer 24 not converged (global=1.7269 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3263 global=1.7927\n",
            "  step 10: local=0.3333 global=1.8421\n",
            "  step 20: local=0.3315 global=1.8266\n",
            "  step 30: local=0.3303 global=1.7747\n",
            "  step 40: local=0.3339 global=1.7434\n",
            "  step 50: local=0.3398 global=1.7963\n",
            "  step 60: local=0.3193 global=1.7042\n",
            "  step 70: local=0.3391 global=1.7576\n",
            "  step 80: local=0.3330 global=1.7121\n",
            "  step 90: local=0.3227 global=1.7254\n",
            "  Layer 24 not converged (global=1.7794 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3318 global=1.7727\n",
            "  step 10: local=0.3266 global=1.7062\n",
            "  step 20: local=0.3218 global=1.7380\n",
            "  step 30: local=0.3221 global=1.7511\n",
            "  step 40: local=0.3269 global=1.7291\n",
            "  step 50: local=0.3274 global=1.8615\n",
            "  step 60: local=0.3258 global=1.9068\n",
            "  step 70: local=0.3204 global=1.7523\n",
            "  step 80: local=0.3208 global=1.7560\n",
            "  step 90: local=0.3280 global=1.8258\n",
            "  Layer 24 not converged (global=1.7438 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3224 global=1.8378\n",
            "  step 10: local=0.3193 global=1.7695\n",
            "  step 20: local=0.3292 global=1.8079\n",
            "  step 30: local=0.3264 global=1.8063\n",
            "  step 40: local=0.3295 global=1.7433\n",
            "  step 50: local=0.3166 global=1.7516\n",
            "  step 60: local=0.3336 global=1.8342\n",
            "  step 70: local=0.3245 global=1.8157\n",
            "  step 80: local=0.3239 global=1.6930\n",
            "  step 90: local=0.3260 global=1.7244\n",
            "  Layer 24 not converged (global=1.8166 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3276 global=1.7140\n",
            "  step 10: local=0.3283 global=1.7686\n",
            "  step 20: local=0.3277 global=1.7295\n",
            "  step 30: local=0.3323 global=1.6816\n",
            "  step 40: local=0.3214 global=1.7314\n",
            "  step 50: local=0.3262 global=1.8334\n",
            "  step 60: local=0.3227 global=1.6359\n",
            "  step 70: local=0.3275 global=1.6991\n",
            "  step 80: local=0.3236 global=1.6969\n",
            "  step 90: local=0.3249 global=1.9647\n",
            "  Layer 24 not converged (global=1.7087 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3247 global=1.7192\n",
            "  step 10: local=0.3323 global=1.7897\n",
            "  step 20: local=0.3158 global=1.7636\n",
            "  step 30: local=0.3206 global=1.7913\n",
            "  step 40: local=0.3256 global=1.8116\n",
            "  step 50: local=0.3233 global=1.8994\n",
            "  step 60: local=0.3211 global=1.7943\n",
            "  step 70: local=0.3207 global=1.7117\n",
            "  step 80: local=0.3278 global=1.8917\n",
            "  step 90: local=0.3333 global=1.7682\n",
            "  Layer 24 not converged (global=1.7962 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3264 global=1.6528\n",
            "  step 10: local=0.3266 global=1.7665\n",
            "  step 20: local=0.3241 global=1.7289\n",
            "  step 30: local=0.3366 global=1.8783\n",
            "  step 40: local=0.3182 global=1.7973\n",
            "  step 50: local=0.3180 global=1.6180\n",
            "  step 60: local=0.3251 global=1.7224\n",
            "  step 70: local=0.3254 global=1.8036\n",
            "  step 80: local=0.3247 global=1.8620\n",
            "  step 90: local=0.3295 global=1.6646\n",
            "  Layer 24 not converged (global=1.8007 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3253 global=1.7225\n",
            "  step 10: local=0.3138 global=1.7164\n",
            "  step 20: local=0.3256 global=1.7283\n",
            "  step 30: local=0.3230 global=1.7767\n",
            "  step 40: local=0.3205 global=1.6891\n",
            "  step 50: local=0.3240 global=1.8733\n",
            "  step 60: local=0.3197 global=1.7457\n",
            "  step 70: local=0.3249 global=1.8116\n",
            "  step 80: local=0.3178 global=1.7483\n",
            "  step 90: local=0.3280 global=1.8129\n",
            "  Layer 24 not converged (global=1.8623 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3194 global=1.6808\n",
            "  step 10: local=0.3154 global=1.7691\n",
            "  step 20: local=0.3235 global=1.7987\n",
            "  step 30: local=0.3297 global=1.6926\n",
            "  step 40: local=0.3267 global=1.7257\n",
            "  step 50: local=0.3238 global=1.7283\n",
            "  step 60: local=0.3236 global=1.8391\n",
            "  step 70: local=0.3258 global=1.7565\n",
            "  step 80: local=0.3301 global=1.7508\n",
            "  step 90: local=0.3285 global=1.7568\n",
            "  Layer 24 not converged (global=1.7101 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3240 global=1.8050\n",
            "  step 10: local=0.3263 global=1.8125\n",
            "  step 20: local=0.3237 global=1.8673\n",
            "  step 30: local=0.3210 global=1.8527\n",
            "  step 40: local=0.3220 global=1.8780\n",
            "  step 50: local=0.3192 global=1.7465\n",
            "  step 60: local=0.3254 global=1.8154\n",
            "  step 70: local=0.3270 global=1.8174\n",
            "  step 80: local=0.3157 global=1.8081\n",
            "  step 90: local=0.3273 global=1.8708\n",
            "  Layer 24 not converged (global=1.8144 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3241 global=1.8405\n",
            "  step 10: local=0.3215 global=1.7673\n",
            "  step 20: local=0.3287 global=1.7049\n",
            "  step 30: local=0.3182 global=1.7346\n",
            "  step 40: local=0.3255 global=1.7578\n",
            "  step 50: local=0.3232 global=1.8489\n",
            "  step 60: local=0.3226 global=1.6912\n",
            "  step 70: local=0.3286 global=1.8177\n",
            "  step 80: local=0.3186 global=1.8480\n",
            "  step 90: local=0.3246 global=1.7865\n",
            "  [WARN] Layer 24 did not converge after 20 repeats (global=1.7264)\n",
            "\n",
            "--- Layer 25/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5150 global=1.7409\n",
            "  step 10: local=0.4871 global=1.9456\n",
            "  step 20: local=0.5053 global=1.7951\n",
            "  step 30: local=0.5105 global=1.8431\n",
            "  step 40: local=0.4883 global=1.8185\n",
            "  step 50: local=0.5014 global=1.8672\n",
            "  step 60: local=0.4934 global=1.8640\n",
            "  step 70: local=0.4947 global=1.7966\n",
            "  step 80: local=0.4931 global=1.7701\n",
            "  step 90: local=0.4726 global=1.8209\n",
            "  Layer 25 not converged (global=1.7167 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4756 global=1.7306\n",
            "  step 10: local=0.4764 global=1.7812\n",
            "  step 20: local=0.4838 global=1.7363\n",
            "  step 30: local=0.4820 global=1.7532\n",
            "  step 40: local=0.4683 global=1.7959\n",
            "  step 50: local=0.4639 global=1.7358\n",
            "  step 60: local=0.4652 global=1.7614\n",
            "  step 70: local=0.4800 global=1.7808\n",
            "  step 80: local=0.4608 global=1.7552\n",
            "  step 90: local=0.4644 global=1.8781\n",
            "  Layer 25 not converged (global=1.8224 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4563 global=1.9322\n",
            "  step 10: local=0.4743 global=1.7709\n",
            "  step 20: local=0.4482 global=1.7756\n",
            "  step 30: local=0.4554 global=1.8447\n",
            "  step 40: local=0.4636 global=1.8582\n",
            "  step 50: local=0.4532 global=1.7880\n",
            "  step 60: local=0.4410 global=1.8254\n",
            "  step 70: local=0.4530 global=1.8248\n",
            "  step 80: local=0.4485 global=1.7589\n",
            "  step 90: local=0.4498 global=1.7652\n",
            "  Layer 25 not converged (global=1.7364 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4400 global=1.8575\n",
            "  step 10: local=0.4445 global=1.8266\n",
            "  step 20: local=0.4440 global=1.7114\n",
            "  step 30: local=0.4389 global=1.7422\n",
            "  step 40: local=0.4463 global=1.7321\n",
            "  step 50: local=0.4402 global=1.7853\n",
            "  step 60: local=0.4377 global=1.7466\n",
            "  step 70: local=0.4369 global=1.6906\n",
            "  step 80: local=0.4471 global=1.7506\n",
            "  step 90: local=0.4229 global=1.8459\n",
            "  Layer 25 not converged (global=1.7570 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4309 global=1.6486\n",
            "  step 10: local=0.4376 global=1.7133\n",
            "  step 20: local=0.4497 global=1.7097\n",
            "  step 30: local=0.4405 global=1.9766\n",
            "  step 40: local=0.4312 global=1.7315\n",
            "  step 50: local=0.4264 global=1.8056\n",
            "  step 60: local=0.4313 global=1.7787\n",
            "  step 70: local=0.4172 global=1.8094\n",
            "  step 80: local=0.4436 global=1.8248\n",
            "  step 90: local=0.4291 global=1.9122\n",
            "  Layer 25 not converged (global=1.8251 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4427 global=1.8094\n",
            "  step 10: local=0.4272 global=1.7281\n",
            "  step 20: local=0.4210 global=1.9031\n",
            "  step 30: local=0.4276 global=1.7842\n",
            "  step 40: local=0.4367 global=1.6663\n",
            "  step 50: local=0.4330 global=1.7862\n",
            "  step 60: local=0.4296 global=1.7444\n",
            "  step 70: local=0.4308 global=1.8969\n",
            "  step 80: local=0.4256 global=1.8085\n",
            "  step 90: local=0.4305 global=1.6323\n",
            "  Layer 25 not converged (global=1.7864 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4202 global=1.7349\n",
            "  step 10: local=0.4289 global=1.8187\n",
            "  step 20: local=0.4242 global=1.8788\n",
            "  step 30: local=0.4253 global=1.6773\n",
            "  step 40: local=0.4282 global=1.7319\n",
            "  step 50: local=0.4384 global=1.7293\n",
            "  step 60: local=0.4165 global=1.7361\n",
            "  step 70: local=0.4482 global=1.7935\n",
            "  step 80: local=0.4189 global=1.7027\n",
            "  step 90: local=0.4057 global=1.8854\n",
            "  Layer 25 not converged (global=1.8053 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4180 global=1.7576\n",
            "  step 10: local=0.4243 global=1.8237\n",
            "  step 20: local=0.4234 global=1.7720\n",
            "  step 30: local=0.4123 global=1.8275\n",
            "  step 40: local=0.4358 global=1.6931\n",
            "  step 50: local=0.4238 global=1.7795\n",
            "  step 60: local=0.4190 global=1.8142\n",
            "  step 70: local=0.4083 global=1.7088\n",
            "  step 80: local=0.4126 global=1.7372\n",
            "  step 90: local=0.4072 global=1.7440\n",
            "  Layer 25 not converged (global=1.7822 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4159 global=1.8489\n",
            "  step 10: local=0.4162 global=1.7652\n",
            "  step 20: local=0.4086 global=1.7601\n",
            "  step 30: local=0.4087 global=1.7733\n",
            "  step 40: local=0.4079 global=1.8175\n",
            "  step 50: local=0.4112 global=1.8265\n",
            "  step 60: local=0.4307 global=1.8850\n",
            "  step 70: local=0.4223 global=1.8676\n",
            "  step 80: local=0.4171 global=1.8915\n",
            "  step 90: local=0.4331 global=1.7588\n",
            "  Layer 25 not converged (global=1.7828 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4207 global=1.8226\n",
            "  step 10: local=0.4244 global=1.8298\n",
            "  step 20: local=0.4127 global=1.8222\n",
            "  step 30: local=0.4210 global=1.8839\n",
            "  step 40: local=0.4268 global=1.8547\n",
            "  step 50: local=0.4155 global=1.7774\n",
            "  step 60: local=0.4250 global=1.7247\n",
            "  step 70: local=0.4145 global=1.7463\n",
            "  step 80: local=0.4158 global=1.7682\n",
            "  step 90: local=0.4238 global=1.8606\n",
            "  Layer 25 not converged (global=1.7076 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4275 global=1.7053\n",
            "  step 10: local=0.4240 global=1.8336\n",
            "  step 20: local=0.4216 global=1.8673\n",
            "  step 30: local=0.4212 global=1.7970\n",
            "  step 40: local=0.4304 global=1.7136\n",
            "  step 50: local=0.4036 global=1.9133\n",
            "  step 60: local=0.4142 global=1.7662\n",
            "  step 70: local=0.4274 global=1.8139\n",
            "  step 80: local=0.4115 global=1.7948\n",
            "  step 90: local=0.4083 global=1.8426\n",
            "  Layer 25 not converged (global=1.7953 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4307 global=1.8395\n",
            "  step 10: local=0.4235 global=1.7749\n",
            "  step 20: local=0.4278 global=1.7482\n",
            "  step 30: local=0.4162 global=1.7975\n",
            "  step 40: local=0.4239 global=1.7109\n",
            "  step 50: local=0.4207 global=1.7627\n",
            "  step 60: local=0.4073 global=1.7177\n",
            "  step 70: local=0.4292 global=1.7354\n",
            "  step 80: local=0.4111 global=1.7792\n",
            "  step 90: local=0.4127 global=1.7182\n",
            "  Layer 25 not converged (global=1.8448 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4139 global=1.7448\n",
            "  step 10: local=0.4140 global=1.7645\n",
            "  step 20: local=0.4295 global=1.7390\n",
            "  step 30: local=0.4236 global=1.8629\n",
            "  step 40: local=0.4170 global=1.9176\n",
            "  step 50: local=0.4160 global=1.7584\n",
            "  step 60: local=0.4101 global=1.7630\n",
            "  step 70: local=0.4245 global=1.8314\n",
            "  step 80: local=0.4221 global=1.8449\n",
            "  step 90: local=0.4120 global=1.7762\n",
            "  Layer 25 not converged (global=1.9035 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4141 global=1.8126\n",
            "  step 10: local=0.4031 global=1.8128\n",
            "  step 20: local=0.4059 global=1.7482\n",
            "  step 30: local=0.4210 global=1.7544\n",
            "  step 40: local=0.4230 global=1.8464\n",
            "  step 50: local=0.4104 global=1.8162\n",
            "  step 60: local=0.4110 global=1.7021\n",
            "  step 70: local=0.4084 global=1.7329\n",
            "  step 80: local=0.3984 global=1.7235\n",
            "  step 90: local=0.4037 global=1.7759\n",
            "  Layer 25 not converged (global=1.6697 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4076 global=1.7394\n",
            "  step 10: local=0.4142 global=1.6833\n",
            "  step 20: local=0.4245 global=1.7414\n",
            "  step 30: local=0.4111 global=1.8380\n",
            "  step 40: local=0.4106 global=1.6409\n",
            "  step 50: local=0.4085 global=1.7065\n",
            "  step 60: local=0.4184 global=1.7017\n",
            "  step 70: local=0.4185 global=1.9688\n",
            "  step 80: local=0.4168 global=1.7252\n",
            "  step 90: local=0.4265 global=1.7986\n",
            "  Layer 25 not converged (global=1.7693 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4175 global=1.7721\n",
            "  step 10: local=0.4126 global=1.8022\n",
            "  step 20: local=0.4079 global=1.8194\n",
            "  step 30: local=0.4179 global=1.9053\n",
            "  step 40: local=0.4045 global=1.8028\n",
            "  step 50: local=0.4065 global=1.7215\n",
            "  step 60: local=0.4291 global=1.8956\n",
            "  step 70: local=0.4140 global=1.7775\n",
            "  step 80: local=0.4085 global=1.6612\n",
            "  step 90: local=0.4135 global=1.7807\n",
            "  Layer 25 not converged (global=1.7406 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4154 global=1.7389\n",
            "  step 10: local=0.4120 global=1.8918\n",
            "  step 20: local=0.4173 global=1.8040\n",
            "  step 30: local=0.4227 global=1.6269\n",
            "  step 40: local=0.4153 global=1.7292\n",
            "  step 50: local=0.4090 global=1.8129\n",
            "  step 60: local=0.4069 global=1.8732\n",
            "  step 70: local=0.4180 global=1.6725\n",
            "  step 80: local=0.4152 global=1.7273\n",
            "  step 90: local=0.4154 global=1.7250\n",
            "  Layer 25 not converged (global=1.7010 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4190 global=1.7311\n",
            "  step 10: local=0.4222 global=1.7892\n",
            "  step 20: local=0.4215 global=1.6976\n",
            "  step 30: local=0.4014 global=1.8790\n",
            "  step 40: local=0.3986 global=1.7522\n",
            "  step 50: local=0.4260 global=1.8188\n",
            "  step 60: local=0.4141 global=1.7672\n",
            "  step 70: local=0.4013 global=1.8226\n",
            "  step 80: local=0.4249 global=1.6891\n",
            "  step 90: local=0.4067 global=1.7756\n",
            "  Layer 25 not converged (global=1.7661 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3991 global=1.8109\n",
            "  step 10: local=0.3992 global=1.7043\n",
            "  step 20: local=0.4199 global=1.7327\n",
            "  step 30: local=0.4172 global=1.7402\n",
            "  step 40: local=0.4090 global=1.8434\n",
            "  step 50: local=0.4162 global=1.7610\n",
            "  step 60: local=0.4126 global=1.7568\n",
            "  step 70: local=0.4003 global=1.7694\n",
            "  step 80: local=0.4116 global=1.8132\n",
            "  step 90: local=0.4077 global=1.8224\n",
            "  Layer 25 not converged (global=1.7612 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3988 global=1.8810\n",
            "  step 10: local=0.4054 global=1.8633\n",
            "  step 20: local=0.4186 global=1.8871\n",
            "  step 30: local=0.4149 global=1.7550\n",
            "  step 40: local=0.4183 global=1.8189\n",
            "  step 50: local=0.4131 global=1.8264\n",
            "  step 60: local=0.4067 global=1.8179\n",
            "  step 70: local=0.3997 global=1.8800\n",
            "  step 80: local=0.4297 global=1.8506\n",
            "  step 90: local=0.4046 global=1.7741\n",
            "  [WARN] Layer 25 did not converge after 20 repeats (global=1.8659)\n",
            "\n",
            "--- Layer 26/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4960 global=1.7807\n",
            "  step 10: local=0.4918 global=1.7939\n",
            "  step 20: local=0.4977 global=1.8203\n",
            "  step 30: local=0.4795 global=1.9027\n",
            "  step 40: local=0.4558 global=1.7487\n",
            "  step 50: local=0.4691 global=1.8810\n",
            "  step 60: local=0.4778 global=1.9071\n",
            "  step 70: local=0.4782 global=1.8387\n",
            "  step 80: local=0.4788 global=1.7528\n",
            "  step 90: local=0.4535 global=1.9562\n",
            "  Layer 26 not converged (global=1.8831 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4554 global=1.8098\n",
            "  step 10: local=0.4662 global=1.8540\n",
            "  step 20: local=0.4371 global=1.8251\n",
            "  step 30: local=0.4342 global=1.8751\n",
            "  step 40: local=0.4427 global=1.8658\n",
            "  step 50: local=0.4422 global=1.8054\n",
            "  step 60: local=0.4370 global=1.7817\n",
            "  step 70: local=0.4417 global=1.8240\n",
            "  step 80: local=0.4255 global=1.7489\n",
            "  step 90: local=0.4364 global=1.7873\n",
            "  Layer 26 not converged (global=1.8169 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4176 global=1.7459\n",
            "  step 10: local=0.4387 global=1.7639\n",
            "  step 20: local=0.4293 global=1.8016\n",
            "  step 30: local=0.4209 global=1.7485\n",
            "  step 40: local=0.4237 global=1.7685\n",
            "  step 50: local=0.4175 global=1.7893\n",
            "  step 60: local=0.4162 global=1.7644\n",
            "  step 70: local=0.4159 global=1.8868\n",
            "  step 80: local=0.4105 global=1.9452\n",
            "  step 90: local=0.4105 global=1.7825\n",
            "  Layer 26 not converged (global=1.7564 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4077 global=1.7831\n",
            "  step 10: local=0.4044 global=1.8568\n",
            "  step 20: local=0.4115 global=1.8716\n",
            "  step 30: local=0.4137 global=1.7980\n",
            "  step 40: local=0.4106 global=1.8413\n",
            "  step 50: local=0.4068 global=1.8408\n",
            "  step 60: local=0.4101 global=1.7671\n",
            "  step 70: local=0.3988 global=1.7730\n",
            "  step 80: local=0.4132 global=1.8734\n",
            "  step 90: local=0.4116 global=1.8389\n",
            "  Layer 26 not converged (global=1.9641 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4122 global=1.7227\n",
            "  step 10: local=0.4183 global=1.7563\n",
            "  step 20: local=0.4109 global=1.7359\n",
            "  step 30: local=0.3883 global=1.7991\n",
            "  step 40: local=0.3961 global=1.7613\n",
            "  step 50: local=0.3966 global=1.6995\n",
            "  step 60: local=0.4045 global=1.7545\n",
            "  step 70: local=0.3961 global=1.8580\n",
            "  step 80: local=0.3962 global=1.6591\n",
            "  step 90: local=0.3979 global=1.7233\n",
            "  Layer 26 not converged (global=1.7046 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4053 global=1.7172\n",
            "  step 10: local=0.3961 global=1.9856\n",
            "  step 20: local=0.3924 global=1.7447\n",
            "  step 30: local=0.3979 global=1.8179\n",
            "  step 40: local=0.3950 global=1.7945\n",
            "  step 50: local=0.4044 global=1.8241\n",
            "  step 60: local=0.4083 global=1.8371\n",
            "  step 70: local=0.4054 global=1.9281\n",
            "  step 80: local=0.3988 global=1.8160\n",
            "  step 90: local=0.3867 global=1.7376\n",
            "  Layer 26 not converged (global=1.7353 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3759 global=1.9135\n",
            "  step 10: local=0.4017 global=1.7936\n",
            "  step 20: local=0.4006 global=1.6761\n",
            "  step 30: local=0.3962 global=1.7956\n",
            "  step 40: local=0.3947 global=1.7629\n",
            "  step 50: local=0.4009 global=1.9073\n",
            "  step 60: local=0.3956 global=1.8210\n",
            "  step 70: local=0.3898 global=1.6433\n",
            "  step 80: local=0.3940 global=1.7396\n",
            "  step 90: local=0.4010 global=1.8280\n",
            "  Layer 26 not converged (global=1.8285 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3956 global=1.8910\n",
            "  step 10: local=0.4027 global=1.6847\n",
            "  step 20: local=0.3906 global=1.7391\n",
            "  step 30: local=0.3963 global=1.7393\n",
            "  step 40: local=0.3935 global=1.7456\n",
            "  step 50: local=0.3957 global=1.8082\n",
            "  step 60: local=0.3936 global=1.7106\n",
            "  step 70: local=0.3871 global=1.8915\n",
            "  step 80: local=0.3783 global=1.7672\n",
            "  step 90: local=0.4031 global=1.8302\n",
            "  Layer 26 not converged (global=1.7943 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3875 global=1.7823\n",
            "  step 10: local=0.3743 global=1.8447\n",
            "  step 20: local=0.3858 global=1.6996\n",
            "  step 30: local=0.3997 global=1.7878\n",
            "  step 40: local=0.3805 global=1.8294\n",
            "  step 50: local=0.3808 global=1.7192\n",
            "  step 60: local=0.3954 global=1.7516\n",
            "  step 70: local=0.3896 global=1.7488\n",
            "  step 80: local=0.3935 global=1.8635\n",
            "  step 90: local=0.3853 global=1.7812\n",
            "  Layer 26 not converged (global=1.7780 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4000 global=1.7703\n",
            "  step 10: local=0.3879 global=1.7808\n",
            "  step 20: local=0.3846 global=1.8322\n",
            "  step 30: local=0.4007 global=1.8398\n",
            "  step 40: local=0.3893 global=1.8943\n",
            "  step 50: local=0.3886 global=1.8813\n",
            "  step 60: local=0.4010 global=1.9064\n",
            "  step 70: local=0.3984 global=1.7672\n",
            "  step 80: local=0.3851 global=1.8369\n",
            "  step 90: local=0.3748 global=1.8409\n",
            "  Layer 26 not converged (global=1.7699 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3839 global=1.8335\n",
            "  step 10: local=0.3894 global=1.8945\n",
            "  step 20: local=0.3875 global=1.8650\n",
            "  step 30: local=0.3807 global=1.7878\n",
            "  step 40: local=0.3878 global=1.7349\n",
            "  step 50: local=0.4058 global=1.7539\n",
            "  step 60: local=0.3880 global=1.7790\n",
            "  step 70: local=0.3914 global=1.8715\n",
            "  step 80: local=0.3860 global=1.7126\n",
            "  step 90: local=0.3884 global=1.8469\n",
            "  Layer 26 not converged (global=1.7961 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3861 global=1.8748\n",
            "  step 10: local=0.3930 global=1.8080\n",
            "  step 20: local=0.3950 global=1.7246\n",
            "  step 30: local=0.3834 global=1.9257\n",
            "  step 40: local=0.3761 global=1.7811\n",
            "  step 50: local=0.3849 global=1.8280\n",
            "  step 60: local=0.3965 global=1.8020\n",
            "  step 70: local=0.3805 global=1.8498\n",
            "  step 80: local=0.3880 global=1.8433\n",
            "  step 90: local=0.3817 global=1.7837\n",
            "  Layer 26 not converged (global=1.7904 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3948 global=1.7598\n",
            "  step 10: local=0.3809 global=1.8007\n",
            "  step 20: local=0.3836 global=1.7285\n",
            "  step 30: local=0.3884 global=1.7692\n",
            "  step 40: local=0.3757 global=1.7285\n",
            "  step 50: local=0.3789 global=1.7439\n",
            "  step 60: local=0.3925 global=1.7863\n",
            "  step 70: local=0.3782 global=1.7319\n",
            "  step 80: local=0.3787 global=1.7524\n",
            "  step 90: local=0.3786 global=1.7744\n",
            "  Layer 26 not converged (global=1.6848 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3710 global=1.7484\n",
            "  step 10: local=0.3838 global=1.8725\n",
            "  step 20: local=0.3823 global=1.9308\n",
            "  step 30: local=0.3764 global=1.7683\n",
            "  step 40: local=0.3747 global=1.7710\n",
            "  step 50: local=0.3854 global=1.8430\n",
            "  step 60: local=0.3816 global=1.8589\n",
            "  step 70: local=0.3731 global=1.7861\n",
            "  step 80: local=0.3714 global=1.8284\n",
            "  step 90: local=0.3848 global=1.8305\n",
            "  Layer 26 not converged (global=1.8009 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3875 global=1.7558\n",
            "  step 10: local=0.3864 global=1.7619\n",
            "  step 20: local=0.3831 global=1.8647\n",
            "  step 30: local=0.3853 global=1.8281\n",
            "  step 40: local=0.3781 global=1.7147\n",
            "  step 50: local=0.3797 global=1.7467\n",
            "  step 60: local=0.3704 global=1.7270\n",
            "  step 70: local=0.3749 global=1.7906\n",
            "  step 80: local=0.3781 global=1.7536\n",
            "  step 90: local=0.3839 global=1.6925\n",
            "  Layer 26 not converged (global=1.7704 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3790 global=1.7460\n",
            "  step 10: local=0.3777 global=1.8491\n",
            "  step 20: local=0.3676 global=1.6518\n",
            "  step 30: local=0.3707 global=1.7146\n",
            "  step 40: local=0.3840 global=1.7103\n",
            "  step 50: local=0.3839 global=1.9781\n",
            "  step 60: local=0.3940 global=1.7365\n",
            "  step 70: local=0.3752 global=1.8113\n",
            "  step 80: local=0.3723 global=1.7874\n",
            "  step 90: local=0.3855 global=1.8161\n",
            "  Layer 26 not converged (global=1.7674 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3964 global=1.8306\n",
            "  step 10: local=0.3830 global=1.9213\n",
            "  step 20: local=0.3777 global=1.8090\n",
            "  step 30: local=0.3881 global=1.7309\n",
            "  step 40: local=0.3836 global=1.9064\n",
            "  step 50: local=0.3847 global=1.7872\n",
            "  step 60: local=0.3795 global=1.6705\n",
            "  step 70: local=0.3853 global=1.7881\n",
            "  step 80: local=0.3645 global=1.7566\n",
            "  step 90: local=0.4021 global=1.8990\n",
            "  Layer 26 not converged (global=1.8543 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3856 global=1.8158\n",
            "  step 10: local=0.3783 global=1.6374\n",
            "  step 20: local=0.3782 global=1.7336\n",
            "  step 30: local=0.3740 global=1.8219\n",
            "  step 40: local=0.3827 global=1.8837\n",
            "  step 50: local=0.3968 global=1.6804\n",
            "  step 60: local=0.3689 global=1.7337\n",
            "  step 70: local=0.3770 global=1.7341\n",
            "  step 80: local=0.3826 global=1.7404\n",
            "  step 90: local=0.3900 global=1.8038\n",
            "  Layer 26 not converged (global=1.8012 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3807 global=1.7052\n",
            "  step 10: local=0.3768 global=1.8866\n",
            "  step 20: local=0.3767 global=1.7612\n",
            "  step 30: local=0.3941 global=1.8240\n",
            "  step 40: local=0.3906 global=1.7773\n",
            "  step 50: local=0.3725 global=1.8383\n",
            "  step 60: local=0.3808 global=1.6948\n",
            "  step 70: local=0.3795 global=1.7840\n",
            "  step 80: local=0.3815 global=1.8249\n",
            "  step 90: local=0.3772 global=1.7146\n",
            "  Layer 26 not converged (global=1.8560 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3968 global=1.7485\n",
            "  step 10: local=0.3896 global=1.7445\n",
            "  step 20: local=0.3764 global=1.8579\n",
            "  step 30: local=0.3758 global=1.7770\n",
            "  step 40: local=0.3822 global=1.7658\n",
            "  step 50: local=0.3793 global=1.7751\n",
            "  step 60: local=0.3735 global=1.8271\n",
            "  step 70: local=0.3754 global=1.8358\n",
            "  step 80: local=0.3891 global=1.8889\n",
            "  step 90: local=0.3863 global=1.8764\n",
            "  [WARN] Layer 26 did not converge after 20 repeats (global=1.8428)\n",
            "\n",
            "--- Layer 27/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4800 global=2.4191\n",
            "  step 10: local=0.4939 global=2.2278\n",
            "  step 20: local=0.4826 global=2.1359\n",
            "  step 30: local=0.3930 global=2.1649\n",
            "  step 40: local=0.3957 global=2.1580\n",
            "  step 50: local=0.4719 global=2.1892\n",
            "  step 60: local=0.4513 global=2.1653\n",
            "  step 70: local=0.4665 global=2.0366\n",
            "  step 80: local=0.4413 global=1.9860\n",
            "  step 90: local=0.4532 global=1.9972\n",
            "  Layer 27 not converged (global=1.9546 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3498 global=2.0105\n",
            "  step 10: local=0.3598 global=2.0888\n",
            "  step 20: local=0.3534 global=1.9253\n",
            "  step 30: local=0.3310 global=2.0738\n",
            "  step 40: local=0.3380 global=2.0816\n",
            "  step 50: local=0.3334 global=2.0066\n",
            "  step 60: local=0.4529 global=1.9057\n",
            "  step 70: local=0.4061 global=2.1012\n",
            "  step 80: local=0.3481 global=1.9786\n",
            "  step 90: local=0.3715 global=2.0143\n",
            "  Layer 27 not converged (global=1.9177 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3348 global=1.9688\n",
            "  step 10: local=0.4403 global=2.0192\n",
            "  step 20: local=0.3249 global=2.0273\n",
            "  step 30: local=0.4613 global=1.9612\n",
            "  step 40: local=0.4285 global=1.9314\n",
            "  step 50: local=0.3250 global=1.9686\n",
            "  step 60: local=0.3241 global=1.9002\n",
            "  step 70: local=0.4053 global=1.9105\n",
            "  step 80: local=0.4234 global=1.8921\n",
            "  step 90: local=0.4107 global=1.9111\n",
            "  Layer 27 not converged (global=1.9502 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4258 global=1.9254\n",
            "  step 10: local=0.3024 global=1.8740\n",
            "  step 20: local=0.4175 global=1.8922\n",
            "  step 30: local=0.3171 global=1.9248\n",
            "  step 40: local=0.3120 global=1.8875\n",
            "  step 50: local=0.3163 global=2.0169\n",
            "  step 60: local=0.3123 global=2.0811\n",
            "  step 70: local=0.2750 global=1.8902\n",
            "  step 80: local=0.4079 global=1.8856\n",
            "  step 90: local=0.3138 global=1.9832\n",
            "  Layer 27 not converged (global=1.8921 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3121 global=1.9832\n",
            "  step 10: local=0.3062 global=1.9056\n",
            "  step 20: local=0.4089 global=1.9386\n",
            "  step 30: local=0.4006 global=1.9626\n",
            "  step 40: local=0.2975 global=1.8837\n",
            "  step 50: local=0.3169 global=1.9068\n",
            "  step 60: local=0.3954 global=1.9815\n",
            "  step 70: local=0.3226 global=1.9546\n",
            "  step 80: local=0.4049 global=1.8194\n",
            "  step 90: local=0.3187 global=1.8565\n",
            "  Layer 27 not converged (global=1.9506 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3876 global=1.8568\n",
            "  step 10: local=0.3006 global=1.9229\n",
            "  step 20: local=0.4039 global=1.8708\n",
            "  step 30: local=0.4050 global=1.8168\n",
            "  step 40: local=0.3954 global=1.8737\n",
            "  step 50: local=0.3160 global=1.9566\n",
            "  step 60: local=0.3111 global=1.7713\n",
            "  step 70: local=0.3813 global=1.8419\n",
            "  step 80: local=0.4227 global=1.8273\n",
            "  step 90: local=0.3823 global=2.1125\n",
            "  Layer 27 not converged (global=1.8285 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.2836 global=1.8505\n",
            "  step 10: local=0.3210 global=1.9250\n",
            "  step 20: local=0.3785 global=1.8867\n",
            "  step 30: local=0.2828 global=1.9249\n",
            "  step 40: local=0.4135 global=1.9479\n",
            "  step 50: local=0.3043 global=2.0387\n",
            "  step 60: local=0.3816 global=1.9146\n",
            "  step 70: local=0.3894 global=1.8401\n",
            "  step 80: local=0.3156 global=2.0235\n",
            "  step 90: local=0.3953 global=1.9258\n",
            "  Layer 27 not converged (global=1.9238 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3088 global=1.7821\n",
            "  step 10: local=0.4102 global=1.9065\n",
            "  step 20: local=0.3809 global=1.8721\n",
            "  step 30: local=0.3009 global=2.0172\n",
            "  step 40: local=0.3050 global=1.9243\n",
            "  step 50: local=0.3844 global=1.7467\n",
            "  step 60: local=0.3144 global=1.8359\n",
            "  step 70: local=0.3707 global=1.9350\n",
            "  step 80: local=0.3756 global=2.0073\n",
            "  step 90: local=0.2923 global=1.7815\n",
            "  Layer 27 not converged (global=1.9358 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3051 global=1.8485\n",
            "  step 10: local=0.3928 global=1.8415\n",
            "  step 20: local=0.2852 global=1.8559\n",
            "  step 30: local=0.3053 global=1.9134\n",
            "  step 40: local=0.2913 global=1.8166\n",
            "  step 50: local=0.3906 global=2.0108\n",
            "  step 60: local=0.3881 global=1.8632\n",
            "  step 70: local=0.4163 global=1.9262\n",
            "  step 80: local=0.3988 global=1.8852\n",
            "  step 90: local=0.4020 global=1.9469\n",
            "  Layer 27 not converged (global=1.9899 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3910 global=1.8049\n",
            "  step 10: local=0.2952 global=1.8978\n",
            "  step 20: local=0.3998 global=1.9276\n",
            "  step 30: local=0.4015 global=1.8176\n",
            "  step 40: local=0.2914 global=1.8508\n",
            "  step 50: local=0.3972 global=1.8535\n",
            "  step 60: local=0.3993 global=1.9743\n",
            "  step 70: local=0.3052 global=1.8834\n",
            "  step 80: local=0.3905 global=1.8744\n",
            "  step 90: local=0.3022 global=1.8737\n",
            "  Layer 27 not converged (global=1.8401 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3828 global=1.9324\n",
            "  step 10: local=0.2980 global=1.9445\n",
            "  step 20: local=0.3948 global=2.0124\n",
            "  step 30: local=0.3823 global=1.9950\n",
            "  step 40: local=0.2855 global=2.0248\n",
            "  step 50: local=0.3736 global=1.8661\n",
            "  step 60: local=0.3869 global=1.9200\n",
            "  step 70: local=0.3828 global=1.9597\n",
            "  step 80: local=0.3844 global=1.9486\n",
            "  step 90: local=0.3025 global=1.9985\n",
            "  Layer 27 not converged (global=1.9535 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3906 global=1.9842\n",
            "  step 10: local=0.3875 global=1.8891\n",
            "  step 20: local=0.3822 global=1.8400\n",
            "  step 30: local=0.2944 global=1.8601\n",
            "  step 40: local=0.3840 global=1.8854\n",
            "  step 50: local=0.3884 global=1.9772\n",
            "  step 60: local=0.2864 global=1.8142\n",
            "  step 70: local=0.2870 global=1.9675\n",
            "  step 80: local=0.2981 global=1.9752\n",
            "  step 90: local=0.3039 global=1.9085\n",
            "  Layer 27 not converged (global=1.8447 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3720 global=1.8191\n",
            "  step 10: local=0.2774 global=2.0145\n",
            "  step 20: local=0.3060 global=1.8932\n",
            "  step 30: local=0.3932 global=1.9306\n",
            "  step 40: local=0.3654 global=1.8921\n",
            "  step 50: local=0.3766 global=1.9505\n",
            "  step 60: local=0.3966 global=1.9541\n",
            "  step 70: local=0.2842 global=1.8921\n",
            "  step 80: local=0.3929 global=1.8667\n",
            "  step 90: local=0.3102 global=1.9016\n",
            "  Layer 27 not converged (global=1.8057 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4125 global=1.8387\n",
            "  step 10: local=0.3035 global=1.8561\n",
            "  step 20: local=0.3075 global=1.8414\n",
            "  step 30: local=0.2890 global=1.8551\n",
            "  step 40: local=0.3905 global=1.8812\n",
            "  step 50: local=0.2819 global=1.8353\n",
            "  step 60: local=0.2766 global=1.8490\n",
            "  step 70: local=0.3915 global=1.8884\n",
            "  step 80: local=0.2812 global=1.8452\n",
            "  step 90: local=0.4081 global=1.9773\n",
            "  Layer 27 not converged (global=1.9223 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3792 global=2.0427\n",
            "  step 10: local=0.2782 global=1.8581\n",
            "  step 20: local=0.3873 global=1.8544\n",
            "  step 30: local=0.3883 global=1.9490\n",
            "  step 40: local=0.3867 global=1.9609\n",
            "  step 50: local=0.3972 global=1.8772\n",
            "  step 60: local=0.4137 global=1.9163\n",
            "  step 70: local=0.4023 global=1.9432\n",
            "  step 80: local=0.2832 global=1.8597\n",
            "  step 90: local=0.3840 global=1.8818\n",
            "  Layer 27 not converged (global=1.8402 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4004 global=1.9602\n",
            "  step 10: local=0.2918 global=1.9430\n",
            "  step 20: local=0.2843 global=1.8005\n",
            "  step 30: local=0.3897 global=1.8374\n",
            "  step 40: local=0.4081 global=1.8358\n",
            "  step 50: local=0.2994 global=1.9023\n",
            "  step 60: local=0.3819 global=1.8531\n",
            "  step 70: local=0.3995 global=1.8010\n",
            "  step 80: local=0.2969 global=1.8609\n",
            "  step 90: local=0.3940 global=1.9418\n",
            "  Layer 27 not converged (global=1.8728 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3867 global=1.7587\n",
            "  step 10: local=0.3775 global=1.8322\n",
            "  step 20: local=0.2926 global=1.8162\n",
            "  step 30: local=0.3753 global=2.1022\n",
            "  step 40: local=0.3737 global=1.8370\n",
            "  step 50: local=0.3933 global=1.9084\n",
            "  step 60: local=0.3712 global=1.8787\n",
            "  step 70: local=0.2890 global=1.9145\n",
            "  step 80: local=0.3943 global=1.9392\n",
            "  step 90: local=0.2979 global=2.0274\n",
            "  Layer 27 not converged (global=1.9500 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4001 global=1.9077\n",
            "  step 10: local=0.2907 global=1.8333\n",
            "  step 20: local=0.3951 global=2.0108\n",
            "  step 30: local=0.2872 global=1.9115\n",
            "  step 40: local=0.2847 global=1.7684\n",
            "  step 50: local=0.3729 global=1.8940\n",
            "  step 60: local=0.3903 global=1.8625\n",
            "  step 70: local=0.3005 global=2.0063\n",
            "  step 80: local=0.4017 global=1.9169\n",
            "  step 90: local=0.2937 global=1.7411\n",
            "  Layer 27 not converged (global=1.8799 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3939 global=1.8265\n",
            "  step 10: local=0.3848 global=1.9253\n",
            "  step 20: local=0.2904 global=1.9971\n",
            "  step 30: local=0.3985 global=1.7758\n",
            "  step 40: local=0.3808 global=1.8367\n",
            "  step 50: local=0.2873 global=1.8330\n",
            "  step 60: local=0.3119 global=1.8445\n",
            "  step 70: local=0.2847 global=1.9037\n",
            "  step 80: local=0.4002 global=1.8091\n",
            "  step 90: local=0.3923 global=1.9986\n",
            "  Layer 27 not converged (global=1.9082 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3605 global=1.8591\n",
            "  step 10: local=0.2980 global=1.9156\n",
            "  step 20: local=0.3942 global=1.8749\n",
            "  step 30: local=0.3977 global=1.9446\n",
            "  step 40: local=0.3903 global=1.7956\n",
            "  step 50: local=0.3794 global=1.8863\n",
            "  step 60: local=0.3747 global=1.9242\n",
            "  step 70: local=0.3806 global=1.8097\n",
            "  step 80: local=0.3713 global=1.8426\n",
            "  step 90: local=0.3797 global=1.8463\n",
            "  [WARN] Layer 27 did not converge after 20 repeats (global=1.8848)\n",
            "\n",
            "============================================================\n",
            "PASS 4: E2E Quantizer Tuning (f-only)\n",
            "============================================================\n",
            "  Trainable f parameters: 196\n",
            "  step 0: loss=10.2538\n",
            "  step 10: loss=10.4003\n",
            "  step 20: loss=10.1231\n",
            "  step 30: loss=10.5152\n",
            "  step 40: loss=10.2626\n",
            "  step 50: loss=10.3885\n",
            "  step 60: loss=10.5383\n",
            "  step 70: loss=10.6635\n",
            "  step 80: loss=10.2906\n",
            "  step 90: loss=10.1441\n",
            "  step 100: loss=10.6275\n",
            "  step 110: loss=10.6354\n",
            "  step 120: loss=10.2843\n",
            "  step 130: loss=10.7884\n",
            "  step 140: loss=10.2151\n",
            "  step 150: loss=10.5161\n",
            "  step 160: loss=10.3465\n",
            "  step 170: loss=9.9562\n",
            "  step 180: loss=10.1510\n",
            "  step 190: loss=10.5188\n",
            "  step 200: loss=10.3379\n",
            "  step 210: loss=10.3919\n",
            "  step 220: loss=10.4109\n",
            "  step 230: loss=10.1975\n",
            "  step 240: loss=10.2492\n",
            "  step 250: loss=10.8032\n",
            "  step 260: loss=10.1465\n",
            "  step 270: loss=10.2937\n",
            "  step 280: loss=10.2060\n",
            "  step 290: loss=10.3963\n",
            "  step 300: loss=10.6585\n",
            "  step 310: loss=10.4293\n",
            "  step 320: loss=10.3283\n",
            "  step 330: loss=10.4753\n",
            "  step 340: loss=10.6026\n",
            "  step 350: loss=10.2286\n",
            "  step 360: loss=10.4196\n",
            "  step 370: loss=10.0806\n",
            "  step 380: loss=10.2882\n",
            "  step 390: loss=10.1536\n",
            "  step 400: loss=10.4583\n",
            "  step 410: loss=10.3037\n",
            "  step 420: loss=10.0550\n",
            "  step 430: loss=10.5903\n",
            "  step 440: loss=10.5061\n",
            "  step 450: loss=9.8981\n",
            "  step 460: loss=9.5451\n",
            "  step 470: loss=10.3067\n",
            "  step 480: loss=10.0511\n",
            "  step 490: loss=10.2023\n",
            "\n",
            "============================================================\n",
            "Saving outputs\n",
            "============================================================\n",
            "  Model saved to: runs/progressive_qat_q2_v1/qat_state_dict.pt\n",
            "  Loss log saved to: runs/progressive_qat_q2_v1/loss_per_layer.csv\n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# Full Progressive: MLP pass + E2E f-only (skip attention/refinement for v1)\n",
        "# For full 3-pass training, remove --skip_attention_pass and --skip_mlp_refinement\n",
        "# Starting from 4-bit checkpoint for 2-bit training\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/train_qat_progressive.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_CHECKPOINT} \\\n",
        "  --output_dir {RUN_DIR_PROGRESSIVE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --steps_per_layer_mlp {STEPS_PER_LAYER_MLP} \\\n",
        "  --e2e_steps {E2E_STEPS} \\\n",
        "  --local_weight {LOCAL_WEIGHT} \\\n",
        "  --global_weight {GLOBAL_WEIGHT} \\\n",
        "  --local_token_samples {LOCAL_TOKEN_SAMPLES} \\\n",
        "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
        "  --learning_rate {LR_PROGRESSIVE} \\\n",
        "  --e2e_learning_rate {LR_E2E} \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --skip_attention_pass \\\n",
        "  --max_layer_repeats 20 \\\n",
        "  --max_backtrack 5 \\\n",
        "  --layer_converge_threshold 0.4 \\\n",
        "  --skip_mlp_refinement"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SAVE RUN"
      ],
      "metadata": {
        "id": "A9MSJDvwhdnV"
      },
      "id": "A9MSJDvwhdnV"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# SAVE PROGRESSIVE QAT CHECKPOINT TO GOOGLE DRIVE\n",
        "# ============================================================\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive if not already mounted\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Source directory (matches RUN_DIR_PROGRESSIVE from config)\n",
        "RUN_NAME = \"progressive_qat_q2_v1\"\n",
        "RUN_DIR = f\"runs/{RUN_NAME}\"\n",
        "\n",
        "# Destination on Google Drive\n",
        "DEST_DIR_GD = \"/content/drive/MyDrive/qwen3_runs/\"\n",
        "!mkdir -p {DEST_DIR_GD}\n",
        "\n",
        "# Check if run directory exists and has content\n",
        "if os.path.isdir(RUN_DIR) and os.listdir(RUN_DIR):\n",
        "    # Compress the run directory\n",
        "    print(f\"[archive] Compressing {RUN_DIR}...\")\n",
        "    !tar -zcvf {RUN_NAME}.tgz -C runs {RUN_NAME}\n",
        "\n",
        "    # Copy to Google Drive\n",
        "    print(f\"[save] Copying {RUN_NAME}.tgz to Google Drive...\")\n",
        "    !rsync -ah --info=progress2 {RUN_NAME}.tgz {DEST_DIR_GD}\n",
        "\n",
        "    # Verify\n",
        "    gd_size = os.path.getsize(f\"{DEST_DIR_GD}/{RUN_NAME}.tgz\")\n",
        "    print(f\"[save] Saved to Google Drive: {gd_size / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(f\"[save] ERROR: {RUN_DIR} is empty or doesn't exist\")\n",
        "\n",
        "# got to \"Stage 3 resume\" to continue distill treaining\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVSFT6W9cffG",
        "outputId": "2b2a34d7-1473-44a9-f1eb-eddffcee3dd0"
      },
      "id": "DVSFT6W9cffG",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[archive] Compressing runs/progressive_qat_q2_v1...\n",
            "progressive_qat_q2_v1/\n",
            "progressive_qat_q2_v1/loss_per_layer.csv\n",
            "progressive_qat_q2_v1/training_args.json\n",
            "progressive_qat_q2_v1/qat_state_dict.pt\n",
            "[save] Copying progressive_qat_q2_v1.tgz to Google Drive...\n",
            "        945.50M 100%  442.21MB/s    0:00:02 (xfr#1, to-chk=0/1)\n",
            "[save] Saved to Google Drive: 0.88 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# v3: Full 3-pass progressive training\n",
        "# MLP -> Attention -> MLP refinement -> E2E f-only\n",
        "# WARNING: May show instability at later layers for 2-bit\n",
        "# Starting from 4-bit checkpoint for 2-bit training\n",
        "\n",
        "RUN_DIR_PROGRESSIVE_V3 = \"runs/progressive_qat_q2_v3\"\n",
        "\n",
        "!python scripts/train_qat_progressive.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_CHECKPOINT} \\\n",
        "  --output_dir {RUN_DIR_PROGRESSIVE_V3} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --steps_per_layer_mlp {STEPS_PER_LAYER_MLP} \\\n",
        "  --steps_per_layer_attn {STEPS_PER_LAYER_ATTN} \\\n",
        "  --e2e_steps {E2E_STEPS} \\\n",
        "  --local_weight {LOCAL_WEIGHT} \\\n",
        "  --global_weight {GLOBAL_WEIGHT} \\\n",
        "  --local_token_samples {LOCAL_TOKEN_SAMPLES} \\\n",
        "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
        "  --learning_rate {LR_PROGRESSIVE} \\\n",
        "  --e2e_learning_rate {LR_E2E} \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head"
      ],
      "metadata": {
        "id": "_qK8LP8WrbSB"
      },
      "id": "_qK8LP8WrbSB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dBFCVCZjwBo_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBFCVCZjwBo_",
        "outputId": "8eaba02b-c9ea-4930-df9b-69a5f75e180d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[archive] Compressing runs/qwen3_kdqat_cache_q2_3...\n",
            "qwen3_kdqat_cache_q2_3/\n",
            "qwen3_kdqat_cache_q2_3/special_tokens_map.json\n",
            "qwen3_kdqat_cache_q2_3/loss.csv\n",
            "qwen3_kdqat_cache_q2_3/added_tokens.json\n",
            "qwen3_kdqat_cache_q2_3/tokenizer_config.json\n",
            "qwen3_kdqat_cache_q2_3/run_state.json\n",
            "qwen3_kdqat_cache_q2_3/merges.txt\n",
            "qwen3_kdqat_cache_q2_3/training_args.json\n",
            "qwen3_kdqat_cache_q2_3/chat_template.jinja\n",
            "qwen3_kdqat_cache_q2_3/vocab.json\n",
            "qwen3_kdqat_cache_q2_3/final_state_dict.pt\n",
            "qwen3_kdqat_cache_q2_3/qat_state_dict.pt\n",
            "qwen3_kdqat_cache_q2_3/tokenizer.json\n",
            "[save] Copying qwen3_kdqat_cache_q2_3.tgz to Google Drive...\n",
            "          1.90G 100%  430.50MB/s    0:00:04 (xfr#1, to-chk=0/1)\n",
            "[save] Saved to Google Drive: 1.77 GB\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# SAVE PROGRESSIVE QAT CHECKPOINT TO GOOGLE DRIVE\n",
        "# ============================================================\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "\n",
        "# Mount Google Drive if not already mounted\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Source directory\n",
        "#RUN_NAME = \"progressive_qat_v1\"\n",
        "RUN_NAME = \"progressive_qat_q2_v3\"\n",
        "\n",
        "RUN_DIR = f\"runs/{RUN_NAME}\"\n",
        "\n",
        "# Destination on Google Drive\n",
        "DEST_DIR_GD = \"/content/drive/MyDrive/qwen3_runs/\"\n",
        "!mkdir -p {DEST_DIR_GD}\n",
        "\n",
        "# Check if run directory exists and has content\n",
        "if os.path.isdir(RUN_DIR) and os.listdir(RUN_DIR):\n",
        "    # Compress the run directory\n",
        "    print(f\"[archive] Compressing {RUN_DIR}...\")\n",
        "    !tar -zcvf {RUN_NAME}.tgz -C runs {RUN_NAME}\n",
        "\n",
        "    # Copy to Google Drive\n",
        "    print(f\"[save] Copying {RUN_NAME}.tgz to Google Drive...\")\n",
        "    !rsync -ah --info=progress2 {RUN_NAME}.tgz {DEST_DIR_GD}\n",
        "\n",
        "    # Verify\n",
        "    gd_size = os.path.getsize(f\"{DEST_DIR_GD}/{RUN_NAME}.tgz\")\n",
        "    print(f\"[save] Saved to Google Drive: {gd_size / (1024**3):.2f} GB\")\n",
        "\n",
        "    # Cleanup local archive (optional)\n",
        "    # !rm {RUN_NAME}.tgz\n",
        "else:\n",
        "    print(f\"[save] ERROR: {RUN_DIR} is empty or doesn't exist\")\n",
        "    print(\"[save] Run progressive training first\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qiaq60t466e",
      "metadata": {
        "id": "qiaq60t466e"
      },
      "outputs": [],
      "source": [
        "# v3: Full 3-pass progressive training\n",
        "# MLP -> Attention -> MLP refinement -> E2E f-only\n",
        "# WARNING: May show instability at later layers for 2-bit\n",
        "\n",
        "RUN_DIR_PROGRESSIVE_V3 = \"runs/progressive_qat_v3\"\n",
        "\n",
        "!python scripts/train_qat_progressive.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --output_dir {RUN_DIR_PROGRESSIVE_V3} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --steps_per_layer_mlp {STEPS_PER_LAYER_MLP} \\\n",
        "  --steps_per_layer_attn {STEPS_PER_LAYER_ATTN} \\\n",
        "  --e2e_steps {E2E_STEPS} \\\n",
        "  --local_weight {LOCAL_WEIGHT} \\\n",
        "  --global_weight {GLOBAL_WEIGHT} \\\n",
        "  --local_token_samples {LOCAL_TOKEN_SAMPLES} \\\n",
        "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
        "  --learning_rate {LR_PROGRESSIVE} \\\n",
        "  --e2e_learning_rate {LR_E2E} \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ix3xjm2h0xq",
      "metadata": {
        "id": "ix3xjm2h0xq"
      },
      "outputs": [],
      "source": [
        "# Plot per-layer training progress\n",
        "# Change PLOT_RUN to visualize different runs\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Choose which run to visualize\n",
        "PLOT_RUN = RUN_DIR_E2E_FONLY  # or RUN_DIR_PROGRESSIVE, RUN_DIR_PROGRESSIVE_V3\n",
        "\n",
        "csv_path = f\"{PLOT_RUN}/loss_per_layer.csv\"\n",
        "if not os.path.exists(csv_path):\n",
        "    print(f\"Loss CSV not found at {csv_path}\")\n",
        "    print(\"Run training first or check the path.\")\n",
        "else:\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "    # Pass 1: MLP training (local loss)\n",
        "    mlp_df = df[(df['pass'] == 1) & (df['component'] == 'mlp')]\n",
        "    if not mlp_df.empty and 'local' in mlp_df.columns:\n",
        "        for layer in mlp_df['layer'].unique():\n",
        "            layer_df = mlp_df[mlp_df['layer'] == layer]\n",
        "            axes[0, 0].plot(layer_df['step'], layer_df['local'], label=f'L{layer}', alpha=0.7)\n",
        "        axes[0, 0].set_title('Pass 1: MLP Local Loss per Layer')\n",
        "        axes[0, 0].set_xlabel('Step')\n",
        "        axes[0, 0].set_ylabel('Loss')\n",
        "        axes[0, 0].legend(ncol=4, fontsize=6)\n",
        "    else:\n",
        "        axes[0, 0].set_title('Pass 1: MLP Local Loss (skipped or no local loss)')\n",
        "\n",
        "    # Pass 1: MLP global loss\n",
        "    if not mlp_df.empty and 'global' in mlp_df.columns:\n",
        "        for layer in mlp_df['layer'].unique():\n",
        "            layer_df = mlp_df[mlp_df['layer'] == layer]\n",
        "            axes[0, 1].plot(layer_df['step'], layer_df['global'], label=f'L{layer}', alpha=0.7)\n",
        "        axes[0, 1].set_title('Pass 1: MLP Global KD Loss per Layer')\n",
        "        axes[0, 1].set_xlabel('Step')\n",
        "        axes[0, 1].set_ylabel('Loss')\n",
        "        axes[0, 1].legend(ncol=4, fontsize=6)\n",
        "    else:\n",
        "        axes[0, 1].set_title('Pass 1: MLP Global Loss (skipped)')\n",
        "\n",
        "    # Pass 2: Attention training\n",
        "    attn_df = df[(df['pass'] == 2) & (df['component'] == 'attn')]\n",
        "    if not attn_df.empty and 'global' in attn_df.columns:\n",
        "        for layer in attn_df['layer'].unique():\n",
        "            layer_df = attn_df[attn_df['layer'] == layer]\n",
        "            axes[1, 0].plot(layer_df['step'], layer_df['global'], label=f'L{layer}', alpha=0.7)\n",
        "        axes[1, 0].set_title('Pass 2: Attention Global KD Loss per Layer')\n",
        "        axes[1, 0].set_xlabel('Step')\n",
        "        axes[1, 0].set_ylabel('Loss')\n",
        "        axes[1, 0].legend(ncol=4, fontsize=6)\n",
        "    else:\n",
        "        axes[1, 0].set_title('Pass 2: Attention (skipped)')\n",
        "        axes[1, 0].text(0.5, 0.5, 'Not run', ha='center', va='center', transform=axes[1, 0].transAxes)\n",
        "\n",
        "    # Pass 4: E2E f-only tuning\n",
        "    e2e_df = df[(df['pass'] == 4)]\n",
        "    if not e2e_df.empty and 'global' in e2e_df.columns:\n",
        "        axes[1, 1].plot(e2e_df['step'], e2e_df['global'], 'b-', linewidth=2)\n",
        "        axes[1, 1].set_title('Pass 4: E2E f-only Tuning')\n",
        "        axes[1, 1].set_xlabel('Step')\n",
        "        axes[1, 1].set_ylabel('Global KD Loss')\n",
        "    else:\n",
        "        axes[1, 1].set_title('Pass 4: E2E (not yet run)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{PLOT_RUN}/loss_per_layer.png\", dpi=150)\n",
        "    plt.show()\n",
        "    print(f\"Saved to {PLOT_RUN}/loss_per_layer.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qgtw9rd3cr",
      "metadata": {
        "id": "qgtw9rd3cr"
      },
      "source": [
        "### Inference Check: Progressive QAT Results\n",
        "\n",
        "Test the progressive QAT checkpoint with a quick inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4coakmebsik",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4coakmebsik",
        "outputId": "5ab1ae51-ec40-4a29-c9d0-92df9f482839"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-25 03:57:41.932578: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-25 03:57:41.952444: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766635061.977425    7043 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766635061.982748    7043 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766635061.996415    7043 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766635061.996438    7043 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766635061.996441    7043 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766635061.996444    7043 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-25 03:57:42.000573: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "user\n",
            "What is Apple Neural Engine?\n",
            "assistant\n",
            "<think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Apple Neural Engine (NLP) is a powerful and powerful artificial intelligence system that mimues human behavior, learning, and decision-making. It is a popular machine learning algorithm that is widely used for various AI applications, such as image recognition, language understanding, and language generation. It is also used in finance, finance, and finance, making it a important tool for financial analysts and analysts.\n"
          ]
        }
      ],
      "source": [
        "# Test inference with progressive QAT checkpoint\n",
        "# Change RUN_DIR to test different runs:\n",
        "#   RUN_DIR_E2E_FONLY, RUN_DIR_PROGRESSIVE_FONLY, RUN_DIR_PROGRESSIVE\n",
        "# progressive_qat_v1/qat_state_dict.pt\n",
        "\n",
        "#TEST_RUN = RUN_DIR_E2E_FONLY  # Change this to test other runs\n",
        "\n",
        "# Use the path where the checkpoint was unzipped\n",
        "TEST_RUN =  \"runs/qwen3_kdqat_cache_q2_4\"\n",
        "TEST_RUN =  \"runs/qwen3_kdqat_cache_q2_3\"\n",
        "\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {TEST_RUN}/qat_state_dict.pt \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --prompt \"What is Apple Neural Engine?\" \\\n",
        "  --do_sample true \\\n",
        "  --max_new_tokens 128"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yq8GHX0o3c2y",
      "metadata": {
        "id": "Yq8GHX0o3c2y"
      },
      "source": [
        "## 6) Stage B: LoRA recovery\n",
        "\n",
        "Two options:\n",
        "- **SFT LoRA** (Alpaca-style instruction tuning)\n",
        "- **Cached KD-LoRA** (preserve teacher distribution; no new “skills”)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B1yimj8aS11y",
      "metadata": {
        "id": "B1yimj8aS11y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "LcGFKyxO3c2y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcGFKyxO3c2y",
        "outputId": "979e3378-150b-44fc-83ef-762801383bb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "[device] cuda | amp_dtype=torch.bfloat16 | param_dtype=torch.bfloat16\n",
            "2025-12-25 03:59:07.550602: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-25 03:59:07.570769: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766635147.595924    7465 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766635147.601384    7465 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766635147.615579    7465 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766635147.615602    7465 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766635147.615605    7465 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766635147.615608    7465 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-25 03:59:07.619894: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[qat] weight_bits=2\n",
            "Loaded QAT checkpoint. missing=0 unexpected=0\n",
            "Enabled LoRA on (196, 20185088) layers. Trainable params: 20,185,088\n",
            "[kd_cache] Note: cache max_length=128 (you passed --max_length=1024). --max_length is ignored in cache mode.\n",
            "[kd_cache] cache topk=32\n",
            "[kd_cache] Enabled cached KD-LoRA. T=2.0 weight=1.0 hard_top1=0.02 hard_full_top1=0.01\n",
            "opt_step: 100% 1000/1000 [09:49<00:00,  1.70step/s, loss=1.2426, lr=0.00e+00]\n",
            "Done. Saved LoRA adapter to: runs/qwen3_lora_recovery_cached_r32/lora_only_state_dict.pt\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# STAGE B: LoRA Recovery (Cached KD-LoRA)\n",
        "# ============================================================\n",
        "# Train LoRA adapters on top of QAT checkpoint\n",
        "\n",
        "CACHE_DIR = CACHE_DIR_CHAT\n",
        "#RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2\"\n",
        "RUN_DIR_CACHE  = \"runs/progressive_qat_v1\"\n",
        "RUN_DIR_CACHE  = \"runs/qwen3_kdqat_cache_q2_4\"\n",
        "RUN_DIR_CACHE =  \"runs/qwen3_kdqat_cache_q2_3\"\n",
        "\n",
        "\n",
        "LORA_DIM = 32\n",
        "LORA_RUN_KD = f\"runs/qwen3_lora_recovery_cached_r{LORA_DIM}\"\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/train_lora_recovery.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR_CACHE}/qat_state_dict.pt \\\n",
        "  --output_dir {LORA_RUN_KD} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --gradient_accumulation_steps 2 \\\n",
        "  --learning_rate 1e-5 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 1000 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 2 \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} \\\n",
        "  --lora_alpha {LORA_DIM} \\\n",
        "  --lora_dropout 0.0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.02 \\\n",
        "  --hard-full-top1-weight 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Defule6L3c2y",
      "metadata": {
        "id": "Defule6L3c2y"
      },
      "source": [
        "## 7) Plot loss\n",
        "\n",
        "In Colab, use `--no_show` + `--save` then display the PNG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QM1RNfDh3c2y",
      "metadata": {
        "id": "QM1RNfDh3c2y"
      },
      "outputs": [],
      "source": [
        "!python scripts/plot_loss.py --run_dir {RUN_DIR} --source csv --no_show --save {RUN_DIR}/loss.png\n",
        "from PIL import Image\n",
        "display(Image.open(f\"{RUN_DIR}/loss.png\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p3bOUiMt3c2y",
      "metadata": {
        "id": "p3bOUiMt3c2y"
      },
      "source": [
        "## 8) Inference sanity checks\n",
        "\n",
        "Greedy decode (`--do_sample false`) and keep outputs short (`--max_new_tokens 16`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "aA_cJHvT3c2y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aA_cJHvT3c2y",
        "outputId": "cc468368-92bd-4981-e9f4-e2e894eca16b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "2025-12-25 04:11:49.105856: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-25 04:11:49.126275: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766635909.151365   10948 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766635909.156770   10948 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766635909.170813   10948 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766635909.170836   10948 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766635909.170839   10948 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766635909.170842   10948 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-25 04:11:49.175005: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[lora] loaded adapters for 196 layers from runs/qwen3_lora_recovery_cached_r32/lora_only_state_dict.pt\n",
            "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "user\n",
            "What is Machine Learning?\n",
            "assistant\n",
            "<think>\n",
            "<think>\n",
            "</think>\n",
            "\n",
            "Machine Learning is a type of machine learning that involves combining data sets to predict outcomes based on patterns. It is used in various fields, such as finance, finance, and social media. Machine learning is a powerful field that combines data to make decisions based on patterns. It is used in various fields, such as finance, finance, and social media. Machine learning is a important field that is growing in many industries, including finance, finance, and social media.\n"
          ]
        }
      ],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_3\"\n",
        "#RUN_DIR = \"runs/progressive_qat_v1\"\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --lora_checkpoint \"runs/qwen3_lora_recovery_cached_r32/lora_only_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
        "  --prompt \"What is Machine Learning?\" \\\n",
        "  --do_sample false \\\n",
        "  --enable_thinking true \\\n",
        "  --max_new_tokens 256\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EJlu4sOZCNpY",
      "metadata": {
        "id": "EJlu4sOZCNpY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cD985HdXlm0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cD985HdXlm0",
        "outputId": "65053f76-705a-419c-fddb-8ae1cff1fda1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-24 10:41:54.709024: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-24 10:41:54.729231: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766572914.754556  155967 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766572914.759941  155967 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766572914.773878  155967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572914.773902  155967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572914.773905  155967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572914.773908  155967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 10:41:54.777961: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[lora] loaded adapters for 196 layers from runs/qwen3_lora_recovery_cached_r32/lora_only_state_dict.pt\n",
            "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "user\n",
            "Explain how neural networks learn in simple terms\n",
            "assistant\n",
            "<think>\n",
            "<think>\n",
            "Okay, let's start with the basics. Neural networks are like a brain in a computer, right? They process information in layers, just like a human brain does. Each layer has some connections between neurons, and each neuron can process a single input value. \n",
            "\n",
            "So, if we have a simple example, like a simple neural network that takes a single input and outputs a single output, how does it work? Let's imagine a simple example. Let's say the input is a number, say 10. The first layer is the input layer, and the second layer is the output layer. The input layer has a single neuron, and the output layer has a single neuron. \n",
            "\n",
            "The input layer takes the input number and processes it. Then, the output layer takes the result and passes it back. So, the process is: input → output. \n",
            "\n",
            "But how does this work in practice? It's like a simple machine learning model. The input is the data, the output is the result, and the model learns by adjusting the weights and biases to minimize the error. \n",
            "\n",
            "So, in simple terms, neural networks work by taking a set of inputs, applying some operations to them, and then producing a set of outputs. The process is iterative, and the model learns over time to make better predictions or decisions. \n",
            "\n",
            "I think that's a good explanation. Let me make sure it's simple and not too technical. The key is that the neural network is like a computer, and the process of learning is similar to how a human brain learns. \n",
            "\n",
            "I think that's a good explanation. Let me check if I'm missing anything. No, I think I've covered the basics. If there's anything else, I can add more details, but I'm already confident in this explanation.\n",
            "</think>\n",
            "\n",
            "**Explanation of How Neural Networks Learn in Simple Terms:**\n",
            "\n",
            "**1. Introduction:**\n",
            "- **Neural Networks (NNs):** They are like computers in the brain, where each neuron processes information.\n",
            "- **Input Layer:** Takes the input data and processes it.\n",
            "- **Output Layer:** Produces the output, which is the result of the processing.\n",
            "\n",
            "**2. Simple Example:**\n",
            "- **Input:** A single input value, say 10.\n",
            "- **First Layer (Input Layer):** Processes the input, maybe by adding some weights and biases.\n",
            "- **Output Layer (Output Layer):** Passes the result back, with the final output being the result of the processing.\n",
            "\n",
            "**3. How It Works:**\n",
            "- **Processing:** Each neuron in a layer takes the input and processes it.\n",
            "- **Learning:** The network learns to adjust its weights and biases over time to minimize the error.\n",
            "- **Iteration:** The process is repeated over time, and the model improves its performance.\n",
            "\n",
            "**4. Simple Explanation in Words:**\n",
            "- Neural networks are like a computer that learns by doing tasks. The input is the data, the output is the result, and the model learns to make better predictions.\n",
            "\n",
            "**5. Conclusion:**\n",
            "- Neural networks are a type of machine learning model that processes information in layers, just like a human brain does. They are simple, can be used for various tasks, and are very effective in learning.\n",
            "\n",
            "This explanation is simple, visual, and easy to understand.\n"
          ]
        }
      ],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_4\"\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --lora_checkpoint \"runs/qwen3_lora_recovery_cached_r32/lora_only_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
        "  --prompt \"Explain how neural networks learn in simple terms\" \\\n",
        "  --do_sample false \\\n",
        "  --enable_thinking true \\\n",
        "  --max_new_tokens 1024\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xZ6aSDAa3c2y",
      "metadata": {
        "id": "xZ6aSDAa3c2y"
      },
      "outputs": [],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --lora_checkpoint \"runs/qwen3_lora_recovery_cached_r64/lora_only_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
        "  --prompt \"2+2=\" \\\n",
        "  --do_sample false \\\n",
        "  --enable_thinking true \\\n",
        "  --max_new_tokens 90\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZOIo97VlyAbh",
      "metadata": {
        "id": "ZOIo97VlyAbh"
      },
      "outputs": [],
      "source": [
        "LORA_DIM = 64\n",
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2\"\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
        "  --prompt \"What is capital of France?\" \\\n",
        "  --do_sample true \\\n",
        "  --max_new_tokens 64\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ef_Q8P0i3c2y",
      "metadata": {
        "id": "Ef_Q8P0i3c2y"
      },
      "source": [
        "## 9) Optional: snap weights to the exact grid\n",
        "\n",
        "This produces a float checkpoint with weights snapped to the N-bit codebook (not bitpacked)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jk_qSZIs3c2y",
      "metadata": {
        "id": "jk_qSZIs3c2y"
      },
      "outputs": [],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "!python scripts/hard_quantize_checkpoint.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}/checkpoint_last.pt \\\n",
        "  --output_path {RUN_DIR}/hard_quant_full_state_dict.pt \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7HnE7IFS3dK",
      "metadata": {
        "id": "f7HnE7IFS3dK"
      },
      "outputs": [],
      "source": [
        "%cd qwen3_apple_style_2bit_qat_lora\n",
        "%ls -l runs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beQD2eyDWWnb",
      "metadata": {
        "id": "beQD2eyDWWnb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}