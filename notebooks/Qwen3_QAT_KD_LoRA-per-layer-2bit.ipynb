{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anemll/qwen3_apple_style_2bit_qat_lora/blob/main/notebooks/Qwen3_QAT_KD_LoRA-per-layer-2bit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rRgC0uK43c2v",
      "metadata": {
        "id": "rRgC0uK43c2v"
      },
      "source": [
        "# Qwen3-0.6B — Apple-style QAT (2-bit / 4-bit) + KD + LoRA recovery\n",
        "\n",
        "This notebook mirrors the structure of common “phone deployment” notebooks, but uses **this repo’s** pipeline:\n",
        "\n",
        "- **Stage A (recommended default):** KD-QAT on plain text (C4 streaming) or KD-cache QAT\n",
        "- **Stage B:** LoRA recovery (either SFT or cached KD-LoRA)\n",
        "- Plot `loss.csv`\n",
        "- Run inference sanity checks\n",
        "\n",
        "Notes:\n",
        "- Qwen3 requires `transformers>=4.51.0`.\n",
        "- For disk usage: C4 is huge; prefer `--streaming` unless you explicitly want to download.\n",
        "- Bitwidth: use `-q 2` (default) or `-q 4` (less aggressive). Checkpoints persist the bitwidth per layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i6Pb9Kki3c2w",
      "metadata": {
        "id": "i6Pb9Kki3c2w"
      },
      "source": [
        "## 0) Setup (Colab / local)\n",
        "\n",
        "If you’re in Colab, clone the repo. If you’re already in the repo directory locally, you can skip this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "UXlLPdtkGM35",
      "metadata": {
        "id": "UXlLPdtkGM35"
      },
      "outputs": [],
      "source": [
        "# ---- Config (edit these) ----\n",
        "#MODEL_NAME = 'Qwen/Qwen3-4B-Thinking-2507'\n",
        "MODEL_NAME = 'Qwen/Qwen3-0.6B'\n",
        "TEACHER_NAME = MODEL_NAME\n",
        "QUANT_BITS = 2  # 2 or 4\n",
        "DEVICE = 'auto'\n",
        "AMP_DTYPE = 'auto'\n",
        "PARAM_DTYPE = 'auto'\n",
        "DTYPE = 'auto'\n",
        "\n",
        "# Cache dirs\n",
        "CACHE_DIR_CHAT = 'caches/alpaca_chat_think_both_L128_K32_R256'\n",
        "CACHE_DIR_TEXT = 'caches/c4_qwen3_L64_K32_R256'\n",
        "#CACHE_DIR_CHAT = 'caches/Q4B_alpaca_chat_think_L128_K32_R256'\n",
        "#CACHE_DIR_TEXT = 'caches/Q4B_c4_qwen3_L64_K32_R256'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "u2vonfu23c2w",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u2vonfu23c2w",
        "outputId": "c3e12b52-7835-48a4-d8fa-b0757bdcc911"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'qwen3_apple_style_2bit_qat_lora'...\n",
            "remote: Enumerating objects: 220, done.\u001b[K\n",
            "remote: Counting objects: 100% (220/220), done.\u001b[K\n",
            "remote: Compressing objects: 100% (154/154), done.\u001b[K\n",
            "remote: Total 220 (delta 125), reused 145 (delta 63), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (220/220), 278.75 KiB | 8.20 MiB/s, done.\n",
            "Resolving deltas: 100% (125/125), done.\n",
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "Already up to date.\n",
            "HEAD is now at 2a12323 preping 2-bit notebook for restart\n"
          ]
        }
      ],
      "source": [
        "# Colab-only:\n",
        "%cd /content/\n",
        "!git clone https://github.com/Anemll/qwen3_apple_style_2bit_qat_lora\n",
        "%cd qwen3_apple_style_2bit_qat_lora\n",
        "!git fetch\n",
        "!git pull\n",
        "!git reset --hard HEAD\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZUmiISSL3c2w",
      "metadata": {
        "id": "ZUmiISSL3c2w"
      },
      "source": [
        "## 1) Install dependencies (uv)\n",
        "\n",
        "This repo is set up to work with `uv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ACRjVuVa3c2x",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACRjVuVa3c2x",
        "outputId": "6ad351ae-0f68-4e6c-d45b-25a4b6a89d6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m7 packages\u001b[0m \u001b[2min 95ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m62 packages\u001b[0m \u001b[2min 120ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 738ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 0.37ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 0.80ms\u001b[0m\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mqat-lora\u001b[0m\u001b[2m==0.0.0 (from file:///content/qwen3_apple_style_2bit_qat_lora)\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mqat-lora\u001b[0m\u001b[2m==0.0.0 (from file:///content/qwen3_apple_style_2bit_qat_lora/qwen3_apple_style_2bit_qat_lora)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip -q install uv\n",
        "!uv pip install -r requirements.txt\n",
        "!uv pip install -e .\n",
        "# plotting\n",
        "!uv pip install -q matplotlib\n",
        "!uv pip install -q plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yfoLPbBk3c2x",
      "metadata": {
        "id": "yfoLPbBk3c2x"
      },
      "source": [
        "## 2) Optional: Hugging Face login\n",
        "\n",
        "If you hit gated model/dataset errors, log in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wLMWX7E23c2x",
      "metadata": {
        "id": "wLMWX7E23c2x"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()  # paste token when prompted\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fA37ilf33c2x",
      "metadata": {
        "id": "fA37ilf33c2x"
      },
      "source": [
        "## 3) Quick environment check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VUCXehkU3c2x",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUCXehkU3c2x",
        "outputId": "0377d1f6-830e-429e-c7e4-65de61b795ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch 2.9.0+cu126\n",
            "transformers 4.57.3\n",
            "cuda True\n",
            "mps False\n"
          ]
        }
      ],
      "source": [
        "import torch, transformers\n",
        "print('torch', torch.__version__)\n",
        "print('transformers', transformers.__version__)\n",
        "print('cuda', torch.cuda.is_available())\n",
        "print('mps', torch.backends.mps.is_available())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GCXUYtVC3c2x",
      "metadata": {
        "id": "GCXUYtVC3c2x"
      },
      "source": [
        "## 4) Stage A ((Optional)): KD-QAT on streaming C4\n",
        "\n",
        "This preserves the base model’s behavior under low-bit fake-quant weights.\n",
        "\n",
        "Tips:\n",
        "- Start with a small run (`--max_steps 50`) to validate the pipeline.\n",
        "- Use `-q 4` if 2-bit is too unstable; 4-bit is less aggressive.\n",
        "- On MPS, prefer `--ema_decay 0` for KD-QAT.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jERGktjwjz29",
      "metadata": {
        "id": "jERGktjwjz29"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TpT02cskrs6D",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TpT02cskrs6D",
        "outputId": "5e5648b6-a34d-4ada-85c9-2292e31b316f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[device] cuda | dtype=torch.bfloat16\n",
            "tokenizer_config.json: 9.73kB [00:00, 39.0MB/s]\n",
            "vocab.json: 2.78MB [00:00, 65.7MB/s]\n",
            "merges.txt: 1.67MB [00:00, 132MB/s]\n",
            "tokenizer.json: 100% 11.4M/11.4M [00:00<00:00, 20.4MB/s]\n",
            "config.json: 100% 726/726 [00:00<00:00, 6.09MB/s]\n",
            "2025-12-24 01:22:39.394668: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-24 01:22:39.417464: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766539359.443242    8009 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766539359.449385    8009 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766539359.463433    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766539359.463456    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766539359.463459    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766539359.463461    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 01:22:39.467717: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors: 100% 1.50G/1.50G [00:01<00:00, 935MB/s] \n",
            "generation_config.json: 100% 239/239 [00:00<00:00, 2.03MB/s]\n",
            "README.md: 7.47kB [00:00, 30.6MB/s]\n",
            "data/train-00000-of-00001-a09b74b3ef9c3b(…): 100% 24.2M/24.2M [00:00<00:00, 34.7MB/s]\n",
            "Generating train split: 100% 52002/52002 [00:00<00:00, 336912.65 examples/s]\n",
            "[write] shard_00000.pt | N=512\n",
            "[write] shard_00001.pt | N=512\n",
            "[write] shard_00002.pt | N=512\n",
            "[write] shard_00003.pt | N=512\n",
            "[write] shard_00004.pt | N=512\n",
            "[write] shard_00005.pt | N=512\n",
            "[write] shard_00006.pt | N=512\n",
            "[write] shard_00007.pt | N=512\n",
            "[write] shard_00008.pt | N=512\n",
            "[write] shard_00009.pt | N=512\n",
            "[write] shard_00010.pt | N=512\n",
            "[write] shard_00011.pt | N=512\n",
            "[write] shard_00012.pt | N=512\n",
            "[write] shard_00013.pt | N=512\n",
            "[write] shard_00014.pt | N=512\n",
            "[write] shard_00015.pt | N=512\n",
            "[write] shard_00016.pt | N=512\n",
            "[write] shard_00017.pt | N=512\n",
            "[write] shard_00018.pt | N=512\n",
            "[write] shard_00019.pt | N=512\n",
            "[write] shard_00020.pt | N=512\n",
            "[write] shard_00021.pt | N=512\n",
            "[write] shard_00022.pt | N=512\n",
            "[write] shard_00023.pt | N=512\n",
            "[write] shard_00024.pt | N=512\n",
            "[write] shard_00025.pt | N=512\n",
            "[write] shard_00026.pt | N=512\n",
            "[write] shard_00027.pt | N=512\n",
            "[write] shard_00028.pt | N=512\n",
            "[write] shard_00029.pt | N=512\n",
            "[write] shard_00030.pt | N=512\n",
            "[write] shard_00031.pt | N=512\n",
            "[write] shard_00032.pt | N=512\n",
            "[write] shard_00033.pt | N=512\n",
            "[write] shard_00034.pt | N=512\n",
            "[write] shard_00035.pt | N=512\n",
            "[write] shard_00036.pt | N=512\n",
            "[write] shard_00037.pt | N=512\n",
            "[write] shard_00038.pt | N=512\n",
            "[write] shard_00039.pt | N=32\n",
            "Done. Cached sequences=20000 | out_dir=caches/alpaca_chat_think_L128_K32_R256\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# GENERATE THINKING DATASET (Alpaca chat format)\n",
        "# ============================================================\n",
        "# SKIP THIS CELL if you already have the cache on Google Drive!\n",
        "# Use the \"LOAD FROM GOOGLE DRIVE\" cell instead.\n",
        "\n",
        "!python scripts/precompute_teacher_topk.py \\\n",
        "  --teacher_model_name_or_path {MODEL_NAME} \\\n",
        "  --dataset_name tatsu-lab/alpaca \\\n",
        "  --dataset_split train \\\n",
        "  --dataset_format alpaca_chat \\\n",
        "  --enable_thinking true \\\n",
        "  --max_length 128 \\\n",
        "  --topk 32 \\\n",
        "  --rand_neg 256 \\\n",
        "  --num_sequences 20000 \\\n",
        "  --batch_size 1 \\\n",
        "  --shard_size 512 \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  --output_dir {CACHE_DIR_CHAT}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rlxJ3X8a3c2x",
      "metadata": {
        "id": "rlxJ3X8a3c2x"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# GENERATE TEXT DATASET (C4 streaming)\n",
        "# ============================================================\n",
        "# SKIP THIS CELL if you already have the cache on Google Drive!\n",
        "# Use the \"LOAD FROM GOOGLE DRIVE\" cell instead.\n",
        "\n",
        "import os\n",
        "\n",
        "CACHE_DIR = CACHE_DIR_TEXT\n",
        "\n",
        "if not os.path.isdir(CACHE_DIR):\n",
        "    print(f\"[cache] {CACHE_DIR} not found -> generating cache...\")\n",
        "\n",
        "    !python scripts/precompute_teacher_topk.py \\\n",
        "      --teacher_model_name_or_path {MODEL_NAME} \\\n",
        "      --dataset_name allenai/c4 \\\n",
        "      --dataset_config_name en \\\n",
        "      --dataset_split train \\\n",
        "      --dataset_text_field text \\\n",
        "      --streaming \\\n",
        "      --shuffle_buffer 10000 \\\n",
        "      --max_length 64 \\\n",
        "      --topk 32 \\\n",
        "      --rand_neg 256 \\\n",
        "      --num_sequences 2000 \\\n",
        "      --batch_size 1 \\\n",
        "      --shard_size 512 \\\n",
        "      --device {DEVICE} \\\n",
        "      --dtype {DTYPE} \\\n",
        "      --output_dir {CACHE_DIR}\n",
        "\n",
        "else:\n",
        "    print(f\"[cache] {CACHE_DIR} already exists -> skipping generation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Nt45qTnlKO7a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt45qTnlKO7a",
        "outputId": "be3ace25-b2da-42ec-df47-30b79fc479a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[gzip] Compressing caches/alpaca_chat_think_L128_K32_R256...\n",
            "caches/alpaca_chat_think_L128_K32_R256/\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00018.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00011.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00033.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00031.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00036.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00038.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00030.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00000.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00032.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00010.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00039.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00008.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00005.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00025.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/meta.json\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00015.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00028.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00016.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00009.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00021.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00037.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00001.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00035.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00029.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00003.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00006.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00014.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00007.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00002.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00013.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00024.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00034.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00022.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00020.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00004.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00012.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00019.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00027.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00026.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00017.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00023.pt\n",
            "[gzip] Done: 2.84 GB\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# COMPRESS CHAT CACHE (for Google Drive upload)\n",
        "# ============================================================\n",
        "# SKIP if cache is already compressed or loaded from Google Drive\n",
        "\n",
        "import os\n",
        "\n",
        "if os.path.isdir(CACHE_DIR_CHAT):\n",
        "    print(f\"[gzip] Compressing {CACHE_DIR_CHAT}...\")\n",
        "    !tar -zcvf {CACHE_DIR_CHAT}.tgz {CACHE_DIR_CHAT}\n",
        "    compressed_size = os.path.getsize(f\"{CACHE_DIR_CHAT}.tgz\")\n",
        "    print(f\"[gzip] Done: {compressed_size / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(f\"[gzip] Directory {CACHE_DIR_CHAT} not found. Skipping.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IIIWEkllwGEA",
      "metadata": {
        "collapsed": true,
        "id": "IIIWEkllwGEA"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SAVE CACHED KD DATA TO GOOGLE DRIVE (run after generating cache)\n",
        "# ============================================================\n",
        "# This saves the generated cache to Google Drive for future sessions\n",
        "# Only run this AFTER you've generated the cache with precompute_teacher_topk.py\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create destination directory\n",
        "!mkdir -p /content/drive/MyDrive/qwen3_caches\n",
        "\n",
        "# Choose which cache to save (should match what you generated)\n",
        "CACHE_NAME = \"alpaca_chat_think_both_L128_K32_R256\"\n",
        "\n",
        "import os\n",
        "\n",
        "# Check if cache exists - copy folder directly (no compression needed)\n",
        "if os.path.isdir(f\"caches/{CACHE_NAME}\"):\n",
        "    # Copy folder to Google Drive\n",
        "    print(f\"[save] Copying {CACHE_NAME} to Google Drive...\")\n",
        "    !rsync -ah --info=progress2 caches/{CACHE_NAME}/ /content/drive/MyDrive/qwen3_caches/{CACHE_NAME}/\n",
        "\n",
        "    # Verify\n",
        "    num_shards = len([f for f in os.listdir(f\"/content/drive/MyDrive/qwen3_caches/{CACHE_NAME}\") if f.startswith(\"shard_\")])\n",
        "    print(f\"[save] Saved to Google Drive: {num_shards} shards\")\n",
        "else:\n",
        "    print(f\"[save] ERROR: Cache directory caches/{CACHE_NAME} not found\")\n",
        "    print(\"[save] Run precompute_teacher_topk.py first to generate the cache\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n4snxtz2q6q",
      "metadata": {
        "id": "n4snxtz2q6q"
      },
      "source": [
        "## 4.5) Google Drive Cache Management\n",
        "\n",
        "**Workflow for KD Cache:**\n",
        "\n",
        "1. **First time setup** (slow):\n",
        "   - Run `precompute_teacher_topk.py` to generate cache\n",
        "   - Run \"SAVE TO GOOGLE DRIVE\" cell to persist\n",
        "   \n",
        "2. **Subsequent sessions** (fast):\n",
        "   - Run \"LOAD FROM GOOGLE DRIVE\" cell to restore cache\n",
        "   - Skip cache generation step\n",
        "\n",
        "The cached KD data (~2-3 GB compressed) contains precomputed teacher logits for knowledge distillation training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7745eb2",
      "metadata": {
        "id": "d7745eb2"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# COMPRESS TEXT CACHE (for Google Drive upload)\n",
        "# ============================================================\n",
        "# SKIP if cache is already compressed or loaded from Google Drive\n",
        "\n",
        "import os\n",
        "\n",
        "if os.path.isdir(CACHE_DIR_TEXT):\n",
        "    print(f\"[gzip] Compressing {CACHE_DIR_TEXT}...\")\n",
        "    !tar -zcvf {CACHE_DIR_TEXT}.tgz {CACHE_DIR_TEXT}\n",
        "    compressed_size = os.path.getsize(f\"{CACHE_DIR_TEXT}.tgz\")\n",
        "    print(f\"[gzip] Done: {compressed_size / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(f\"[gzip] Directory {CACHE_DIR_TEXT} not found. Skipping.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "vlhuS4N9GbN4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlhuS4N9GbN4",
        "outputId": "7ac4bee6-b28f-4ed8-ed8d-3650298fd7cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[cache] Copying alpaca_chat_think_both_L128_K32_R256 from Google Drive...\n",
            "          4.40G 100%   24.79MB/s    0:02:49 (xfr#41, to-chk=0/42)\n",
            "[cache] Successfully loaded alpaca_chat_think_both_L128_K32_R256 with 40 shards\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# LOAD CACHED KD DATA FROM GOOGLE DRIVE (run this cell first!)\n",
        "# ============================================================\n",
        "# Mount Google Drive and copy cached KD data back to local storage\n",
        "# This avoids regenerating the cache every session\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create local cache directory\n",
        "!mkdir -p caches\n",
        "\n",
        "# Cache folder to load (copy folder directly, no .tgz)\n",
        "CACHE_NAME = \"alpaca_chat_think_both_L128_K32_R256\"\n",
        "\n",
        "# Copy folder directly from Google Drive\n",
        "SRC_PATH = f\"/content/drive/MyDrive/qwen3_caches/{CACHE_NAME}\"\n",
        "DST_PATH = f\"caches/{CACHE_NAME}\"\n",
        "\n",
        "print(f\"[cache] Copying {CACHE_NAME} from Google Drive...\")\n",
        "!rsync -ah --info=progress2 {SRC_PATH}/ {DST_PATH}/\n",
        "\n",
        "# Verify copy\n",
        "import os\n",
        "if os.path.isdir(DST_PATH):\n",
        "    num_shards = len([f for f in os.listdir(DST_PATH) if f.startswith(\"shard_\")])\n",
        "    print(f\"[cache] Successfully loaded {CACHE_NAME} with {num_shards} shards\")\n",
        "else:\n",
        "    print(f\"[cache] ERROR: Failed to copy {CACHE_NAME}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "NKhv4eaYs4qR",
      "metadata": {
        "id": "NKhv4eaYs4qR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a28b3d7b-eb60-4800-88c2-8459cec2acbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[checkpoint] Copying qwen3_kdqat_cache_q2_4.tgz from Google Drive...\n",
            "\r              0   0%    0.00kB/s    0:00:00 (xfr#0, to-chk=0/1)\n",
            "[checkpoint] Checking tarball structure...\n",
            "qwen3_kdqat_cache_q2_4/\n",
            "qwen3_kdqat_cache_q2_4/special_tokens_map.json\n",
            "qwen3_kdqat_cache_q2_4/loss.csv\n",
            "qwen3_kdqat_cache_q2_4/added_tokens.json\n",
            "qwen3_kdqat_cache_q2_4/tokenizer_config.json\n",
            "[checkpoint] Extracting qwen3_kdqat_cache_q2_4.tgz...\n",
            "[checkpoint] Successfully loaded qwen3_kdqat_cache_q2_4 with 12 files:\n",
            "  - added_tokens.json\n",
            "  - chat_template.jinja\n",
            "  - final_state_dict.pt\n",
            "  - loss.csv\n",
            "  - merges.txt\n",
            "  ... and 7 more\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# LOAD 4-BIT CHECKPOINT FROM GOOGLE DRIVE (for 2-bit initialization)\n",
        "# ============================================================\n",
        "# Copy the 4-bit trained checkpoint to use as starting point for 2-bit training\n",
        "\n",
        "import os\n",
        "\n",
        "# Create runs directory\n",
        "!mkdir -p runs\n",
        "\n",
        "# 4-bit checkpoint to load (best result from 4-bit training)\n",
        "CHECKPOINT_NAME = \"qwen3_kdqat_cache_q2_4\"\n",
        "SRC_PATH = f\"/content/drive/MyDrive/qwen3_runs/{CHECKPOINT_NAME}.tgz\"\n",
        "DST_PATH = f\"runs/{CHECKPOINT_NAME}.tgz\"\n",
        "\n",
        "# Copy from Google Drive\n",
        "print(f\"[checkpoint] Copying {CHECKPOINT_NAME}.tgz from Google Drive...\")\n",
        "!rsync -ah --info=progress2 {SRC_PATH} {DST_PATH}\n",
        "\n",
        "# Check tarball structure first\n",
        "print(f\"[checkpoint] Checking tarball structure...\")\n",
        "!tar -tzf {DST_PATH} | head -5\n",
        "\n",
        "# Extract to runs/ directory (tarball contains folder without runs/ prefix)\n",
        "print(f\"[checkpoint] Extracting {CHECKPOINT_NAME}.tgz...\")\n",
        "!tar -xzf {DST_PATH} -C runs/\n",
        "\n",
        "# Verify extraction\n",
        "if os.path.isdir(f\"runs/{CHECKPOINT_NAME}\"):\n",
        "    files = os.listdir(f\"runs/{CHECKPOINT_NAME}\")\n",
        "    print(f\"[checkpoint] Successfully loaded {CHECKPOINT_NAME} with {len(files)} files:\")\n",
        "    for f in sorted(files)[:5]:\n",
        "        print(f\"  - {f}\")\n",
        "    if len(files) > 5:\n",
        "        print(f\"  ... and {len(files)-5} more\")\n",
        "else:\n",
        "    # Try to find where it extracted\n",
        "    print(f\"[checkpoint] Checking runs/ directory...\")\n",
        "    !ls -la runs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AHSsmdhzsBTo",
      "metadata": {
        "id": "AHSsmdhzsBTo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A7UPbdfK3c2x",
      "metadata": {
        "id": "A7UPbdfK3c2x"
      },
      "outputs": [],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_stream_q2\"\n",
        "\n",
        "# DISABLED --- NOTE used! see #5 for first QAT step\n",
        "# Construct the command string in Python to ensure variable interpolation\n",
        "command_str = f\"\"\"python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --teacher_model_name_or_path {MODEL_NAME} \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --dataset_name allenai/c4 \\\n",
        "  --dataset_config_name en \\\n",
        "  --dataset_split train \\\n",
        "  --dataset_format text \\\n",
        "  --dataset_text_field text \\\n",
        "  --streaming \\\n",
        "  --shuffle_buffer 10000 \\\n",
        "  --output_dir {RUN_DIR} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 1 \\\n",
        "  --gradient_accumulation_steps 16 \\\n",
        "  --learning_rate 5e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 50 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --logging_steps 10 \\\n",
        "  --save_steps 50\"\"\"\n",
        "\n",
        "# Execute the constructed command string\n",
        "!{command_str}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UmgbC-RI3c2x",
      "metadata": {
        "id": "UmgbC-RI3c2x"
      },
      "source": [
        "### (Optional) Resume\n",
        "\n",
        "`--resume_from_checkpoint auto` resolves to `checkpoint_last.pt` if it exists in the output directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mJqdmXSA3c2x",
      "metadata": {
        "id": "mJqdmXSA3c2x"
      },
      "outputs": [],
      "source": [
        "# !python scripts/train_qat.py ... --output_dir {RUN_DIR} --max_steps 500 --resume_from_checkpoint auto\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R7UZBhau3c2x",
      "metadata": {
        "id": "R7UZBhau3c2x"
      },
      "source": [
        "## 5)  KD-cache: precompute teacher top-k + negatives\n",
        "\n",
        "Cache mode is MPS-friendly:\n",
        "- no teacher model during training\n",
        "- no full-vocab logits\n",
        "\n",
        "If you see good KD loss but bad greedy decoding, increase negative coverage (`--rand_neg`) and/or add hard top-1 terms:\n",
        "- `--hard-top1-weight 0.05`\n",
        "- `--hard-full-top1-weight 0.02`–`0.05`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g4s_PzQW3c2x",
      "metadata": {
        "id": "g4s_PzQW3c2x"
      },
      "source": [
        "### KD-cache QAT training\n",
        "\n",
        "This uses cached teacher signals + candidate softmax."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06LaYj0vPIE7",
      "metadata": {
        "id": "06LaYj0vPIE7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1Li8Aysa3c2x",
      "metadata": {
        "id": "1Li8Aysa3c2x"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STAGE 1: KD-QAT (Conservative - freeze MLP/Attention)\n",
        "# ============================================================\n",
        "# First training stage with frozen output layers for stability\n",
        "\n",
        "%pwd\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "CACHE_DIR = CACHE_DIR_CHAT\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2\"\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --output_dir {RUN_DIR_CACHE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 1 \\\n",
        "  --gradient_accumulation_steps 4 \\\n",
        "  --learning_rate 5e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 1000 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 5 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.05 \\\n",
        "  --hard-full-top1-weight 0.03 \\\n",
        "  --ov-freeze \\\n",
        "  --freeze-last-mlp \\\n",
        "  --freeze-last-mlp-layers 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UKgDA-7OuA8m",
      "metadata": {
        "id": "UKgDA-7OuA8m"
      },
      "outputs": [],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --prompt \"What capital city of France is?\" \\\n",
        "  --do_sample true \\\n",
        "  --max_new_tokens 64\n",
        "\n",
        "#  --prompt \"What is Capital of france?\" \\\n",
        "#   --prompt \"What is Apple Neural Engine?\" \\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sexJNkyJyMgv",
      "metadata": {
        "id": "sexJNkyJyMgv"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb19fb17",
      "metadata": {
        "id": "eb19fb17"
      },
      "outputs": [],
      "source": [
        "# Define source and destination paths\n",
        "SOURCE_FILE = \"runs/qwen3_kdqat_cache_q2_2/qat_state_dict.pt\"\n",
        "DEST_DIR_GD = \"/content/drive/MyDrive/runs/Q4B/q2_2/\"\n",
        "\n",
        "# Ensure the destination directory exists on Google Drive\n",
        "!mkdir -p {DEST_DIR_GD}\n",
        "\n",
        "# Copy the file to Google Drive\n",
        "!cp -v {SOURCE_FILE} {DEST_DIR_GD}\n",
        "print(f\"Copied {SOURCE_FILE} to {DEST_DIR_GD}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hbfDaYP5yN6-",
      "metadata": {
        "id": "hbfDaYP5yN6-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JUvQIIDeRUF6",
      "metadata": {
        "id": "JUvQIIDeRUF6"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STAGE 2: KD-QAT (Unfrozen layers, resume from Stage 1)\n",
        "# ============================================================\n",
        "# Continue training with all layers unfrozen\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "CACHE_DIR = CACHE_DIR_CHAT\n",
        "INIT_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2\"\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_DIR_CACHE}/qat_state_dict.pt \\\n",
        "  --output_dir {RUN_DIR_CACHE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 1 \\\n",
        "  --gradient_accumulation_steps 4 \\\n",
        "  --learning_rate 5e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 1000 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --ov-freeze \\\n",
        "  --hard-top1-weight 0.02 \\\n",
        "  --hard-full-top1-weight 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qsfHq1gvtpg-",
      "metadata": {
        "id": "qsfHq1gvtpg-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KXQUTgEJfzl4",
      "metadata": {
        "id": "KXQUTgEJfzl4"
      },
      "outputs": [],
      "source": [
        "# Stage 3 resume KD-QAT with unfrozen attention and relaxed hard-top/full!\n",
        "\n",
        "CACHE_DIR = CACHE_DIR_CHAT  # Use config variable\n",
        "#INIT_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "INIT_DIR_CACHE =  \"runs/progressive_qat_v1\"\n",
        "\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_3\"\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_DIR_CACHE}/qat_state_dict.pt \\\n",
        "  --output_dir {RUN_DIR_CACHE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 128 \\\n",
        "  --gradient_accumulation_steps 1 \\\n",
        "  --learning_rate 5e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 1000 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.00 \\\n",
        "  --hard-full-top1-weight 0.0005\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q1Zjl_WnHZ4O",
      "metadata": {
        "id": "Q1Zjl_WnHZ4O"
      },
      "outputs": [],
      "source": [
        "# Stage 4 resume KD-QAT with unfrozen attention and relaxed hard-top/full!\n",
        "#   --hard-full-top1-weight 0.0000\n",
        "#   learning_rate 2e-6\n",
        "CACHE_DIR = CACHE_DIR_CHAT  # Use config variable\n",
        "INIT_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_3\"\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_4\"\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_DIR_CACHE}/qat_state_dict.pt \\\n",
        "  --output_dir {RUN_DIR_CACHE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 160 \\\n",
        "  --gradient_accumulation_steps 1 \\\n",
        "  --learning_rate 2e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 500 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 5 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.00 \\\n",
        "  --hard-full-top1-weight 0.0000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acak6adahpf",
      "metadata": {
        "id": "acak6adahpf"
      },
      "source": [
        "## 5.5) Progressive Layer-by-Layer QAT (Experimental)\n",
        "\n",
        "This approach trains one layer at a time with:\n",
        "- **Local reconstruction loss**: MSE between quantized and fp MLP outputs\n",
        "- **Global KD loss**: Cached teacher logits\n",
        "- **Prefix quantized / suffix fp**: Earlier layers stay quantized, later layers use full precision\n",
        "\n",
        "### Recommended Training Order (most stable first):\n",
        "\n",
        "1. **E2E f-only** (Option 1): Train ALL f parameters at once\n",
        "   - Skip progressive passes, just run Pass 4\n",
        "   - Most stable, fastest validation\n",
        "   \n",
        "2. **Progressive f-only** (Option 2): Layer-by-layer f-param training\n",
        "   - Uses `--train_f_only` flag\n",
        "   - Disable local loss with `--local_weight 0.0`\n",
        "   \n",
        "3. **Full progressive** (Option 3): Train weights + f per layer\n",
        "   - Most aggressive, may show instability at later layers\n",
        "\n",
        "### GPU Configuration:\n",
        "\n",
        "| GPU | Recommended batch_size |\n",
        "|-----|------------------------|\n",
        "| T4 (15GB) | 2-4 |\n",
        "| V100 (32GB) | 4-8 |\n",
        "| A100 (40GB) | 8-16 |\n",
        "| A100 (80GB) / H100 | 16-32 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "scxwylalctd",
      "metadata": {
        "id": "scxwylalctd"
      },
      "outputs": [],
      "source": [
        "# ---- Progressive QAT Config (2-bit from 4-bit checkpoint) ----\n",
        "# Starting from 4-bit trained checkpoint for better 2-bit initialization\n",
        "\n",
        "# 4-bit checkpoint as initialization (loaded from Google Drive)\n",
        "INIT_CHECKPOINT = \"runs/qwen3_kdqat_cache_q2_4/qat_state_dict.pt\"\n",
        "\n",
        "# Adjust batch_size for your GPU (A100: 8-16, V100: 4-8, T4: 2-4)\n",
        "BATCH_SIZE = 96                # Increase for faster instances (A100/H100)\n",
        "STEPS_PER_LAYER_MLP = 100      # Steps per MLP layer (Pass 1 + Pass 3)\n",
        "STEPS_PER_LAYER_ATTN = 30      # Steps per attention layer (Pass 2)\n",
        "E2E_STEPS = 500                # E2E quantizer tuning steps (Pass 4)\n",
        "LOCAL_WEIGHT = 0.3             # Local reconstruction loss weight\n",
        "GLOBAL_WEIGHT = 1.0            # Global KD loss weight\n",
        "LOCAL_TOKEN_SAMPLES = 128      # Tokens to sample for local loss\n",
        "MAX_GRAD_NORM = 1.0            # Gradient clipping (important for 2-bit)\n",
        "\n",
        "# Learning rates (lower for 2-bit stability)\n",
        "LR_PROGRESSIVE = 2e-6          # Learning rate for progressive passes\n",
        "LR_E2E = 5e-5                  # Learning rate for E2E f-only tuning\n",
        "\n",
        "# Output directories (2-bit versions)\n",
        "RUN_DIR_E2E_FONLY = \"runs/e2e_f_only_q2\"\n",
        "RUN_DIR_PROGRESSIVE_FONLY = \"runs/progressive_f_only_q2\"\n",
        "RUN_DIR_PROGRESSIVE = \"runs/progressive_qat_q2_v1\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q9f32hx0qo",
      "metadata": {
        "id": "q9f32hx0qo"
      },
      "source": [
        "### Option 1: E2E f-only Training (Recommended First)\n",
        "\n",
        "**Most stable approach** - trains ALL `_f_param` (quantization scales) simultaneously.\n",
        "Skip all progressive layer-by-layer passes and go straight to Pass 4.\n",
        "\n",
        "This is recommended when:\n",
        "- Progressive layer-by-layer shows instability (local loss hitting 10.0)\n",
        "- You want to validate the infrastructure works before trying progressive\n",
        "- You have limited time and want the fastest path to a working checkpoint\n",
        "\n",
        "The `f` parameter is the learnable quantization scale from Apple-style quantization:\n",
        "- Actual scale `s = softplus(f)` ensures positivity\n",
        "- Training only `f` keeps weights frozen - more stable for ultra-low-bit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vh3iifkgpj",
      "metadata": {
        "id": "vh3iifkgpj"
      },
      "outputs": [],
      "source": [
        "# E2E f-only: Skip ALL progressive passes, train all f parameters at once\n",
        "# This is the simplest and most stable approach\n",
        "# Starting from 4-bit checkpoint for 2-bit training\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/train_qat_progressive.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_CHECKPOINT} \\\n",
        "  --output_dir {RUN_DIR_E2E_FONLY} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --skip_mlp_pass \\\n",
        "  --skip_attention_pass \\\n",
        "  --skip_mlp_refinement \\\n",
        "  --e2e_steps {E2E_STEPS} \\\n",
        "  --e2e_learning_rate {LR_E2E} \\\n",
        "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vab9x1zlqhs",
      "metadata": {
        "id": "vab9x1zlqhs"
      },
      "source": [
        "### Option 2: Progressive f-only Training\n",
        "\n",
        "Layer-by-layer training but only trains `_f_param` (quantization scales), not weights.\n",
        "More stable than full progressive training, but may still see instability at later layers.\n",
        "\n",
        "Use `--train_f_only` flag to freeze weights and only train quantization scales per layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oqxrx3c2fe",
      "metadata": {
        "id": "oqxrx3c2fe"
      },
      "outputs": [],
      "source": [
        "# Progressive f-only: Layer-by-layer, but only train quantization scales\n",
        "# Use --train_f_only for more stable training\n",
        "# Starting from 4-bit checkpoint for 2-bit training\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/train_qat_progressive.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_CHECKPOINT} \\\n",
        "  --output_dir {RUN_DIR_PROGRESSIVE_FONLY} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --train_f_only \\\n",
        "  --steps_per_layer_mlp {STEPS_PER_LAYER_MLP} \\\n",
        "  --steps_per_layer_attn {STEPS_PER_LAYER_ATTN} \\\n",
        "  --e2e_steps {E2E_STEPS} \\\n",
        "  --local_weight 0.0 \\\n",
        "  --global_weight {GLOBAL_WEIGHT} \\\n",
        "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
        "  --learning_rate {LR_PROGRESSIVE} \\\n",
        "  --e2e_learning_rate {LR_E2E} \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --skip_mlp_refinement"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e0gsxkcc4",
      "metadata": {
        "id": "8e0gsxkcc4"
      },
      "source": [
        "**bold text**### Option 3: Full Progressive Training (weights + f)\n",
        "\n",
        "Full layer-by-layer training with weights and quantization scales.\n",
        "Most aggressive but potentially unstable for ultra-low-bit (2-bit).\n",
        "\n",
        "**Training Order (3-pass v3):**\n",
        "1. **Pass 1**: Train MLP layers (local reconstruction + global KD)\n",
        "2. **Pass 2**: Train attention layers (global KD only)\n",
        "3. **Pass 3**: MLP refinement (addresses MLP-attention coupling)\n",
        "4. **Pass 4**: E2E quantizer-only tuning (f-param only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7qzyw0rd18e",
      "metadata": {
        "id": "7qzyw0rd18e"
      },
      "outputs": [],
      "source": [
        "# Full Progressive: MLP pass + E2E f-only (skip attention/refinement for v1)\n",
        "# For full 3-pass training, remove --skip_attention_pass and --skip_mlp_refinement\n",
        "# Starting from 4-bit checkpoint for 2-bit training\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/train_qat_progressive.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_CHECKPOINT} \\\n",
        "  --output_dir {RUN_DIR_PROGRESSIVE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --steps_per_layer_mlp {STEPS_PER_LAYER_MLP} \\\n",
        "  --e2e_steps {E2E_STEPS} \\\n",
        "  --local_weight {LOCAL_WEIGHT} \\\n",
        "  --global_weight {GLOBAL_WEIGHT} \\\n",
        "  --local_token_samples {LOCAL_TOKEN_SAMPLES} \\\n",
        "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
        "  --learning_rate {LR_PROGRESSIVE} \\\n",
        "  --e2e_learning_rate {LR_E2E} \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --skip_attention_pass \\\n",
        "  --max_layer_repeats 20 \\\n",
        "  --max_backtrack 5 \\\n",
        "  --layer_converge_threshold 0.4 \\\n",
        "  --skip_mlp_refinement"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# v3: Full 3-pass progressive training\n",
        "# MLP -> Attention -> MLP refinement -> E2E f-only\n",
        "# WARNING: May show instability at later layers for 2-bit\n",
        "# Starting from 4-bit checkpoint for 2-bit training\n",
        "\n",
        "RUN_DIR_PROGRESSIVE_V3 = \"runs/progressive_qat_q2_v3\"\n",
        "\n",
        "!python scripts/train_qat_progressive.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_CHECKPOINT} \\\n",
        "  --output_dir {RUN_DIR_PROGRESSIVE_V3} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --steps_per_layer_mlp {STEPS_PER_LAYER_MLP} \\\n",
        "  --steps_per_layer_attn {STEPS_PER_LAYER_ATTN} \\\n",
        "  --e2e_steps {E2E_STEPS} \\\n",
        "  --local_weight {LOCAL_WEIGHT} \\\n",
        "  --global_weight {GLOBAL_WEIGHT} \\\n",
        "  --local_token_samples {LOCAL_TOKEN_SAMPLES} \\\n",
        "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
        "  --learning_rate {LR_PROGRESSIVE} \\\n",
        "  --e2e_learning_rate {LR_E2E} \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head"
      ],
      "metadata": {
        "id": "_qK8LP8WrbSB"
      },
      "id": "_qK8LP8WrbSB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dBFCVCZjwBo_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBFCVCZjwBo_",
        "outputId": "8eaba02b-c9ea-4930-df9b-69a5f75e180d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[archive] Compressing runs/qwen3_kdqat_cache_q2_3...\n",
            "qwen3_kdqat_cache_q2_3/\n",
            "qwen3_kdqat_cache_q2_3/special_tokens_map.json\n",
            "qwen3_kdqat_cache_q2_3/loss.csv\n",
            "qwen3_kdqat_cache_q2_3/added_tokens.json\n",
            "qwen3_kdqat_cache_q2_3/tokenizer_config.json\n",
            "qwen3_kdqat_cache_q2_3/run_state.json\n",
            "qwen3_kdqat_cache_q2_3/merges.txt\n",
            "qwen3_kdqat_cache_q2_3/training_args.json\n",
            "qwen3_kdqat_cache_q2_3/chat_template.jinja\n",
            "qwen3_kdqat_cache_q2_3/vocab.json\n",
            "qwen3_kdqat_cache_q2_3/final_state_dict.pt\n",
            "qwen3_kdqat_cache_q2_3/qat_state_dict.pt\n",
            "qwen3_kdqat_cache_q2_3/tokenizer.json\n",
            "[save] Copying qwen3_kdqat_cache_q2_3.tgz to Google Drive...\n",
            "          1.90G 100%  430.50MB/s    0:00:04 (xfr#1, to-chk=0/1)\n",
            "[save] Saved to Google Drive: 1.77 GB\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# SAVE PROGRESSIVE QAT CHECKPOINT TO GOOGLE DRIVE\n",
        "# ============================================================\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "\n",
        "# Mount Google Drive if not already mounted\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Source directory\n",
        "RUN_NAME = \"progressive_qat_v1\"\n",
        "RUN_NAME = \"qwen3_kdqat_cache_q2_3\"\n",
        "\n",
        "RUN_DIR = f\"runs/{RUN_NAME}\"\n",
        "\n",
        "# Destination on Google Drive\n",
        "DEST_DIR_GD = \"/content/drive/MyDrive/qwen3_runs/\"\n",
        "!mkdir -p {DEST_DIR_GD}\n",
        "\n",
        "# Check if run directory exists and has content\n",
        "if os.path.isdir(RUN_DIR) and os.listdir(RUN_DIR):\n",
        "    # Compress the run directory\n",
        "    print(f\"[archive] Compressing {RUN_DIR}...\")\n",
        "    !tar -zcvf {RUN_NAME}.tgz -C runs {RUN_NAME}\n",
        "\n",
        "    # Copy to Google Drive\n",
        "    print(f\"[save] Copying {RUN_NAME}.tgz to Google Drive...\")\n",
        "    !rsync -ah --info=progress2 {RUN_NAME}.tgz {DEST_DIR_GD}\n",
        "\n",
        "    # Verify\n",
        "    gd_size = os.path.getsize(f\"{DEST_DIR_GD}/{RUN_NAME}.tgz\")\n",
        "    print(f\"[save] Saved to Google Drive: {gd_size / (1024**3):.2f} GB\")\n",
        "\n",
        "    # Cleanup local archive (optional)\n",
        "    # !rm {RUN_NAME}.tgz\n",
        "else:\n",
        "    print(f\"[save] ERROR: {RUN_DIR} is empty or doesn't exist\")\n",
        "    print(\"[save] Run progressive training first\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qiaq60t466e",
      "metadata": {
        "id": "qiaq60t466e"
      },
      "outputs": [],
      "source": [
        "# v3: Full 3-pass progressive training\n",
        "# MLP -> Attention -> MLP refinement -> E2E f-only\n",
        "# WARNING: May show instability at later layers for 2-bit\n",
        "\n",
        "RUN_DIR_PROGRESSIVE_V3 = \"runs/progressive_qat_v3\"\n",
        "\n",
        "!python scripts/train_qat_progressive.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --output_dir {RUN_DIR_PROGRESSIVE_V3} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --steps_per_layer_mlp {STEPS_PER_LAYER_MLP} \\\n",
        "  --steps_per_layer_attn {STEPS_PER_LAYER_ATTN} \\\n",
        "  --e2e_steps {E2E_STEPS} \\\n",
        "  --local_weight {LOCAL_WEIGHT} \\\n",
        "  --global_weight {GLOBAL_WEIGHT} \\\n",
        "  --local_token_samples {LOCAL_TOKEN_SAMPLES} \\\n",
        "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
        "  --learning_rate {LR_PROGRESSIVE} \\\n",
        "  --e2e_learning_rate {LR_E2E} \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ix3xjm2h0xq",
      "metadata": {
        "id": "ix3xjm2h0xq"
      },
      "outputs": [],
      "source": [
        "# Plot per-layer training progress\n",
        "# Change PLOT_RUN to visualize different runs\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Choose which run to visualize\n",
        "PLOT_RUN = RUN_DIR_E2E_FONLY  # or RUN_DIR_PROGRESSIVE, RUN_DIR_PROGRESSIVE_V3\n",
        "\n",
        "csv_path = f\"{PLOT_RUN}/loss_per_layer.csv\"\n",
        "if not os.path.exists(csv_path):\n",
        "    print(f\"Loss CSV not found at {csv_path}\")\n",
        "    print(\"Run training first or check the path.\")\n",
        "else:\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "    # Pass 1: MLP training (local loss)\n",
        "    mlp_df = df[(df['pass'] == 1) & (df['component'] == 'mlp')]\n",
        "    if not mlp_df.empty and 'local' in mlp_df.columns:\n",
        "        for layer in mlp_df['layer'].unique():\n",
        "            layer_df = mlp_df[mlp_df['layer'] == layer]\n",
        "            axes[0, 0].plot(layer_df['step'], layer_df['local'], label=f'L{layer}', alpha=0.7)\n",
        "        axes[0, 0].set_title('Pass 1: MLP Local Loss per Layer')\n",
        "        axes[0, 0].set_xlabel('Step')\n",
        "        axes[0, 0].set_ylabel('Loss')\n",
        "        axes[0, 0].legend(ncol=4, fontsize=6)\n",
        "    else:\n",
        "        axes[0, 0].set_title('Pass 1: MLP Local Loss (skipped or no local loss)')\n",
        "\n",
        "    # Pass 1: MLP global loss\n",
        "    if not mlp_df.empty and 'global' in mlp_df.columns:\n",
        "        for layer in mlp_df['layer'].unique():\n",
        "            layer_df = mlp_df[mlp_df['layer'] == layer]\n",
        "            axes[0, 1].plot(layer_df['step'], layer_df['global'], label=f'L{layer}', alpha=0.7)\n",
        "        axes[0, 1].set_title('Pass 1: MLP Global KD Loss per Layer')\n",
        "        axes[0, 1].set_xlabel('Step')\n",
        "        axes[0, 1].set_ylabel('Loss')\n",
        "        axes[0, 1].legend(ncol=4, fontsize=6)\n",
        "    else:\n",
        "        axes[0, 1].set_title('Pass 1: MLP Global Loss (skipped)')\n",
        "\n",
        "    # Pass 2: Attention training\n",
        "    attn_df = df[(df['pass'] == 2) & (df['component'] == 'attn')]\n",
        "    if not attn_df.empty and 'global' in attn_df.columns:\n",
        "        for layer in attn_df['layer'].unique():\n",
        "            layer_df = attn_df[attn_df['layer'] == layer]\n",
        "            axes[1, 0].plot(layer_df['step'], layer_df['global'], label=f'L{layer}', alpha=0.7)\n",
        "        axes[1, 0].set_title('Pass 2: Attention Global KD Loss per Layer')\n",
        "        axes[1, 0].set_xlabel('Step')\n",
        "        axes[1, 0].set_ylabel('Loss')\n",
        "        axes[1, 0].legend(ncol=4, fontsize=6)\n",
        "    else:\n",
        "        axes[1, 0].set_title('Pass 2: Attention (skipped)')\n",
        "        axes[1, 0].text(0.5, 0.5, 'Not run', ha='center', va='center', transform=axes[1, 0].transAxes)\n",
        "\n",
        "    # Pass 4: E2E f-only tuning\n",
        "    e2e_df = df[(df['pass'] == 4)]\n",
        "    if not e2e_df.empty and 'global' in e2e_df.columns:\n",
        "        axes[1, 1].plot(e2e_df['step'], e2e_df['global'], 'b-', linewidth=2)\n",
        "        axes[1, 1].set_title('Pass 4: E2E f-only Tuning')\n",
        "        axes[1, 1].set_xlabel('Step')\n",
        "        axes[1, 1].set_ylabel('Global KD Loss')\n",
        "    else:\n",
        "        axes[1, 1].set_title('Pass 4: E2E (not yet run)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{PLOT_RUN}/loss_per_layer.png\", dpi=150)\n",
        "    plt.show()\n",
        "    print(f\"Saved to {PLOT_RUN}/loss_per_layer.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qgtw9rd3cr",
      "metadata": {
        "id": "qgtw9rd3cr"
      },
      "source": [
        "### Inference Check: Progressive QAT Results\n",
        "\n",
        "Test the progressive QAT checkpoint with a quick inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4coakmebsik",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4coakmebsik",
        "outputId": "49e3bafc-7fe3-4ed6-cde9-de22e77cb02d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-24 10:27:10.822627: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-24 10:27:10.842835: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766572030.868200  151847 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766572030.873694  151847 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766572030.887622  151847 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572030.887657  151847 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572030.887660  151847 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572030.887663  151847 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 10:27:10.891867: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "user\n",
            "What is the capital of France?\n",
            "assistant\n",
            "<think>\n",
            "\n",
            "</think>\n",
            "\n",
            "The capital of France is **Paris**.\n"
          ]
        }
      ],
      "source": [
        "# Test inference with progressive QAT checkpoint\n",
        "# Change RUN_DIR to test different runs:\n",
        "#   RUN_DIR_E2E_FONLY, RUN_DIR_PROGRESSIVE_FONLY, RUN_DIR_PROGRESSIVE\n",
        "# progressive_qat_v1/qat_state_dict.pt\n",
        "\n",
        "#TEST_RUN = RUN_DIR_E2E_FONLY  # Change this to test other runs\n",
        "\n",
        "TEST_RUN =  \"runs/progressive_qat_v1\"\n",
        "TEST_RUN =  \"runs/qwen3_kdqat_cache_q2_4\"\n",
        "\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {TEST_RUN}/qat_state_dict.pt \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --prompt \"What is the capital of France?\" \\\n",
        "  --do_sample true \\\n",
        "  --max_new_tokens 64"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yq8GHX0o3c2y",
      "metadata": {
        "id": "Yq8GHX0o3c2y"
      },
      "source": [
        "## 6) Stage B: LoRA recovery\n",
        "\n",
        "Two options:\n",
        "- **SFT LoRA** (Alpaca-style instruction tuning)\n",
        "- **Cached KD-LoRA** (preserve teacher distribution; no new “skills”)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B1yimj8aS11y",
      "metadata": {
        "id": "B1yimj8aS11y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LcGFKyxO3c2y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcGFKyxO3c2y",
        "outputId": "b358c6fd-868d-4290-d664-c05e50f0831f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "[device] cuda | amp_dtype=torch.bfloat16 | param_dtype=torch.bfloat16\n",
            "2025-12-24 10:27:43.643798: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-24 10:27:43.663498: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766572063.688940  152049 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766572063.694401  152049 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766572063.707764  152049 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572063.707788  152049 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572063.707791  152049 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572063.707794  152049 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 10:27:43.711890: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[qat] weight_bits=4\n",
            "Loaded QAT checkpoint. missing=0 unexpected=0\n",
            "Enabled LoRA on (196, 20185088) layers. Trainable params: 20,185,088\n",
            "[kd_cache] Note: cache max_length=128 (you passed --max_length=1024). --max_length is ignored in cache mode.\n",
            "[kd_cache] cache topk=32\n",
            "[kd_cache] Enabled cached KD-LoRA. T=2.0 weight=1.0 hard_top1=0.02 hard_full_top1=0.01\n",
            "[loss.csv] Rotated existing loss.csv (last_step=1000) -> loss_prev_20251224_102754.csv\n",
            "opt_step: 100% 1000/1000 [09:39<00:00,  1.72step/s, loss=0.4478, lr=0.00e+00]\n",
            "Done. Saved LoRA adapter to: runs/qwen3_lora_recovery_cached_r32/lora_only_state_dict.pt\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# STAGE B: LoRA Recovery (Cached KD-LoRA)\n",
        "# ============================================================\n",
        "# Train LoRA adapters on top of QAT checkpoint\n",
        "\n",
        "CACHE_DIR = CACHE_DIR_CHAT\n",
        "#RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2\"\n",
        "RUN_DIR_CACHE  = \"runs/progressive_qat_v1\"\n",
        "RUN_DIR_CACHE  = \"runs/qwen3_kdqat_cache_q2_4\"\n",
        "\n",
        "LORA_DIM = 32\n",
        "LORA_RUN_KD = f\"runs/qwen3_lora_recovery_cached_r{LORA_DIM}\"\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/train_lora_recovery.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR_CACHE}/qat_state_dict.pt \\\n",
        "  --output_dir {LORA_RUN_KD} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --gradient_accumulation_steps 2 \\\n",
        "  --learning_rate 1e-5 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 1000 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 2 \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} \\\n",
        "  --lora_alpha {LORA_DIM} \\\n",
        "  --lora_dropout 0.0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.02 \\\n",
        "  --hard-full-top1-weight 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Defule6L3c2y",
      "metadata": {
        "id": "Defule6L3c2y"
      },
      "source": [
        "## 7) Plot loss\n",
        "\n",
        "In Colab, use `--no_show` + `--save` then display the PNG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QM1RNfDh3c2y",
      "metadata": {
        "id": "QM1RNfDh3c2y"
      },
      "outputs": [],
      "source": [
        "!python scripts/plot_loss.py --run_dir {RUN_DIR} --source csv --no_show --save {RUN_DIR}/loss.png\n",
        "from PIL import Image\n",
        "display(Image.open(f\"{RUN_DIR}/loss.png\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p3bOUiMt3c2y",
      "metadata": {
        "id": "p3bOUiMt3c2y"
      },
      "source": [
        "## 8) Inference sanity checks\n",
        "\n",
        "Greedy decode (`--do_sample false`) and keep outputs short (`--max_new_tokens 16`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aA_cJHvT3c2y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aA_cJHvT3c2y",
        "outputId": "be3b0f24-91a2-4d52-e9ec-314d00de7ea7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "2025-12-24 10:39:05.017278: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-24 10:39:05.036974: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766572745.061841  155066 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766572745.067157  155066 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766572745.080472  155066 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572745.080496  155066 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572745.080499  155066 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572745.080502  155066 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 10:39:05.084617: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[lora] loaded adapters for 196 layers from runs/qwen3_lora_recovery_cached_r32/lora_only_state_dict.pt\n",
            "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "user\n",
            "What is capital of France?\n",
            "assistant\n",
            "<think>\n",
            "<think>\n",
            "Okay, I need to figure out the capital of France. Let me start by recalling some basic geography facts. France is a European country, and I know that the capital is Paris\n"
          ]
        }
      ],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_4\"\n",
        "#RUN_DIR = \"runs/progressive_qat_v1\"\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --lora_checkpoint \"runs/qwen3_lora_recovery_cached_r32/lora_only_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
        "  --prompt \"What is capital of France?\" \\\n",
        "  --do_sample false \\\n",
        "  --enable_thinking true \\\n",
        "  --max_new_tokens 40\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EJlu4sOZCNpY",
      "metadata": {
        "id": "EJlu4sOZCNpY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cD985HdXlm0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cD985HdXlm0",
        "outputId": "65053f76-705a-419c-fddb-8ae1cff1fda1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-24 10:41:54.709024: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-24 10:41:54.729231: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766572914.754556  155967 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766572914.759941  155967 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766572914.773878  155967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572914.773902  155967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572914.773905  155967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572914.773908  155967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 10:41:54.777961: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[lora] loaded adapters for 196 layers from runs/qwen3_lora_recovery_cached_r32/lora_only_state_dict.pt\n",
            "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "user\n",
            "Explain how neural networks learn in simple terms\n",
            "assistant\n",
            "<think>\n",
            "<think>\n",
            "Okay, let's start with the basics. Neural networks are like a brain in a computer, right? They process information in layers, just like a human brain does. Each layer has some connections between neurons, and each neuron can process a single input value. \n",
            "\n",
            "So, if we have a simple example, like a simple neural network that takes a single input and outputs a single output, how does it work? Let's imagine a simple example. Let's say the input is a number, say 10. The first layer is the input layer, and the second layer is the output layer. The input layer has a single neuron, and the output layer has a single neuron. \n",
            "\n",
            "The input layer takes the input number and processes it. Then, the output layer takes the result and passes it back. So, the process is: input → output. \n",
            "\n",
            "But how does this work in practice? It's like a simple machine learning model. The input is the data, the output is the result, and the model learns by adjusting the weights and biases to minimize the error. \n",
            "\n",
            "So, in simple terms, neural networks work by taking a set of inputs, applying some operations to them, and then producing a set of outputs. The process is iterative, and the model learns over time to make better predictions or decisions. \n",
            "\n",
            "I think that's a good explanation. Let me make sure it's simple and not too technical. The key is that the neural network is like a computer, and the process of learning is similar to how a human brain learns. \n",
            "\n",
            "I think that's a good explanation. Let me check if I'm missing anything. No, I think I've covered the basics. If there's anything else, I can add more details, but I'm already confident in this explanation.\n",
            "</think>\n",
            "\n",
            "**Explanation of How Neural Networks Learn in Simple Terms:**\n",
            "\n",
            "**1. Introduction:**\n",
            "- **Neural Networks (NNs):** They are like computers in the brain, where each neuron processes information.\n",
            "- **Input Layer:** Takes the input data and processes it.\n",
            "- **Output Layer:** Produces the output, which is the result of the processing.\n",
            "\n",
            "**2. Simple Example:**\n",
            "- **Input:** A single input value, say 10.\n",
            "- **First Layer (Input Layer):** Processes the input, maybe by adding some weights and biases.\n",
            "- **Output Layer (Output Layer):** Passes the result back, with the final output being the result of the processing.\n",
            "\n",
            "**3. How It Works:**\n",
            "- **Processing:** Each neuron in a layer takes the input and processes it.\n",
            "- **Learning:** The network learns to adjust its weights and biases over time to minimize the error.\n",
            "- **Iteration:** The process is repeated over time, and the model improves its performance.\n",
            "\n",
            "**4. Simple Explanation in Words:**\n",
            "- Neural networks are like a computer that learns by doing tasks. The input is the data, the output is the result, and the model learns to make better predictions.\n",
            "\n",
            "**5. Conclusion:**\n",
            "- Neural networks are a type of machine learning model that processes information in layers, just like a human brain does. They are simple, can be used for various tasks, and are very effective in learning.\n",
            "\n",
            "This explanation is simple, visual, and easy to understand.\n"
          ]
        }
      ],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_4\"\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --lora_checkpoint \"runs/qwen3_lora_recovery_cached_r32/lora_only_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
        "  --prompt \"Explain how neural networks learn in simple terms\" \\\n",
        "  --do_sample false \\\n",
        "  --enable_thinking true \\\n",
        "  --max_new_tokens 1024\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xZ6aSDAa3c2y",
      "metadata": {
        "id": "xZ6aSDAa3c2y"
      },
      "outputs": [],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --lora_checkpoint \"runs/qwen3_lora_recovery_cached_r64/lora_only_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
        "  --prompt \"2+2=\" \\\n",
        "  --do_sample false \\\n",
        "  --enable_thinking true \\\n",
        "  --max_new_tokens 90\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZOIo97VlyAbh",
      "metadata": {
        "id": "ZOIo97VlyAbh"
      },
      "outputs": [],
      "source": [
        "LORA_DIM = 64\n",
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2\"\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
        "  --prompt \"What is capital of France?\" \\\n",
        "  --do_sample true \\\n",
        "  --max_new_tokens 64\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ef_Q8P0i3c2y",
      "metadata": {
        "id": "Ef_Q8P0i3c2y"
      },
      "source": [
        "## 9) Optional: snap weights to the exact grid\n",
        "\n",
        "This produces a float checkpoint with weights snapped to the N-bit codebook (not bitpacked)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jk_qSZIs3c2y",
      "metadata": {
        "id": "jk_qSZIs3c2y"
      },
      "outputs": [],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "!python scripts/hard_quantize_checkpoint.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}/checkpoint_last.pt \\\n",
        "  --output_path {RUN_DIR}/hard_quant_full_state_dict.pt \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7HnE7IFS3dK",
      "metadata": {
        "id": "f7HnE7IFS3dK"
      },
      "outputs": [],
      "source": [
        "%cd qwen3_apple_style_2bit_qat_lora\n",
        "%ls -l runs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beQD2eyDWWnb",
      "metadata": {
        "id": "beQD2eyDWWnb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}