{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anemll/qwen3_apple_style_2bit_qat_lora/blob/main/notebooks/Qwen3_QAT_KD_LoRA-per-layer-2bit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rRgC0uK43c2v",
      "metadata": {
        "id": "rRgC0uK43c2v"
      },
      "source": [
        "# Qwen3-0.6B — Apple-style QAT (2-bit / 4-bit) + KD + LoRA recovery\n",
        "\n",
        "This notebook mirrors the structure of common “phone deployment” notebooks, but uses **this repo’s** pipeline:\n",
        "\n",
        "- **Stage A (recommended default):** KD-QAT on plain text (C4 streaming) or KD-cache QAT\n",
        "- **Stage B:** LoRA recovery (either SFT or cached KD-LoRA)\n",
        "- Plot `loss.csv`\n",
        "- Run inference sanity checks\n",
        "\n",
        "Notes:\n",
        "- Qwen3 requires `transformers>=4.51.0`.\n",
        "- For disk usage: C4 is huge; prefer `--streaming` unless you explicitly want to download.\n",
        "- Bitwidth: use `-q 2` (default) or `-q 4` (less aggressive). Checkpoints persist the bitwidth per layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i6Pb9Kki3c2w",
      "metadata": {
        "id": "i6Pb9Kki3c2w"
      },
      "source": [
        "## 0) Setup (Colab / local)\n",
        "\n",
        "If you’re in Colab, clone the repo. If you’re already in the repo directory locally, you can skip this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "UXlLPdtkGM35",
      "metadata": {
        "id": "UXlLPdtkGM35"
      },
      "outputs": [],
      "source": [
        "# ---- Config (edit these) ----\n",
        "#MODEL_NAME = 'Qwen/Qwen3-4B-Thinking-2507'\n",
        "MODEL_NAME = 'Qwen/Qwen3-0.6B'\n",
        "TEACHER_NAME = MODEL_NAME\n",
        "QUANT_BITS = 2  # 2 or 4\n",
        "DEVICE = 'auto'\n",
        "AMP_DTYPE = 'auto'\n",
        "PARAM_DTYPE = 'auto'\n",
        "DTYPE = 'auto'\n",
        "\n",
        "# Cache dirs\n",
        "CACHE_DIR_CHAT = 'caches/alpaca_chat_think_both_L128_K32_R256'\n",
        "CACHE_DIR_TEXT = 'caches/c4_qwen3_L64_K32_R256'\n",
        "#CACHE_DIR_CHAT = 'caches/Q4B_alpaca_chat_think_L128_K32_R256'\n",
        "#CACHE_DIR_TEXT = 'caches/Q4B_c4_qwen3_L64_K32_R256'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "u2vonfu23c2w",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u2vonfu23c2w",
        "outputId": "55d95536-f8e3-4307-8693-05cec37b09db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'qwen3_apple_style_2bit_qat_lora' already exists and is not an empty directory.\n",
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "Already up to date.\n",
            "HEAD is now at a4fd586 Add timing metrics to progressive QAT training\n"
          ]
        }
      ],
      "source": [
        "# Colab-only:\n",
        "%cd /content/\n",
        "!git clone https://github.com/Anemll/qwen3_apple_style_2bit_qat_lora\n",
        "%cd qwen3_apple_style_2bit_qat_lora\n",
        "!git fetch\n",
        "!git pull\n",
        "!git reset --hard HEAD\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZUmiISSL3c2w",
      "metadata": {
        "id": "ZUmiISSL3c2w"
      },
      "source": [
        "## 1) Install dependencies (uv)\n",
        "\n",
        "This repo is set up to work with `uv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ACRjVuVa3c2x",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACRjVuVa3c2x",
        "outputId": "8a59c6dc-87a1-4be7-9b52-61df3af5cf4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/22.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/22.2 MB\u001b[0m \u001b[31m205.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/22.2 MB\u001b[0m \u001b[31m234.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m246.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m246.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m114.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m7 packages\u001b[0m \u001b[2min 137ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m62 packages\u001b[0m \u001b[2min 342ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 797ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 0.98ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mqat-lora\u001b[0m\u001b[2m==0.0.0 (from file:///content/qwen3_apple_style_2bit_qat_lora)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip -q install uv\n",
        "!uv pip install -r requirements.txt\n",
        "!uv pip install -e .\n",
        "# plotting\n",
        "!uv pip install -q matplotlib\n",
        "!uv pip install -q plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yfoLPbBk3c2x",
      "metadata": {
        "id": "yfoLPbBk3c2x"
      },
      "source": [
        "## 2) Optional: Hugging Face login\n",
        "\n",
        "If you hit gated model/dataset errors, log in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wLMWX7E23c2x",
      "metadata": {
        "id": "wLMWX7E23c2x"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()  # paste token when prompted\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fA37ilf33c2x",
      "metadata": {
        "id": "fA37ilf33c2x"
      },
      "source": [
        "## 3) Quick environment check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "VUCXehkU3c2x",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUCXehkU3c2x",
        "outputId": "da600363-4122-4d54-ba48-d6f8f2c73913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch 2.9.0+cu126\n",
            "transformers 4.57.3\n",
            "cuda True\n",
            "mps False\n"
          ]
        }
      ],
      "source": [
        "import torch, transformers\n",
        "print('torch', torch.__version__)\n",
        "print('transformers', transformers.__version__)\n",
        "print('cuda', torch.cuda.is_available())\n",
        "print('mps', torch.backends.mps.is_available())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GCXUYtVC3c2x",
      "metadata": {
        "id": "GCXUYtVC3c2x"
      },
      "source": [
        "## 4) Stage A ((Optional)): KD-QAT on streaming C4\n",
        "\n",
        "This preserves the base model’s behavior under low-bit fake-quant weights.\n",
        "\n",
        "Tips:\n",
        "- Start with a small run (`--max_steps 50`) to validate the pipeline.\n",
        "- Use `-q 4` if 2-bit is too unstable; 4-bit is less aggressive.\n",
        "- On MPS, prefer `--ema_decay 0` for KD-QAT.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jERGktjwjz29",
      "metadata": {
        "id": "jERGktjwjz29"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TpT02cskrs6D",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TpT02cskrs6D",
        "outputId": "5e5648b6-a34d-4ada-85c9-2292e31b316f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[device] cuda | dtype=torch.bfloat16\n",
            "tokenizer_config.json: 9.73kB [00:00, 39.0MB/s]\n",
            "vocab.json: 2.78MB [00:00, 65.7MB/s]\n",
            "merges.txt: 1.67MB [00:00, 132MB/s]\n",
            "tokenizer.json: 100% 11.4M/11.4M [00:00<00:00, 20.4MB/s]\n",
            "config.json: 100% 726/726 [00:00<00:00, 6.09MB/s]\n",
            "2025-12-24 01:22:39.394668: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-24 01:22:39.417464: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766539359.443242    8009 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766539359.449385    8009 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766539359.463433    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766539359.463456    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766539359.463459    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766539359.463461    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 01:22:39.467717: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors: 100% 1.50G/1.50G [00:01<00:00, 935MB/s] \n",
            "generation_config.json: 100% 239/239 [00:00<00:00, 2.03MB/s]\n",
            "README.md: 7.47kB [00:00, 30.6MB/s]\n",
            "data/train-00000-of-00001-a09b74b3ef9c3b(…): 100% 24.2M/24.2M [00:00<00:00, 34.7MB/s]\n",
            "Generating train split: 100% 52002/52002 [00:00<00:00, 336912.65 examples/s]\n",
            "[write] shard_00000.pt | N=512\n",
            "[write] shard_00001.pt | N=512\n",
            "[write] shard_00002.pt | N=512\n",
            "[write] shard_00003.pt | N=512\n",
            "[write] shard_00004.pt | N=512\n",
            "[write] shard_00005.pt | N=512\n",
            "[write] shard_00006.pt | N=512\n",
            "[write] shard_00007.pt | N=512\n",
            "[write] shard_00008.pt | N=512\n",
            "[write] shard_00009.pt | N=512\n",
            "[write] shard_00010.pt | N=512\n",
            "[write] shard_00011.pt | N=512\n",
            "[write] shard_00012.pt | N=512\n",
            "[write] shard_00013.pt | N=512\n",
            "[write] shard_00014.pt | N=512\n",
            "[write] shard_00015.pt | N=512\n",
            "[write] shard_00016.pt | N=512\n",
            "[write] shard_00017.pt | N=512\n",
            "[write] shard_00018.pt | N=512\n",
            "[write] shard_00019.pt | N=512\n",
            "[write] shard_00020.pt | N=512\n",
            "[write] shard_00021.pt | N=512\n",
            "[write] shard_00022.pt | N=512\n",
            "[write] shard_00023.pt | N=512\n",
            "[write] shard_00024.pt | N=512\n",
            "[write] shard_00025.pt | N=512\n",
            "[write] shard_00026.pt | N=512\n",
            "[write] shard_00027.pt | N=512\n",
            "[write] shard_00028.pt | N=512\n",
            "[write] shard_00029.pt | N=512\n",
            "[write] shard_00030.pt | N=512\n",
            "[write] shard_00031.pt | N=512\n",
            "[write] shard_00032.pt | N=512\n",
            "[write] shard_00033.pt | N=512\n",
            "[write] shard_00034.pt | N=512\n",
            "[write] shard_00035.pt | N=512\n",
            "[write] shard_00036.pt | N=512\n",
            "[write] shard_00037.pt | N=512\n",
            "[write] shard_00038.pt | N=512\n",
            "[write] shard_00039.pt | N=32\n",
            "Done. Cached sequences=20000 | out_dir=caches/alpaca_chat_think_L128_K32_R256\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# GENERATE THINKING DATASET (Alpaca chat format)\n",
        "# ============================================================\n",
        "# SKIP THIS CELL if you already have the cache on Google Drive!\n",
        "# Use the \"LOAD FROM GOOGLE DRIVE\" cell instead.\n",
        "\n",
        "!python scripts/precompute_teacher_topk.py \\\n",
        "  --teacher_model_name_or_path {MODEL_NAME} \\\n",
        "  --dataset_name tatsu-lab/alpaca \\\n",
        "  --dataset_split train \\\n",
        "  --dataset_format alpaca_chat \\\n",
        "  --enable_thinking true \\\n",
        "  --max_length 128 \\\n",
        "  --topk 32 \\\n",
        "  --rand_neg 256 \\\n",
        "  --num_sequences 20000 \\\n",
        "  --batch_size 1 \\\n",
        "  --shard_size 512 \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  --output_dir {CACHE_DIR_CHAT}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rlxJ3X8a3c2x",
      "metadata": {
        "id": "rlxJ3X8a3c2x"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# GENERATE TEXT DATASET (C4 streaming)\n",
        "# ============================================================\n",
        "# SKIP THIS CELL if you already have the cache on Google Drive!\n",
        "# Use the \"LOAD FROM GOOGLE DRIVE\" cell instead.\n",
        "\n",
        "import os\n",
        "\n",
        "CACHE_DIR = CACHE_DIR_TEXT\n",
        "\n",
        "if not os.path.isdir(CACHE_DIR):\n",
        "    print(f\"[cache] {CACHE_DIR} not found -> generating cache...\")\n",
        "\n",
        "    !python scripts/precompute_teacher_topk.py \\\n",
        "      --teacher_model_name_or_path {MODEL_NAME} \\\n",
        "      --dataset_name allenai/c4 \\\n",
        "      --dataset_config_name en \\\n",
        "      --dataset_split train \\\n",
        "      --dataset_text_field text \\\n",
        "      --streaming \\\n",
        "      --shuffle_buffer 10000 \\\n",
        "      --max_length 64 \\\n",
        "      --topk 32 \\\n",
        "      --rand_neg 256 \\\n",
        "      --num_sequences 2000 \\\n",
        "      --batch_size 1 \\\n",
        "      --shard_size 512 \\\n",
        "      --device {DEVICE} \\\n",
        "      --dtype {DTYPE} \\\n",
        "      --output_dir {CACHE_DIR}\n",
        "\n",
        "else:\n",
        "    print(f\"[cache] {CACHE_DIR} already exists -> skipping generation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Nt45qTnlKO7a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt45qTnlKO7a",
        "outputId": "be3ace25-b2da-42ec-df47-30b79fc479a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[gzip] Compressing caches/alpaca_chat_think_L128_K32_R256...\n",
            "caches/alpaca_chat_think_L128_K32_R256/\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00018.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00011.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00033.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00031.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00036.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00038.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00030.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00000.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00032.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00010.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00039.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00008.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00005.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00025.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/meta.json\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00015.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00028.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00016.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00009.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00021.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00037.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00001.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00035.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00029.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00003.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00006.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00014.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00007.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00002.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00013.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00024.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00034.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00022.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00020.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00004.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00012.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00019.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00027.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00026.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00017.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00023.pt\n",
            "[gzip] Done: 2.84 GB\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# COMPRESS CHAT CACHE (for Google Drive upload)\n",
        "# ============================================================\n",
        "# SKIP if cache is already compressed or loaded from Google Drive\n",
        "\n",
        "import os\n",
        "\n",
        "if os.path.isdir(CACHE_DIR_CHAT):\n",
        "    print(f\"[gzip] Compressing {CACHE_DIR_CHAT}...\")\n",
        "    !tar -zcvf {CACHE_DIR_CHAT}.tgz {CACHE_DIR_CHAT}\n",
        "    compressed_size = os.path.getsize(f\"{CACHE_DIR_CHAT}.tgz\")\n",
        "    print(f\"[gzip] Done: {compressed_size / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(f\"[gzip] Directory {CACHE_DIR_CHAT} not found. Skipping.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IIIWEkllwGEA",
      "metadata": {
        "collapsed": true,
        "id": "IIIWEkllwGEA"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SAVE CACHED KD DATA TO GOOGLE DRIVE (run after generating cache)\n",
        "# ============================================================\n",
        "# This saves the generated cache to Google Drive for future sessions\n",
        "# Only run this AFTER you've generated the cache with precompute_teacher_topk.py\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create destination directory\n",
        "!mkdir -p /content/drive/MyDrive/qwen3_caches\n",
        "\n",
        "# Choose which cache to save (should match what you generated)\n",
        "CACHE_NAME = \"alpaca_chat_think_both_L128_K32_R256\"\n",
        "\n",
        "import os\n",
        "\n",
        "# Check if cache exists - copy folder directly (no compression needed)\n",
        "if os.path.isdir(f\"caches/{CACHE_NAME}\"):\n",
        "    # Copy folder to Google Drive\n",
        "    print(f\"[save] Copying {CACHE_NAME} to Google Drive...\")\n",
        "    !rsync -ah --info=progress2 caches/{CACHE_NAME}/ /content/drive/MyDrive/qwen3_caches/{CACHE_NAME}/\n",
        "\n",
        "    # Verify\n",
        "    num_shards = len([f for f in os.listdir(f\"/content/drive/MyDrive/qwen3_caches/{CACHE_NAME}\") if f.startswith(\"shard_\")])\n",
        "    print(f\"[save] Saved to Google Drive: {num_shards} shards\")\n",
        "else:\n",
        "    print(f\"[save] ERROR: Cache directory caches/{CACHE_NAME} not found\")\n",
        "    print(\"[save] Run precompute_teacher_topk.py first to generate the cache\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n4snxtz2q6q",
      "metadata": {
        "id": "n4snxtz2q6q"
      },
      "source": [
        "## 4.5) Google Drive Cache Management\n",
        "\n",
        "**Workflow for KD Cache:**\n",
        "\n",
        "1. **First time setup** (slow):\n",
        "   - Run `precompute_teacher_topk.py` to generate cache\n",
        "   - Run \"SAVE TO GOOGLE DRIVE\" cell to persist\n",
        "   \n",
        "2. **Subsequent sessions** (fast):\n",
        "   - Run \"LOAD FROM GOOGLE DRIVE\" cell to restore cache\n",
        "   - Skip cache generation step\n",
        "\n",
        "The cached KD data (~2-3 GB compressed) contains precomputed teacher logits for knowledge distillation training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7745eb2",
      "metadata": {
        "id": "d7745eb2"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# COMPRESS TEXT CACHE (for Google Drive upload)\n",
        "# ============================================================\n",
        "# SKIP if cache is already compressed or loaded from Google Drive\n",
        "\n",
        "import os\n",
        "\n",
        "if os.path.isdir(CACHE_DIR_TEXT):\n",
        "    print(f\"[gzip] Compressing {CACHE_DIR_TEXT}...\")\n",
        "    !tar -zcvf {CACHE_DIR_TEXT}.tgz {CACHE_DIR_TEXT}\n",
        "    compressed_size = os.path.getsize(f\"{CACHE_DIR_TEXT}.tgz\")\n",
        "    print(f\"[gzip] Done: {compressed_size / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(f\"[gzip] Directory {CACHE_DIR_TEXT} not found. Skipping.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (!!) LOAD 128 KD CACHE FROM GOOGLE DRIVE"
      ],
      "metadata": {
        "id": "_mFprGAwPBB3"
      },
      "id": "_mFprGAwPBB3"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# LOAD KD CACHE FROM GOOGLE DRIVE\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "\n",
        "# Try K=64 first, fall back to K=32\n",
        "CACHE_OPTIONS = [\n",
        "    'alpaca_chat_think_both_L128_K64_R512.tgz',\n",
        "    #'alpaca_chat_think_both_L128_K128_R512.tgz',\n",
        "    #'alpaca_chat_think_both_L128_K32_R256',\n",
        "]\n",
        "\n",
        "GD_CACHE_DIR = '/content/drive/MyDrive/qwen3_caches'\n",
        "\n",
        "cache_loaded = False\n",
        "for cache_name in CACHE_OPTIONS:\n",
        "    gd_cache_path = f\"{GD_CACHE_DIR}/{cache_name}\"\n",
        "\n",
        "    # Determine the directory name without the .tgz extension\n",
        "    # This will be the name of the directory created in 'caches/'\n",
        "    target_dir_name = cache_name.replace('.tgz', '')\n",
        "    local_cache_path = f'caches/{target_dir_name}'\n",
        "\n",
        "    if os.path.exists(gd_cache_path):\n",
        "        if cache_name.endswith('.tgz'):\n",
        "            print(f\"[cache] Found archive {cache_name}, extracting...\")\n",
        "            !mkdir -p caches # Ensure the 'caches' parent directory exists\n",
        "            !tar -xzf {gd_cache_path} -C caches/ # Extract into 'caches/'\n",
        "            CACHE_DIR_CHAT = local_cache_path\n",
        "            cache_loaded = True\n",
        "            break\n",
        "        elif os.path.isdir(gd_cache_path):\n",
        "            print(f\"[cache] Found directory {cache_name}, copying...\")\n",
        "            !mkdir -p caches\n",
        "            !rsync -ah --info=progress2 {gd_cache_path}/ {local_cache_path}/\n",
        "            CACHE_DIR_CHAT = local_cache_path\n",
        "            cache_loaded = True\n",
        "            break\n",
        "\n",
        "if cache_loaded:\n",
        "    print(f\"[cache] Using: {CACHE_DIR_CHAT}\")\n",
        "else:\n",
        "    print(\"[cache] ERROR: No KD cache found. Generate one first using Generate_KD_Cache_K64_K128.ipynb\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n69REDdkO2ee",
        "outputId": "e6ce8a17-ff90-4c44-f114-b70875ce684b"
      },
      "id": "n69REDdkO2ee",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[cache] Found archive alpaca_chat_think_both_L128_K64_R512.tgz, extracting...\n",
            "[cache] Using: caches/alpaca_chat_think_both_L128_K64_R512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (!!) LOAD CACHED KD DATA FROM GOOGLE DRIVE\n"
      ],
      "metadata": {
        "id": "JhOpTWn8hthM"
      },
      "id": "JhOpTWn8hthM"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "vlhuS4N9GbN4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlhuS4N9GbN4",
        "outputId": "7dc650d2-14bf-47c5-9c44-4a6c6269530d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[cache] Copying alpaca_chat_think_both_L128_K32_R256 from Google Drive...\n",
            "              0   0%    0.00kB/s    0:00:00 (xfr#0, to-chk=0/42)\n",
            "[cache] Successfully loaded alpaca_chat_think_both_L128_K32_R256 with 40 shards\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# LOAD CACHED KD DATA FROM GOOGLE DRIVE (run this cell first!)\n",
        "# ============================================================\n",
        "# Mount Google Drive and copy cached KD data back to local storage\n",
        "# This avoids regenerating the cache every session\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create local cache directory\n",
        "!mkdir -p caches\n",
        "\n",
        "# Cache folder to load (copy folder directly, no .tgz)\n",
        "CACHE_NAME = \"alpaca_chat_think_both_L128_K32_R256\"\n",
        "\n",
        "# Copy folder directly from Google Drive\n",
        "SRC_PATH = f\"/content/drive/MyDrive/qwen3_caches/{CACHE_NAME}\"\n",
        "DST_PATH = f\"caches/{CACHE_NAME}\"\n",
        "\n",
        "print(f\"[cache] Copying {CACHE_NAME} from Google Drive...\")\n",
        "!rsync -ah --info=progress2 {SRC_PATH}/ {DST_PATH}/\n",
        "\n",
        "# Verify copy\n",
        "import os\n",
        "if os.path.isdir(DST_PATH):\n",
        "    num_shards = len([f for f in os.listdir(DST_PATH) if f.startswith(\"shard_\")])\n",
        "    print(f\"[cache] Successfully loaded {CACHE_NAME} with {num_shards} shards\")\n",
        "else:\n",
        "    print(f\"[cache] ERROR: Failed to copy {CACHE_NAME}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NKhv4eaYs4qR",
      "metadata": {
        "id": "NKhv4eaYs4qR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a28b3d7b-eb60-4800-88c2-8459cec2acbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[checkpoint] Copying qwen3_kdqat_cache_q2_4.tgz from Google Drive...\n",
            "\r              0   0%    0.00kB/s    0:00:00 (xfr#0, to-chk=0/1)\n",
            "[checkpoint] Checking tarball structure...\n",
            "qwen3_kdqat_cache_q2_4/\n",
            "qwen3_kdqat_cache_q2_4/special_tokens_map.json\n",
            "qwen3_kdqat_cache_q2_4/loss.csv\n",
            "qwen3_kdqat_cache_q2_4/added_tokens.json\n",
            "qwen3_kdqat_cache_q2_4/tokenizer_config.json\n",
            "[checkpoint] Extracting qwen3_kdqat_cache_q2_4.tgz...\n",
            "[checkpoint] Successfully loaded qwen3_kdqat_cache_q2_4 with 12 files:\n",
            "  - added_tokens.json\n",
            "  - chat_template.jinja\n",
            "  - final_state_dict.pt\n",
            "  - loss.csv\n",
            "  - merges.txt\n",
            "  ... and 7 more\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# LOAD 4-BIT CHECKPOINT FROM GOOGLE DRIVE (for 2-bit initialization)\n",
        "# ============================================================\n",
        "# Copy the 4-bit trained checkpoint to use as starting point for 2-bit training\n",
        "\n",
        "import os\n",
        "\n",
        "# Create runs directory\n",
        "!mkdir -p runs\n",
        "\n",
        "# 4-bit checkpoint to load (best result from 4-bit training)\n",
        "CHECKPOINT_NAME = \"qwen3_kdqat_cache_q2_4\"\n",
        "SRC_PATH = f\"/content/drive/MyDrive/qwen3_runs/{CHECKPOINT_NAME}.tgz\"\n",
        "DST_PATH = f\"runs/{CHECKPOINT_NAME}.tgz\"\n",
        "\n",
        "# Copy from Google Drive\n",
        "print(f\"[checkpoint] Copying {CHECKPOINT_NAME}.tgz from Google Drive...\")\n",
        "!rsync -ah --info=progress2 {SRC_PATH} {DST_PATH}\n",
        "\n",
        "# Check tarball structure first\n",
        "print(f\"[checkpoint] Checking tarball structure...\")\n",
        "!tar -tzf {DST_PATH} | head -5\n",
        "\n",
        "# Extract to runs/ directory (tarball contains folder without runs/ prefix)\n",
        "print(f\"[checkpoint] Extracting {CHECKPOINT_NAME}.tgz...\")\n",
        "!tar -xzf {DST_PATH} -C runs/\n",
        "\n",
        "# Verify extraction\n",
        "if os.path.isdir(f\"runs/{CHECKPOINT_NAME}\"):\n",
        "    files = os.listdir(f\"runs/{CHECKPOINT_NAME}\")\n",
        "    print(f\"[checkpoint] Successfully loaded {CHECKPOINT_NAME} with {len(files)} files:\")\n",
        "    for f in sorted(files)[:5]:\n",
        "        print(f\"  - {f}\")\n",
        "    if len(files) > 5:\n",
        "        print(f\"  ... and {len(files)-5} more\")\n",
        "else:\n",
        "    # Try to find where it extracted\n",
        "    print(f\"[checkpoint] Checking runs/ directory...\")\n",
        "    !ls -la runs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AHSsmdhzsBTo",
      "metadata": {
        "id": "AHSsmdhzsBTo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A7UPbdfK3c2x",
      "metadata": {
        "id": "A7UPbdfK3c2x"
      },
      "outputs": [],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_stream_q2\"\n",
        "\n",
        "# DISABLED --- NOTE used! see #5 for first QAT step\n",
        "# Construct the command string in Python to ensure variable interpolation\n",
        "command_str = f\"\"\"python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --teacher_model_name_or_path {MODEL_NAME} \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --dataset_name allenai/c4 \\\n",
        "  --dataset_config_name en \\\n",
        "  --dataset_split train \\\n",
        "  --dataset_format text \\\n",
        "  --dataset_text_field text \\\n",
        "  --streaming \\\n",
        "  --shuffle_buffer 10000 \\\n",
        "  --output_dir {RUN_DIR} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 1 \\\n",
        "  --gradient_accumulation_steps 16 \\\n",
        "  --learning_rate 5e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 50 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --logging_steps 10 \\\n",
        "  --save_steps 50\"\"\"\n",
        "\n",
        "# Execute the constructed command string\n",
        "!{command_str}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UmgbC-RI3c2x",
      "metadata": {
        "id": "UmgbC-RI3c2x"
      },
      "source": [
        "### (Optional) Resume\n",
        "\n",
        "`--resume_from_checkpoint auto` resolves to `checkpoint_last.pt` if it exists in the output directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mJqdmXSA3c2x",
      "metadata": {
        "id": "mJqdmXSA3c2x"
      },
      "outputs": [],
      "source": [
        "# !python scripts/train_qat.py ... --output_dir {RUN_DIR} --max_steps 500 --resume_from_checkpoint auto\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R7UZBhau3c2x",
      "metadata": {
        "id": "R7UZBhau3c2x"
      },
      "source": [
        "## 5)  KD-cache: precompute teacher top-k + negatives\n",
        "\n",
        "Cache mode is MPS-friendly:\n",
        "- no teacher model during training\n",
        "- no full-vocab logits\n",
        "\n",
        "If you see good KD loss but bad greedy decoding, increase negative coverage (`--rand_neg`) and/or add hard top-1 terms:\n",
        "- `--hard-top1-weight 0.05`\n",
        "- `--hard-full-top1-weight 0.02`–`0.05`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g4s_PzQW3c2x",
      "metadata": {
        "id": "g4s_PzQW3c2x"
      },
      "source": [
        "### KD-cache QAT training\n",
        "\n",
        "This uses cached teacher signals + candidate softmax."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m7q7F2_jgN6Z"
      },
      "id": "m7q7F2_jgN6Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "sexJNkyJyMgv",
      "metadata": {
        "id": "sexJNkyJyMgv"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb19fb17",
      "metadata": {
        "id": "eb19fb17"
      },
      "outputs": [],
      "source": [
        "# Define source and destination paths\n",
        "SOURCE_FILE = \"runs/qwen3_kdqat_cache_q2_2/qat_state_dict.pt\"\n",
        "DEST_DIR_GD = \"/content/drive/MyDrive/runs/Q4B/q2_2/\"\n",
        "\n",
        "# Ensure the destination directory exists on Google Drive\n",
        "!mkdir -p {DEST_DIR_GD}\n",
        "\n",
        "# Copy the file to Google Drive\n",
        "!cp -v {SOURCE_FILE} {DEST_DIR_GD}\n",
        "print(f\"Copied {SOURCE_FILE} to {DEST_DIR_GD}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hbfDaYP5yN6-",
      "metadata": {
        "id": "hbfDaYP5yN6-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (!!!!)Stage 3 resume KD-QAT with\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "GJ7WaTFCgRHN"
      },
      "id": "GJ7WaTFCgRHN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KXQUTgEJfzl4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXQUTgEJfzl4",
        "outputId": "4e7def09-96ca-4b06-dbf2-c9ff086a69ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-25 20:53:31.639885: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-25 20:53:31.656162: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766696011.677584   78223 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766696011.684489   78223 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766696011.701241   78223 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766696011.701266   78223 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766696011.701269   78223 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766696011.701272   78223 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-25 20:53:31.706221: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[device] cuda | amp_dtype=torch.bfloat16 | param_dtype=torch.bfloat16\n",
            "[qat] weight_bits=2\n",
            "[init] loading model state from runs/progressive_qat_q2_v1_fresh/qat_state_dict.pt\n",
            "[init] model weights loaded.\n",
            "[kd-cache] cache topk=32\n",
            "[kd-cache] dir=caches/alpaca_chat_think_both_L128_K32_R256 | weight=1.0 | T=2.0 | hard_top1=0.0 | hard_full_top1=0.0005\n",
            "opt_step:  41% 1227/3000 [21:53<31:29,  1.07s/step, loss=1.4682, lr=2.97e-06]"
          ]
        }
      ],
      "source": [
        "# Stage 3 resume KD-QAT with unfrozen attention and relaxed hard-top/full!\n",
        "\n",
        "CACHE_DIR_CHAT = 'caches/alpaca_chat_think_both_L128_K32_R256'\n",
        "CACHE_DIR = CACHE_DIR_CHAT  # Use config variable\n",
        "\n",
        "INIT_DIR_CACHE =  \"runs/progressive_qat_q2_v1_fresh\"\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_3_fresh\"\n",
        "\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_DIR_CACHE}/qat_state_dict.pt \\\n",
        "  --output_dir {RUN_DIR_CACHE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 128 \\\n",
        "  --gradient_accumulation_steps 1 \\\n",
        "  --learning_rate 5e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 3000 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.00 \\\n",
        "  --hard-full-top1-weight 0.0005\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#save run\n"
      ],
      "metadata": {
        "id": "ESr5L_EStT6y"
      },
      "id": "ESr5L_EStT6y"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# SAVE PROGRESSIVE QAT CHECKPOINT TO GOOGLE DRIVE\n",
        "# ============================================================\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive if not already mounted\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Source directory (matches RUN_DIR_PROGRESSIVE from config)\n",
        "RUN_NAME = \"qwen3_kdqat_cache_q2_3\"\n",
        "RUN_DIR = f\"runs/{RUN_NAME}\"\n",
        "\n",
        "# Destination on Google Drive\n",
        "DEST_DIR_GD = \"/content/drive/MyDrive/qwen3_runs/\"\n",
        "!mkdir -p {DEST_DIR_GD}\n",
        "\n",
        "# Check if run directory exists and has content\n",
        "if os.path.isdir(RUN_DIR) and os.listdir(RUN_DIR):\n",
        "    # Compress the run directory\n",
        "    print(f\"[archive] Compressing {RUN_DIR}...\")\n",
        "    !tar -zcvf {RUN_NAME}.tgz -C runs {RUN_NAME}\n",
        "\n",
        "    # Copy to Google Drive\n",
        "    print(f\"[save] Copying {RUN_NAME}.tgz to Google Drive...\")\n",
        "    !rsync -ah --info=progress2 {RUN_NAME}.tgz {DEST_DIR_GD}\n",
        "\n",
        "    # Verify\n",
        "    gd_size = os.path.getsize(f\"{DEST_DIR_GD}/{RUN_NAME}.tgz\")\n",
        "    print(f\"[save] Saved to Google Drive: {gd_size / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(f\"[save] ERROR: {RUN_DIR} is empty or doesn't exist\")\n",
        "\n",
        "# got to \"Stage 3 resume\" to continue distill treaining"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPk8pkD3tUeu",
        "outputId": "6e0433c4-8709-4eda-8aec-d27443c92fc6"
      },
      "id": "RPk8pkD3tUeu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[archive] Compressing runs/qwen3_kdqat_cache_q2_3...\n",
            "qwen3_kdqat_cache_q2_3/\n",
            "qwen3_kdqat_cache_q2_3/special_tokens_map.json\n",
            "qwen3_kdqat_cache_q2_3/loss.csv\n",
            "qwen3_kdqat_cache_q2_3/added_tokens.json\n",
            "qwen3_kdqat_cache_q2_3/tokenizer_config.json\n",
            "qwen3_kdqat_cache_q2_3/run_state.json\n",
            "qwen3_kdqat_cache_q2_3/merges.txt\n",
            "qwen3_kdqat_cache_q2_3/training_args.json\n",
            "qwen3_kdqat_cache_q2_3/chat_template.jinja\n",
            "qwen3_kdqat_cache_q2_3/vocab.json\n",
            "qwen3_kdqat_cache_q2_3/final_state_dict.pt\n",
            "qwen3_kdqat_cache_q2_3/qat_state_dict.pt\n",
            "qwen3_kdqat_cache_q2_3/checkpoint_step3000.pt\n",
            "qwen3_kdqat_cache_q2_3/checkpoint_last.pt\n",
            "qwen3_kdqat_cache_q2_3/tokenizer.json\n",
            "[save] Copying qwen3_kdqat_cache_q2_3.tgz to Google Drive...\n",
            "          7.46G 100%  332.14MB/s    0:00:21 (xfr#1, to-chk=0/1)\n",
            "[save] Saved to Google Drive: 6.95 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "U5THSkU3oo2V"
      },
      "id": "U5THSkU3oo2V"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28cb12c7"
      },
      "source": [
        "#### Pull and Unzip Progressive QAT Checkpoint"
      ],
      "id": "28cb12c7"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the checkpoint name to pull\n",
        "RUN_NAME = \"qwen3_kdqat_cache_q2_3\"\n",
        "\n",
        "# Source path on Google Drive\n",
        "SRC_PATH_GD = f\"/content/drive/MyDrive/qwen3_runs/{RUN_NAME}.tgz\"\n",
        "# Destination path locally\n",
        "DST_PATH_LOCAL = f\"{RUN_NAME}.tgz\"\n",
        "\n",
        "# Create runs directory if it doesn't exist\n",
        "!mkdir -p runs\n",
        "\n",
        "print(f\"[pull] Copying {RUN_NAME}.tgz from Google Drive...\")\n",
        "!rsync -ah --info=progress2 {SRC_PATH_GD} {DST_PATH_LOCAL}\n",
        "\n",
        "# Check if the tarball was copied successfully\n",
        "if os.path.exists(DST_PATH_LOCAL):\n",
        "    print(f\"[pull] Extracting {RUN_NAME}.tgz...\")\n",
        "    !tar -xzf {DST_PATH_LOCAL} -C runs/\n",
        "    print(f\"[pull] Successfully extracted to runs/{RUN_NAME}\")\n",
        "\n",
        "    # Optionally, remove the tarball after extraction to save space\n",
        "    # !rm {DST_PATH_LOCAL}\n",
        "else:\n",
        "    print(f\"[pull] ERROR: {RUN_NAME}.tgz not found on Google Drive. Make sure it was saved correctly.\")\n",
        "\n",
        "# Use the path where the checkpoint was unzipped to the inference check code cell\n",
        "#(4coakmebsik). This comment clarifies that the TEST_RUN variable should point to the d\n",
        "#irectory where the QAT checkpoint was extracted for inference."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVx16qzAodl2",
        "outputId": "ae861164-dfef-41b1-aec9-24e6d1005dcd"
      },
      "id": "uVx16qzAodl2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[pull] Copying qwen3_kdqat_cache_q2_3.tgz from Google Drive...\n",
            "          7.46G 100%  136.13MB/s    0:00:52 (xfr#1, to-chk=0/1)\n",
            "[pull] Extracting qwen3_kdqat_cache_q2_3.tgz...\n",
            "[pull] Successfully extracted to runs/qwen3_kdqat_cache_q2_3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q1Zjl_WnHZ4O",
      "metadata": {
        "id": "Q1Zjl_WnHZ4O"
      },
      "outputs": [],
      "source": [
        "# Stage 4 resume KD-QAT with unfrozen attention and relaxed hard-top/full!\n",
        "#   --hard-full-top1-weight 0.0000\n",
        "#   learning_rate 2e-6\n",
        "CACHE_DIR = CACHE_DIR_CHAT  # Use config variable\n",
        "INIT_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_3\"\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_4\"\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_DIR_CACHE}/qat_state_dict.pt \\\n",
        "  --output_dir {RUN_DIR_CACHE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 160 \\\n",
        "  --gradient_accumulation_steps 1 \\\n",
        "  --learning_rate 2e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 500 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 5 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.00 \\\n",
        "  --hard-full-top1-weight 0.0000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acak6adahpf",
      "metadata": {
        "id": "acak6adahpf"
      },
      "source": [
        "## 5.5) Progressive Layer-by-Layer QAT (Experimental)\n",
        "\n",
        "This approach trains one layer at a time with:\n",
        "- **Local reconstruction loss**: MSE between quantized and fp MLP outputs\n",
        "- **Global KD loss**: Cached teacher logits\n",
        "- **Prefix quantized / suffix fp**: Earlier layers stay quantized, later layers use full precision\n",
        "\n",
        "### Recommended Training Order (most stable first):\n",
        "\n",
        "1. **E2E f-only** (Option 1): Train ALL f parameters at once\n",
        "   - Skip progressive passes, just run Pass 4\n",
        "   - Most stable, fastest validation\n",
        "   \n",
        "2. **Progressive f-only** (Option 2): Layer-by-layer f-param training\n",
        "   - Uses `--train_f_only` flag\n",
        "   - Disable local loss with `--local_weight 0.0`\n",
        "   \n",
        "3. **Full progressive** (Option 3): Train weights + f per layer\n",
        "   - Most aggressive, may show instability at later layers\n",
        "\n",
        "### GPU Configuration:\n",
        "\n",
        "| GPU | Recommended batch_size |\n",
        "|-----|------------------------|\n",
        "| T4 (15GB) | 2-4 |\n",
        "| V100 (32GB) | 4-8 |\n",
        "| A100 (40GB) | 8-16 |\n",
        "| A100 (80GB) / H100 | 16-32 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "scxwylalctd",
      "metadata": {
        "id": "scxwylalctd"
      },
      "outputs": [],
      "source": [
        "# ---- Progressive QAT Config (2-bit from 4-bit checkpoint) ----\n",
        "# Starting from 4-bit trained checkpoint for better 2-bit initialization\n",
        "\n",
        "# 4-bit checkpoint as initialization (loaded from Google Drive)\n",
        "#INIT_CHECKPOINT = \"runs/qwen3_kdqat_cache_q2_4/qat_state_dict.pt\"\n",
        "\n",
        "# Adjust batch_size for your GPU (A100: 8-16, V100: 4-8, T4: 2-4)\n",
        "BATCH_SIZE = 96                # Increase for faster instances (A100/H100)\n",
        "STEPS_PER_LAYER_MLP = 100      # Steps per MLP layer (Pass 1 + Pass 3)\n",
        "STEPS_PER_LAYER_ATTN = 30      # Steps per attention layer (Pass 2)\n",
        "E2E_STEPS = 500                # E2E quantizer tuning steps (Pass 4)\n",
        "LOCAL_WEIGHT = 0.3             # Local reconstruction loss weight\n",
        "GLOBAL_WEIGHT = 1.0            # Global KD loss weight\n",
        "LOCAL_TOKEN_SAMPLES = 128      # Tokens to sample for local loss\n",
        "MAX_GRAD_NORM = 1.0            # Gradient clipping (important for 2-bit)\n",
        "\n",
        "# Learning rates (lower for 2-bit stability)\n",
        "LR_PROGRESSIVE = 2e-6          # Learning rate for progressive passes\n",
        "LR_E2E = 5e-5                  # Learning rate for E2E f-only tuning\n",
        "\n",
        "# Output directories (2-bit versions)\n",
        "RUN_DIR_E2E_FONLY = \"runs/e2e_f_only_q2\"\n",
        "RUN_DIR_PROGRESSIVE_FONLY = \"runs/progressive_f_only_q2\"\n",
        "RUN_DIR_PROGRESSIVE = \"runs/progressive_qat_q2_v1\"\n",
        "RUN_DIR_PROGRESSIVE_FRESH = \"runs/progressive_qat_q2_v1_fresh\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q9f32hx0qo",
      "metadata": {
        "id": "q9f32hx0qo"
      },
      "source": [
        "### Option 1: E2E f-only Training (Recommended First)\n",
        "\n",
        "**Most stable approach** - trains ALL `_f_param` (quantization scales) simultaneously.\n",
        "Skip all progressive layer-by-layer passes and go straight to Pass 4.\n",
        "\n",
        "This is recommended when:\n",
        "- Progressive layer-by-layer shows instability (local loss hitting 10.0)\n",
        "- You want to validate the infrastructure works before trying progressive\n",
        "- You have limited time and want the fastest path to a working checkpoint\n",
        "\n",
        "The `f` parameter is the learnable quantization scale from Apple-style quantization:\n",
        "- Actual scale `s = softplus(f)` ensures positivity\n",
        "- Training only `f` keeps weights frozen - more stable for ultra-low-bit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vh3iifkgpj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vh3iifkgpj",
        "outputId": "c0ebe461-dfaa-44e1-a38e-6c5fdb2c9a87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "[device] cuda | amp_dtype=torch.bfloat16 | param_dtype=torch.bfloat16\n",
            "tokenizer_config.json: 9.73kB [00:00, 38.5MB/s]\n",
            "vocab.json: 2.78MB [00:00, 52.2MB/s]\n",
            "merges.txt: 1.67MB [00:00, 139MB/s]\n",
            "tokenizer.json: 100% 11.4M/11.4M [00:01<00:00, 10.2MB/s]\n",
            "[model] Loading Qwen/Qwen3-0.6B\n",
            "config.json: 100% 726/726 [00:00<00:00, 8.23MB/s]\n",
            "2025-12-24 18:47:22.719539: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-24 18:47:22.740144: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766602042.765073    9837 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766602042.770583    9837 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766602042.784760    9837 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766602042.784783    9837 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766602042.784786    9837 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766602042.784789    9837 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 18:47:22.789000: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors: 100% 1.50G/1.50G [00:02<00:00, 570MB/s]\n",
            "generation_config.json: 100% 239/239 [00:00<00:00, 2.04MB/s]\n",
            "[init] Loading model state from runs/qwen3_kdqat_cache_q2_4/qat_state_dict.pt\n",
            "[qat] weight_bits=2\n",
            "[kd-cache] Loading from caches/alpaca_chat_think_both_L128_K32_R256\n",
            "[model] 28 transformer layers\n",
            "[training] batch_size=96 steps_per_mlp=50 e2e_steps=500\n",
            "\n",
            "============================================================\n",
            "PASS 4: E2E Quantizer Tuning (f-only)\n",
            "============================================================\n",
            "  Trainable f parameters: 196\n",
            "  step 0: loss=12.1407\n",
            "  step 10: loss=11.5678\n",
            "  step 20: loss=11.8305\n",
            "  step 30: loss=11.9946\n",
            "  step 40: loss=11.7304\n",
            "  step 50: loss=11.8770\n",
            "  step 60: loss=12.0282\n",
            "  step 70: loss=11.6242\n",
            "  step 80: loss=11.5261\n",
            "  step 90: loss=11.6720\n",
            "  step 100: loss=12.3927\n",
            "  step 110: loss=11.5418\n",
            "  step 120: loss=12.0243\n",
            "  step 130: loss=11.7296\n",
            "  step 140: loss=12.1050\n",
            "  step 150: loss=11.6462\n",
            "  step 160: loss=11.9153\n",
            "  step 170: loss=11.9587\n",
            "  step 180: loss=11.9907\n",
            "  step 190: loss=11.9216\n",
            "  step 200: loss=12.2080\n",
            "  step 210: loss=12.0106\n",
            "  step 220: loss=11.6917\n",
            "  step 230: loss=12.0309\n",
            "  step 240: loss=11.9478\n",
            "  step 250: loss=11.7766\n",
            "  step 260: loss=11.9367\n",
            "  step 270: loss=11.8300\n",
            "  step 280: loss=12.2159\n",
            "  step 290: loss=11.9723\n",
            "  step 300: loss=11.9617\n",
            "  step 310: loss=11.8662\n",
            "  step 320: loss=11.9585\n",
            "  step 330: loss=11.6636\n",
            "  step 340: loss=11.9637\n",
            "  step 350: loss=11.8636\n",
            "  step 360: loss=11.8626\n",
            "  step 370: loss=12.0558\n",
            "  step 380: loss=11.7337\n",
            "  step 390: loss=11.9441\n",
            "  step 400: loss=11.9119\n",
            "  step 410: loss=12.0105\n",
            "  step 420: loss=11.6374\n",
            "  step 430: loss=11.8217\n",
            "  step 440: loss=11.6547\n",
            "  step 450: loss=11.7303\n",
            "  step 460: loss=11.8293\n",
            "  step 470: loss=11.6669\n",
            "  step 480: loss=11.8896\n",
            "  step 490: loss=11.7591\n",
            "\n",
            "============================================================\n",
            "Saving outputs\n",
            "============================================================\n",
            "  Model saved to: runs/e2e_f_only_q2/qat_state_dict.pt\n",
            "  Loss log saved to: runs/e2e_f_only_q2/loss_per_layer.csv\n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# E2E f-only: Skip ALL progressive passes, train all f parameters at once\n",
        "# This is the simplest and most stable approach\n",
        "# Starting from 4-bit checkpoint for 2-bit training\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/train_qat_progressive.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_CHECKPOINT} \\\n",
        "  --output_dir {RUN_DIR_E2E_FONLY} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --skip_mlp_pass \\\n",
        "  --skip_attention_pass \\\n",
        "  --skip_mlp_refinement \\\n",
        "  --e2e_steps {E2E_STEPS} \\\n",
        "  --e2e_learning_rate {LR_E2E} \\\n",
        "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vab9x1zlqhs",
      "metadata": {
        "id": "vab9x1zlqhs"
      },
      "source": [
        "### Option 2: Progressive f-only Training\n",
        "\n",
        "Layer-by-layer training but only trains `_f_param` (quantization scales), not weights.\n",
        "More stable than full progressive training, but may still see instability at later layers.\n",
        "\n",
        "Use `--train_f_only` flag to freeze weights and only train quantization scales per layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oqxrx3c2fe",
      "metadata": {
        "id": "oqxrx3c2fe"
      },
      "outputs": [],
      "source": [
        "# Progressive f-only: Layer-by-layer, but only train quantization scales\n",
        "# Use --train_f_only for more stable training\n",
        "# Starting from 4-bit checkpoint for 2-bit training\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/train_qat_progressive.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_CHECKPOINT} \\\n",
        "  --output_dir {RUN_DIR_PROGRESSIVE_FONLY} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --train_f_only \\\n",
        "  --steps_per_layer_mlp {STEPS_PER_LAYER_MLP} \\\n",
        "  --steps_per_layer_attn {STEPS_PER_LAYER_ATTN} \\\n",
        "  --e2e_steps {E2E_STEPS} \\\n",
        "  --local_weight 0.0 \\\n",
        "  --global_weight {GLOBAL_WEIGHT} \\\n",
        "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
        "  --learning_rate {LR_PROGRESSIVE} \\\n",
        "  --e2e_learning_rate {LR_E2E} \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --skip_mlp_refinement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b30bc51f"
      },
      "source": [
        "# ---- Config (edit these) ----\n",
        "#MODEL_NAME = 'Qwen/Qwen3-4B-Thinking-2507'\n",
        "MODEL_NAME = 'Qwen/Qwen3-0.6B'\n",
        "TEACHER_NAME = MODEL_NAME\n",
        "QUANT_BITS = 2  # 2 or 4\n",
        "DEVICE = 'auto'\n",
        "AMP_DTYPE = 'auto'\n",
        "PARAM_DTYPE = 'auto'\n",
        "DTYPE = 'auto'\n",
        "\n",
        "# Cache dirs\n",
        "CACHE_DIR_CHAT = 'caches/alpaca_chat_think_both_L128_K32_R256'\n",
        "CACHE_DIR_TEXT = 'caches/c4_qwen3_L64_K32_R256'\n",
        "#CACHE_DIR_CHAT = 'caches/Q4B_alpaca_chat_think_L128_K32_R256'\n",
        "#CACHE_DIR_TEXT = 'caches/Q4B_c4_qwen3_L64_K32_R256'"
      ],
      "id": "b30bc51f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8e0gsxkcc4",
      "metadata": {
        "id": "8e0gsxkcc4"
      },
      "source": [
        "### (!!!) Option 3: Full Progressive Training (weights + f)\n",
        "\n",
        "Full layer-by-layer training with weights and quantization scales.\n",
        "Most aggressive but potentially unstable for ultra-low-bit (2-bit).\n",
        "\n",
        "**Training Order (3-pass v3):**\n",
        "1. **Pass 1**: Train MLP layers (local reconstruction + global KD)\n",
        "2. **Pass 2**: Train attention layers (global KD only)\n",
        "3. **Pass 3**: MLP refinement (addresses MLP-attention coupling)\n",
        "4. **Pass 4**: E2E quantizer-only tuning (f-param only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7qzyw0rd18e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qzyw0rd18e",
        "outputId": "5c8fe245-8637-4c12-e519-ffb3eac2e5bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  step 70: local=0.3900 global=1.5076\n",
            "  step 80: local=0.3982 global=1.4915\n",
            "  step 90: local=0.3910 global=1.4774\n",
            "  Layer 10 not converged (global=1.5578 > 0.8), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3972 global=1.5382\n",
            "  step 10: local=0.3940 global=1.5609\n",
            "  step 20: local=0.3810 global=1.5606\n",
            "  step 30: local=0.4043 global=1.5206\n",
            "  step 40: local=0.3878 global=1.5106\n",
            "  step 50: local=0.3954 global=1.5723\n",
            "  step 60: local=0.4033 global=1.6047\n",
            "  step 70: local=0.3961 global=1.5958\n",
            "  step 80: local=0.3861 global=1.4519\n",
            "  step 90: local=0.3870 global=1.4778\n",
            "  Layer 10 not converged (global=1.5904 > 0.8), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4013 global=1.5547\n",
            "  step 10: local=0.3861 global=1.5464\n",
            "  step 20: local=0.3842 global=1.4988\n",
            "  step 30: local=0.3912 global=1.5509\n",
            "  step 40: local=0.3814 global=1.5978\n",
            "  step 50: local=0.3802 global=1.4569\n",
            "  step 60: local=0.3985 global=1.4947\n",
            "  step 70: local=0.3782 global=1.6233\n",
            "  step 80: local=0.3851 global=1.5748\n",
            "  step 90: local=0.3921 global=1.5987\n",
            "  Layer 10 not converged (global=1.5170 > 0.8), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3840 global=1.5148\n",
            "  step 10: local=0.3937 global=1.5409\n",
            "  step 20: local=0.3770 global=1.4531\n",
            "  step 30: local=0.3884 global=1.5019\n",
            "  step 40: local=0.4013 global=1.5419\n",
            "  step 50: local=0.3968 global=1.5534\n",
            "  step 60: local=0.3923 global=1.5430\n",
            "  step 70: local=0.3952 global=1.4877\n",
            "  step 80: local=0.3853 global=1.4226\n",
            "  step 90: local=0.3956 global=1.4667\n",
            "  Layer 10 not converged (global=1.5112 > 0.8), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3895 global=1.4804\n",
            "  step 10: local=0.3844 global=1.5345\n",
            "  step 20: local=0.3808 global=1.4814\n",
            "  step 30: local=0.3854 global=1.6085\n",
            "  step 40: local=0.3892 global=1.4946\n",
            "  step 50: local=0.3894 global=1.5486\n",
            "  step 60: local=0.3882 global=1.5171\n",
            "  step 70: local=0.3858 global=1.5504\n",
            "  step 80: local=0.3902 global=1.4591\n",
            "  step 90: local=0.3893 global=1.5396\n",
            "  Layer 10 not converged (global=1.5508 > 0.8), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3837 global=1.5704\n",
            "  step 10: local=0.3744 global=1.4086\n",
            "  step 20: local=0.3835 global=1.4968\n",
            "  step 30: local=0.3933 global=1.5657\n",
            "  step 40: local=0.3824 global=1.4262\n",
            "  step 50: local=0.3726 global=1.5498\n",
            "  step 60: local=0.3909 global=1.5029\n",
            "  step 70: local=0.3862 global=1.4730\n",
            "  step 80: local=0.3767 global=1.5079\n",
            "  step 90: local=0.3929 global=1.5471\n",
            "  Layer 10 not converged (global=1.5421 > 0.8), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3879 global=1.5071\n",
            "  step 10: local=0.3806 global=1.5012\n",
            "  step 20: local=0.3964 global=1.6420\n",
            "  step 30: local=0.3938 global=1.5704\n",
            "  step 40: local=0.3819 global=1.4804\n",
            "  step 50: local=0.3928 global=1.5939\n",
            "  step 60: local=0.3862 global=1.4968\n",
            "  step 70: local=0.3800 global=1.5407\n",
            "  step 80: local=0.3876 global=1.6446\n",
            "  step 90: local=0.3847 global=1.6139\n",
            "  Layer 10 not converged (global=1.5307 > 0.8), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3894 global=1.5400\n",
            "  step 10: local=0.3867 global=1.4435\n",
            "  step 20: local=0.3829 global=1.5464\n",
            "  step 30: local=0.3818 global=1.5007\n",
            "  step 40: local=0.3822 global=1.5084\n",
            "  step 50: local=0.3861 global=1.5603\n",
            "  step 60: local=0.3886 global=1.4491\n",
            "  step 70: local=0.3876 global=1.6353\n",
            "  step 80: local=0.3930 global=1.5500\n",
            "  step 90: local=0.3847 global=1.5229\n",
            "  Layer 10 not converged (global=1.5449 > 0.8), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3818 global=1.5191\n",
            "  step 10: local=0.3872 global=1.5918\n",
            "  step 20: local=0.3840 global=1.5470\n",
            "  step 30: local=0.3828 global=1.5518\n",
            "  step 40: local=0.3768 global=1.5595\n",
            "  step 50: local=0.3883 global=1.4852\n",
            "  step 60: local=0.3778 global=1.5008\n",
            "  step 70: local=0.3882 global=1.4619\n",
            "  step 80: local=0.3869 global=1.6645\n",
            "  step 90: local=0.3789 global=1.6624\n",
            "  Layer 10 not converged (global=1.5174 > 0.8), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3928 global=1.4625\n",
            "  step 10: local=0.3836 global=1.4651\n",
            "  step 20: local=0.3792 global=1.4599\n",
            "  step 30: local=0.3818 global=1.4855\n",
            "  step 40: local=0.3806 global=1.5678\n",
            "  step 50: local=0.3879 global=1.4922\n",
            "  step 60: local=0.3841 global=1.5714\n",
            "  step 70: local=0.3752 global=1.6139\n",
            "  step 80: local=0.3730 global=1.5379\n",
            "  step 90: local=0.3789 global=1.5105\n",
            "  Layer 10 not converged (global=1.4841 > 0.8), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3875 global=1.5184\n",
            "  step 10: local=0.3811 global=1.4707\n",
            "  step 20: local=0.3844 global=1.5061\n",
            "  step 30: local=0.3842 global=1.5931\n",
            "  step 40: local=0.3719 global=1.4747\n",
            "  step 50: local=0.3849 global=1.5390\n",
            "  step 60: local=0.3828 global=1.6173\n",
            "  step 70: local=0.3749 global=1.5611\n",
            "  step 80: local=0.3793 global=1.5447\n",
            "  step 90: local=0.3846 global=1.4067\n",
            "  Layer 10 not converged (global=1.5820 > 0.8), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3873 global=1.5418\n",
            "  step 10: local=0.3928 global=1.4966\n",
            "  step 20: local=0.3805 global=1.4802\n",
            "  step 30: local=0.3825 global=1.4674\n",
            "  step 40: local=0.3829 global=1.5262\n",
            "  step 50: local=0.3868 global=1.5508\n",
            "  step 60: local=0.3824 global=1.5501\n",
            "  step 70: local=0.3828 global=1.5112\n",
            "  step 80: local=0.3745 global=1.5003\n",
            "  step 90: local=0.3867 global=1.5614\n",
            "  [WARN] Layer 10 did not converge after 20 repeats (global=1.5056)\n",
            "  [TIME] Layer 10 took 678.0s | Total: 115.9min | ETA: 123.1min (17 layers left)\n",
            "\n",
            "--- Layer 11/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5409 global=1.7240\n",
            "  step 10: local=0.5202 global=1.6868\n",
            "  step 20: local=0.5321 global=1.5394\n",
            "  step 30: local=0.5094 global=1.5638\n",
            "  step 40: local=0.5073 global=1.6335\n",
            "  step 50: local=0.4830 global=1.6150\n",
            "  step 60: local=0.4827 global=1.5710\n",
            "  step 70: local=0.4842 global=1.6169\n",
            "  step 80: local=0.4746 global=1.6601\n",
            "  step 90: local=0.4713 global=1.5155\n",
            "  Layer 11 not converged (global=1.5270 > 0.8), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4857 global=1.5502\n",
            "  step 10: local=0.4803 global=1.6823\n",
            "  step 20: local=0.4677 global=1.6322\n",
            "  step 30: local=0.4703 global=1.6426\n",
            "  step 40: local=0.4468 global=1.5560\n",
            "  step 50: local=0.4655 global=1.5851\n",
            "  step 60: local=0.4442 global=1.4840\n",
            "  step 70: local=0.4453 global=1.5390\n",
            "  step 80: local=0.4460 global=1.5754\n",
            "  step 90: local=0.4182 global=1.5809\n",
            "  Layer 11 not converged (global=1.5840 > 0.8), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4332 global=1.5767\n",
            "  step 10: local=0.4232 global=1.5179\n",
            "  step 20: local=0.4282 global=1.4546\n",
            "  step 30: local=0.4176 global=1.5094\n",
            "  step 40: local=0.4155 global=1.5036\n",
            "  step 50: local=0.4148 global=1.5604\n",
            "  step 60: local=0.4024 global=1.5054\n",
            "  step 70: local=0.3962 global=1.6280\n",
            "  step 80: local=0.3905 global=1.5174\n",
            "  step 90: local=0.4086 global=1.5620\n",
            "  Layer 11 not converged (global=1.5470 > 0.8), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3892 global=1.5335\n",
            "  step 10: local=0.3974 global=1.5671\n",
            "  step 20: local=0.3880 global=1.4658\n",
            "  step 30: local=0.3933 global=1.5534\n",
            "  step 40: local=0.3849 global=1.5796\n",
            "  step 50: local=0.3890 global=1.4161\n",
            "  step 60: local=0.3816 global=1.5101\n",
            "  step 70: local=0.3845 global=1.5670\n",
            "  step 80: local=0.3838 global=1.4282\n",
            "  step 90: local=0.3794 global=1.5594\n",
            "  Layer 11 not converged (global=1.6112 > 0.8), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3814 global=1.5136\n",
            "  step 10: local=0.3895 global=1.4826\n",
            "  step 20: local=0.3847 global=1.5094\n",
            "  step 30: local=0.3754 global=1.5491\n",
            "  step 40: local=0.3819 global=1.5106\n",
            "  step 50: local=0.3834 global=1.5115\n",
            "  step 60: local=0.3883 global=1.6407\n",
            "  step 70: local=0.3773 global=1.5708\n",
            "  step 80: local=0.3812 global=1.4855\n",
            "  step 90: local=0.3767 global=1.6014\n",
            "  Layer 11 not converged (global=1.5240 > 0.8), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3739 global=1.5010\n",
            "  step 10: local=0.3783 global=1.5472\n",
            "  step 20: local=0.3673 global=1.6461\n",
            "  step 30: local=0.3745 global=1.6101\n",
            "  step 40: local=0.3942 global=1.5437\n",
            "  step 50: local=0.3705 global=1.4441\n",
            "  step 60: local=0.3597 global=1.5412\n",
            "  step 70: local=0.3607 global=1.4944\n",
            "  step 80: local=0.3806 global=1.5005\n",
            "  step 90: local=0.3701 global=1.5588\n",
            "  Layer 11 not converged (global=1.4684 > 0.8), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3745 global=1.4452\n",
            "  step 10: local=0.3725 global=1.6339\n",
            "  step 20: local=0.3827 global=1.5457\n",
            "  step 30: local=0.3651 global=1.5172\n",
            "  step 40: local=0.3737 global=1.5112\n",
            "  step 50: local=0.3756 global=1.5958\n",
            "  step 60: local=0.3703 global=1.5461\n",
            "  step 70: local=0.3698 global=1.5495\n",
            "  step 80: local=0.3759 global=1.5621\n",
            "  step 90: local=0.3660 global=1.4791\n",
            "  Layer 11 not converged (global=1.5818 > 0.8), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3623 global=1.5031\n",
            "  step 10: local=0.3738 global=1.4569\n",
            "  step 20: local=0.3736 global=1.6568\n",
            "  step 30: local=0.3630 global=1.6581\n",
            "  step 40: local=0.3639 global=1.4565\n",
            "  step 50: local=0.3708 global=1.4581\n",
            "  step 60: local=0.3612 global=1.4504\n",
            "  step 70: local=0.3701 global=1.4853\n",
            "  step 80: local=0.3712 global=1.5622\n",
            "  step 90: local=0.3600 global=1.4895\n",
            "  Layer 11 not converged (global=1.5006 > 0.8), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3694 global=1.5644\n",
            "  step 10: local=0.3661 global=1.6081\n",
            "  step 20: local=0.3677 global=1.5334\n",
            "  step 30: local=0.3644 global=1.5052\n",
            "  step 40: local=0.3676 global=1.5161\n",
            "  step 50: local=0.3585 global=1.4658\n",
            "  step 60: local=0.3598 global=1.4983\n",
            "  step 70: local=0.3553 global=1.5859\n",
            "  step 80: local=0.3719 global=1.4741\n",
            "  step 90: local=0.3572 global=1.5322\n",
            "  Layer 11 not converged (global=1.6202 > 0.8), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3757 global=1.6159\n",
            "  step 10: local=0.3621 global=1.5584\n",
            "  step 20: local=0.3667 global=1.5320\n",
            "  step 30: local=0.3579 global=1.4051\n",
            "  step 40: local=0.3698 global=1.5295\n",
            "  step 50: local=0.3652 global=1.4915\n",
            "  step 60: local=0.3655 global=1.4727\n",
            "  step 70: local=0.3614 global=1.4600\n",
            "  step 80: local=0.3586 global=1.5084\n",
            "  step 90: local=0.3529 global=1.5440\n",
            "  Layer 11 not converged (global=1.5587 > 0.8), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3751 global=1.5381\n",
            "  step 10: local=0.3651 global=1.5080\n",
            "  step 20: local=0.3610 global=1.4892\n",
            "  step 30: local=0.3649 global=1.5511\n",
            "  step 40: local=0.3711 global=1.5901\n",
            "  step 50: local=0.3586 global=1.5755\n",
            "  step 60: local=0.3601 global=1.4336\n",
            "  step 70: local=0.3609 global=1.4660\n",
            "  step 80: local=0.3641 global=1.5401\n",
            "  step 90: local=0.3614 global=1.5333\n",
            "  Layer 11 not converged (global=1.5263 > 0.8), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3641 global=1.4834\n",
            "  step 10: local=0.3641 global=1.5345\n",
            "  step 20: local=0.3591 global=1.5822\n",
            "  step 30: local=0.3495 global=1.4423\n",
            "  step 40: local=0.3758 global=1.4769\n",
            "  step 50: local=0.3522 global=1.6118\n",
            "  step 60: local=0.3645 global=1.5644\n",
            "  step 70: local=0.3566 global=1.5828\n",
            "  step 80: local=0.3522 global=1.4969\n",
            "  step 90: local=0.3663 global=1.5293\n",
            "  Layer 11 not converged (global=1.4860 > 0.8), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3630 global=1.4283\n",
            "  step 10: local=0.3653 global=1.4843\n",
            "  step 20: local=0.3725 global=1.5217\n",
            "  step 30: local=0.3627 global=1.5310\n",
            "  step 40: local=0.3748 global=1.5264\n",
            "  step 50: local=0.3607 global=1.4741\n",
            "  step 60: local=0.3606 global=1.4124\n",
            "  step 70: local=0.3651 global=1.4592\n",
            "  step 80: local=0.3559 global=1.4657\n",
            "  step 90: local=0.3710 global=1.5210\n",
            "  Layer 11 not converged (global=1.5505 > 0.8), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3648 global=1.4671\n",
            "  step 10: local=0.3587 global=1.5911\n",
            "  step 20: local=0.3578 global=1.4847\n",
            "  step 30: local=0.3714 global=1.5314\n",
            "  step 40: local=0.3594 global=1.5014\n",
            "  step 50: local=0.3570 global=1.5377\n",
            "  step 60: local=0.3708 global=1.4393\n",
            "  step 70: local=0.3662 global=1.5252\n",
            "  step 80: local=0.3633 global=1.5526\n",
            "  step 90: local=0.3615 global=1.3884\n",
            "  Layer 11 not converged (global=1.5141 > 0.8), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3678 global=1.4860\n",
            "  step 10: local=0.3633 global=1.5429\n",
            "  step 20: local=0.3622 global=1.4059\n",
            "  step 30: local=0.3496 global=1.5367\n",
            "  step 40: local=0.3603 global=1.4859\n",
            "  step 50: local=0.3630 global=1.4604\n",
            "  step 60: local=0.3609 global=1.4896\n",
            "  step 70: local=0.3583 global=1.5305\n",
            "  step 80: local=0.3604 global=1.4922\n",
            "  step 90: local=0.3644 global=1.4915\n",
            "  Layer 11 not converged (global=1.4858 > 0.8), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3621 global=1.6228\n",
            "  step 10: local=0.3600 global=1.5533\n",
            "  step 20: local=0.3558 global=1.4681\n",
            "  step 30: local=0.3612 global=1.5839\n",
            "  step 40: local=0.3674 global=1.4833\n",
            "  step 50: local=0.3666 global=1.5306\n",
            "  step 60: local=0.3669 global=1.6296\n",
            "  step 70: local=0.3577 global=1.5941\n",
            "  step 80: local=0.3606 global=1.5278\n",
            "  step 90: local=0.3578 global=1.4306\n",
            "  Layer 11 not converged (global=1.5928 > 0.8), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3562 global=1.5263\n",
            "  step 10: local=0.3622 global=1.4804\n",
            "  step 20: local=0.3656 global=1.4856\n",
            "  step 30: local=0.3627 global=1.5445\n",
            "  step 40: local=0.3516 global=1.4310\n",
            "  step 50: local=0.3592 global=1.6191\n",
            "  step 60: local=0.3644 global=1.5324\n",
            "  step 70: local=0.3545 global=1.5062\n",
            "  step 80: local=0.3559 global=1.5008\n",
            "  step 90: local=0.3641 global=1.5828\n",
            "  Layer 11 not converged (global=1.4720 > 0.8), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3630 global=1.5345\n",
            "  step 10: local=0.3617 global=1.5378\n",
            "  step 20: local=0.3515 global=1.5488\n",
            "  step 30: local=0.3531 global=1.4679\n",
            "  step 40: local=0.3649 global=1.4909\n",
            "  step 50: local=0.3634 global=1.4460\n",
            "  step 60: local=0.3596 global=1.6450\n",
            "  step 70: local=0.3437 global=1.6466\n",
            "  step 80: local=0.3574 global=1.4444\n",
            "  step 90: local=0.3594 global=1.4474\n",
            "  Layer 11 not converged (global=1.5218 > 0.8), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3590 global=1.4413\n",
            "  step 10: local=0.3572 global=1.4737\n",
            "  step 20: local=0.3568 global=1.5518\n",
            "  step 30: local=0.3570 global=1.4790\n",
            "  step 40: local=0.3547 global=1.5546\n",
            "  step 50: local=0.3594 global=1.5964\n",
            "  step 60: local=0.3681 global=1.5236\n",
            "  step 70: local=0.3592 global=1.4952\n",
            "  step 80: local=0.3532 global=1.5073\n",
            "  step 90: local=0.3556 global=1.4559\n",
            "  Layer 11 not converged (global=1.5220 > 0.8), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3690 global=1.4893\n",
            "  step 10: local=0.3574 global=1.5756\n",
            "  step 20: local=0.3633 global=1.4647\n",
            "  step 30: local=0.3512 global=1.5228\n",
            "  step 40: local=0.3638 global=1.6059\n",
            "  step 50: local=0.3547 global=1.5492\n",
            "  step 60: local=0.3563 global=1.5241\n",
            "  step 70: local=0.3491 global=1.3968\n",
            "  step 80: local=0.3570 global=1.5206\n",
            "  step 90: local=0.3453 global=1.4831\n",
            "  [WARN] Layer 11 did not converge after 20 repeats (global=1.5501)\n",
            "  [TIME] Layer 11 took 662.3s | Total: 126.9min | ETA: 119.4min (16 layers left)\n",
            "\n",
            "--- Layer 12/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4998 global=1.5673\n",
            "  step 10: local=0.5065 global=1.5365\n",
            "  step 20: local=0.4806 global=1.5795\n",
            "  step 30: local=0.4779 global=1.6117\n",
            "  step 40: local=0.4743 global=1.6099\n",
            "  step 50: local=0.4833 global=1.5734\n",
            "  step 60: local=0.4608 global=1.5489\n",
            "  step 70: local=0.4679 global=1.6126\n",
            "  step 80: local=0.4580 global=1.6507\n",
            "  step 90: local=0.4487 global=1.6285\n",
            "  Layer 12 not converged (global=1.5841 > 0.8), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4536 global=1.4768\n",
            "  step 10: local=0.4404 global=1.5085\n",
            "  step 20: local=0.4426 global=1.5822\n",
            "  step 30: local=0.4297 global=1.5641\n",
            "  step 40: local=0.4303 global=1.5258\n",
            "  step 50: local=0.4175 global=1.5740\n",
            "  step 60: local=0.4126 global=1.6165\n",
            "  step 70: local=0.4064 global=1.4718\n",
            "  step 80: local=0.4178 global=1.5072\n",
            "  step 90: local=0.4211 global=1.6361\n",
            "  Layer 12 not converged (global=1.5311 > 0.8), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4015 global=1.5880\n",
            "  step 10: local=0.4049 global=1.6060\n",
            "  step 20: local=0.3972 global=1.5182\n",
            "  step 30: local=0.3965 global=1.5560\n",
            "  step 40: local=0.3884 global=1.4418\n",
            "  step 50: local=0.3948 global=1.5023\n",
            "  step 60: local=0.3981 global=1.5352\n",
            "  step 70: local=0.3921 global=1.5443\n",
            "  step 80: local=0.3907 global=1.5395\n",
            "  step 90: local=0.3802 global=1.4800\n",
            "  Layer 12 not converged (global=1.5696 > 0.8), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3814 global=1.4228\n",
            "  step 10: local=0.3871 global=1.4804\n",
            "  step 20: local=0.3838 global=1.4810\n",
            "  step 30: local=0.3795 global=1.5219\n",
            "  step 40: local=0.3752 global=1.4699\n",
            "  step 50: local=0.3732 global=1.5985\n",
            "  step 60: local=0.3725 global=1.4845\n",
            "  step 70: local=0.3696 global=1.5397\n",
            "  step 80: local=0.3729 global=1.5085\n",
            "  step 90: local=0.3655 global=1.5406\n",
            "  Layer 12 not converged (global=1.6403 > 0.8), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3712 global=1.4369\n",
            "  step 10: local=0.3735 global=1.5289\n",
            "  step 20: local=0.3700 global=1.5534\n",
            "  step 30: local=0.3700 global=1.3842\n",
            "  step 40: local=0.3608 global=1.4820\n",
            "  step 50: local=0.3632 global=1.5475\n",
            "  step 60: local=0.3656 global=1.4048\n",
            "  step 70: local=0.3626 global=1.5293\n",
            "  step 80: local=0.3590 global=1.4999\n",
            "  step 90: local=0.3700 global=1.4555\n",
            "  Layer 12 not converged (global=1.5884 > 0.8), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3663 global=1.4906\n",
            "  step 10: local=0.3525 global=1.5231\n",
            "  step 20: local=0.3708 global=1.4841\n",
            "  step 30: local=0.3568 global=1.4894\n",
            "  step 40: local=0.3703 global=1.6196\n",
            "  step 50: local=0.3502 global=1.5477\n",
            "  step 60: local=0.3541 global=1.4626\n",
            "  step 70: local=0.3503 global=1.5843\n",
            "  step 80: local=0.3539 global=1.4733\n",
            "  step 90: local=0.3600 global=1.5226\n",
            "  Layer 12 not converged (global=1.5581 > 0.8), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3535 global=1.6223\n",
            "  step 10: local=0.3629 global=1.5896\n",
            "  step 20: local=0.3629 global=1.5220\n",
            "  step 30: local=0.3553 global=1.4263\n",
            "  step 40: local=0.3557 global=1.5186\n",
            "  step 50: local=0.3588 global=1.4718\n",
            "  step 60: local=0.3459 global=1.4746\n",
            "  step 70: local=0.3579 global=1.5356\n",
            "  step 80: local=0.3515 global=1.4246\n",
            "  step 90: local=0.3589 global=1.6100\n",
            "  Layer 12 not converged (global=1.5196 > 0.8), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3565 global=1.5206\n",
            "  step 10: local=0.3592 global=1.4954\n",
            "  step 20: local=0.3523 global=1.4919\n",
            "  step 30: local=0.3566 global=1.5820\n",
            "  step 40: local=0.3548 global=1.5204\n",
            "  step 50: local=0.3636 global=1.5317\n",
            "  step 60: local=0.3513 global=1.5386\n",
            "  step 70: local=0.3529 global=1.4595\n",
            "  step 80: local=0.3527 global=1.4799\n",
            "  step 90: local=0.3519 global=1.4336\n",
            "  Layer 12 not converged (global=1.4680 > 0.8), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3463 global=1.6408\n",
            "  step 10: local=0.3577 global=1.6332\n",
            "  step 20: local=0.3580 global=1.4359\n",
            "  step 30: local=0.3460 global=1.4390\n",
            "  step 40: local=0.3534 global=1.4325\n",
            "  step 50: local=0.3484 global=1.4627\n",
            "  step 60: local=0.3534 global=1.5438\n",
            "  step 70: local=0.3594 global=1.4683\n",
            "  step 80: local=0.3512 global=1.5421\n",
            "  step 90: local=0.3540 global=1.5823\n",
            "  Layer 12 not converged (global=1.5542 > 0.8), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3521 global=1.5056\n",
            "  step 10: local=0.3494 global=1.4824\n",
            "  step 20: local=0.3615 global=1.4971\n",
            "  step 30: local=0.3448 global=1.4485\n",
            "  step 40: local=0.3530 global=1.4776\n",
            "  step 50: local=0.3467 global=1.5618\n",
            "  step 60: local=0.3557 global=1.4504\n",
            "  step 70: local=0.3501 global=1.5105\n",
            "  step 80: local=0.3477 global=1.5904\n",
            "  step 90: local=0.3498 global=1.5366\n",
            "  Layer 12 not converged (global=1.4703 > 0.8), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3532 global=1.5114\n",
            "  step 10: local=0.3406 global=1.3881\n",
            "  step 20: local=0.3470 global=1.5124\n",
            "  step 30: local=0.3533 global=1.4752\n",
            "  step 40: local=0.3420 global=1.4562\n",
            "  step 50: local=0.3429 global=1.4330\n",
            "  step 60: local=0.3442 global=1.4877\n",
            "  step 70: local=0.3441 global=1.5162\n",
            "  step 80: local=0.3406 global=1.5166\n",
            "  step 90: local=0.3527 global=1.4857\n",
            "  Layer 12 not converged (global=1.5239 > 0.8), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3523 global=1.4696\n",
            "  step 10: local=0.3436 global=1.5347\n",
            "  step 20: local=0.3535 global=1.5722\n",
            "  step 30: local=0.3449 global=1.5603\n",
            "  step 40: local=0.3490 global=1.4130\n",
            "  step 50: local=0.3521 global=1.4448\n",
            "  step 60: local=0.3531 global=1.5207\n",
            "  step 70: local=0.3394 global=1.5082\n",
            "  step 80: local=0.3413 global=1.4655\n",
            "  step 90: local=0.3501 global=1.5199\n",
            "  Layer 12 not converged (global=1.4750 > 0.8), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3511 global=1.5626\n",
            "  step 10: local=0.3367 global=1.4242\n",
            "  step 20: local=0.3451 global=1.4610\n",
            "  step 30: local=0.3452 global=1.5900\n",
            "  step 40: local=0.3470 global=1.5426\n",
            "  step 50: local=0.3457 global=1.5630\n",
            "  step 60: local=0.3405 global=1.4778\n",
            "  step 70: local=0.3543 global=1.5175\n",
            "  step 80: local=0.3462 global=1.4056\n",
            "  step 90: local=0.3497 global=1.4633\n",
            "  Layer 12 not converged (global=1.5071 > 0.8), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3491 global=1.4994\n",
            "  step 10: local=0.3562 global=1.5105\n",
            "  step 20: local=0.3490 global=1.5057\n",
            "  step 30: local=0.3451 global=1.4513\n",
            "  step 40: local=0.3403 global=1.3946\n",
            "  step 50: local=0.3487 global=1.4461\n",
            "  step 60: local=0.3451 global=1.4555\n",
            "  step 70: local=0.3568 global=1.4961\n",
            "  step 80: local=0.3484 global=1.4447\n",
            "  step 90: local=0.3373 global=1.5723\n",
            "  Layer 12 not converged (global=1.4845 > 0.8), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3408 global=1.4610\n",
            "  step 10: local=0.3425 global=1.5150\n",
            "  step 20: local=0.3412 global=1.4892\n",
            "  step 30: local=0.3440 global=1.5183\n",
            "  step 40: local=0.3428 global=1.4142\n",
            "  step 50: local=0.3430 global=1.5094\n",
            "  step 60: local=0.3404 global=1.5343\n",
            "  step 70: local=0.3540 global=1.3641\n",
            "  step 80: local=0.3410 global=1.4619\n",
            "  step 90: local=0.3441 global=1.5298\n",
            "  Layer 12 not converged (global=1.5320 > 0.8), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3414 global=1.3893\n",
            "  step 10: local=0.3391 global=1.5092\n",
            "  step 20: local=0.3534 global=1.4784\n",
            "  step 30: local=0.3475 global=1.4402\n",
            "  step 40: local=0.3421 global=1.4729\n",
            "  step 50: local=0.3396 global=1.5084\n",
            "  step 60: local=0.3388 global=1.4677\n",
            "  step 70: local=0.3475 global=1.4749\n",
            "  step 80: local=0.3537 global=1.6055\n",
            "  step 90: local=0.3416 global=1.5316\n",
            "  Layer 12 not converged (global=1.6795 > 0.8), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3444 global=1.4498\n",
            "  step 10: local=0.3467 global=1.5696\n",
            "  step 20: local=0.3498 global=1.4593\n",
            "  step 30: local=0.3458 global=1.5102\n",
            "  step 40: local=0.3490 global=1.6065\n",
            "  step 50: local=0.3403 global=1.5759\n",
            "  step 60: local=0.3391 global=1.5105\n",
            "  step 70: local=0.3529 global=1.4141\n",
            "  step 80: local=0.3532 global=1.5087\n",
            "  step 90: local=0.3447 global=1.4616\n",
            "  Layer 12 not converged (global=1.4815 > 0.8), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3471 global=1.4631\n",
            "  step 10: local=0.3553 global=1.5233\n",
            "  step 20: local=0.3496 global=1.4119\n",
            "  step 30: local=0.3455 global=1.5994\n",
            "  step 40: local=0.3471 global=1.5083\n",
            "  step 50: local=0.3440 global=1.4848\n",
            "  step 60: local=0.3413 global=1.4813\n",
            "  step 70: local=0.3503 global=1.5719\n",
            "  step 80: local=0.3410 global=1.5110\n",
            "  step 90: local=0.3360 global=1.5217\n",
            "  Layer 12 not converged (global=1.5416 > 0.8), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3464 global=1.5291\n",
            "  step 10: local=0.3505 global=1.4490\n",
            "  step 20: local=0.3450 global=1.4733\n",
            "  step 30: local=0.3419 global=1.4239\n",
            "  step 40: local=0.3494 global=1.6304\n",
            "  step 50: local=0.3386 global=1.6223\n",
            "  step 60: local=0.3532 global=1.4269\n",
            "  step 70: local=0.3392 global=1.4316\n",
            "  step 80: local=0.3451 global=1.4252\n",
            "  step 90: local=0.3400 global=1.4540\n",
            "  Layer 12 not converged (global=1.4923 > 0.8), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3426 global=1.5348\n",
            "  step 10: local=0.3417 global=1.4615\n",
            "  step 20: local=0.3430 global=1.5341\n",
            "  step 30: local=0.3516 global=1.5739\n",
            "  step 40: local=0.3431 global=1.4991\n",
            "  step 50: local=0.3412 global=1.4744\n",
            "  step 60: local=0.3477 global=1.4886\n",
            "  step 70: local=0.3487 global=1.4397\n",
            "  step 80: local=0.3512 global=1.4703\n",
            "  step 90: local=0.3384 global=1.5551\n",
            "  [WARN] Layer 12 did not converge after 20 repeats (global=1.4614)\n",
            "  [TIME] Layer 12 took 646.4s | Total: 137.7min | ETA: 114.7min (15 layers left)\n",
            "\n",
            "--- Layer 13/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5561 global=1.5546\n",
            "  step 10: local=0.5241 global=1.5968\n",
            "  step 20: local=0.5424 global=1.6739\n",
            "  step 30: local=0.5192 global=1.6238\n",
            "  step 40: local=0.5150 global=1.5751\n",
            "  step 50: local=0.4805 global=1.4588\n",
            "  step 60: local=0.4977 global=1.5821\n",
            "  step 70: local=0.4950 global=1.5298\n",
            "  step 80: local=0.4832 global=1.5068\n",
            "  step 90: local=0.4765 global=1.4822\n",
            "  Layer 13 not converged (global=1.5625 > 0.8), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4718 global=1.5285\n",
            "  step 10: local=0.4574 global=1.5574\n",
            "  step 20: local=0.4585 global=1.5589\n",
            "  step 30: local=0.4529 global=1.5217\n",
            "  step 40: local=0.4441 global=1.4994\n",
            "  step 50: local=0.4309 global=1.5725\n",
            "  step 60: local=0.4394 global=1.6104\n",
            "  step 70: local=0.4373 global=1.5910\n",
            "  step 80: local=0.4356 global=1.4354\n",
            "  step 90: local=0.4291 global=1.4707\n",
            "  Layer 13 not converged (global=1.5687 > 0.8), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4279 global=1.5419\n",
            "  step 10: local=0.4178 global=1.5279\n",
            "  step 20: local=0.4132 global=1.4827\n",
            "  step 30: local=0.4112 global=1.5390\n",
            "  step 40: local=0.4094 global=1.5781\n",
            "  step 50: local=0.3998 global=1.4351\n",
            "  step 60: local=0.4101 global=1.4827\n",
            "  step 70: local=0.4049 global=1.6018\n",
            "  step 80: local=0.4034 global=1.5528\n",
            "  step 90: local=0.3980 global=1.5727\n",
            "  Layer 13 not converged (global=1.4866 > 0.8), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4102 global=1.4891\n",
            "  step 10: local=0.4022 global=1.5284\n",
            "  step 20: local=0.3862 global=1.4146\n",
            "  step 30: local=0.3995 global=1.4704\n",
            "  step 40: local=0.3925 global=1.5032\n",
            "  step 50: local=0.3935 global=1.5140\n",
            "  step 60: local=0.3946 global=1.5086\n",
            "  step 70: local=0.3842 global=1.4508\n",
            "  step 80: local=0.3903 global=1.3936\n",
            "  step 90: local=0.3902 global=1.4478\n",
            "  Layer 13 not converged (global=1.4761 > 0.8), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3784 global=1.4548\n",
            "  step 10: local=0.3687 global=1.4939\n",
            "  step 20: local=0.3725 global=1.4489\n",
            "  step 30: local=0.3760 global=1.5735\n",
            "  step 40: local=0.3760 global=1.4628\n",
            "  step 50: local=0.3836 global=1.5121\n",
            "  step 60: local=0.3655 global=1.4902\n",
            "  step 70: local=0.3772 global=1.5180\n",
            "  step 80: local=0.3679 global=1.4057\n",
            "  step 90: local=0.3702 global=1.5149\n",
            "  Layer 13 not converged (global=1.5174 > 0.8), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3729 global=1.5273\n",
            "  step 10: local=0.3773 global=1.3593\n",
            "  step 20: local=0.3791 global=1.4593\n",
            "  step 30: local=0.3689 global=1.5242\n",
            "  step 40: local=0.3665 global=1.3827\n",
            "  step 50: local=0.3642 global=1.5030\n",
            "  step 60: local=0.3736 global=1.4855\n",
            "  step 70: local=0.3636 global=1.4339\n",
            "  step 80: local=0.3610 global=1.4689\n",
            "  step 90: local=0.3626 global=1.4979\n",
            "  Layer 13 not converged (global=1.5110 > 0.8), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3667 global=1.4647\n",
            "  step 10: local=0.3670 global=1.4565\n",
            "  step 20: local=0.3625 global=1.5973\n",
            "  step 30: local=0.3713 global=1.5247\n",
            "  step 40: local=0.3636 global=1.4381\n",
            "  step 50: local=0.3613 global=1.5639\n",
            "  step 60: local=0.3627 global=1.4505\n",
            "  step 70: local=0.3680 global=1.5033\n",
            "  step 80: local=0.3565 global=1.5947\n",
            "  step 90: local=0.3664 global=1.5684\n",
            "  Layer 13 not converged (global=1.4841 > 0.8), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3634 global=1.4960\n",
            "  step 10: local=0.3666 global=1.4096\n",
            "  step 20: local=0.3552 global=1.5021\n",
            "  step 30: local=0.3577 global=1.4510\n",
            "  step 40: local=0.3685 global=1.4526\n",
            "  step 50: local=0.3625 global=1.5126\n",
            "  step 60: local=0.3617 global=1.4042\n",
            "  step 70: local=0.3605 global=1.5848\n",
            "  step 80: local=0.3661 global=1.4963\n",
            "  step 90: local=0.3630 global=1.4768\n",
            "  Layer 13 not converged (global=1.5107 > 0.8), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3731 global=1.4688\n",
            "  step 10: local=0.3580 global=1.5575\n",
            "  step 20: local=0.3587 global=1.4998\n",
            "  step 30: local=0.3691 global=1.5149\n",
            "  step 40: local=0.3571 global=1.5177\n",
            "  step 50: local=0.3573 global=1.4394\n",
            "  step 60: local=0.3652 global=1.4610\n",
            "  step 70: local=0.3715 global=1.4116\n",
            "  step 80: local=0.3594 global=1.6203\n",
            "  step 90: local=0.3642 global=1.6121\n",
            "  Layer 13 not converged (global=1.4674 > 0.8), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3569 global=1.4161\n",
            "  step 10: local=0.3645 global=1.4241\n",
            "  step 20: local=0.3586 global=1.4156\n",
            "  step 30: local=0.3569 global=1.4445\n",
            "  step 40: local=0.3577 global=1.5228\n",
            "  step 50: local=0.3525 global=1.4481\n",
            "  step 60: local=0.3520 global=1.5191\n",
            "  step 70: local=0.3551 global=1.5588\n",
            "  step 80: local=0.3552 global=1.4868\n",
            "  step 90: local=0.3574 global=1.4602\n",
            "  Layer 13 not converged (global=1.4482 > 0.8), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3652 global=1.4796\n",
            "  step 10: local=0.3567 global=1.4262\n",
            "  step 20: local=0.3697 global=1.4601\n",
            "  step 30: local=0.3546 global=1.5442\n",
            "  step 40: local=0.3553 global=1.4290\n",
            "  step 50: local=0.3582 global=1.4873\n",
            "  step 60: local=0.3666 global=1.5689\n",
            "  step 70: local=0.3607 global=1.5191\n",
            "  step 80: local=0.3617 global=1.4871\n",
            "  step 90: local=0.3513 global=1.3771\n",
            "  Layer 13 not converged (global=1.5328 > 0.8), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3548 global=1.4938\n",
            "  step 10: local=0.3555 global=1.4576\n",
            "  step 20: local=0.3518 global=1.4397\n",
            "  step 30: local=0.3565 global=1.4155\n",
            "  step 40: local=0.3590 global=1.4706\n",
            "  step 50: local=0.3657 global=1.4948\n",
            "  step 60: local=0.3573 global=1.4973\n",
            "  step 70: local=0.3627 global=1.4652\n",
            "  step 80: local=0.3607 global=1.4438\n",
            "  step 90: local=0.3600 global=1.5184\n",
            "  Layer 13 not converged (global=1.4629 > 0.8), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3698 global=1.5518\n",
            "  step 10: local=0.3627 global=1.5438\n",
            "  step 20: local=0.3514 global=1.3918\n",
            "  step 30: local=0.3627 global=1.4270\n",
            "  step 40: local=0.3574 global=1.4994\n",
            "  step 50: local=0.3616 global=1.4913\n",
            "  step 60: local=0.3617 global=1.4455\n",
            "  step 70: local=0.3566 global=1.5004\n",
            "  step 80: local=0.3651 global=1.5426\n",
            "  step 90: local=0.3550 global=1.4052\n",
            "  Layer 13 not converged (global=1.4143 > 0.8), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3570 global=1.4481\n",
            "  step 10: local=0.3542 global=1.5689\n",
            "  step 20: local=0.3543 global=1.5231\n",
            "  step 30: local=0.3626 global=1.5428\n",
            "  step 40: local=0.3485 global=1.4598\n",
            "  step 50: local=0.3641 global=1.5025\n",
            "  step 60: local=0.3490 global=1.3896\n",
            "  step 70: local=0.3611 global=1.4452\n",
            "  step 80: local=0.3485 global=1.4795\n",
            "  step 90: local=0.3618 global=1.4901\n",
            "  Layer 13 not converged (global=1.5016 > 0.8), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3553 global=1.4868\n",
            "  step 10: local=0.3510 global=1.4300\n",
            "  step 20: local=0.3523 global=1.3737\n",
            "  step 30: local=0.3607 global=1.4268\n",
            "  step 40: local=0.3577 global=1.4369\n",
            "  step 50: local=0.3496 global=1.4738\n",
            "  step 60: local=0.3585 global=1.4330\n",
            "  step 70: local=0.3551 global=1.5550\n",
            "  step 80: local=0.3495 global=1.4483\n",
            "  step 90: local=0.3598 global=1.4962\n",
            "  Layer 13 not converged (global=1.4841 > 0.8), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3604 global=1.4762\n",
            "  step 10: local=0.3582 global=1.5026\n",
            "  step 20: local=0.3498 global=1.3934\n",
            "  step 30: local=0.3645 global=1.5005\n",
            "  step 40: local=0.3691 global=1.5120\n",
            "  step 50: local=0.3528 global=1.3467\n",
            "  step 60: local=0.3554 global=1.4459\n",
            "  step 70: local=0.3637 global=1.5119\n",
            "  step 80: local=0.3393 global=1.3718\n",
            "  step 90: local=0.3519 global=1.4913\n",
            "  Layer 13 not converged (global=1.5474 > 0.8), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3587 global=1.4662\n",
            "  step 10: local=0.3470 global=1.4234\n",
            "  step 20: local=0.3525 global=1.4561\n",
            "  step 30: local=0.3563 global=1.4869\n",
            "  step 40: local=0.3522 global=1.4526\n",
            "  step 50: local=0.3450 global=1.4470\n",
            "  step 60: local=0.3573 global=1.5843\n",
            "  step 70: local=0.3530 global=1.5156\n",
            "  step 80: local=0.3479 global=1.4277\n",
            "  step 90: local=0.3536 global=1.5531\n",
            "  Layer 13 not converged (global=1.4622 > 0.8), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3540 global=1.4388\n",
            "  step 10: local=0.3588 global=1.4947\n",
            "  step 20: local=0.3565 global=1.5834\n",
            "  step 30: local=0.3549 global=1.5596\n",
            "  step 40: local=0.3620 global=1.4843\n",
            "  step 50: local=0.3596 global=1.3992\n",
            "  step 60: local=0.3543 global=1.4922\n",
            "  step 70: local=0.3503 global=1.4420\n",
            "  step 80: local=0.3539 global=1.4444\n",
            "  step 90: local=0.3549 global=1.5039\n",
            "  Layer 13 not converged (global=1.4135 > 0.8), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3584 global=1.3959\n",
            "  step 10: local=0.3482 global=1.5758\n",
            "  step 20: local=0.3616 global=1.4872\n",
            "  step 30: local=0.3568 global=1.4682\n",
            "  step 40: local=0.3535 global=1.4621\n",
            "  step 50: local=0.3527 global=1.5471\n",
            "  step 60: local=0.3536 global=1.4925\n",
            "  step 70: local=0.3545 global=1.5062\n",
            "  step 80: local=0.3491 global=1.5096\n",
            "  step 90: local=0.3552 global=1.4326\n",
            "  Layer 13 not converged (global=1.5432 > 0.8), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3563 global=1.4539\n",
            "  step 10: local=0.3666 global=1.4034\n",
            "  step 20: local=0.3605 global=1.6127\n",
            "  step 30: local=0.3472 global=1.6034\n",
            "  step 40: local=0.3639 global=1.4097\n",
            "  step 50: local=0.3606 global=1.4150\n",
            "  step 60: local=0.3520 global=1.4088\n",
            "  step 70: local=0.3521 global=1.4394\n",
            "  step 80: local=0.3600 global=1.5173\n",
            "  step 90: local=0.3618 global=1.4413\n",
            "  [WARN] Layer 13 did not converge after 20 repeats (global=1.4524)\n",
            "  [TIME] Layer 13 took 630.8s | Total: 148.2min | ETA: 109.2min (14 layers left)\n",
            "\n",
            "--- Layer 14/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4666 global=1.6261\n",
            "  step 10: local=0.4740 global=1.6570\n",
            "  step 20: local=0.4598 global=1.5582\n",
            "  step 30: local=0.4535 global=1.5332\n",
            "  step 40: local=0.4554 global=1.5525\n",
            "  step 50: local=0.4449 global=1.4938\n",
            "  step 60: local=0.4485 global=1.5226\n",
            "  step 70: local=0.4262 global=1.5973\n",
            "  step 80: local=0.4320 global=1.4865\n",
            "  step 90: local=0.4235 global=1.5488\n",
            "  Layer 14 not converged (global=1.6212 > 0.8), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4195 global=1.6227\n",
            "  step 10: local=0.4226 global=1.5701\n",
            "  step 20: local=0.4140 global=1.5249\n",
            "  step 30: local=0.4087 global=1.4194\n",
            "  step 40: local=0.4103 global=1.5289\n",
            "  step 50: local=0.4116 global=1.4984\n",
            "  step 60: local=0.4134 global=1.4775\n",
            "  step 70: local=0.4070 global=1.4474\n",
            "  step 80: local=0.3986 global=1.5011\n",
            "  step 90: local=0.3948 global=1.5325\n",
            "  Layer 14 not converged (global=1.5553 > 0.8), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3894 global=1.5303\n",
            "  step 10: local=0.3893 global=1.4924\n",
            "  step 20: local=0.3873 global=1.4716\n",
            "  step 30: local=0.3909 global=1.5434\n",
            "  step 40: local=0.3843 global=1.5757\n",
            "  step 50: local=0.3811 global=1.5701\n",
            "  step 60: local=0.3836 global=1.4143\n",
            "  step 70: local=0.3735 global=1.4518\n",
            "  step 80: local=0.3770 global=1.5184\n",
            "  step 90: local=0.3698 global=1.5159\n",
            "  Layer 14 not converged (global=1.4901 > 0.8), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3579 global=1.4609\n",
            "  step 10: local=0.3649 global=1.5153\n",
            "  step 20: local=0.3574 global=1.5630\n",
            "  step 30: local=0.3590 global=1.4223\n",
            "  step 40: local=0.3642 global=1.4636\n",
            "  step 50: local=0.3535 global=1.5803\n",
            "  step 60: local=0.3593 global=1.5332\n",
            "  step 70: local=0.3541 global=1.5486\n",
            "  step 80: local=0.3581 global=1.4714\n",
            "  step 90: local=0.3561 global=1.5120\n",
            "  Layer 14 not converged (global=1.4671 > 0.8), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3534 global=1.3951\n",
            "  step 10: local=0.3543 global=1.4513\n",
            "  step 20: local=0.3664 global=1.4904\n",
            "  step 30: local=0.3607 global=1.4964\n",
            "  step 40: local=0.3451 global=1.4968\n",
            "  step 50: local=0.3563 global=1.4301\n",
            "  step 60: local=0.3434 global=1.3806\n",
            "  step 70: local=0.3513 global=1.4332\n",
            "  step 80: local=0.3467 global=1.4442\n",
            "  step 90: local=0.3508 global=1.4805\n",
            "  Layer 14 not converged (global=1.5184 > 0.8), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3512 global=1.4401\n",
            "  step 10: local=0.3534 global=1.5623\n",
            "  step 20: local=0.3545 global=1.4576\n",
            "  step 30: local=0.3423 global=1.5032\n",
            "  step 40: local=0.3407 global=1.4838\n",
            "  step 50: local=0.3445 global=1.5078\n",
            "  step 60: local=0.3490 global=1.3910\n",
            "  step 70: local=0.3524 global=1.5054\n",
            "  step 80: local=0.3475 global=1.5176\n",
            "  step 90: local=0.3414 global=1.3517\n",
            "  Layer 14 not converged (global=1.4773 > 0.8), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3414 global=1.4432\n",
            "  step 10: local=0.3494 global=1.5136\n",
            "  step 20: local=0.3455 global=1.3742\n",
            "  step 30: local=0.3389 global=1.4978\n",
            "  step 40: local=0.3530 global=1.4730\n",
            "  step 50: local=0.3478 global=1.4312\n",
            "  step 60: local=0.3439 global=1.4587\n",
            "  step 70: local=0.3386 global=1.4884\n",
            "  step 80: local=0.3451 global=1.4562\n",
            "  step 90: local=0.3359 global=1.4447\n",
            "  Layer 14 not converged (global=1.4463 > 0.8), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3495 global=1.5883\n",
            "  step 10: local=0.3467 global=1.5165\n",
            "  step 20: local=0.3420 global=1.4320\n",
            "  step 30: local=0.3464 global=1.5562\n",
            "  step 40: local=0.3404 global=1.4391\n",
            "  step 50: local=0.3376 global=1.4908\n",
            "  step 60: local=0.3364 global=1.5837\n",
            "  step 70: local=0.3395 global=1.5515\n",
            "  step 80: local=0.3431 global=1.4800\n",
            "  step 90: local=0.3369 global=1.3987\n",
            "  Layer 14 not converged (global=1.5524 > 0.8), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3370 global=1.4914\n",
            "  step 10: local=0.3367 global=1.4391\n",
            "  step 20: local=0.3460 global=1.4399\n",
            "  step 30: local=0.3483 global=1.5026\n",
            "  step 40: local=0.3504 global=1.3904\n",
            "  step 50: local=0.3397 global=1.5769\n",
            "  step 60: local=0.3397 global=1.4862\n",
            "  step 70: local=0.3438 global=1.4669\n",
            "  step 80: local=0.3394 global=1.4589\n",
            "  step 90: local=0.3339 global=1.5399\n",
            "  Layer 14 not converged (global=1.4378 > 0.8), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3315 global=1.4891\n",
            "  step 10: local=0.3359 global=1.5074\n",
            "  step 20: local=0.3394 global=1.5128\n",
            "  step 30: local=0.3427 global=1.4276\n",
            "  step 40: local=0.3358 global=1.4479\n",
            "  step 50: local=0.3363 global=1.4005\n",
            "  step 60: local=0.3391 global=1.6105\n",
            "  step 70: local=0.3335 global=1.6033\n",
            "  step 80: local=0.3389 global=1.4051\n",
            "  step 90: local=0.3367 global=1.4100\n",
            "  Layer 14 not converged (global=1.4768 > 0.8), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3387 global=1.4040\n",
            "  step 10: local=0.3372 global=1.4347\n",
            "  step 20: local=0.3385 global=1.5125\n",
            "  step 30: local=0.3374 global=1.4395\n",
            "  step 40: local=0.3317 global=1.5062\n",
            "  step 50: local=0.3376 global=1.5493\n",
            "  step 60: local=0.3389 global=1.4743\n",
            "  step 70: local=0.3373 global=1.4446\n",
            "  step 80: local=0.3322 global=1.4699\n",
            "  step 90: local=0.3411 global=1.4164\n",
            "  Layer 14 not converged (global=1.4742 > 0.8), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3342 global=1.4471\n",
            "  step 10: local=0.3257 global=1.5270\n",
            "  step 20: local=0.3418 global=1.4224\n",
            "  step 30: local=0.3370 global=1.4785\n",
            "  step 40: local=0.3326 global=1.5593\n",
            "  step 50: local=0.3409 global=1.5082\n",
            "  step 60: local=0.3347 global=1.4716\n",
            "  step 70: local=0.3300 global=1.3652\n",
            "  step 80: local=0.3402 global=1.4754\n",
            "  step 90: local=0.3353 global=1.4493\n",
            "  Layer 14 not converged (global=1.5043 > 0.8), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3351 global=1.4294\n",
            "  step 10: local=0.3388 global=1.4023\n",
            "  step 20: local=0.3395 global=1.4548\n",
            "  step 30: local=0.3375 global=1.4865\n",
            "  step 40: local=0.3362 global=1.4833\n",
            "  step 50: local=0.3331 global=1.4512\n",
            "  step 60: local=0.3281 global=1.4329\n",
            "  step 70: local=0.3295 global=1.5052\n",
            "  step 80: local=0.3364 global=1.5384\n",
            "  step 90: local=0.3305 global=1.5349\n",
            "  Layer 14 not converged (global=1.4892 > 0.8), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3306 global=1.3806\n",
            "  step 10: local=0.3405 global=1.4199\n",
            "  step 20: local=0.3348 global=1.4877\n",
            "  step 30: local=0.3463 global=1.4845\n",
            "  step 40: local=0.3274 global=1.4319\n",
            "  step 50: local=0.3264 global=1.4867\n",
            "  step 60: local=0.3339 global=1.5357\n",
            "  step 70: local=0.3295 global=1.3979\n",
            "  step 80: local=0.3339 global=1.4395\n",
            "  step 90: local=0.3340 global=1.5548\n",
            "  Layer 14 not converged (global=1.4615 > 0.8), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3331 global=1.5086\n",
            "  step 10: local=0.3353 global=1.5274\n",
            "  step 20: local=0.3289 global=1.4501\n",
            "  step 30: local=0.3362 global=1.4901\n",
            "  step 40: local=0.3289 global=1.3747\n",
            "  step 50: local=0.3324 global=1.4327\n",
            "  step 60: local=0.3319 global=1.4708\n",
            "  step 70: local=0.3307 global=1.4781\n",
            "  step 80: local=0.3407 global=1.4792\n",
            "  step 90: local=0.3349 global=1.4139\n",
            "  Layer 14 not converged (global=1.5118 > 0.8), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3326 global=1.3640\n",
            "  step 10: local=0.3328 global=1.4125\n",
            "  step 20: local=0.3380 global=1.4306\n",
            "  step 30: local=0.3311 global=1.4657\n",
            "  step 40: local=0.3288 global=1.4246\n",
            "  step 50: local=0.3269 global=1.5475\n",
            "  step 60: local=0.3287 global=1.4421\n",
            "  step 70: local=0.3272 global=1.4896\n",
            "  step 80: local=0.3268 global=1.4709\n",
            "  step 90: local=0.3323 global=1.4949\n",
            "  Layer 14 not converged (global=1.5890 > 0.8), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3337 global=1.3787\n",
            "  step 10: local=0.3296 global=1.4905\n",
            "  step 20: local=0.3285 global=1.5028\n",
            "  step 30: local=0.3328 global=1.3371\n",
            "  step 40: local=0.3327 global=1.4309\n",
            "  step 50: local=0.3259 global=1.5008\n",
            "  step 60: local=0.3299 global=1.3638\n",
            "  step 70: local=0.3304 global=1.4869\n",
            "  step 80: local=0.3257 global=1.4556\n",
            "  step 90: local=0.3356 global=1.4196\n",
            "  Layer 14 not converged (global=1.5440 > 0.8), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3350 global=1.4464\n",
            "  step 10: local=0.3202 global=1.4768\n",
            "  step 20: local=0.3268 global=1.4471\n",
            "  step 30: local=0.3274 global=1.4337\n",
            "  step 40: local=0.3361 global=1.5772\n",
            "  step 50: local=0.3303 global=1.5043\n",
            "  step 60: local=0.3221 global=1.4213\n",
            "  step 70: local=0.3374 global=1.5461\n",
            "  step 80: local=0.3233 global=1.4277\n",
            "  step 90: local=0.3329 global=1.4808\n",
            "  Layer 14 not converged (global=1.5111 > 0.8), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3282 global=1.5739\n",
            "  step 10: local=0.3372 global=1.5405\n",
            "  step 20: local=0.3378 global=1.4702\n",
            "  step 30: local=0.3408 global=1.3902\n",
            "  step 40: local=0.3342 global=1.4803\n",
            "  step 50: local=0.3208 global=1.4311\n",
            "  step 60: local=0.3330 global=1.4320\n",
            "  step 70: local=0.3392 global=1.4944\n",
            "  step 80: local=0.3298 global=1.3820\n",
            "  step 90: local=0.3334 global=1.5657\n",
            "  Layer 14 not converged (global=1.4803 > 0.8), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3284 global=1.4785\n",
            "  step 10: local=0.3338 global=1.4600\n",
            "  step 20: local=0.3296 global=1.4508\n",
            "  step 30: local=0.3219 global=1.5319\n",
            "  step 40: local=0.3390 global=1.4792\n",
            "  step 50: local=0.3323 global=1.4988\n",
            "  step 60: local=0.3356 global=1.5027\n",
            "  step 70: local=0.3293 global=1.4197\n",
            "  step 80: local=0.3300 global=1.4416\n",
            "  step 90: local=0.3352 global=1.3923\n",
            "  [WARN] Layer 14 did not converge after 20 repeats (global=1.4230)\n",
            "  [TIME] Layer 14 took 615.4s | Total: 158.5min | ETA: 103.0min (13 layers left)\n",
            "\n",
            "--- Layer 15/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5626 global=1.7279\n",
            "  step 10: local=0.5420 global=1.7030\n",
            "  step 20: local=0.5240 global=1.5030\n",
            "  step 30: local=0.5334 global=1.4921\n",
            "  step 40: local=0.5178 global=1.4860\n",
            "  step 50: local=0.5027 global=1.5133\n",
            "  step 60: local=0.5030 global=1.5925\n",
            "  step 70: local=0.4867 global=1.5120\n",
            "  step 80: local=0.4761 global=1.5796\n",
            "  step 90: local=0.4696 global=1.6135\n",
            "  Layer 15 not converged (global=1.5786 > 0.8), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4579 global=1.5290\n",
            "  step 10: local=0.4635 global=1.4970\n",
            "  step 20: local=0.4563 global=1.5271\n",
            "  step 30: local=0.4528 global=1.4678\n",
            "  step 40: local=0.4441 global=1.4980\n",
            "  step 50: local=0.4397 global=1.5714\n",
            "  step 60: local=0.4297 global=1.4687\n",
            "  step 70: local=0.4460 global=1.5245\n",
            "  step 80: local=0.4212 global=1.5980\n",
            "  step 90: local=0.4129 global=1.5518\n",
            "  Layer 15 not converged (global=1.4793 > 0.8), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4124 global=1.5024\n",
            "  step 10: local=0.4097 global=1.4045\n",
            "  step 20: local=0.3948 global=1.5126\n",
            "  step 30: local=0.3990 global=1.4780\n",
            "  step 40: local=0.3956 global=1.4564\n",
            "  step 50: local=0.3991 global=1.4234\n",
            "  step 60: local=0.3872 global=1.4788\n",
            "  step 70: local=0.3799 global=1.5201\n",
            "  step 80: local=0.3801 global=1.5153\n",
            "  step 90: local=0.3849 global=1.4697\n",
            "  Layer 15 not converged (global=1.5003 > 0.8), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3761 global=1.4520\n",
            "  step 10: local=0.3751 global=1.5259\n",
            "  step 20: local=0.3751 global=1.5591\n",
            "  step 30: local=0.3652 global=1.5555\n",
            "  step 40: local=0.3614 global=1.3959\n",
            "  step 50: local=0.3644 global=1.4340\n",
            "  step 60: local=0.3746 global=1.5005\n",
            "  step 70: local=0.3560 global=1.4941\n",
            "  step 80: local=0.3602 global=1.4367\n",
            "  step 90: local=0.3646 global=1.5002\n",
            "  Layer 15 not converged (global=1.4610 > 0.8), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3538 global=1.5467\n",
            "  step 10: local=0.3431 global=1.4072\n",
            "  step 20: local=0.3599 global=1.4498\n",
            "  step 30: local=0.3514 global=1.5666\n",
            "  step 40: local=0.3542 global=1.5124\n",
            "  step 50: local=0.3504 global=1.5253\n",
            "  step 60: local=0.3548 global=1.4569\n",
            "  step 70: local=0.3544 global=1.5004\n",
            "  step 80: local=0.3391 global=1.3760\n",
            "  step 90: local=0.3528 global=1.4353\n",
            "  Layer 15 not converged (global=1.4959 > 0.8), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3524 global=1.4720\n",
            "  step 10: local=0.3513 global=1.4908\n",
            "  step 20: local=0.3533 global=1.4845\n",
            "  step 30: local=0.3470 global=1.4169\n",
            "  step 40: local=0.3486 global=1.3638\n",
            "  step 50: local=0.3451 global=1.4283\n",
            "  step 60: local=0.3404 global=1.4307\n",
            "  step 70: local=0.3415 global=1.4658\n",
            "  step 80: local=0.3496 global=1.4264\n",
            "  step 90: local=0.3405 global=1.5461\n",
            "  Layer 15 not converged (global=1.4512 > 0.8), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3424 global=1.4474\n",
            "  step 10: local=0.3395 global=1.4934\n",
            "  step 20: local=0.3402 global=1.4679\n",
            "  step 30: local=0.3380 global=1.4901\n",
            "  step 40: local=0.3432 global=1.3781\n",
            "  step 50: local=0.3463 global=1.4975\n",
            "  step 60: local=0.3348 global=1.5025\n",
            "  step 70: local=0.3376 global=1.3442\n",
            "  step 80: local=0.3491 global=1.4259\n",
            "  step 90: local=0.3474 global=1.4974\n",
            "  Layer 15 not converged (global=1.5036 > 0.8), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3385 global=1.3595\n",
            "  step 10: local=0.3382 global=1.4883\n",
            "  step 20: local=0.3368 global=1.4629\n",
            "  step 30: local=0.3342 global=1.4107\n",
            "  step 40: local=0.3351 global=1.4460\n",
            "  step 50: local=0.3373 global=1.4743\n",
            "  step 60: local=0.3374 global=1.4445\n",
            "  step 70: local=0.3407 global=1.4312\n",
            "  step 80: local=0.3407 global=1.5708\n",
            "  step 90: local=0.3354 global=1.4970\n",
            "  Layer 15 not converged (global=1.6459 > 0.8), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3388 global=1.4262\n",
            "  step 10: local=0.3346 global=1.5495\n",
            "  step 20: local=0.3303 global=1.4240\n",
            "  step 30: local=0.3385 global=1.4783\n",
            "  step 40: local=0.3394 global=1.5742\n",
            "  step 50: local=0.3393 global=1.5329\n",
            "  step 60: local=0.3408 global=1.4684\n",
            "  step 70: local=0.3338 global=1.3866\n",
            "  step 80: local=0.3312 global=1.4785\n",
            "  step 90: local=0.3382 global=1.4277\n",
            "  Layer 15 not converged (global=1.4511 > 0.8), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3312 global=1.4287\n",
            "  step 10: local=0.3320 global=1.4871\n",
            "  step 20: local=0.3402 global=1.3779\n",
            "  step 30: local=0.3448 global=1.5576\n",
            "  step 40: local=0.3311 global=1.4774\n",
            "  step 50: local=0.3350 global=1.4488\n",
            "  step 60: local=0.3349 global=1.4483\n",
            "  step 70: local=0.3340 global=1.5201\n",
            "  step 80: local=0.3369 global=1.4857\n",
            "  step 90: local=0.3331 global=1.4984\n",
            "  Layer 15 not converged (global=1.5024 > 0.8), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3370 global=1.5014\n",
            "  step 10: local=0.3417 global=1.4212\n",
            "  step 20: local=0.3372 global=1.4360\n",
            "  step 30: local=0.3397 global=1.3869\n",
            "  step 40: local=0.3336 global=1.5941\n",
            "  step 50: local=0.3304 global=1.5931\n",
            "  step 60: local=0.3327 global=1.3969\n",
            "  step 70: local=0.3365 global=1.3997\n",
            "  step 80: local=0.3247 global=1.3950\n",
            "  step 90: local=0.3347 global=1.4196\n",
            "  Layer 15 not converged (global=1.4688 > 0.8), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3330 global=1.5030\n",
            "  step 10: local=0.3282 global=1.4283\n",
            "  step 20: local=0.3344 global=1.4924\n",
            "  step 30: local=0.3228 global=1.5364\n",
            "  step 40: local=0.3345 global=1.4578\n",
            "  step 50: local=0.3283 global=1.4270\n",
            "  step 60: local=0.3383 global=1.4613\n",
            "  step 70: local=0.3386 global=1.4042\n",
            "  step 80: local=0.3319 global=1.4359\n",
            "  step 90: local=0.3193 global=1.5106\n",
            "  Layer 15 not converged (global=1.4226 > 0.8), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3379 global=1.4135\n",
            "  step 10: local=0.3325 global=1.4684\n",
            "  step 20: local=0.3276 global=1.5446\n",
            "  step 30: local=0.3316 global=1.5001\n",
            "  step 40: local=0.3273 global=1.4565\n",
            "  step 50: local=0.3346 global=1.3614\n",
            "  step 60: local=0.3265 global=1.4655\n",
            "  step 70: local=0.3256 global=1.4365\n",
            "  step 80: local=0.3353 global=1.4182\n",
            "  step 90: local=0.3340 global=1.3829\n",
            "  Layer 15 not converged (global=1.4735 > 0.8), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3339 global=1.4412\n",
            "  step 10: local=0.3278 global=1.4795\n",
            "  step 20: local=0.3223 global=1.4776\n",
            "  step 30: local=0.3345 global=1.4355\n",
            "  step 40: local=0.3265 global=1.4179\n",
            "  step 50: local=0.3370 global=1.4973\n",
            "  step 60: local=0.3274 global=1.5288\n",
            "  step 70: local=0.3293 global=1.5251\n",
            "  step 80: local=0.3331 global=1.3691\n",
            "  step 90: local=0.3298 global=1.4075\n",
            "  Layer 15 not converged (global=1.5019 > 0.8), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3377 global=1.4777\n",
            "  step 10: local=0.3280 global=1.4694\n",
            "  step 20: local=0.3256 global=1.4154\n",
            "  step 30: local=0.3246 global=1.4761\n",
            "  step 40: local=0.3275 global=1.5250\n",
            "  step 50: local=0.3288 global=1.3867\n",
            "  step 60: local=0.3301 global=1.4296\n",
            "  step 70: local=0.3317 global=1.5449\n",
            "  step 80: local=0.3288 global=1.4937\n",
            "  step 90: local=0.3272 global=1.5082\n",
            "  Layer 15 not converged (global=1.4305 > 0.8), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3323 global=1.4410\n",
            "  step 10: local=0.3273 global=1.4840\n",
            "  step 20: local=0.3289 global=1.3612\n",
            "  step 30: local=0.3398 global=1.4195\n",
            "  step 40: local=0.3246 global=1.4545\n",
            "  step 50: local=0.3346 global=1.4748\n",
            "  step 60: local=0.3261 global=1.4698\n",
            "  step 70: local=0.3278 global=1.4042\n",
            "  step 80: local=0.3330 global=1.3514\n",
            "  step 90: local=0.3231 global=1.4099\n",
            "  Layer 15 not converged (global=1.4344 > 0.8), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3288 global=1.4184\n",
            "  step 10: local=0.3332 global=1.4538\n",
            "  step 20: local=0.3338 global=1.4152\n",
            "  step 30: local=0.3319 global=1.5333\n",
            "  step 40: local=0.3291 global=1.4345\n",
            "  step 50: local=0.3273 global=1.4805\n",
            "  step 60: local=0.3271 global=1.4548\n",
            "  step 70: local=0.3306 global=1.4784\n",
            "  step 80: local=0.3341 global=1.3662\n",
            "  step 90: local=0.3294 global=1.4863\n",
            "  Layer 15 not converged (global=1.4835 > 0.8), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3178 global=1.4901\n",
            "  step 10: local=0.3327 global=1.3338\n",
            "  step 20: local=0.3308 global=1.4151\n",
            "  step 30: local=0.3260 global=1.4879\n",
            "  step 40: local=0.3273 global=1.3472\n",
            "  step 50: local=0.3229 global=1.4776\n",
            "  step 60: local=0.3237 global=1.4496\n",
            "  step 70: local=0.3335 global=1.4001\n",
            "  step 80: local=0.3278 global=1.4335\n",
            "  step 90: local=0.3225 global=1.4650\n",
            "  Layer 15 not converged (global=1.4772 > 0.8), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3325 global=1.4354\n",
            "  step 10: local=0.3279 global=1.4240\n",
            "  step 20: local=0.3310 global=1.5607\n",
            "  step 30: local=0.3245 global=1.4878\n",
            "  step 40: local=0.3270 global=1.4143\n",
            "  step 50: local=0.3352 global=1.5396\n",
            "  step 60: local=0.3250 global=1.4139\n",
            "  step 70: local=0.3169 global=1.4682\n",
            "  step 80: local=0.3200 global=1.5661\n",
            "  step 90: local=0.3286 global=1.5233\n",
            "  Layer 15 not converged (global=1.4519 > 0.8), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3257 global=1.4601\n",
            "  step 10: local=0.3330 global=1.3795\n",
            "  step 20: local=0.3279 global=1.4719\n",
            "  step 30: local=0.3194 global=1.4196\n",
            "  step 40: local=0.3228 global=1.4212\n",
            "  step 50: local=0.3318 global=1.4791\n",
            "  step 60: local=0.3289 global=1.3719\n",
            "  step 70: local=0.3267 global=1.5485\n",
            "  step 80: local=0.3265 global=1.4712\n",
            "  step 90: local=0.3363 global=1.4423\n",
            "  [WARN] Layer 15 did not converge after 20 repeats (global=1.4772)\n",
            "  [TIME] Layer 15 took 599.5s | Total: 168.5min | ETA: 96.2min (12 layers left)\n",
            "\n",
            "--- Layer 16/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.6694 global=1.7388\n",
            "  step 10: local=0.6669 global=1.8008\n",
            "  step 20: local=0.6410 global=1.7661\n",
            "  step 30: local=0.6445 global=1.7354\n",
            "  step 40: local=0.6331 global=1.7335\n",
            "  step 50: local=0.6298 global=1.6516\n",
            "  step 60: local=0.6238 global=1.6439\n",
            "  step 70: local=0.6198 global=1.5903\n",
            "  step 80: local=0.6129 global=1.7937\n",
            "  step 90: local=0.6027 global=1.8101\n",
            "  Layer 16 not converged (global=1.6504 > 0.8), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.6025 global=1.5879\n",
            "  step 10: local=0.6065 global=1.5734\n",
            "  step 20: local=0.5987 global=1.5701\n",
            "  step 30: local=0.5741 global=1.5887\n",
            "  step 40: local=0.5882 global=1.6766\n",
            "  step 50: local=0.5786 global=1.5916\n",
            "  step 60: local=0.5873 global=1.6616\n",
            "  step 70: local=0.5631 global=1.7007\n",
            "  step 80: local=0.5773 global=1.6119\n",
            "  step 90: local=0.5698 global=1.5777\n",
            "  Layer 16 not converged (global=1.5734 > 0.8), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5597 global=1.6082\n",
            "  step 10: local=0.5664 global=1.5387\n",
            "  step 20: local=0.5646 global=1.5815\n",
            "  step 30: local=0.5520 global=1.6650\n",
            "  step 40: local=0.5610 global=1.5549\n",
            "  step 50: local=0.5345 global=1.6128\n",
            "  step 60: local=0.5408 global=1.6800\n",
            "  step 70: local=0.5363 global=1.6374\n",
            "  step 80: local=0.5420 global=1.5769\n",
            "  step 90: local=0.5307 global=1.4877\n",
            "  Layer 16 not converged (global=1.6449 > 0.8), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5184 global=1.5993\n",
            "  step 10: local=0.5378 global=1.5522\n",
            "  step 20: local=0.5396 global=1.5326\n",
            "  step 30: local=0.5374 global=1.4972\n",
            "  step 40: local=0.5228 global=1.5504\n",
            "  step 50: local=0.5349 global=1.6231\n",
            "  step 60: local=0.5146 global=1.6101\n",
            "  step 70: local=0.5274 global=1.5483\n",
            "  step 80: local=0.5118 global=1.5286\n",
            "  step 90: local=0.5083 global=1.6130\n",
            "  Layer 16 not converged (global=1.5413 > 0.8), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5091 global=1.6603\n",
            "  step 10: local=0.5047 global=1.6326\n",
            "  step 20: local=0.5121 global=1.4756\n",
            "  step 30: local=0.5220 global=1.5150\n",
            "  step 40: local=0.5225 global=1.5842\n",
            "  step 50: local=0.5218 global=1.5947\n",
            "  step 60: local=0.5030 global=1.5245\n",
            "  step 70: local=0.5114 global=1.5664\n",
            "  step 80: local=0.5084 global=1.6261\n",
            "  step 90: local=0.5210 global=1.4930\n",
            "  Layer 16 not converged (global=1.5036 > 0.8), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5036 global=1.5301\n",
            "  step 10: local=0.5123 global=1.6438\n",
            "  step 20: local=0.4956 global=1.5900\n",
            "  step 30: local=0.5113 global=1.6112\n",
            "  step 40: local=0.5001 global=1.5419\n",
            "  step 50: local=0.5190 global=1.5758\n",
            "  step 60: local=0.5014 global=1.4507\n",
            "  step 70: local=0.5130 global=1.5142\n",
            "  step 80: local=0.5138 global=1.5574\n",
            "  step 90: local=0.5071 global=1.5843\n",
            "  Layer 16 not converged (global=1.5779 > 0.8), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4941 global=1.5668\n",
            "  step 10: local=0.4916 global=1.4950\n",
            "  step 20: local=0.4914 global=1.4402\n",
            "  step 30: local=0.5011 global=1.5075\n",
            "  step 40: local=0.4957 global=1.5004\n",
            "  step 50: local=0.5055 global=1.5361\n",
            "  step 60: local=0.5132 global=1.5002\n",
            "  step 70: local=0.5149 global=1.6265\n",
            "  step 80: local=0.4982 global=1.5280\n",
            "  step 90: local=0.5065 global=1.5752\n",
            "  Layer 16 not converged (global=1.5409 > 0.8), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5138 global=1.5454\n",
            "  step 10: local=0.5100 global=1.5604\n",
            "  step 20: local=0.4831 global=1.4588\n",
            "  step 30: local=0.5002 global=1.5774\n",
            "  step 40: local=0.5171 global=1.5797\n",
            "  step 50: local=0.4965 global=1.4243\n",
            "  step 60: local=0.5028 global=1.5024\n",
            "  step 70: local=0.5064 global=1.5772\n",
            "  step 80: local=0.4872 global=1.4406\n",
            "  step 90: local=0.5091 global=1.5650\n",
            "  Layer 16 not converged (global=1.6202 > 0.8), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4993 global=1.5369\n",
            "  step 10: local=0.4935 global=1.4976\n",
            "  step 20: local=0.5098 global=1.5196\n",
            "  step 30: local=0.4923 global=1.5529\n",
            "  step 40: local=0.4896 global=1.5274\n",
            "  step 50: local=0.4981 global=1.5139\n",
            "  step 60: local=0.5008 global=1.6583\n",
            "  step 70: local=0.5082 global=1.5746\n",
            "  step 80: local=0.4852 global=1.5090\n",
            "  step 90: local=0.5088 global=1.6334\n",
            "  Layer 16 not converged (global=1.5187 > 0.8), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4870 global=1.4924\n",
            "  step 10: local=0.4995 global=1.5600\n",
            "  step 20: local=0.5020 global=1.6625\n",
            "  step 30: local=0.4904 global=1.6168\n",
            "  step 40: local=0.5022 global=1.5517\n",
            "  step 50: local=0.5146 global=1.4627\n",
            "  step 60: local=0.4863 global=1.5720\n",
            "  step 70: local=0.5028 global=1.5099\n",
            "  step 80: local=0.4840 global=1.5098\n",
            "  step 90: local=0.4855 global=1.5631\n",
            "  Layer 16 not converged (global=1.4865 > 0.8), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4869 global=1.4612\n",
            "  step 10: local=0.4925 global=1.6456\n",
            "  step 20: local=0.4979 global=1.5642\n",
            "  step 30: local=0.4869 global=1.5299\n",
            "  step 40: local=0.4837 global=1.5324\n",
            "  step 50: local=0.5026 global=1.6013\n",
            "  step 60: local=0.4971 global=1.5775\n",
            "  step 70: local=0.4968 global=1.5792\n",
            "  step 80: local=0.4813 global=1.5791\n",
            "  step 90: local=0.4908 global=1.5007\n",
            "  Layer 16 not converged (global=1.6054 > 0.8), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4942 global=1.5111\n",
            "  step 10: local=0.5013 global=1.4601\n",
            "  step 20: local=0.5075 global=1.6757\n",
            "  step 30: local=0.4825 global=1.6830\n",
            "  step 40: local=0.4779 global=1.4780\n",
            "  step 50: local=0.5068 global=1.4742\n",
            "  step 60: local=0.4963 global=1.4745\n",
            "  step 70: local=0.4915 global=1.4957\n",
            "  step 80: local=0.4846 global=1.5876\n",
            "  step 90: local=0.4959 global=1.5056\n",
            "  Layer 16 not converged (global=1.5154 > 0.8), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4907 global=1.5815\n",
            "  step 10: local=0.4768 global=1.6170\n",
            "  step 20: local=0.4815 global=1.5372\n",
            "  step 30: local=0.4752 global=1.5015\n",
            "  step 40: local=0.5020 global=1.5432\n",
            "  step 50: local=0.4939 global=1.4736\n",
            "  step 60: local=0.4846 global=1.5186\n",
            "  step 70: local=0.4967 global=1.5996\n",
            "  step 80: local=0.4836 global=1.4981\n",
            "  step 90: local=0.4991 global=1.5516\n",
            "  Layer 16 not converged (global=1.6204 > 0.8), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4882 global=1.6240\n",
            "  step 10: local=0.4985 global=1.5845\n",
            "  step 20: local=0.4886 global=1.5297\n",
            "  step 30: local=0.4891 global=1.4431\n",
            "  step 40: local=0.4888 global=1.5489\n",
            "  step 50: local=0.4973 global=1.5098\n",
            "  step 60: local=0.4922 global=1.4919\n",
            "  step 70: local=0.4972 global=1.4564\n",
            "  step 80: local=0.4977 global=1.5113\n",
            "  step 90: local=0.4838 global=1.5825\n",
            "  Layer 16 not converged (global=1.5701 > 0.8), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4764 global=1.5675\n",
            "  step 10: local=0.4890 global=1.5172\n",
            "  step 20: local=0.4763 global=1.4966\n",
            "  step 30: local=0.4858 global=1.5822\n",
            "  step 40: local=0.5005 global=1.6231\n",
            "  step 50: local=0.4879 global=1.5995\n",
            "  step 60: local=0.4885 global=1.4489\n",
            "  step 70: local=0.4893 global=1.4863\n",
            "  step 80: local=0.4885 global=1.5608\n",
            "  step 90: local=0.4869 global=1.5692\n",
            "  Layer 16 not converged (global=1.5294 > 0.8), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5051 global=1.4979\n",
            "  step 10: local=0.4986 global=1.5414\n",
            "  step 20: local=0.4966 global=1.5998\n",
            "  step 30: local=0.4820 global=1.4723\n",
            "  step 40: local=0.4916 global=1.5054\n",
            "  step 50: local=0.4909 global=1.6235\n",
            "  step 60: local=0.4850 global=1.5707\n",
            "  step 70: local=0.4850 global=1.5894\n",
            "  step 80: local=0.4841 global=1.5236\n",
            "  step 90: local=0.4972 global=1.5586\n",
            "  Layer 16 not converged (global=1.5058 > 0.8), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4798 global=1.4351\n",
            "  step 10: local=0.4922 global=1.4964\n",
            "  step 20: local=0.5002 global=1.5427\n",
            "  step 30: local=0.4844 global=1.5626\n",
            "  step 40: local=0.4899 global=1.5469\n",
            "  step 50: local=0.4926 global=1.4744\n",
            "  step 60: local=0.4935 global=1.4219\n",
            "  step 70: local=0.4873 global=1.4896\n",
            "  step 80: local=0.4749 global=1.4881\n",
            "  step 90: local=0.4805 global=1.5167\n",
            "  Layer 16 not converged (global=1.5707 > 0.8), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4863 global=1.4857\n",
            "  step 10: local=0.4831 global=1.6092\n",
            "  step 20: local=0.4883 global=1.5125\n",
            "  step 30: local=0.4922 global=1.5581\n",
            "  step 40: local=0.4922 global=1.5316\n",
            "  step 50: local=0.4788 global=1.5494\n",
            "  step 60: local=0.4826 global=1.4437\n",
            "  step 70: local=0.4897 global=1.5624\n",
            "  step 80: local=0.4873 global=1.5699\n",
            "  step 90: local=0.4985 global=1.4097\n",
            "  Layer 16 not converged (global=1.5344 > 0.8), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4970 global=1.4909\n",
            "  step 10: local=0.4879 global=1.5701\n",
            "  step 20: local=0.4671 global=1.4263\n",
            "  step 30: local=0.4952 global=1.5530\n",
            "  step 40: local=0.4863 global=1.5215\n",
            "  step 50: local=0.4873 global=1.4879\n",
            "  step 60: local=0.4839 global=1.5059\n",
            "  step 70: local=0.4986 global=1.5422\n",
            "  step 80: local=0.4993 global=1.5153\n",
            "  step 90: local=0.4896 global=1.4999\n",
            "  Layer 16 not converged (global=1.4993 > 0.8), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5056 global=1.6518\n",
            "  step 10: local=0.4812 global=1.5632\n",
            "  step 20: local=0.4835 global=1.4976\n",
            "  step 30: local=0.4983 global=1.6218\n",
            "  step 40: local=0.4904 global=1.4791\n",
            "  step 50: local=0.4938 global=1.5505\n",
            "  step 60: local=0.4738 global=1.6486\n",
            "  step 70: local=0.4879 global=1.6027\n",
            "  step 80: local=0.4925 global=1.5415\n",
            "  step 90: local=0.4965 global=1.4513\n",
            "  [WARN] Layer 16 did not converge after 20 repeats (global=1.6123)\n",
            "  [TIME] Layer 16 took 583.9s | Total: 178.2min | ETA: 89.1min (11 layers left)\n",
            "\n",
            "--- Layer 17/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.7701 global=1.9618\n",
            "  step 10: local=0.7590 global=1.9023\n",
            "  step 20: local=0.7607 global=1.8889\n",
            "  step 30: local=0.7658 global=1.9478\n",
            "  step 40: local=0.7598 global=1.8556\n",
            "  step 50: local=0.7517 global=2.0465\n",
            "  step 60: local=0.7381 global=1.9427\n",
            "  step 70: local=0.7277 global=1.9200\n",
            "  step 80: local=0.7335 global=1.9195\n",
            "  step 90: local=0.7185 global=2.0034\n",
            "  Layer 17 not converged (global=1.8650 > 0.8), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.7178 global=1.9680\n",
            "  step 10: local=0.7037 global=1.9690\n",
            "  step 20: local=0.7187 global=1.9820\n",
            "  step 30: local=0.7221 global=1.8748\n",
            "  step 40: local=0.7219 global=1.8942\n",
            "  step 50: local=0.7105 global=1.8316\n",
            "  step 60: local=0.6994 global=2.0605\n",
            "  step 70: local=0.6914 global=2.0883\n",
            "  step 80: local=0.7069 global=1.8794\n",
            "  step 90: local=0.6980 global=1.8374\n",
            "  Layer 17 not converged (global=1.9093 > 0.8), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.6884 global=1.8438\n",
            "  step 10: local=0.6899 global=1.8752\n",
            "  step 20: local=0.6903 global=2.0000\n",
            "  step 30: local=0.6845 global=1.8999\n",
            "  step 40: local=0.6826 global=1.9673\n",
            "  step 50: local=0.6761 global=2.0443\n",
            "  step 60: local=0.6751 global=1.9161\n",
            "  step 70: local=0.6773 global=1.8978\n",
            "  step 80: local=0.6707 global=1.9207\n",
            "  step 90: local=0.6738 global=1.8663\n",
            "  Layer 17 not converged (global=1.9274 > 0.8), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.6790 global=1.9101\n",
            "  step 10: local=0.6496 global=2.0009\n",
            "  step 20: local=0.6507 global=1.9007\n",
            "  step 30: local=0.6535 global=1.9412\n",
            "  step 40: local=0.6482 global=2.0173\n",
            "  step 50: local=0.6482 global=1.9767\n",
            "  step 60: local=0.6416 global=1.8928\n",
            "  step 70: local=0.6277 global=1.8070\n",
            "  step 80: local=0.6360 global=1.9411\n",
            "  step 90: local=0.6421 global=1.8892\n",
            "  Layer 17 not converged (global=1.9481 > 0.8), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.6278 global=1.8697\n",
            "  step 10: local=0.6448 global=1.8426\n",
            "  step 20: local=0.6303 global=1.9301\n",
            "  step 30: local=0.6314 global=2.0110\n",
            "  step 40: local=0.6180 global=1.9448\n",
            "  step 50: local=0.6239 global=1.9028\n",
            "  step 60: local=0.6117 global=1.9048\n",
            "  step 70: local=0.6284 global=1.9708\n",
            "  step 80: local=0.6389 global=2.0517\n",
            "  step 90: local=0.6228 global=1.9992\n",
            "  Layer 17 not converged (global=1.9624 > 0.8), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.6249 global=1.8398\n",
            "  step 10: local=0.6369 global=1.8616\n",
            "  step 20: local=0.6326 global=1.9604\n",
            "  step 30: local=0.6188 global=1.9588\n",
            "  step 40: local=0.6180 global=1.9067\n",
            "  step 50: local=0.6213 global=1.9180\n",
            "  step 60: local=0.6245 global=2.0090\n",
            "  step 70: local=0.6186 global=1.8660\n",
            "  step 80: local=0.6291 global=1.8946\n",
            "  step 90: local=0.6183 global=2.0209\n",
            "  Layer 17 not converged (global=1.9146 > 0.8), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.6169 global=1.9530\n",
            "  step 10: local=0.6192 global=1.9796\n",
            "  step 20: local=0.6197 global=1.9076\n",
            "  step 30: local=0.6304 global=1.9702\n",
            "  step 40: local=0.6085 global=1.8050\n",
            "  step 50: local=0.6346 global=1.8761\n",
            "  step 60: local=0.6337 global=1.9213\n",
            "  step 70: local=0.6250 global=1.9824\n",
            "  step 80: local=0.6212 global=1.9405\n",
            "  step 90: local=0.6169 global=1.8558\n",
            "  Layer 17 not converged (global=1.9616 > 0.8), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.6107 global=1.8095\n",
            "  step 10: local=0.6150 global=1.9014\n",
            "  step 20: local=0.6171 global=1.8654\n",
            "  step 30: local=0.6149 global=1.9283\n",
            "  step 40: local=0.6056 global=1.8613\n",
            "  step 50: local=0.6186 global=2.0027\n",
            "  step 60: local=0.6201 global=1.9291\n",
            "  step 70: local=0.6251 global=1.9440\n",
            "  step 80: local=0.6136 global=1.9228\n",
            "  step 90: local=0.6352 global=1.9321\n",
            "  Layer 17 not converged (global=2.0756 > 0.8), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.6172 global=1.8340\n",
            "  step 10: local=0.6224 global=1.9861\n",
            "  step 20: local=0.6150 global=1.9442\n",
            "  step 30: local=0.6180 global=1.8125\n",
            "  step 40: local=0.6310 global=1.8584\n",
            "  step 50: local=0.6275 global=1.9450\n",
            "  step 60: local=0.6120 global=1.8253\n",
            "  step 70: local=0.6081 global=1.9514\n",
            "  step 80: local=0.6314 global=1.9519\n",
            "  step 90: local=0.6115 global=1.8805\n",
            "  Layer 17 not converged (global=2.0045 > 0.8), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.6178 global=1.8887\n",
            "  step 10: local=0.6175 global=1.9131\n",
            "  step 20: local=0.6194 global=1.9166\n",
            "  step 30: local=0.6128 global=1.8830\n",
            "  step 40: local=0.6214 global=2.0552\n",
            "  step 50: local=0.6123 global=1.9546\n",
            "  step 60: local=0.6123 global=1.8838\n",
            "  step 70: local=0.6274 global=2.0359\n",
            "  step 80: local=0.6127 global=1.8614\n",
            "  step 90: local=0.6118 global=1.9334\n",
            "  Layer 17 not converged (global=1.9594 > 0.8), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.6099 global=2.0247\n",
            "  step 10: local=0.6128 global=1.9890\n",
            "  step 20: local=0.6184 global=1.9499\n",
            "  step 30: local=0.6153 global=1.8249\n",
            "  step 40: local=0.6135 global=1.9620\n",
            "  step 50: local=0.6070 global=1.9024\n",
            "  step 60: local=0.6031 global=1.8884\n",
            "  step 70: local=0.6242 global=1.9481\n",
            "  step 80: local=0.6203 global=1.8559\n",
            "  step 90: local=0.6181 global=2.0463\n",
            "  Layer 17 not converged (global=1.9447 > 0.8), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.6164 global=1.9426\n",
            "  step 10: local=0.6224 global=1.9207\n",
            "  step 20: local=0.6111 global=1.9190\n",
            "  step 30: local=0.6231 global=2.0035\n",
            "  step 40: local=0.6107 global=1.9681\n",
            "  step 50: local=0.6046 global=1.9687\n",
            "  step 60: local=0.6158 global=1.9820\n",
            "  step 70: local=0.6183 global=1.8749\n",
            "  step 80: local=0.6061 global=1.8944\n",
            "  step 90: local=0.6128 global=1.8318\n",
            "  Layer 17 not converged (global=1.8770 > 0.8), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.6248 global=2.0607\n",
            "  step 10: local=0.6020 global=2.0883\n",
            "  step 20: local=0.6164 global=1.8793\n",
            "  step 30: local=0.6231 global=1.8375\n",
            "  step 40: local=0.6219 global=1.8439\n",
            "  step 50: local=0.6074 global=1.8749\n",
            "  step 60: local=0.6096 global=2.0001\n",
            "  step 70: local=0.6152 global=1.9002\n",
            "  step 80: local=0.6095 global=1.9676\n",
            "  step 90: local=0.6269 global=2.0440\n",
            "  Layer 17 not converged (global=1.9901 > 0.8), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.6010 global=1.9159\n",
            "  step 10: local=0.6156 global=1.8979\n",
            "  step 20: local=0.6273 global=1.9207\n",
            "  step 30: local=0.6201 global=1.8662\n",
            "  step 40: local=0.6201 global=1.9100\n",
            "  step 50: local=0.6063 global=2.0007\n",
            "  step 60: local=0.6095 global=1.9011\n",
            "  step 70: local=0.6213 global=1.9412\n",
            "  step 80: local=0.6124 global=2.0171\n",
            "  step 90: local=0.6193 global=1.9771\n",
            "  Layer 17 not converged (global=1.8973 > 0.8), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.6034 global=1.8930\n",
            "  step 10: local=0.6030 global=1.8072\n",
            "  step 20: local=0.6030 global=1.9410\n",
            "  step 30: local=0.6052 global=1.8891\n",
            "  step 40: local=0.6124 global=1.8702\n",
            "  step 50: local=0.6111 global=1.8431\n",
            "  step 60: local=0.6076 global=1.9305\n",
            "  step 70: local=0.6068 global=2.0106\n",
            "  step 80: local=0.5992 global=1.9445\n",
            "  step 90: local=0.6129 global=1.9028\n",
            "  Layer 17 not converged (global=1.9267 > 0.8), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5927 global=1.9053\n",
            "  step 10: local=0.6053 global=1.9707\n",
            "  step 20: local=0.6113 global=2.0519\n",
            "  step 30: local=0.6092 global=1.9991\n",
            "  step 40: local=0.6120 global=1.8395\n",
            "  step 50: local=0.6128 global=1.8608\n",
            "  step 60: local=0.6207 global=1.9603\n",
            "  step 70: local=0.6236 global=1.9584\n",
            "  step 80: local=0.5984 global=1.9064\n",
            "  step 90: local=0.6164 global=1.9177\n",
            "  Layer 17 not converged (global=1.8962 > 0.8), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.6260 global=2.0086\n",
            "  step 10: local=0.6101 global=1.8661\n",
            "  step 20: local=0.6102 global=1.8945\n",
            "  step 30: local=0.6065 global=2.0208\n",
            "  step 40: local=0.6094 global=1.9528\n",
            "  step 50: local=0.6055 global=1.9795\n",
            "  step 60: local=0.6100 global=1.9075\n",
            "  step 70: local=0.6190 global=1.9702\n",
            "  step 80: local=0.5973 global=1.8047\n",
            "  step 90: local=0.6196 global=1.8763\n",
            "  Layer 17 not converged (global=1.9702 > 0.8), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.6208 global=1.9215\n",
            "  step 10: local=0.6176 global=1.9827\n",
            "  step 20: local=0.6096 global=1.9405\n",
            "  step 30: local=0.6095 global=1.8559\n",
            "  step 40: local=0.6147 global=1.8095\n",
            "  step 50: local=0.6111 global=1.9013\n",
            "  step 60: local=0.6026 global=1.8654\n",
            "  step 70: local=0.6035 global=1.9290\n",
            "  step 80: local=0.6110 global=1.8613\n",
            "  step 90: local=0.6105 global=2.0023\n",
            "  Layer 17 not converged (global=1.8986 > 0.8), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.6129 global=1.9294\n",
            "  step 10: local=0.6120 global=1.9442\n",
            "  step 20: local=0.5994 global=1.9230\n",
            "  step 30: local=0.5982 global=1.9314\n",
            "  step 40: local=0.6162 global=1.8341\n",
            "  step 50: local=0.6086 global=1.9859\n",
            "  step 60: local=0.6184 global=1.9442\n",
            "  step 70: local=0.6117 global=1.8126\n",
            "  step 80: local=0.6195 global=1.8585\n",
            "  step 90: local=0.6136 global=1.9447\n",
            "  Layer 17 not converged (global=1.9697 > 0.8), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.6167 global=1.8244\n",
            "  step 10: local=0.6070 global=1.9510\n",
            "  step 20: local=0.6112 global=1.9514\n",
            "  step 30: local=0.6097 global=1.8802\n",
            "  step 40: local=0.5984 global=1.8885\n",
            "  step 50: local=0.5984 global=1.9134\n",
            "  step 60: local=0.6064 global=1.9166\n",
            "  step 70: local=0.6058 global=1.8835\n",
            "  step 80: local=0.6200 global=2.0548\n",
            "  step 90: local=0.6053 global=1.9548\n",
            "  [WARN] Layer 17 did not converge after 20 repeats (global=2.1875)\n",
            "  [TIME] Layer 17 took 568.3s | Total: 187.7min | ETA: 81.6min (10 layers left)\n",
            "\n",
            "--- Layer 18/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5288 global=2.2077\n",
            "  step 10: local=0.5194 global=2.3297\n",
            "  step 20: local=0.5235 global=2.1079\n",
            "  step 30: local=0.5100 global=2.1641\n",
            "  step 40: local=0.5173 global=2.2458\n",
            "  step 50: local=0.5057 global=2.2043\n",
            "  step 60: local=0.5098 global=2.1444\n",
            "  step 70: local=0.5172 global=1.9841\n",
            "  step 80: local=0.4948 global=2.1153\n",
            "  step 90: local=0.4885 global=2.0512\n",
            "  Layer 18 not converged (global=2.0533 > 0.8), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4797 global=2.0088\n",
            "  step 10: local=0.4881 global=2.0485\n",
            "  step 20: local=0.4811 global=1.9487\n",
            "  step 30: local=0.4806 global=2.1460\n",
            "  step 40: local=0.4753 global=2.0298\n",
            "  step 50: local=0.4688 global=1.9786\n",
            "  step 60: local=0.4687 global=2.0028\n",
            "  step 70: local=0.4626 global=2.0739\n",
            "  step 80: local=0.4612 global=2.0336\n",
            "  step 90: local=0.4625 global=2.0100\n",
            "  Layer 18 not converged (global=2.0163 > 0.8), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4454 global=2.0255\n",
            "  step 10: local=0.4458 global=1.8937\n",
            "  step 20: local=0.4472 global=1.9161\n",
            "  step 30: local=0.4506 global=1.8432\n",
            "  step 40: local=0.4562 global=2.0649\n",
            "  step 50: local=0.4485 global=2.0892\n",
            "  step 60: local=0.4426 global=1.8774\n",
            "  step 70: local=0.4465 global=1.8094\n",
            "  step 80: local=0.4444 global=1.8214\n",
            "  step 90: local=0.4251 global=1.8401\n",
            "  Layer 18 not converged (global=1.9092 > 0.8), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4417 global=1.9712\n",
            "  step 10: local=0.4296 global=1.8623\n",
            "  step 20: local=0.4282 global=1.9330\n",
            "  step 30: local=0.4274 global=1.9994\n",
            "  step 40: local=0.4362 global=1.8624\n",
            "  step 50: local=0.4359 global=1.8331\n",
            "  step 60: local=0.4289 global=1.8716\n",
            "  step 70: local=0.4312 global=1.8030\n",
            "  step 80: local=0.4214 global=1.8444\n",
            "  step 90: local=0.4237 global=1.9376\n",
            "  Layer 18 not converged (global=1.7962 > 0.8), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4165 global=1.8372\n",
            "  step 10: local=0.4211 global=1.8728\n",
            "  step 20: local=0.4238 global=1.9599\n",
            "  step 30: local=0.4154 global=1.9140\n",
            "  step 40: local=0.4217 global=1.8052\n",
            "  step 50: local=0.4152 global=1.7191\n",
            "  step 60: local=0.4206 global=1.8628\n",
            "  step 70: local=0.4159 global=1.8088\n",
            "  step 80: local=0.4149 global=1.7779\n",
            "  step 90: local=0.4180 global=1.7496\n",
            "  Layer 18 not converged (global=1.8607 > 0.8), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4144 global=1.8462\n",
            "  step 10: local=0.4037 global=1.9427\n",
            "  step 20: local=0.4080 global=1.8591\n",
            "  step 30: local=0.4127 global=1.8013\n",
            "  step 40: local=0.4062 global=1.7952\n",
            "  step 50: local=0.4007 global=1.8859\n",
            "  step 60: local=0.4047 global=1.9662\n",
            "  step 70: local=0.4083 global=1.9181\n",
            "  step 80: local=0.4156 global=1.7408\n",
            "  step 90: local=0.4088 global=1.7701\n",
            "  Layer 18 not converged (global=1.8551 > 0.8), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4110 global=1.8653\n",
            "  step 10: local=0.3984 global=1.8496\n",
            "  step 20: local=0.4067 global=1.7925\n",
            "  step 30: local=0.4044 global=1.8367\n",
            "  step 40: local=0.4056 global=1.8979\n",
            "  step 50: local=0.3942 global=1.7636\n",
            "  step 60: local=0.4037 global=1.8069\n",
            "  step 70: local=0.3960 global=1.9055\n",
            "  step 80: local=0.3956 global=1.8548\n",
            "  step 90: local=0.4048 global=1.8728\n",
            "  Layer 18 not converged (global=1.7744 > 0.8), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4113 global=1.8016\n",
            "  step 10: local=0.4216 global=1.8762\n",
            "  step 20: local=0.4007 global=1.6996\n",
            "  step 30: local=0.4085 global=1.7865\n",
            "  step 40: local=0.4053 global=1.8216\n",
            "  step 50: local=0.4030 global=1.8777\n",
            "  step 60: local=0.3987 global=1.8427\n",
            "  step 70: local=0.4039 global=1.7445\n",
            "  step 80: local=0.4057 global=1.7024\n",
            "  step 90: local=0.4002 global=1.8064\n",
            "  Layer 18 not converged (global=1.7767 > 0.8), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3994 global=1.7598\n",
            "  step 10: local=0.4046 global=1.8220\n",
            "  step 20: local=0.3985 global=1.7617\n",
            "  step 30: local=0.3950 global=1.8885\n",
            "  step 40: local=0.4048 global=1.8032\n",
            "  step 50: local=0.4036 global=1.8444\n",
            "  step 60: local=0.4018 global=1.8294\n",
            "  step 70: local=0.4004 global=1.8157\n",
            "  step 80: local=0.3995 global=1.7229\n",
            "  step 90: local=0.3962 global=1.8614\n",
            "  Layer 18 not converged (global=1.8584 > 0.8), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4030 global=1.8470\n",
            "  step 10: local=0.3989 global=1.6839\n",
            "  step 20: local=0.4002 global=1.7454\n",
            "  step 30: local=0.4011 global=1.8344\n",
            "  step 40: local=0.3993 global=1.6969\n",
            "  step 50: local=0.3928 global=1.8202\n",
            "  step 60: local=0.4000 global=1.8728\n",
            "  step 70: local=0.4052 global=1.7720\n",
            "  step 80: local=0.3980 global=1.7795\n",
            "  step 90: local=0.4049 global=1.8044\n",
            "  Layer 18 not converged (global=1.8746 > 0.8), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3954 global=1.8029\n",
            "  step 10: local=0.4012 global=1.7804\n",
            "  step 20: local=0.4070 global=1.9481\n",
            "  step 30: local=0.3996 global=1.8308\n",
            "  step 40: local=0.3910 global=1.7699\n",
            "  step 50: local=0.3939 global=1.9206\n",
            "  step 60: local=0.3994 global=1.7455\n",
            "  step 70: local=0.4051 global=1.8096\n",
            "  step 80: local=0.3957 global=1.9197\n",
            "  step 90: local=0.3993 global=1.8876\n",
            "  Layer 18 not converged (global=1.8181 > 0.8), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3981 global=1.8385\n",
            "  step 10: local=0.3980 global=1.7240\n",
            "  step 20: local=0.3930 global=1.8409\n",
            "  step 30: local=0.3968 global=1.7784\n",
            "  step 40: local=0.3982 global=1.7764\n",
            "  step 50: local=0.4007 global=1.8296\n",
            "  step 60: local=0.3902 global=1.7278\n",
            "  step 70: local=0.4102 global=1.9188\n",
            "  step 80: local=0.4000 global=1.8338\n",
            "  step 90: local=0.4009 global=1.7934\n",
            "  Layer 18 not converged (global=1.8347 > 0.8), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4037 global=1.8167\n",
            "  step 10: local=0.4043 global=1.8802\n",
            "  step 20: local=0.3999 global=1.8643\n",
            "  step 30: local=0.4042 global=1.8496\n",
            "  step 40: local=0.3857 global=1.8673\n",
            "  step 50: local=0.3977 global=1.7510\n",
            "  step 60: local=0.4014 global=1.7756\n",
            "  step 70: local=0.3983 global=1.7169\n",
            "  step 80: local=0.4040 global=1.9379\n",
            "  step 90: local=0.3888 global=1.9588\n",
            "  Layer 18 not converged (global=1.7881 > 0.8), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3986 global=1.7652\n",
            "  step 10: local=0.3975 global=1.7117\n",
            "  step 20: local=0.4000 global=1.7249\n",
            "  step 30: local=0.3973 global=1.7509\n",
            "  step 40: local=0.3922 global=1.8743\n",
            "  step 50: local=0.3913 global=1.7795\n",
            "  step 60: local=0.4021 global=1.8453\n",
            "  step 70: local=0.4014 global=1.9140\n",
            "  step 80: local=0.4035 global=1.7918\n",
            "  step 90: local=0.3924 global=1.7661\n",
            "  Layer 18 not converged (global=1.7554 > 0.8), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3975 global=1.8067\n",
            "  step 10: local=0.3938 global=1.7418\n",
            "  step 20: local=0.3899 global=1.7812\n",
            "  step 30: local=0.3879 global=1.8762\n",
            "  step 40: local=0.3968 global=1.7770\n",
            "  step 50: local=0.3910 global=1.8200\n",
            "  step 60: local=0.3995 global=1.9086\n",
            "  step 70: local=0.3946 global=1.8664\n",
            "  step 80: local=0.3984 global=1.7622\n",
            "  step 90: local=0.3894 global=1.6782\n",
            "  Layer 18 not converged (global=1.8718 > 0.8), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3892 global=1.8159\n",
            "  step 10: local=0.3947 global=1.7671\n",
            "  step 20: local=0.3845 global=1.7371\n",
            "  step 30: local=0.4027 global=1.7104\n",
            "  step 40: local=0.3902 global=1.8029\n",
            "  step 50: local=0.4052 global=1.8997\n",
            "  step 60: local=0.3812 global=1.8256\n",
            "  step 70: local=0.4022 global=1.7671\n",
            "  step 80: local=0.3932 global=1.7615\n",
            "  step 90: local=0.3848 global=1.8551\n",
            "  Layer 18 not converged (global=1.7502 > 0.8), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3967 global=1.9349\n",
            "  step 10: local=0.3822 global=1.8846\n",
            "  step 20: local=0.3975 global=1.7112\n",
            "  step 30: local=0.3882 global=1.7399\n",
            "  step 40: local=0.3942 global=1.8368\n",
            "  step 50: local=0.3925 global=1.8232\n",
            "  step 60: local=0.3976 global=1.7679\n",
            "  step 70: local=0.3926 global=1.8099\n",
            "  step 80: local=0.3991 global=1.8719\n",
            "  step 90: local=0.3885 global=1.7401\n",
            "  Layer 18 not converged (global=1.7556 > 0.8), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4043 global=1.7863\n",
            "  step 10: local=0.3780 global=1.8795\n",
            "  step 20: local=0.3906 global=1.8336\n",
            "  step 30: local=0.3968 global=1.8489\n",
            "  step 40: local=0.3890 global=1.7777\n",
            "  step 50: local=0.3940 global=1.8564\n",
            "  step 60: local=0.3885 global=1.6758\n",
            "  step 70: local=0.3963 global=1.7614\n",
            "  step 80: local=0.3875 global=1.8047\n",
            "  step 90: local=0.3928 global=1.8550\n",
            "  Layer 18 not converged (global=1.8509 > 0.8), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3946 global=1.8211\n",
            "  step 10: local=0.3895 global=1.7256\n",
            "  step 20: local=0.3848 global=1.6833\n",
            "  step 30: local=0.3867 global=1.7858\n",
            "  step 40: local=0.3953 global=1.7427\n",
            "  step 50: local=0.4011 global=1.8005\n",
            "  step 60: local=0.3915 global=1.7450\n",
            "  step 70: local=0.3952 global=1.8677\n",
            "  step 80: local=0.4035 global=1.7847\n",
            "  step 90: local=0.3970 global=1.8278\n",
            "  Layer 18 not converged (global=1.7896 > 0.8), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3919 global=1.8095\n",
            "  step 10: local=0.3919 global=1.8013\n",
            "  step 20: local=0.3848 global=1.7067\n",
            "  step 30: local=0.3890 global=1.8432\n",
            "  step 40: local=0.4027 global=1.8296\n",
            "  step 50: local=0.3910 global=1.6697\n",
            "  step 60: local=0.3936 global=1.7302\n",
            "  step 70: local=0.3899 global=1.8194\n",
            "  step 80: local=0.3898 global=1.6820\n",
            "  step 90: local=0.3847 global=1.8056\n",
            "  [WARN] Layer 18 did not converge after 20 repeats (global=1.8711)\n",
            "  [TIME] Layer 18 took 552.6s | Total: 196.9min | ETA: 73.8min (9 layers left)\n",
            "\n",
            "--- Layer 19/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5463 global=2.3632\n",
            "  step 10: local=0.5357 global=2.2442\n",
            "  step 20: local=0.5579 global=2.1965\n",
            "  step 30: local=0.5269 global=2.1991\n",
            "  step 40: local=0.5471 global=2.1946\n",
            "  step 50: local=0.5148 global=2.1600\n",
            "  step 60: local=0.5165 global=2.3377\n",
            "  step 70: local=0.5108 global=2.1708\n",
            "  step 80: local=0.5115 global=2.1037\n",
            "  step 90: local=0.5343 global=2.2506\n",
            "  Layer 19 not converged (global=2.0849 > 0.8), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5151 global=2.0594\n",
            "  step 10: local=0.4945 global=2.1083\n",
            "  step 20: local=0.4917 global=2.2088\n",
            "  step 30: local=0.4716 global=2.1928\n",
            "  step 40: local=0.4989 global=2.1092\n",
            "  step 50: local=0.4692 global=1.9947\n",
            "  step 60: local=0.4954 global=2.1109\n",
            "  step 70: local=0.4729 global=2.0582\n",
            "  step 80: local=0.4616 global=2.0304\n",
            "  step 90: local=0.4824 global=2.0528\n",
            "  Layer 19 not converged (global=1.9816 > 0.8), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4587 global=1.9714\n",
            "  step 10: local=0.4925 global=2.1724\n",
            "  step 20: local=0.4618 global=2.0644\n",
            "  step 30: local=0.4634 global=2.0178\n",
            "  step 40: local=0.4407 global=2.0340\n",
            "  step 50: local=0.4345 global=2.1057\n",
            "  step 60: local=0.4440 global=2.0887\n",
            "  step 70: local=0.4747 global=2.0467\n",
            "  step 80: local=0.4719 global=2.0640\n",
            "  step 90: local=0.4706 global=1.9346\n",
            "  Layer 19 not converged (global=2.0568 > 0.8), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4382 global=1.9574\n",
            "  step 10: local=0.4565 global=1.8952\n",
            "  step 20: local=0.4473 global=2.1199\n",
            "  step 30: local=0.4255 global=2.1454\n",
            "  step 40: local=0.4484 global=1.9215\n",
            "  step 50: local=0.4258 global=1.8804\n",
            "  step 60: local=0.4471 global=1.8699\n",
            "  step 70: local=0.4514 global=1.9034\n",
            "  step 80: local=0.4257 global=2.0514\n",
            "  step 90: local=0.4452 global=1.9083\n",
            "  Layer 19 not converged (global=1.9300 > 0.8), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4287 global=1.9864\n",
            "  step 10: local=0.4502 global=2.0800\n",
            "  step 20: local=0.4411 global=1.9243\n",
            "  step 30: local=0.4274 global=1.9028\n",
            "  step 40: local=0.4413 global=1.9511\n",
            "  step 50: local=0.4173 global=1.8816\n",
            "  step 60: local=0.4294 global=1.9338\n",
            "  step 70: local=0.4291 global=2.0302\n",
            "  step 80: local=0.4337 global=1.9102\n",
            "  step 90: local=0.4398 global=1.9599\n",
            "  Layer 19 not converged (global=2.0356 > 0.8), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4199 global=2.0505\n",
            "  step 10: local=0.4443 global=2.0152\n",
            "  step 20: local=0.4323 global=1.8793\n",
            "  step 30: local=0.4426 global=1.8076\n",
            "  step 40: local=0.4351 global=1.9503\n",
            "  step 50: local=0.4293 global=1.8972\n",
            "  step 60: local=0.4289 global=1.8607\n",
            "  step 70: local=0.4393 global=1.8331\n",
            "  step 80: local=0.4464 global=1.9412\n",
            "  step 90: local=0.4249 global=2.0497\n",
            "  Layer 19 not converged (global=1.9576 > 0.8), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4172 global=1.9590\n",
            "  step 10: local=0.4331 global=1.8954\n",
            "  step 20: local=0.4248 global=1.8974\n",
            "  step 30: local=0.4200 global=1.9806\n",
            "  step 40: local=0.4117 global=2.0685\n",
            "  step 50: local=0.4264 global=2.0114\n",
            "  step 60: local=0.4154 global=1.8331\n",
            "  step 70: local=0.4108 global=1.8567\n",
            "  step 80: local=0.4102 global=1.9560\n",
            "  step 90: local=0.4228 global=1.9538\n",
            "  Layer 19 not converged (global=1.8984 > 0.8), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4203 global=1.8889\n",
            "  step 10: local=0.4345 global=1.9235\n",
            "  step 20: local=0.4016 global=1.9862\n",
            "  step 30: local=0.4018 global=1.8662\n",
            "  step 40: local=0.4234 global=1.9135\n",
            "  step 50: local=0.4108 global=2.0078\n",
            "  step 60: local=0.4341 global=1.9459\n",
            "  step 70: local=0.4184 global=1.9644\n",
            "  step 80: local=0.4036 global=1.8901\n",
            "  step 90: local=0.4399 global=1.9593\n",
            "  Layer 19 not converged (global=1.8869 > 0.8), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4315 global=1.7852\n",
            "  step 10: local=0.4286 global=1.8762\n",
            "  step 20: local=0.4225 global=1.9167\n",
            "  step 30: local=0.4124 global=1.9910\n",
            "  step 40: local=0.4019 global=1.9331\n",
            "  step 50: local=0.4320 global=1.8443\n",
            "  step 60: local=0.4294 global=1.7862\n",
            "  step 70: local=0.4120 global=1.9113\n",
            "  step 80: local=0.4044 global=1.8508\n",
            "  step 90: local=0.4016 global=1.9141\n",
            "  Layer 19 not converged (global=1.9463 > 0.8), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4398 global=1.8654\n",
            "  step 10: local=0.3984 global=2.0023\n",
            "  step 20: local=0.4244 global=1.8824\n",
            "  step 30: local=0.4134 global=1.9437\n",
            "  step 40: local=0.4223 global=1.9369\n",
            "  step 50: local=0.4386 global=1.9218\n",
            "  step 60: local=0.4064 global=1.8057\n",
            "  step 70: local=0.4130 global=1.9576\n",
            "  step 80: local=0.4207 global=1.9570\n",
            "  step 90: local=0.4142 global=1.7759\n",
            "  Layer 19 not converged (global=1.9377 > 0.8), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4203 global=1.8337\n",
            "  step 10: local=0.4170 global=1.9385\n",
            "  step 20: local=0.4280 global=1.7897\n",
            "  step 30: local=0.4327 global=1.9234\n",
            "  step 40: local=0.4117 global=1.9896\n",
            "  step 50: local=0.4164 global=1.8816\n",
            "  step 60: local=0.4097 global=1.8667\n",
            "  step 70: local=0.4388 global=1.8987\n",
            "  step 80: local=0.4083 global=1.8937\n",
            "  step 90: local=0.4364 global=1.8838\n",
            "  Layer 19 not converged (global=1.8542 > 0.8), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4337 global=2.0642\n",
            "  step 10: local=0.4240 global=1.9257\n",
            "  step 20: local=0.4024 global=1.8653\n",
            "  step 30: local=0.4298 global=2.0271\n",
            "  step 40: local=0.4180 global=1.8520\n",
            "  step 50: local=0.4155 global=1.9000\n",
            "  step 60: local=0.4276 global=2.0185\n",
            "  step 70: local=0.4256 global=1.9975\n",
            "  step 80: local=0.3979 global=1.9324\n",
            "  step 90: local=0.4152 global=1.8353\n",
            "  Layer 19 not converged (global=1.9852 > 0.8), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4163 global=1.9405\n",
            "  step 10: local=0.4117 global=1.8984\n",
            "  step 20: local=0.4027 global=1.8779\n",
            "  step 30: local=0.4016 global=1.9178\n",
            "  step 40: local=0.3839 global=1.8260\n",
            "  step 50: local=0.3920 global=2.0229\n",
            "  step 60: local=0.3976 global=1.9386\n",
            "  step 70: local=0.3973 global=1.8888\n",
            "  step 80: local=0.4245 global=1.9148\n",
            "  step 90: local=0.4034 global=1.9913\n",
            "  Layer 19 not converged (global=1.8327 > 0.8), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4055 global=1.9786\n",
            "  step 10: local=0.4034 global=1.9460\n",
            "  step 20: local=0.4165 global=1.9631\n",
            "  step 30: local=0.4093 global=1.8439\n",
            "  step 40: local=0.4063 global=1.8682\n",
            "  step 50: local=0.4218 global=1.8115\n",
            "  step 60: local=0.4080 global=2.0388\n",
            "  step 70: local=0.4176 global=2.0613\n",
            "  step 80: local=0.4146 global=1.8499\n",
            "  step 90: local=0.4261 global=1.8123\n",
            "  Layer 19 not converged (global=1.8973 > 0.8), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4169 global=1.8134\n",
            "  step 10: local=0.4241 global=1.8413\n",
            "  step 20: local=0.3990 global=1.9878\n",
            "  step 30: local=0.4076 global=1.8545\n",
            "  step 40: local=0.4039 global=1.9296\n",
            "  step 50: local=0.3958 global=2.0256\n",
            "  step 60: local=0.4030 global=1.8782\n",
            "  step 70: local=0.4254 global=1.8573\n",
            "  step 80: local=0.4000 global=1.9065\n",
            "  step 90: local=0.3996 global=1.8372\n",
            "  Layer 19 not converged (global=1.9022 > 0.8), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4450 global=1.8888\n",
            "  step 10: local=0.4181 global=1.9892\n",
            "  step 20: local=0.4158 global=1.8691\n",
            "  step 30: local=0.4190 global=1.9222\n",
            "  step 40: local=0.4268 global=2.0097\n",
            "  step 50: local=0.4353 global=1.9770\n",
            "  step 60: local=0.4138 global=1.8475\n",
            "  step 70: local=0.4278 global=1.7732\n",
            "  step 80: local=0.4141 global=1.9138\n",
            "  step 90: local=0.3986 global=1.8664\n",
            "  Layer 19 not converged (global=1.9289 > 0.8), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3954 global=1.8286\n",
            "  step 10: local=0.4051 global=1.8045\n",
            "  step 20: local=0.4108 global=1.9142\n",
            "  step 30: local=0.3949 global=2.0176\n",
            "  step 40: local=0.3938 global=1.9283\n",
            "  step 50: local=0.4219 global=1.8678\n",
            "  step 60: local=0.4207 global=1.8665\n",
            "  step 70: local=0.4019 global=1.9523\n",
            "  step 80: local=0.4294 global=2.0404\n",
            "  step 90: local=0.4048 global=1.9859\n",
            "  Layer 19 not converged (global=1.9407 > 0.8), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4061 global=1.8061\n",
            "  step 10: local=0.4189 global=1.8317\n",
            "  step 20: local=0.4105 global=1.9327\n",
            "  step 30: local=0.4010 global=1.9282\n",
            "  step 40: local=0.4109 global=1.8637\n",
            "  step 50: local=0.4003 global=1.9033\n",
            "  step 60: local=0.4121 global=1.9609\n",
            "  step 70: local=0.4298 global=1.8399\n",
            "  step 80: local=0.4261 global=1.8908\n",
            "  step 90: local=0.4132 global=1.9842\n",
            "  Layer 19 not converged (global=1.8876 > 0.8), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4120 global=1.9237\n",
            "  step 10: local=0.3909 global=1.9425\n",
            "  step 20: local=0.3939 global=1.8705\n",
            "  step 30: local=0.4158 global=1.9388\n",
            "  step 40: local=0.3942 global=1.7654\n",
            "  step 50: local=0.3908 global=1.8568\n",
            "  step 60: local=0.4230 global=1.8975\n",
            "  step 70: local=0.4054 global=1.9696\n",
            "  step 80: local=0.3905 global=1.9117\n",
            "  step 90: local=0.4050 global=1.8262\n",
            "  Layer 19 not converged (global=1.9376 > 0.8), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4261 global=1.7664\n",
            "  step 10: local=0.4095 global=1.8893\n",
            "  step 20: local=0.4057 global=1.8359\n",
            "  step 30: local=0.4140 global=1.8960\n",
            "  step 40: local=0.3964 global=1.8449\n",
            "  step 50: local=0.4102 global=1.9830\n",
            "  step 60: local=0.4028 global=1.8658\n",
            "  step 70: local=0.4084 global=1.9269\n",
            "  step 80: local=0.4067 global=1.9160\n",
            "  step 90: local=0.3878 global=1.9072\n",
            "  [WARN] Layer 19 did not converge after 20 repeats (global=2.0465)\n",
            "  [TIME] Layer 19 took 536.9s | Total: 205.8min | ETA: 65.9min (8 layers left)\n",
            "\n",
            "--- Layer 20/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5405 global=2.0002\n",
            "  step 10: local=0.5404 global=2.1729\n",
            "  step 20: local=0.5446 global=2.1629\n",
            "  step 30: local=0.5278 global=1.9666\n",
            "  step 40: local=0.5198 global=2.0108\n",
            "  step 50: local=0.5234 global=2.1155\n",
            "  step 60: local=0.5039 global=1.9621\n",
            "  step 70: local=0.5018 global=2.1048\n",
            "  step 80: local=0.4977 global=2.1720\n",
            "  step 90: local=0.4925 global=2.0528\n",
            "  Layer 20 not converged (global=2.1711 > 0.8), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4887 global=2.0131\n",
            "  step 10: local=0.4940 global=2.0423\n",
            "  step 20: local=0.4747 global=2.0529\n",
            "  step 30: local=0.4745 global=2.0305\n",
            "  step 40: local=0.4768 global=2.2125\n",
            "  step 50: local=0.4715 global=2.0537\n",
            "  step 60: local=0.4570 global=1.9982\n",
            "  step 70: local=0.4654 global=2.1646\n",
            "  step 80: local=0.4551 global=1.9650\n",
            "  step 90: local=0.4501 global=2.0166\n",
            "  Layer 20 not converged (global=2.0797 > 0.8), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4530 global=2.1503\n",
            "  step 10: local=0.4555 global=2.1243\n",
            "  step 20: local=0.4476 global=2.0488\n",
            "  step 30: local=0.4531 global=1.9308\n",
            "  step 40: local=0.4367 global=2.0572\n",
            "  step 50: local=0.4298 global=1.9984\n",
            "  step 60: local=0.4384 global=1.9726\n",
            "  step 70: local=0.4324 global=2.0086\n",
            "  step 80: local=0.4277 global=1.9221\n",
            "  step 90: local=0.4458 global=2.1264\n",
            "  Layer 20 not converged (global=2.0156 > 0.8), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4401 global=2.0275\n",
            "  step 10: local=0.4259 global=1.9774\n",
            "  step 20: local=0.4275 global=2.0024\n",
            "  step 30: local=0.4186 global=2.0847\n",
            "  step 40: local=0.4203 global=2.0735\n",
            "  step 50: local=0.4247 global=2.0271\n",
            "  step 60: local=0.4167 global=2.0364\n",
            "  step 70: local=0.4093 global=1.9081\n",
            "  step 80: local=0.4222 global=1.9395\n",
            "  step 90: local=0.4140 global=1.8815\n",
            "  Layer 20 not converged (global=1.9292 > 0.8), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4093 global=2.1167\n",
            "  step 10: local=0.4248 global=2.1512\n",
            "  step 20: local=0.4157 global=1.9309\n",
            "  step 30: local=0.4073 global=1.8850\n",
            "  step 40: local=0.4107 global=1.8773\n",
            "  step 50: local=0.4057 global=1.9057\n",
            "  step 60: local=0.4146 global=2.0731\n",
            "  step 70: local=0.4109 global=1.9261\n",
            "  step 80: local=0.4118 global=2.0001\n",
            "  step 90: local=0.3976 global=2.1044\n",
            "  Layer 20 not converged (global=2.0299 > 0.8), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4052 global=1.9371\n",
            "  step 10: local=0.4031 global=1.9180\n",
            "  step 20: local=0.4031 global=1.9664\n",
            "  step 30: local=0.3991 global=1.9025\n",
            "  step 40: local=0.4066 global=1.9482\n",
            "  step 50: local=0.4106 global=2.0542\n",
            "  step 60: local=0.3978 global=1.9271\n",
            "  step 70: local=0.3986 global=1.9897\n",
            "  step 80: local=0.4154 global=2.0625\n",
            "  step 90: local=0.3972 global=2.0673\n",
            "  Layer 20 not converged (global=1.9216 > 0.8), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4070 global=1.9061\n",
            "  step 10: local=0.3968 global=1.8471\n",
            "  step 20: local=0.3913 global=1.9892\n",
            "  step 30: local=0.4095 global=1.9380\n",
            "  step 40: local=0.4116 global=1.8873\n",
            "  step 50: local=0.3972 global=1.8718\n",
            "  step 60: local=0.3938 global=1.9772\n",
            "  step 70: local=0.3971 global=2.0717\n",
            "  step 80: local=0.3948 global=1.9887\n",
            "  step 90: local=0.3978 global=1.9184\n",
            "  Layer 20 not converged (global=1.9642 > 0.8), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3948 global=1.9284\n",
            "  step 10: local=0.3977 global=2.0313\n",
            "  step 20: local=0.4183 global=2.1129\n",
            "  step 30: local=0.3978 global=2.0381\n",
            "  step 40: local=0.4059 global=1.8631\n",
            "  step 50: local=0.4019 global=1.8990\n",
            "  step 60: local=0.3889 global=1.9875\n",
            "  step 70: local=0.3970 global=1.9981\n",
            "  step 80: local=0.3853 global=1.9335\n",
            "  step 90: local=0.3950 global=1.9596\n",
            "  Layer 20 not converged (global=1.9289 > 0.8), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3988 global=2.0222\n",
            "  step 10: local=0.4032 global=1.9029\n",
            "  step 20: local=0.3955 global=1.9481\n",
            "  step 30: local=0.3939 global=2.0417\n",
            "  step 40: local=0.4029 global=1.9731\n",
            "  step 50: local=0.3950 global=2.0104\n",
            "  step 60: local=0.3939 global=1.9221\n",
            "  step 70: local=0.3915 global=2.0023\n",
            "  step 80: local=0.3879 global=1.8169\n",
            "  step 90: local=0.4037 global=1.9038\n",
            "  Layer 20 not converged (global=2.0591 > 0.8), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3958 global=1.9506\n",
            "  step 10: local=0.3900 global=2.0345\n",
            "  step 20: local=0.3946 global=1.9662\n",
            "  step 30: local=0.3975 global=1.8727\n",
            "  step 40: local=0.3947 global=1.8169\n",
            "  step 50: local=0.4013 global=1.9554\n",
            "  step 60: local=0.3866 global=1.8920\n",
            "  step 70: local=0.3968 global=1.9552\n",
            "  step 80: local=0.4055 global=1.8973\n",
            "  step 90: local=0.3949 global=2.0502\n",
            "  Layer 20 not converged (global=1.9353 > 0.8), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3878 global=1.9263\n",
            "  step 10: local=0.3891 global=1.9739\n",
            "  step 20: local=0.3908 global=1.9936\n",
            "  step 30: local=0.3920 global=1.9657\n",
            "  step 40: local=0.3862 global=1.8432\n",
            "  step 50: local=0.3878 global=2.0063\n",
            "  step 60: local=0.3962 global=2.0047\n",
            "  step 70: local=0.3885 global=1.8149\n",
            "  step 80: local=0.4022 global=1.8723\n",
            "  step 90: local=0.3882 global=1.9794\n",
            "  Layer 20 not converged (global=1.9918 > 0.8), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3961 global=1.8281\n",
            "  step 10: local=0.3872 global=1.9635\n",
            "  step 20: local=0.3866 global=2.0278\n",
            "  step 30: local=0.3876 global=1.9241\n",
            "  step 40: local=0.3860 global=1.9016\n",
            "  step 50: local=0.3954 global=1.9369\n",
            "  step 60: local=0.3947 global=1.9375\n",
            "  step 70: local=0.3878 global=1.9279\n",
            "  step 80: local=0.4035 global=2.1112\n",
            "  step 90: local=0.3841 global=1.9634\n",
            "  Layer 20 not converged (global=2.2148 > 0.8), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3851 global=1.9081\n",
            "  step 10: local=0.3884 global=2.0684\n",
            "  step 20: local=0.3840 global=1.8802\n",
            "  step 30: local=0.3954 global=1.9339\n",
            "  step 40: local=0.3844 global=2.0644\n",
            "  step 50: local=0.3955 global=2.0475\n",
            "  step 60: local=0.3827 global=1.9736\n",
            "  step 70: local=0.3957 global=1.8681\n",
            "  step 80: local=0.3897 global=1.9879\n",
            "  step 90: local=0.3841 global=1.9351\n",
            "  Layer 20 not converged (global=1.9511 > 0.8), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3961 global=1.9162\n",
            "  step 10: local=0.3807 global=1.9530\n",
            "  step 20: local=0.3910 global=1.8654\n",
            "  step 30: local=0.3911 global=2.0687\n",
            "  step 40: local=0.3901 global=1.9748\n",
            "  step 50: local=0.3918 global=1.9277\n",
            "  step 60: local=0.3962 global=1.9541\n",
            "  step 70: local=0.3821 global=2.0375\n",
            "  step 80: local=0.3897 global=2.0306\n",
            "  step 90: local=0.3800 global=1.9861\n",
            "  Layer 20 not converged (global=2.0006 > 0.8), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3731 global=1.9951\n",
            "  step 10: local=0.3921 global=1.8688\n",
            "  step 20: local=0.3822 global=1.9027\n",
            "  step 30: local=0.3835 global=1.8489\n",
            "  step 40: local=0.4014 global=2.0797\n",
            "  step 50: local=0.3902 global=2.1124\n",
            "  step 60: local=0.4035 global=1.8968\n",
            "  step 70: local=0.3976 global=1.8547\n",
            "  step 80: local=0.3759 global=1.8475\n",
            "  step 90: local=0.3806 global=1.8761\n",
            "  Layer 20 not converged (global=1.9542 > 0.8), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3990 global=2.0406\n",
            "  step 10: local=0.3867 global=1.8982\n",
            "  step 20: local=0.3849 global=1.9717\n",
            "  step 30: local=0.3878 global=2.0749\n",
            "  step 40: local=0.3829 global=1.9095\n",
            "  step 50: local=0.3833 global=1.8928\n",
            "  step 60: local=0.4022 global=1.9417\n",
            "  step 70: local=0.3818 global=1.8768\n",
            "  step 80: local=0.3937 global=1.9240\n",
            "  step 90: local=0.3866 global=2.0265\n",
            "  Layer 20 not converged (global=1.8825 > 0.8), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3913 global=1.9053\n",
            "  step 10: local=0.3910 global=1.9673\n",
            "  step 20: local=0.3849 global=2.0409\n",
            "  step 30: local=0.3887 global=2.0442\n",
            "  step 40: local=0.3959 global=1.8866\n",
            "  step 50: local=0.3786 global=1.8260\n",
            "  step 60: local=0.3796 global=1.9673\n",
            "  step 70: local=0.3928 global=1.9155\n",
            "  step 80: local=0.3843 global=1.8673\n",
            "  step 90: local=0.3814 global=1.8511\n",
            "  Layer 20 not converged (global=1.9817 > 0.8), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3808 global=1.9575\n",
            "  step 10: local=0.3903 global=2.0473\n",
            "  step 20: local=0.3913 global=1.9671\n",
            "  step 30: local=0.3920 global=1.9019\n",
            "  step 40: local=0.3823 global=1.9065\n",
            "  step 50: local=0.3825 global=2.0117\n",
            "  step 60: local=0.3814 global=2.0965\n",
            "  step 70: local=0.3781 global=2.0207\n",
            "  step 80: local=0.3855 global=1.8441\n",
            "  step 90: local=0.3920 global=1.8811\n",
            "  Layer 20 not converged (global=1.9668 > 0.8), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3869 global=1.9737\n",
            "  step 10: local=0.3831 global=1.9780\n",
            "  step 20: local=0.3748 global=1.9178\n",
            "  step 30: local=0.3885 global=1.9441\n",
            "  step 40: local=0.3815 global=2.0051\n",
            "  step 50: local=0.3846 global=1.8882\n",
            "  step 60: local=0.3855 global=1.9315\n",
            "  step 70: local=0.3820 global=2.0233\n",
            "  step 80: local=0.3806 global=1.9596\n",
            "  step 90: local=0.3888 global=1.9958\n",
            "  Layer 20 not converged (global=1.8880 > 0.8), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3885 global=1.9066\n",
            "  step 10: local=0.3959 global=1.9885\n",
            "  step 20: local=0.3770 global=1.8026\n",
            "  step 30: local=0.3929 global=1.8909\n",
            "  step 40: local=0.3831 global=1.9371\n",
            "  step 50: local=0.3991 global=2.0198\n",
            "  step 60: local=0.3913 global=1.9516\n",
            "  step 70: local=0.3802 global=1.8560\n",
            "  step 80: local=0.3833 global=1.8038\n",
            "  step 90: local=0.3859 global=1.9391\n",
            "  [WARN] Layer 20 did not converge after 20 repeats (global=1.8821)\n",
            "  [TIME] Layer 20 took 521.4s | Total: 214.5min | ETA: 57.7min (7 layers left)\n",
            "\n",
            "--- Layer 21/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5061 global=2.0031\n",
            "  step 10: local=0.4925 global=2.0748\n",
            "  step 20: local=0.4787 global=2.0073\n",
            "  step 30: local=0.4717 global=2.1704\n",
            "  step 40: local=0.4778 global=2.0459\n",
            "  step 50: local=0.4632 global=2.0742\n",
            "  step 60: local=0.4618 global=2.0963\n",
            "  step 70: local=0.4561 global=2.0638\n",
            "  step 80: local=0.4464 global=1.9393\n",
            "  step 90: local=0.4491 global=2.1105\n",
            "  Layer 21 not converged (global=2.0967 > 0.8), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4467 global=2.1033\n",
            "  step 10: local=0.4419 global=1.9034\n",
            "  step 20: local=0.4347 global=1.9704\n",
            "  step 30: local=0.4362 global=2.0694\n",
            "  step 40: local=0.4243 global=1.9077\n",
            "  step 50: local=0.4233 global=2.0401\n",
            "  step 60: local=0.4235 global=2.1077\n",
            "  step 70: local=0.4106 global=2.0003\n",
            "  step 80: local=0.4102 global=1.9726\n",
            "  step 90: local=0.4145 global=2.0054\n",
            "  Layer 21 not converged (global=2.0937 > 0.8), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4004 global=2.0082\n",
            "  step 10: local=0.4060 global=1.9912\n",
            "  step 20: local=0.3904 global=2.1742\n",
            "  step 30: local=0.3986 global=2.0270\n",
            "  step 40: local=0.3893 global=1.9641\n",
            "  step 50: local=0.3952 global=2.1347\n",
            "  step 60: local=0.3919 global=1.9354\n",
            "  step 70: local=0.3878 global=1.9791\n",
            "  step 80: local=0.3837 global=2.1149\n",
            "  step 90: local=0.3853 global=2.1017\n",
            "  Layer 21 not converged (global=2.0022 > 0.8), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3798 global=2.0276\n",
            "  step 10: local=0.3773 global=1.9036\n",
            "  step 20: local=0.3785 global=2.0346\n",
            "  step 30: local=0.3721 global=1.9909\n",
            "  step 40: local=0.3695 global=1.9543\n",
            "  step 50: local=0.3724 global=1.9969\n",
            "  step 60: local=0.3744 global=1.9109\n",
            "  step 70: local=0.3733 global=2.1135\n",
            "  step 80: local=0.3663 global=2.0145\n",
            "  step 90: local=0.3527 global=1.9674\n",
            "  Layer 21 not converged (global=1.9942 > 0.8), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3619 global=1.9956\n",
            "  step 10: local=0.3653 global=2.0768\n",
            "  step 20: local=0.3589 global=2.0714\n",
            "  step 30: local=0.3586 global=2.0225\n",
            "  step 40: local=0.3695 global=2.0276\n",
            "  step 50: local=0.3537 global=1.9100\n",
            "  step 60: local=0.3627 global=1.9395\n",
            "  step 70: local=0.3643 global=1.8783\n",
            "  step 80: local=0.3603 global=2.1071\n",
            "  step 90: local=0.3560 global=2.1499\n",
            "  Layer 21 not converged (global=1.9948 > 0.8), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3604 global=1.9211\n",
            "  step 10: local=0.3578 global=1.8815\n",
            "  step 20: local=0.3559 global=1.8729\n",
            "  step 30: local=0.3574 global=1.9106\n",
            "  step 40: local=0.3554 global=2.0753\n",
            "  step 50: local=0.3607 global=1.9251\n",
            "  step 60: local=0.3556 global=2.0066\n",
            "  step 70: local=0.3554 global=2.1008\n",
            "  step 80: local=0.3548 global=1.9389\n",
            "  step 90: local=0.3555 global=1.9193\n",
            "  Layer 21 not converged (global=1.9090 > 0.8), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3437 global=1.9719\n",
            "  step 10: local=0.3559 global=1.9067\n",
            "  step 20: local=0.3583 global=1.9522\n",
            "  step 30: local=0.3508 global=2.0598\n",
            "  step 40: local=0.3501 global=1.9300\n",
            "  step 50: local=0.3511 global=1.9871\n",
            "  step 60: local=0.3549 global=2.0763\n",
            "  step 70: local=0.3553 global=2.0652\n",
            "  step 80: local=0.3484 global=1.9122\n",
            "  step 90: local=0.3485 global=1.8497\n",
            "  Layer 21 not converged (global=2.0545 > 0.8), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3496 global=1.9936\n",
            "  step 10: local=0.3497 global=1.9472\n",
            "  step 20: local=0.3470 global=1.8999\n",
            "  step 30: local=0.3531 global=1.8836\n",
            "  step 40: local=0.3468 global=1.9966\n",
            "  step 50: local=0.3433 global=2.0747\n",
            "  step 60: local=0.3410 global=1.9972\n",
            "  step 70: local=0.3462 global=1.9259\n",
            "  step 80: local=0.3444 global=1.9266\n",
            "  step 90: local=0.3499 global=2.0408\n",
            "  Layer 21 not converged (global=1.9144 > 0.8), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3533 global=2.1177\n",
            "  step 10: local=0.3509 global=2.0427\n",
            "  step 20: local=0.3525 global=1.8824\n",
            "  step 30: local=0.3499 global=1.9015\n",
            "  step 40: local=0.3530 global=1.9849\n",
            "  step 50: local=0.3465 global=2.0001\n",
            "  step 60: local=0.3374 global=1.9423\n",
            "  step 70: local=0.3475 global=1.9741\n",
            "  step 80: local=0.3426 global=2.0191\n",
            "  step 90: local=0.3447 global=1.9137\n",
            "  Layer 21 not converged (global=1.9186 > 0.8), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3590 global=1.9558\n",
            "  step 10: local=0.3395 global=2.0463\n",
            "  step 20: local=0.3426 global=1.9813\n",
            "  step 30: local=0.3467 global=2.0298\n",
            "  step 40: local=0.3353 global=1.9286\n",
            "  step 50: local=0.3538 global=2.0146\n",
            "  step 60: local=0.3347 global=1.8235\n",
            "  step 70: local=0.3492 global=1.9158\n",
            "  step 80: local=0.3397 global=1.9579\n",
            "  step 90: local=0.3446 global=2.0438\n",
            "  Layer 21 not converged (global=2.0111 > 0.8), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3407 global=1.9719\n",
            "  step 10: local=0.3507 global=1.8735\n",
            "  step 20: local=0.3428 global=1.8226\n",
            "  step 30: local=0.3368 global=1.9500\n",
            "  step 40: local=0.3443 global=1.8891\n",
            "  step 50: local=0.3465 global=1.9599\n",
            "  step 60: local=0.3431 global=1.9043\n",
            "  step 70: local=0.3404 global=2.0603\n",
            "  step 80: local=0.3447 global=1.9345\n",
            "  step 90: local=0.3415 global=1.9814\n",
            "  Layer 21 not converged (global=1.9464 > 0.8), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3410 global=1.9962\n",
            "  step 10: local=0.3449 global=1.9790\n",
            "  step 20: local=0.3318 global=1.8568\n",
            "  step 30: local=0.3524 global=2.0240\n",
            "  step 40: local=0.3485 global=2.0199\n",
            "  step 50: local=0.3398 global=1.8221\n",
            "  step 60: local=0.3439 global=1.8901\n",
            "  step 70: local=0.3419 global=1.9992\n",
            "  step 80: local=0.3348 global=1.8346\n",
            "  step 90: local=0.3381 global=1.9693\n",
            "  Layer 21 not converged (global=2.0134 > 0.8), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3489 global=2.0374\n",
            "  step 10: local=0.3502 global=1.9314\n",
            "  step 20: local=0.3316 global=1.9128\n",
            "  step 30: local=0.3364 global=1.9470\n",
            "  step 40: local=0.3363 global=1.9480\n",
            "  step 50: local=0.3349 global=1.9346\n",
            "  step 60: local=0.3474 global=2.1198\n",
            "  step 70: local=0.3436 global=1.9755\n",
            "  step 80: local=0.3352 global=1.9156\n",
            "  step 90: local=0.3395 global=2.0839\n",
            "  Layer 21 not converged (global=1.9242 > 0.8), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3456 global=1.8915\n",
            "  step 10: local=0.3385 global=1.9355\n",
            "  step 20: local=0.3356 global=2.0728\n",
            "  step 30: local=0.3417 global=2.0596\n",
            "  step 40: local=0.3396 global=1.9871\n",
            "  step 50: local=0.3428 global=1.8684\n",
            "  step 60: local=0.3256 global=1.9961\n",
            "  step 70: local=0.3397 global=1.9528\n",
            "  step 80: local=0.3333 global=1.9252\n",
            "  step 90: local=0.3364 global=1.9668\n",
            "  Layer 21 not converged (global=1.8895 > 0.8), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3408 global=1.8805\n",
            "  step 10: local=0.3411 global=2.0797\n",
            "  step 20: local=0.3405 global=1.9855\n",
            "  step 30: local=0.3432 global=1.9392\n",
            "  step 40: local=0.3393 global=1.9679\n",
            "  step 50: local=0.3432 global=2.0481\n",
            "  step 60: local=0.3403 global=2.0413\n",
            "  step 70: local=0.3421 global=1.9994\n",
            "  step 80: local=0.3440 global=2.0009\n",
            "  step 90: local=0.3449 global=1.8874\n",
            "  Layer 21 not converged (global=2.0334 > 0.8), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3427 global=1.9145\n",
            "  step 10: local=0.3384 global=1.8573\n",
            "  step 20: local=0.3404 global=2.0867\n",
            "  step 30: local=0.3335 global=2.1259\n",
            "  step 40: local=0.3392 global=1.9002\n",
            "  step 50: local=0.3477 global=1.8607\n",
            "  step 60: local=0.3343 global=1.8530\n",
            "  step 70: local=0.3383 global=1.8892\n",
            "  step 80: local=0.3394 global=2.0516\n",
            "  step 90: local=0.3408 global=1.9078\n",
            "  Layer 21 not converged (global=1.9399 > 0.8), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3403 global=1.9874\n",
            "  step 10: local=0.3427 global=2.0805\n",
            "  step 20: local=0.3347 global=1.9206\n",
            "  step 30: local=0.3388 global=1.9018\n",
            "  step 40: local=0.3370 global=1.9521\n",
            "  step 50: local=0.3360 global=1.8894\n",
            "  step 60: local=0.3402 global=1.9325\n",
            "  step 70: local=0.3329 global=2.0398\n",
            "  step 80: local=0.3447 global=1.9141\n",
            "  step 90: local=0.3385 global=1.9713\n",
            "  Layer 21 not converged (global=2.0542 > 0.8), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3324 global=2.0601\n",
            "  step 10: local=0.3377 global=2.0482\n",
            "  step 20: local=0.3280 global=1.8980\n",
            "  step 30: local=0.3337 global=1.8352\n",
            "  step 40: local=0.3354 global=1.9780\n",
            "  step 50: local=0.3350 global=1.9312\n",
            "  step 60: local=0.3351 global=1.8842\n",
            "  step 70: local=0.3297 global=1.8670\n",
            "  step 80: local=0.3287 global=1.9812\n",
            "  step 90: local=0.3381 global=2.0571\n",
            "  Layer 21 not converged (global=1.9802 > 0.8), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3322 global=1.9821\n",
            "  step 10: local=0.3367 global=1.9110\n",
            "  step 20: local=0.3337 global=1.9139\n",
            "  step 30: local=0.3387 global=2.0262\n",
            "  step 40: local=0.3396 global=2.1043\n",
            "  step 50: local=0.3299 global=2.0296\n",
            "  step 60: local=0.3353 global=1.8673\n",
            "  step 70: local=0.3310 global=1.8891\n",
            "  step 80: local=0.3379 global=1.9733\n",
            "  step 90: local=0.3354 global=1.9844\n",
            "  Layer 21 not converged (global=1.9304 > 0.8), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3334 global=1.9296\n",
            "  step 10: local=0.3315 global=1.9628\n",
            "  step 20: local=0.3318 global=2.0077\n",
            "  step 30: local=0.3373 global=1.9020\n",
            "  step 40: local=0.3328 global=1.9426\n",
            "  step 50: local=0.3358 global=2.0330\n",
            "  step 60: local=0.3342 global=1.9680\n",
            "  step 70: local=0.3335 global=2.0166\n",
            "  step 80: local=0.3349 global=1.9192\n",
            "  step 90: local=0.3390 global=2.0041\n",
            "  [WARN] Layer 21 did not converge after 20 repeats (global=1.9235)\n",
            "  [TIME] Layer 21 took 505.6s | Total: 222.9min | ETA: 49.5min (6 layers left)\n",
            "\n",
            "--- Layer 22/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4872 global=1.9205\n",
            "  step 10: local=0.4810 global=2.0145\n",
            "  step 20: local=0.4735 global=2.0540\n",
            "  step 30: local=0.4761 global=2.1386\n",
            "  step 40: local=0.4749 global=2.0682\n",
            "  step 50: local=0.4681 global=1.9542\n",
            "  step 60: local=0.4659 global=1.8998\n",
            "  step 70: local=0.4565 global=2.0353\n",
            "  step 80: local=0.4509 global=1.9634\n",
            "  step 90: local=0.4397 global=2.0345\n",
            "  Layer 22 not converged (global=2.0746 > 0.8), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4466 global=1.9877\n",
            "  step 10: local=0.4435 global=2.1309\n",
            "  step 20: local=0.4364 global=2.0133\n",
            "  step 30: local=0.4454 global=2.0537\n",
            "  step 40: local=0.4345 global=2.0693\n",
            "  step 50: local=0.4281 global=2.0508\n",
            "  step 60: local=0.4206 global=1.9225\n",
            "  step 70: local=0.4230 global=2.0877\n",
            "  step 80: local=0.4180 global=2.0766\n",
            "  step 90: local=0.4246 global=1.8737\n",
            "  Layer 22 not converged (global=2.0740 > 0.8), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4176 global=1.9414\n",
            "  step 10: local=0.4082 global=2.0543\n",
            "  step 20: local=0.3957 global=1.9029\n",
            "  step 30: local=0.4055 global=2.0218\n",
            "  step 40: local=0.4075 global=2.0994\n",
            "  step 50: local=0.3996 global=1.9841\n",
            "  step 60: local=0.3894 global=1.9753\n",
            "  step 70: local=0.3869 global=1.9991\n",
            "  step 80: local=0.3863 global=1.9973\n",
            "  step 90: local=0.3832 global=1.9818\n",
            "  Layer 22 not converged (global=1.9481 > 0.8), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3897 global=2.1780\n",
            "  step 10: local=0.3773 global=2.0289\n",
            "  step 20: local=0.3711 global=1.9551\n",
            "  step 30: local=0.3720 global=2.1351\n",
            "  step 40: local=0.3761 global=1.9231\n",
            "  step 50: local=0.3803 global=1.9745\n",
            "  step 60: local=0.3636 global=2.1240\n",
            "  step 70: local=0.3641 global=2.1045\n",
            "  step 80: local=0.3739 global=2.0277\n",
            "  step 90: local=0.3662 global=1.9094\n",
            "  Layer 22 not converged (global=2.0743 > 0.8), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3718 global=2.0375\n",
            "  step 10: local=0.3694 global=1.9946\n",
            "  step 20: local=0.3750 global=1.9656\n",
            "  step 30: local=0.3613 global=2.0057\n",
            "  step 40: local=0.3713 global=1.9164\n",
            "  step 50: local=0.3754 global=2.1108\n",
            "  step 60: local=0.3579 global=2.0239\n",
            "  step 70: local=0.3696 global=1.9807\n",
            "  step 80: local=0.3620 global=1.9968\n",
            "  step 90: local=0.3632 global=2.0893\n",
            "  Layer 22 not converged (global=1.9298 > 0.8), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3674 global=2.0827\n",
            "  step 10: local=0.3598 global=2.0369\n",
            "  step 20: local=0.3598 global=2.0327\n",
            "  step 30: local=0.3558 global=1.9256\n",
            "  step 40: local=0.3631 global=1.9476\n",
            "  step 50: local=0.3671 global=1.8890\n",
            "  step 60: local=0.3519 global=2.1272\n",
            "  step 70: local=0.3537 global=2.1606\n",
            "  step 80: local=0.3524 global=1.9271\n",
            "  step 90: local=0.3580 global=1.9065\n",
            "  Layer 22 not converged (global=1.9824 > 0.8), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3525 global=1.8848\n",
            "  step 10: local=0.3561 global=1.9214\n",
            "  step 20: local=0.3644 global=2.0918\n",
            "  step 30: local=0.3634 global=1.9372\n",
            "  step 40: local=0.3541 global=2.0132\n",
            "  step 50: local=0.3601 global=2.1198\n",
            "  step 60: local=0.3592 global=1.9517\n",
            "  step 70: local=0.3544 global=1.9284\n",
            "  step 80: local=0.3567 global=1.9819\n",
            "  step 90: local=0.3590 global=1.9247\n",
            "  Layer 22 not converged (global=1.9772 > 0.8), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3479 global=1.9667\n",
            "  step 10: local=0.3474 global=2.0800\n",
            "  step 20: local=0.3525 global=1.9436\n",
            "  step 30: local=0.3626 global=2.0015\n",
            "  step 40: local=0.3589 global=2.0932\n",
            "  step 50: local=0.3433 global=2.0731\n",
            "  step 60: local=0.3526 global=1.9257\n",
            "  step 70: local=0.3454 global=1.8599\n",
            "  step 80: local=0.3440 global=2.0190\n",
            "  step 90: local=0.3509 global=1.9618\n",
            "  Layer 22 not converged (global=2.0111 > 0.8), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3539 global=1.9196\n",
            "  step 10: local=0.3492 global=1.8967\n",
            "  step 20: local=0.3435 global=2.0072\n",
            "  step 30: local=0.3458 global=2.0860\n",
            "  step 40: local=0.3477 global=2.0042\n",
            "  step 50: local=0.3493 global=1.9478\n",
            "  step 60: local=0.3545 global=1.9450\n",
            "  step 70: local=0.3533 global=2.0548\n",
            "  step 80: local=0.3585 global=2.1337\n",
            "  step 90: local=0.3508 global=2.0463\n",
            "  Layer 22 not converged (global=2.0157 > 0.8), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3535 global=1.8958\n",
            "  step 10: local=0.3529 global=1.9183\n",
            "  step 20: local=0.3553 global=2.0029\n",
            "  step 30: local=0.3484 global=2.0149\n",
            "  step 40: local=0.3433 global=1.9572\n",
            "  step 50: local=0.3478 global=1.9876\n",
            "  step 60: local=0.3466 global=2.0325\n",
            "  step 70: local=0.3558 global=1.9264\n",
            "  step 80: local=0.3490 global=1.9694\n",
            "  step 90: local=0.3508 global=2.0562\n",
            "  Layer 22 not converged (global=1.9742 > 0.8), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3497 global=2.0030\n",
            "  step 10: local=0.3467 global=2.0415\n",
            "  step 20: local=0.3469 global=1.9522\n",
            "  step 30: local=0.3550 global=2.0359\n",
            "  step 40: local=0.3445 global=1.8364\n",
            "  step 50: local=0.3401 global=1.9340\n",
            "  step 60: local=0.3432 global=1.9783\n",
            "  step 70: local=0.3470 global=2.0620\n",
            "  step 80: local=0.3495 global=1.9921\n",
            "  step 90: local=0.3395 global=1.8872\n",
            "  Layer 22 not converged (global=2.0174 > 0.8), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3486 global=1.8362\n",
            "  step 10: local=0.3499 global=1.9659\n",
            "  step 20: local=0.3392 global=1.9025\n",
            "  step 30: local=0.3417 global=1.9736\n",
            "  step 40: local=0.3589 global=1.9263\n",
            "  step 50: local=0.3422 global=2.0681\n",
            "  step 60: local=0.3444 global=1.9568\n",
            "  step 70: local=0.3572 global=1.9997\n",
            "  step 80: local=0.3476 global=2.0108\n",
            "  step 90: local=0.3444 global=2.0018\n",
            "  Layer 22 not converged (global=2.1454 > 0.8), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3457 global=1.8750\n",
            "  step 10: local=0.3531 global=2.0392\n",
            "  step 20: local=0.3500 global=2.0267\n",
            "  step 30: local=0.3416 global=1.8329\n",
            "  step 40: local=0.3461 global=1.8999\n",
            "  step 50: local=0.3474 global=2.0125\n",
            "  step 60: local=0.3378 global=1.8598\n",
            "  step 70: local=0.3391 global=1.9822\n",
            "  step 80: local=0.3479 global=2.0586\n",
            "  step 90: local=0.3462 global=1.9496\n",
            "  Layer 22 not converged (global=2.0875 > 0.8), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3433 global=1.9408\n",
            "  step 10: local=0.3375 global=1.9663\n",
            "  step 20: local=0.3413 global=1.9653\n",
            "  step 30: local=0.3379 global=1.9512\n",
            "  step 40: local=0.3390 global=2.1448\n",
            "  step 50: local=0.3427 global=1.9992\n",
            "  step 60: local=0.3329 global=1.9284\n",
            "  step 70: local=0.3494 global=2.1073\n",
            "  step 80: local=0.3389 global=1.8975\n",
            "  step 90: local=0.3305 global=1.9509\n",
            "  Layer 22 not converged (global=2.0189 > 0.8), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3344 global=2.0979\n",
            "  step 10: local=0.3396 global=2.0792\n",
            "  step 20: local=0.3439 global=2.0023\n",
            "  step 30: local=0.3444 global=1.8863\n",
            "  step 40: local=0.3304 global=2.0138\n",
            "  step 50: local=0.3382 global=1.9728\n",
            "  step 60: local=0.3466 global=1.9451\n",
            "  step 70: local=0.3363 global=1.9866\n",
            "  step 80: local=0.3509 global=1.8970\n",
            "  step 90: local=0.3481 global=2.0912\n",
            "  Layer 22 not converged (global=1.9863 > 0.8), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3402 global=2.0049\n",
            "  step 10: local=0.3476 global=1.9625\n",
            "  step 20: local=0.3378 global=1.9794\n",
            "  step 30: local=0.3378 global=2.0709\n",
            "  step 40: local=0.3547 global=2.0611\n",
            "  step 50: local=0.3495 global=2.0191\n",
            "  step 60: local=0.3410 global=2.0146\n",
            "  step 70: local=0.3383 global=1.9096\n",
            "  step 80: local=0.3354 global=1.9320\n",
            "  step 90: local=0.3368 global=1.8743\n",
            "  Layer 22 not converged (global=1.9280 > 0.8), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3402 global=2.1118\n",
            "  step 10: local=0.3322 global=2.1438\n",
            "  step 20: local=0.3373 global=1.9135\n",
            "  step 30: local=0.3446 global=1.8915\n",
            "  step 40: local=0.3410 global=1.8695\n",
            "  step 50: local=0.3333 global=1.9065\n",
            "  step 60: local=0.3369 global=2.0774\n",
            "  step 70: local=0.3352 global=1.9243\n",
            "  step 80: local=0.3442 global=1.9994\n",
            "  step 90: local=0.3409 global=2.1048\n",
            "  Layer 22 not converged (global=2.0286 > 0.8), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3297 global=1.9381\n",
            "  step 10: local=0.3365 global=1.9144\n",
            "  step 20: local=0.3438 global=1.9699\n",
            "  step 30: local=0.3392 global=1.9134\n",
            "  step 40: local=0.3382 global=1.9522\n",
            "  step 50: local=0.3285 global=2.0666\n",
            "  step 60: local=0.3378 global=1.9312\n",
            "  step 70: local=0.3484 global=1.9913\n",
            "  step 80: local=0.3371 global=2.0815\n",
            "  step 90: local=0.3392 global=2.0609\n",
            "  Layer 22 not converged (global=1.9271 > 0.8), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3304 global=1.9151\n",
            "  step 10: local=0.3396 global=1.8486\n",
            "  step 20: local=0.3388 global=2.0049\n",
            "  step 30: local=0.3398 global=1.9533\n",
            "  step 40: local=0.3403 global=1.9067\n",
            "  step 50: local=0.3387 global=1.8848\n",
            "  step 60: local=0.3311 global=1.9970\n",
            "  step 70: local=0.3476 global=2.0757\n",
            "  step 80: local=0.3303 global=1.9936\n",
            "  step 90: local=0.3362 global=1.9368\n",
            "  Layer 22 not converged (global=1.9822 > 0.8), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3287 global=1.9348\n",
            "  step 10: local=0.3293 global=2.0436\n",
            "  step 20: local=0.3420 global=2.1237\n",
            "  step 30: local=0.3340 global=2.0389\n",
            "  step 40: local=0.3423 global=1.8838\n",
            "  step 50: local=0.3327 global=1.9090\n",
            "  step 60: local=0.3366 global=1.9944\n",
            "  step 70: local=0.3308 global=2.0039\n",
            "  step 80: local=0.3312 global=1.9475\n",
            "  step 90: local=0.3400 global=1.9791\n",
            "  [WARN] Layer 22 did not converge after 20 repeats (global=1.9548)\n",
            "  [TIME] Layer 22 took 489.8s | Total: 231.1min | ETA: 41.3min (5 layers left)\n",
            "\n",
            "--- Layer 23/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4747 global=2.1145\n",
            "  step 10: local=0.4923 global=1.9932\n",
            "  step 20: local=0.4729 global=2.0288\n",
            "  step 30: local=0.4707 global=2.1192\n",
            "  step 40: local=0.4667 global=2.0688\n",
            "  step 50: local=0.4770 global=2.1033\n",
            "  step 60: local=0.4640 global=2.0053\n",
            "  step 70: local=0.4425 global=2.0897\n",
            "  step 80: local=0.4501 global=1.8938\n",
            "  step 90: local=0.4428 global=1.9814\n",
            "  Layer 23 not converged (global=2.1415 > 0.8), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4332 global=2.0286\n",
            "  step 10: local=0.4477 global=2.1133\n",
            "  step 20: local=0.4383 global=2.0400\n",
            "  step 30: local=0.4315 global=1.9288\n",
            "  step 40: local=0.4263 global=1.8820\n",
            "  step 50: local=0.4368 global=2.0165\n",
            "  step 60: local=0.4139 global=1.9430\n",
            "  step 70: local=0.4058 global=2.0175\n",
            "  step 80: local=0.4134 global=1.9675\n",
            "  step 90: local=0.4090 global=2.1100\n",
            "  Layer 23 not converged (global=1.9920 > 0.8), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4041 global=1.9949\n",
            "  step 10: local=0.4012 global=2.0351\n",
            "  step 20: local=0.4079 global=2.0444\n",
            "  step 30: local=0.3981 global=2.0404\n",
            "  step 40: local=0.3873 global=1.9032\n",
            "  step 50: local=0.3941 global=2.0709\n",
            "  step 60: local=0.3979 global=2.0688\n",
            "  step 70: local=0.3901 global=1.8619\n",
            "  step 80: local=0.3839 global=1.9331\n",
            "  step 90: local=0.3725 global=2.0392\n",
            "  Layer 23 not converged (global=2.0450 > 0.8), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3769 global=1.8905\n",
            "  step 10: local=0.3676 global=2.0081\n",
            "  step 20: local=0.3993 global=2.0888\n",
            "  step 30: local=0.3660 global=1.9741\n",
            "  step 40: local=0.3637 global=1.9622\n",
            "  step 50: local=0.3698 global=1.9887\n",
            "  step 60: local=0.3693 global=1.9852\n",
            "  step 70: local=0.3631 global=1.9748\n",
            "  step 80: local=0.3715 global=2.1789\n",
            "  step 90: local=0.3635 global=2.0283\n",
            "  Layer 23 not converged (global=2.2689 > 0.8), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3579 global=1.9446\n",
            "  step 10: local=0.3669 global=2.1262\n",
            "  step 20: local=0.3581 global=1.9199\n",
            "  step 30: local=0.3587 global=1.9744\n",
            "  step 40: local=0.3748 global=2.1223\n",
            "  step 50: local=0.3564 global=2.1029\n",
            "  step 60: local=0.3551 global=2.0280\n",
            "  step 70: local=0.3651 global=1.9002\n",
            "  step 80: local=0.3503 global=2.0417\n",
            "  step 90: local=0.3505 global=1.9956\n",
            "  Layer 23 not converged (global=1.9893 > 0.8), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3518 global=1.9599\n",
            "  step 10: local=0.3575 global=2.0053\n",
            "  step 20: local=0.3508 global=1.9128\n",
            "  step 30: local=0.3487 global=2.1079\n",
            "  step 40: local=0.3507 global=2.0233\n",
            "  step 50: local=0.3517 global=1.9808\n",
            "  step 60: local=0.3555 global=1.9960\n",
            "  step 70: local=0.3593 global=2.0832\n",
            "  step 80: local=0.3610 global=2.0864\n",
            "  step 90: local=0.3444 global=2.0363\n",
            "  Layer 23 not converged (global=2.0437 > 0.8), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3552 global=2.0342\n",
            "  step 10: local=0.3470 global=1.9270\n",
            "  step 20: local=0.3443 global=1.9561\n",
            "  step 30: local=0.3440 global=1.8895\n",
            "  step 40: local=0.3476 global=2.1264\n",
            "  step 50: local=0.3403 global=2.1587\n",
            "  step 60: local=0.3438 global=1.9280\n",
            "  step 70: local=0.3464 global=1.9052\n",
            "  step 80: local=0.3348 global=1.8854\n",
            "  step 90: local=0.3392 global=1.9150\n",
            "  Layer 23 not converged (global=1.9979 > 0.8), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3400 global=2.0891\n",
            "  step 10: local=0.3432 global=1.9450\n",
            "  step 20: local=0.3460 global=2.0244\n",
            "  step 30: local=0.3522 global=2.1130\n",
            "  step 40: local=0.3365 global=1.9470\n",
            "  step 50: local=0.3379 global=1.9242\n",
            "  step 60: local=0.3524 global=1.9797\n",
            "  step 70: local=0.3445 global=1.9248\n",
            "  step 80: local=0.3406 global=1.9590\n",
            "  step 90: local=0.3439 global=2.0815\n",
            "  Layer 23 not converged (global=1.9259 > 0.8), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3438 global=1.9403\n",
            "  step 10: local=0.3445 global=2.0074\n",
            "  step 20: local=0.3357 global=2.0938\n",
            "  step 30: local=0.3370 global=2.0749\n",
            "  step 40: local=0.3471 global=1.9300\n",
            "  step 50: local=0.3372 global=1.8615\n",
            "  step 60: local=0.3408 global=2.0160\n",
            "  step 70: local=0.3410 global=1.9685\n",
            "  step 80: local=0.3467 global=1.9152\n",
            "  step 90: local=0.3373 global=1.8942\n",
            "  Layer 23 not converged (global=2.0199 > 0.8), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3418 global=2.0031\n",
            "  step 10: local=0.3471 global=2.0887\n",
            "  step 20: local=0.3395 global=2.0087\n",
            "  step 30: local=0.3376 global=1.9519\n",
            "  step 40: local=0.3370 global=1.9529\n",
            "  step 50: local=0.3403 global=2.0517\n",
            "  step 60: local=0.3412 global=2.1303\n",
            "  step 70: local=0.3426 global=2.0549\n",
            "  step 80: local=0.3361 global=1.8910\n",
            "  step 90: local=0.3454 global=1.9211\n",
            "  Layer 23 not converged (global=2.0009 > 0.8), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3290 global=1.9994\n",
            "  step 10: local=0.3452 global=2.0180\n",
            "  step 20: local=0.3295 global=1.9613\n",
            "  step 30: local=0.3402 global=1.9923\n",
            "  step 40: local=0.3342 global=2.0428\n",
            "  step 50: local=0.3434 global=1.9247\n",
            "  step 60: local=0.3386 global=1.9708\n",
            "  step 70: local=0.3345 global=2.0586\n",
            "  step 80: local=0.3339 global=2.0108\n",
            "  step 90: local=0.3367 global=2.0484\n",
            "  Layer 23 not converged (global=1.9462 > 0.8), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3373 global=1.9491\n",
            "  step 10: local=0.3333 global=2.0414\n",
            "  step 20: local=0.3385 global=1.8443\n",
            "  step 30: local=0.3411 global=1.9359\n",
            "  step 40: local=0.3321 global=1.9794\n",
            "  step 50: local=0.3462 global=2.0649\n",
            "  step 60: local=0.3516 global=1.9950\n",
            "  step 70: local=0.3318 global=1.8867\n",
            "  step 80: local=0.3279 global=1.8421\n",
            "  step 90: local=0.3430 global=1.9719\n",
            "  Layer 23 not converged (global=1.9283 > 0.8), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3200 global=1.9061\n",
            "  step 10: local=0.3292 global=1.9784\n",
            "  step 20: local=0.3345 global=1.9318\n",
            "  step 30: local=0.3411 global=2.0738\n",
            "  step 40: local=0.3380 global=1.9581\n",
            "  step 50: local=0.3363 global=2.0034\n",
            "  step 60: local=0.3392 global=2.0108\n",
            "  step 70: local=0.3389 global=2.0106\n",
            "  step 80: local=0.3288 global=1.8758\n",
            "  step 90: local=0.3299 global=2.0404\n",
            "  Layer 23 not converged (global=2.0261 > 0.8), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3315 global=2.0390\n",
            "  step 10: local=0.3302 global=1.8352\n",
            "  step 20: local=0.3289 global=1.9066\n",
            "  step 30: local=0.3344 global=2.0142\n",
            "  step 40: local=0.3308 global=1.8655\n",
            "  step 50: local=0.3287 global=1.9837\n",
            "  step 60: local=0.3469 global=2.0661\n",
            "  step 70: local=0.3317 global=1.9523\n",
            "  step 80: local=0.3220 global=1.9425\n",
            "  step 90: local=0.3262 global=1.9700\n",
            "  Layer 23 not converged (global=2.0571 > 0.8), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3253 global=1.9674\n",
            "  step 10: local=0.3263 global=1.9546\n",
            "  step 20: local=0.3307 global=2.1585\n",
            "  step 30: local=0.3333 global=2.0104\n",
            "  step 40: local=0.3217 global=1.9265\n",
            "  step 50: local=0.3398 global=2.1095\n",
            "  step 60: local=0.3309 global=1.9030\n",
            "  step 70: local=0.3366 global=1.9590\n",
            "  step 80: local=0.3410 global=2.1063\n",
            "  step 90: local=0.3339 global=2.0866\n",
            "  Layer 23 not converged (global=1.9895 > 0.8), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3280 global=2.0131\n",
            "  step 10: local=0.3403 global=1.8864\n",
            "  step 20: local=0.3337 global=2.0272\n",
            "  step 30: local=0.3230 global=1.9803\n",
            "  step 40: local=0.3242 global=1.9461\n",
            "  step 50: local=0.3303 global=1.9908\n",
            "  step 60: local=0.3406 global=1.8993\n",
            "  step 70: local=0.3363 global=2.0935\n",
            "  step 80: local=0.3278 global=2.0107\n",
            "  step 90: local=0.3204 global=1.9685\n",
            "  Layer 23 not converged (global=1.9884 > 0.8), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3234 global=1.9826\n",
            "  step 10: local=0.3294 global=2.0707\n",
            "  step 20: local=0.3369 global=2.0726\n",
            "  step 30: local=0.3280 global=2.0253\n",
            "  step 40: local=0.3429 global=2.0214\n",
            "  step 50: local=0.3269 global=1.9149\n",
            "  step 60: local=0.3341 global=1.9451\n",
            "  step 70: local=0.3283 global=1.8796\n",
            "  step 80: local=0.3299 global=2.1159\n",
            "  step 90: local=0.3339 global=2.1476\n",
            "  Layer 23 not converged (global=1.9897 > 0.8), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3240 global=1.9193\n",
            "  step 10: local=0.3340 global=1.8944\n",
            "  step 20: local=0.3171 global=1.8736\n",
            "  step 30: local=0.3251 global=1.9057\n",
            "  step 40: local=0.3316 global=2.0771\n",
            "  step 50: local=0.3318 global=1.9357\n",
            "  step 60: local=0.3331 global=2.0152\n",
            "  step 70: local=0.3433 global=2.1030\n",
            "  step 80: local=0.3218 global=1.9381\n",
            "  step 90: local=0.3223 global=1.9148\n",
            "  Layer 23 not converged (global=1.9143 > 0.8), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3358 global=1.9714\n",
            "  step 10: local=0.3293 global=1.9158\n",
            "  step 20: local=0.3274 global=1.9508\n",
            "  step 30: local=0.3277 global=2.0713\n",
            "  step 40: local=0.3202 global=1.9320\n",
            "  step 50: local=0.3361 global=1.9975\n",
            "  step 60: local=0.3156 global=2.0854\n",
            "  step 70: local=0.3286 global=2.0663\n",
            "  step 80: local=0.3277 global=1.9225\n",
            "  step 90: local=0.3244 global=1.8539\n",
            "  Layer 23 not converged (global=2.0655 > 0.8), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3326 global=2.0088\n",
            "  step 10: local=0.3240 global=1.9609\n",
            "  step 20: local=0.3289 global=1.9087\n",
            "  step 30: local=0.3225 global=1.8869\n",
            "  step 40: local=0.3396 global=1.9963\n",
            "  step 50: local=0.3314 global=2.0805\n",
            "  step 60: local=0.3143 global=1.9998\n",
            "  step 70: local=0.3222 global=1.9462\n",
            "  step 80: local=0.3222 global=1.9454\n",
            "  step 90: local=0.3325 global=2.0443\n",
            "  [WARN] Layer 23 did not converge after 20 repeats (global=1.9260)\n",
            "  [TIME] Layer 23 took 474.4s | Total: 239.0min | ETA: 33.0min (4 layers left)\n",
            "\n",
            "--- Layer 24/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4742 global=2.1798\n",
            "  step 10: local=0.4547 global=2.1072\n",
            "  step 20: local=0.4529 global=1.9462\n",
            "  step 30: local=0.4518 global=1.9648\n",
            "  step 40: local=0.4535 global=2.0417\n",
            "  step 50: local=0.4418 global=2.0629\n",
            "  step 60: local=0.4333 global=2.0086\n",
            "  step 70: local=0.4354 global=2.0367\n",
            "  step 80: local=0.4236 global=2.0856\n",
            "  step 90: local=0.4316 global=1.9615\n",
            "  Layer 24 not converged (global=1.9630 > 0.8), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4355 global=2.0102\n",
            "  step 10: local=0.4224 global=2.1000\n",
            "  step 20: local=0.4202 global=2.0540\n",
            "  step 30: local=0.4160 global=2.0910\n",
            "  step 40: local=0.4067 global=1.9844\n",
            "  step 50: local=0.4159 global=2.0805\n",
            "  step 60: local=0.4058 global=1.8837\n",
            "  step 70: local=0.3989 global=1.9782\n",
            "  step 80: local=0.3983 global=2.0148\n",
            "  step 90: local=0.4040 global=2.0973\n",
            "  Layer 24 not converged (global=2.0647 > 0.8), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4004 global=2.0317\n",
            "  step 10: local=0.3801 global=1.9169\n",
            "  step 20: local=0.3829 global=1.8692\n",
            "  step 30: local=0.3898 global=2.0033\n",
            "  step 40: local=0.3800 global=1.9330\n",
            "  step 50: local=0.3778 global=2.0109\n",
            "  step 60: local=0.3772 global=1.9582\n",
            "  step 70: local=0.3739 global=2.0942\n",
            "  step 80: local=0.3783 global=1.9826\n",
            "  step 90: local=0.3727 global=2.0288\n",
            "  Layer 24 not converged (global=1.9875 > 0.8), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3660 global=2.0319\n",
            "  step 10: local=0.3690 global=2.0329\n",
            "  step 20: local=0.3630 global=1.8945\n",
            "  step 30: local=0.3633 global=2.0682\n",
            "  step 40: local=0.3641 global=2.0622\n",
            "  step 50: local=0.3695 global=1.8555\n",
            "  step 60: local=0.3627 global=1.9266\n",
            "  step 70: local=0.3563 global=2.0339\n",
            "  step 80: local=0.3561 global=1.8841\n",
            "  step 90: local=0.3577 global=1.9933\n",
            "  Layer 24 not converged (global=2.0587 > 0.8), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3606 global=2.0808\n",
            "  step 10: local=0.3598 global=1.9676\n",
            "  step 20: local=0.3505 global=1.9642\n",
            "  step 30: local=0.3458 global=1.9887\n",
            "  step 40: local=0.3525 global=1.9854\n",
            "  step 50: local=0.3451 global=1.9657\n",
            "  step 60: local=0.3482 global=2.1725\n",
            "  step 70: local=0.3432 global=2.0308\n",
            "  step 80: local=0.3449 global=1.9439\n",
            "  step 90: local=0.3565 global=2.1301\n",
            "  Layer 24 not converged (global=1.9631 > 0.8), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3496 global=1.9147\n",
            "  step 10: local=0.3520 global=1.9716\n",
            "  step 20: local=0.3510 global=2.1341\n",
            "  step 30: local=0.3441 global=2.1067\n",
            "  step 40: local=0.3416 global=2.0265\n",
            "  step 50: local=0.3462 global=1.9021\n",
            "  step 60: local=0.3538 global=2.0404\n",
            "  step 70: local=0.3396 global=1.9996\n",
            "  step 80: local=0.3408 global=1.9614\n",
            "  step 90: local=0.3387 global=2.0061\n",
            "  Layer 24 not converged (global=1.9217 > 0.8), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3418 global=1.9092\n",
            "  step 10: local=0.3395 global=2.1112\n",
            "  step 20: local=0.3399 global=2.0253\n",
            "  step 30: local=0.3447 global=1.9824\n",
            "  step 40: local=0.3303 global=1.9950\n",
            "  step 50: local=0.3426 global=2.0871\n",
            "  step 60: local=0.3440 global=2.0906\n",
            "  step 70: local=0.3290 global=2.0352\n",
            "  step 80: local=0.3471 global=2.0301\n",
            "  step 90: local=0.3287 global=1.9231\n",
            "  Layer 24 not converged (global=2.0728 > 0.8), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3248 global=1.9568\n",
            "  step 10: local=0.3323 global=1.8921\n",
            "  step 20: local=0.3275 global=2.1313\n",
            "  step 30: local=0.3316 global=2.1615\n",
            "  step 40: local=0.3319 global=1.9273\n",
            "  step 50: local=0.3359 global=1.9018\n",
            "  step 60: local=0.3348 global=1.8853\n",
            "  step 70: local=0.3294 global=1.9178\n",
            "  step 80: local=0.3404 global=2.0920\n",
            "  step 90: local=0.3266 global=1.9478\n",
            "  Layer 24 not converged (global=1.9834 > 0.8), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3314 global=2.0321\n",
            "  step 10: local=0.3371 global=2.1120\n",
            "  step 20: local=0.3168 global=1.9469\n",
            "  step 30: local=0.3262 global=1.9265\n",
            "  step 40: local=0.3340 global=1.9807\n",
            "  step 50: local=0.3339 global=1.9287\n",
            "  step 60: local=0.3280 global=1.9650\n",
            "  step 70: local=0.3344 global=2.0847\n",
            "  step 80: local=0.3312 global=1.9452\n",
            "  step 90: local=0.3361 global=2.0088\n",
            "  Layer 24 not converged (global=2.0960 > 0.8), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3361 global=2.0977\n",
            "  step 10: local=0.3400 global=2.0833\n",
            "  step 20: local=0.3170 global=1.9389\n",
            "  step 30: local=0.3334 global=1.8673\n",
            "  step 40: local=0.3277 global=2.0244\n",
            "  step 50: local=0.3260 global=1.9723\n",
            "  step 60: local=0.3296 global=1.9205\n",
            "  step 70: local=0.3259 global=1.8976\n",
            "  step 80: local=0.3356 global=2.0052\n",
            "  step 90: local=0.3304 global=2.0951\n",
            "  Layer 24 not converged (global=2.0286 > 0.8), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3234 global=2.0108\n",
            "  step 10: local=0.3302 global=1.9496\n",
            "  step 20: local=0.3295 global=1.9566\n",
            "  step 30: local=0.3354 global=2.0581\n",
            "  step 40: local=0.3398 global=2.1316\n",
            "  step 50: local=0.3278 global=2.0586\n",
            "  step 60: local=0.3309 global=1.8955\n",
            "  step 70: local=0.3175 global=1.9212\n",
            "  step 80: local=0.3375 global=2.0036\n",
            "  step 90: local=0.3286 global=2.0153\n",
            "  Layer 24 not converged (global=1.9541 > 0.8), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3197 global=1.9681\n",
            "  step 10: local=0.3227 global=1.9984\n",
            "  step 20: local=0.3280 global=2.0487\n",
            "  step 30: local=0.3202 global=1.9234\n",
            "  step 40: local=0.3255 global=1.9728\n",
            "  step 50: local=0.3346 global=2.0641\n",
            "  step 60: local=0.3241 global=2.0163\n",
            "  step 70: local=0.3337 global=2.0552\n",
            "  step 80: local=0.3186 global=1.9526\n",
            "  step 90: local=0.3221 global=2.0477\n",
            "  Layer 24 not converged (global=1.9707 > 0.8), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3174 global=1.8532\n",
            "  step 10: local=0.3206 global=1.9466\n",
            "  step 20: local=0.3250 global=1.9851\n",
            "  step 30: local=0.3268 global=2.0672\n",
            "  step 40: local=0.3255 global=2.0041\n",
            "  step 50: local=0.3161 global=1.8911\n",
            "  step 60: local=0.3226 global=1.8469\n",
            "  step 70: local=0.3264 global=1.9747\n",
            "  step 80: local=0.3174 global=1.9114\n",
            "  step 90: local=0.3244 global=1.9900\n",
            "  Layer 24 not converged (global=2.0325 > 0.8), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3264 global=1.9366\n",
            "  step 10: local=0.3168 global=2.0738\n",
            "  step 20: local=0.3310 global=1.9624\n",
            "  step 30: local=0.3206 global=2.0102\n",
            "  step 40: local=0.3238 global=2.0129\n",
            "  step 50: local=0.3307 global=2.0147\n",
            "  step 60: local=0.3261 global=1.8779\n",
            "  step 70: local=0.3253 global=2.0495\n",
            "  step 80: local=0.3244 global=2.0458\n",
            "  step 90: local=0.3204 global=1.8396\n",
            "  Layer 24 not converged (global=2.0285 > 0.8), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3190 global=1.9116\n",
            "  step 10: local=0.3197 global=2.0180\n",
            "  step 20: local=0.3143 global=1.8687\n",
            "  step 30: local=0.3239 global=1.9789\n",
            "  step 40: local=0.3383 global=2.0654\n",
            "  step 50: local=0.3237 global=1.9539\n",
            "  step 60: local=0.3212 global=1.9516\n",
            "  step 70: local=0.3138 global=1.9758\n",
            "  step 80: local=0.3136 global=1.9723\n",
            "  step 90: local=0.3248 global=1.9526\n",
            "  Layer 24 not converged (global=1.9349 > 0.8), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3218 global=2.1602\n",
            "  step 10: local=0.3266 global=2.0190\n",
            "  step 20: local=0.3152 global=1.9325\n",
            "  step 30: local=0.3236 global=2.1193\n",
            "  step 40: local=0.3176 global=1.9026\n",
            "  step 50: local=0.3263 global=1.9619\n",
            "  step 60: local=0.3178 global=2.1229\n",
            "  step 70: local=0.3213 global=2.0949\n",
            "  step 80: local=0.3169 global=2.0161\n",
            "  step 90: local=0.3280 global=1.8913\n",
            "  Layer 24 not converged (global=2.0603 > 0.8), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3231 global=2.0302\n",
            "  step 10: local=0.3105 global=1.9900\n",
            "  step 20: local=0.3101 global=1.9519\n",
            "  step 30: local=0.3238 global=1.9980\n",
            "  step 40: local=0.3195 global=1.8998\n",
            "  step 50: local=0.3246 global=2.1018\n",
            "  step 60: local=0.3198 global=2.0145\n",
            "  step 70: local=0.3218 global=1.9736\n",
            "  step 80: local=0.3233 global=1.9877\n",
            "  step 90: local=0.3165 global=2.0769\n",
            "  Layer 24 not converged (global=1.9284 > 0.8), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3263 global=2.0817\n",
            "  step 10: local=0.3211 global=2.0267\n",
            "  step 20: local=0.3215 global=2.0215\n",
            "  step 30: local=0.3137 global=1.9152\n",
            "  step 40: local=0.3232 global=1.9482\n",
            "  step 50: local=0.3082 global=1.8832\n",
            "  step 60: local=0.3218 global=2.1231\n",
            "  step 70: local=0.3209 global=2.1515\n",
            "  step 80: local=0.3270 global=1.9201\n",
            "  step 90: local=0.3273 global=1.8932\n",
            "  Layer 24 not converged (global=1.9804 > 0.8), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3150 global=1.8785\n",
            "  step 10: local=0.3151 global=1.9105\n",
            "  step 20: local=0.3204 global=2.0845\n",
            "  step 30: local=0.3158 global=1.9423\n",
            "  step 40: local=0.3197 global=2.0241\n",
            "  step 50: local=0.3240 global=2.1044\n",
            "  step 60: local=0.3119 global=1.9402\n",
            "  step 70: local=0.3151 global=1.9201\n",
            "  step 80: local=0.3223 global=1.9743\n",
            "  step 90: local=0.3118 global=1.9214\n",
            "  Layer 24 not converged (global=1.9706 > 0.8), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3216 global=1.9581\n",
            "  step 10: local=0.3159 global=2.0783\n",
            "  step 20: local=0.3163 global=1.9395\n",
            "  step 30: local=0.3234 global=2.0019\n",
            "  step 40: local=0.3163 global=2.0911\n",
            "  step 50: local=0.3226 global=2.0771\n",
            "  step 60: local=0.3074 global=1.9330\n",
            "  step 70: local=0.3188 global=1.8631\n",
            "  step 80: local=0.3114 global=2.0186\n",
            "  step 90: local=0.3229 global=1.9670\n",
            "  [WARN] Layer 24 did not converge after 20 repeats (global=2.0195)\n",
            "  [TIME] Layer 24 took 458.4s | Total: 246.7min | ETA: 24.7min (3 layers left)\n",
            "\n",
            "--- Layer 25/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.6020 global=1.9698\n",
            "  step 10: local=0.6002 global=1.9417\n",
            "  step 20: local=0.5279 global=2.0450\n",
            "  step 30: local=0.5392 global=2.1379\n",
            "  step 40: local=0.5602 global=2.0530\n",
            "  step 50: local=0.5466 global=1.9857\n",
            "  step 60: local=0.5291 global=1.9976\n",
            "  step 70: local=0.5304 global=2.0941\n",
            "  step 80: local=0.5199 global=2.1697\n",
            "  step 90: local=0.5220 global=2.0905\n",
            "  Layer 25 not converged (global=2.0536 > 0.8), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5277 global=1.9296\n",
            "  step 10: local=0.5123 global=1.9527\n",
            "  step 20: local=0.5206 global=2.0348\n",
            "  step 30: local=0.5047 global=2.0510\n",
            "  step 40: local=0.5086 global=1.9975\n",
            "  step 50: local=0.4871 global=2.0261\n",
            "  step 60: local=0.5158 global=2.0730\n",
            "  step 70: local=0.4719 global=1.9480\n",
            "  step 80: local=0.4968 global=1.9987\n",
            "  step 90: local=0.4995 global=2.0917\n",
            "  Layer 25 not converged (global=2.0086 > 0.8), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4911 global=2.0422\n",
            "  step 10: local=0.4908 global=2.0842\n",
            "  step 20: local=0.4815 global=1.9766\n",
            "  step 30: local=0.4845 global=2.0764\n",
            "  step 40: local=0.4591 global=1.8729\n",
            "  step 50: local=0.4821 global=1.9707\n",
            "  step 60: local=0.4841 global=2.0072\n",
            "  step 70: local=0.4901 global=2.0936\n",
            "  step 80: local=0.4739 global=2.0293\n",
            "  step 90: local=0.4695 global=1.9075\n",
            "  Layer 25 not converged (global=2.0521 > 0.8), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4620 global=1.8676\n",
            "  step 10: local=0.4503 global=1.9942\n",
            "  step 20: local=0.4563 global=1.9222\n",
            "  step 30: local=0.4570 global=2.0079\n",
            "  step 40: local=0.4464 global=1.9564\n",
            "  step 50: local=0.4482 global=2.0936\n",
            "  step 60: local=0.4560 global=1.9773\n",
            "  step 70: local=0.4417 global=2.0296\n",
            "  step 80: local=0.4623 global=2.0312\n",
            "  step 90: local=0.4327 global=2.0252\n",
            "  Layer 25 not converged (global=2.1685 > 0.8), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4533 global=1.8922\n",
            "  step 10: local=0.4520 global=2.0628\n",
            "  step 20: local=0.4584 global=2.0555\n",
            "  step 30: local=0.4403 global=1.8570\n",
            "  step 40: local=0.4331 global=1.9261\n",
            "  step 50: local=0.4383 global=2.0322\n",
            "  step 60: local=0.4462 global=1.8820\n",
            "  step 70: local=0.4342 global=1.9948\n",
            "  step 80: local=0.4332 global=2.0848\n",
            "  step 90: local=0.4486 global=1.9689\n",
            "  Layer 25 not converged (global=2.1197 > 0.8), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4381 global=1.9629\n",
            "  step 10: local=0.4378 global=1.9899\n",
            "  step 20: local=0.4498 global=1.9874\n",
            "  step 30: local=0.4396 global=1.9679\n",
            "  step 40: local=0.4617 global=2.1736\n",
            "  step 50: local=0.4390 global=2.0304\n",
            "  step 60: local=0.4306 global=1.9427\n",
            "  step 70: local=0.4404 global=2.1316\n",
            "  step 80: local=0.4171 global=1.9137\n",
            "  step 90: local=0.4385 global=1.9719\n",
            "  Layer 25 not converged (global=2.0429 > 0.8), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4414 global=2.1368\n",
            "  step 10: local=0.4441 global=2.1076\n",
            "  step 20: local=0.4391 global=2.0309\n",
            "  step 30: local=0.4396 global=1.9034\n",
            "  step 40: local=0.4257 global=2.0405\n",
            "  step 50: local=0.4333 global=1.9930\n",
            "  step 60: local=0.4363 global=1.9637\n",
            "  step 70: local=0.4417 global=2.0096\n",
            "  step 80: local=0.4412 global=1.9054\n",
            "  step 90: local=0.4286 global=2.1169\n",
            "  Layer 25 not converged (global=2.0090 > 0.8), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4259 global=2.0288\n",
            "  step 10: local=0.4209 global=1.9813\n",
            "  step 20: local=0.4173 global=1.9993\n",
            "  step 30: local=0.4361 global=2.0844\n",
            "  step 40: local=0.4226 global=2.0944\n",
            "  step 50: local=0.4247 global=2.0360\n",
            "  step 60: local=0.4166 global=2.0351\n",
            "  step 70: local=0.4154 global=1.9313\n",
            "  step 80: local=0.4254 global=1.9571\n",
            "  step 90: local=0.4321 global=1.8933\n",
            "  Layer 25 not converged (global=1.9419 > 0.8), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4444 global=2.1383\n",
            "  step 10: local=0.4240 global=2.1584\n",
            "  step 20: local=0.4393 global=1.9312\n",
            "  step 30: local=0.4391 global=1.9045\n",
            "  step 40: local=0.4174 global=1.8906\n",
            "  step 50: local=0.4088 global=1.9215\n",
            "  step 60: local=0.4473 global=2.0979\n",
            "  step 70: local=0.4206 global=1.9565\n",
            "  step 80: local=0.4212 global=2.0346\n",
            "  step 90: local=0.4216 global=2.1105\n",
            "  Layer 25 not converged (global=2.0556 > 0.8), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4284 global=1.9507\n",
            "  step 10: local=0.4240 global=1.9271\n",
            "  step 20: local=0.4259 global=1.9880\n",
            "  step 30: local=0.4154 global=1.9279\n",
            "  step 40: local=0.4179 global=1.9711\n",
            "  step 50: local=0.3988 global=2.0884\n",
            "  step 60: local=0.4125 global=1.9460\n",
            "  step 70: local=0.4242 global=2.0092\n",
            "  step 80: local=0.4141 global=2.0994\n",
            "  step 90: local=0.4259 global=2.0843\n",
            "  Layer 25 not converged (global=1.9387 > 0.8), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4298 global=1.9396\n",
            "  step 10: local=0.4068 global=1.8772\n",
            "  step 20: local=0.4172 global=2.0272\n",
            "  step 30: local=0.4258 global=1.9782\n",
            "  step 40: local=0.4279 global=1.9223\n",
            "  step 50: local=0.4098 global=1.8990\n",
            "  step 60: local=0.4073 global=1.9997\n",
            "  step 70: local=0.4187 global=2.0988\n",
            "  step 80: local=0.3906 global=2.0131\n",
            "  step 90: local=0.4273 global=1.9510\n",
            "  Layer 25 not converged (global=2.0037 > 0.8), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4138 global=1.9589\n",
            "  step 10: local=0.4098 global=2.0589\n",
            "  step 20: local=0.4267 global=2.1362\n",
            "  step 30: local=0.4181 global=2.0604\n",
            "  step 40: local=0.4057 global=1.8961\n",
            "  step 50: local=0.4003 global=1.9233\n",
            "  step 60: local=0.4162 global=2.0094\n",
            "  step 70: local=0.4173 global=2.0189\n",
            "  step 80: local=0.4020 global=1.9722\n",
            "  step 90: local=0.4081 global=2.0000\n",
            "  Layer 25 not converged (global=1.9760 > 0.8), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4077 global=2.0466\n",
            "  step 10: local=0.4006 global=1.9238\n",
            "  step 20: local=0.4049 global=1.9739\n",
            "  step 30: local=0.4178 global=2.0682\n",
            "  step 40: local=0.4044 global=2.0183\n",
            "  step 50: local=0.4158 global=2.0612\n",
            "  step 60: local=0.4224 global=1.9575\n",
            "  step 70: local=0.3948 global=2.0568\n",
            "  step 80: local=0.4166 global=1.8534\n",
            "  step 90: local=0.4194 global=1.9506\n",
            "  Layer 25 not converged (global=2.0915 > 0.8), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4249 global=1.9892\n",
            "  step 10: local=0.4346 global=2.0761\n",
            "  step 20: local=0.4170 global=2.0125\n",
            "  step 30: local=0.4059 global=1.8915\n",
            "  step 40: local=0.4093 global=1.8516\n",
            "  step 50: local=0.4063 global=1.9780\n",
            "  step 60: local=0.4213 global=1.9082\n",
            "  step 70: local=0.4148 global=1.9961\n",
            "  step 80: local=0.4187 global=1.9421\n",
            "  step 90: local=0.3980 global=2.0808\n",
            "  Layer 25 not converged (global=1.9657 > 0.8), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4036 global=1.9639\n",
            "  step 10: local=0.3987 global=2.0174\n",
            "  step 20: local=0.4231 global=2.0168\n",
            "  step 30: local=0.4066 global=2.0138\n",
            "  step 40: local=0.4185 global=1.8822\n",
            "  step 50: local=0.4028 global=2.0503\n",
            "  step 60: local=0.4145 global=2.0460\n",
            "  step 70: local=0.4012 global=1.8464\n",
            "  step 80: local=0.4180 global=1.9174\n",
            "  step 90: local=0.4174 global=2.0213\n",
            "  Layer 25 not converged (global=2.0364 > 0.8), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3981 global=1.8708\n",
            "  step 10: local=0.4168 global=1.9850\n",
            "  step 20: local=0.4052 global=2.0769\n",
            "  step 30: local=0.4085 global=1.9602\n",
            "  step 40: local=0.4288 global=1.9537\n",
            "  step 50: local=0.4226 global=1.9807\n",
            "  step 60: local=0.4219 global=1.9781\n",
            "  step 70: local=0.4201 global=1.9589\n",
            "  step 80: local=0.4303 global=2.1645\n",
            "  step 90: local=0.4122 global=2.0215\n",
            "  Layer 25 not converged (global=2.2541 > 0.8), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4054 global=1.9345\n",
            "  step 10: local=0.4192 global=2.1239\n",
            "  step 20: local=0.4059 global=1.9068\n",
            "  step 30: local=0.4178 global=1.9652\n",
            "  step 40: local=0.4063 global=2.1282\n",
            "  step 50: local=0.4020 global=2.1000\n",
            "  step 60: local=0.4102 global=2.0238\n",
            "  step 70: local=0.4119 global=1.8963\n",
            "  step 80: local=0.3811 global=2.0330\n",
            "  step 90: local=0.4079 global=1.9859\n",
            "  Layer 25 not converged (global=1.9828 > 0.8), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4208 global=1.9583\n",
            "  step 10: local=0.4143 global=2.0038\n",
            "  step 20: local=0.4008 global=1.8986\n",
            "  step 30: local=0.4212 global=2.1110\n",
            "  step 40: local=0.3992 global=2.0226\n",
            "  step 50: local=0.4062 global=1.9752\n",
            "  step 60: local=0.4089 global=1.9941\n",
            "  step 70: local=0.4190 global=2.0784\n",
            "  step 80: local=0.4154 global=2.0882\n",
            "  step 90: local=0.4064 global=2.0303\n",
            "  Layer 25 not converged (global=2.0405 > 0.8), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3970 global=2.0295\n",
            "  step 10: local=0.4041 global=1.9260\n",
            "  step 20: local=0.4038 global=1.9519\n",
            "  step 30: local=0.4032 global=1.8883\n",
            "  step 40: local=0.4026 global=2.1315\n",
            "  step 50: local=0.3949 global=2.1525\n",
            "  step 60: local=0.3975 global=1.9256\n",
            "  step 70: local=0.4251 global=1.8990\n",
            "  step 80: local=0.4122 global=1.8848\n",
            "  step 90: local=0.4017 global=1.9166\n",
            "  Layer 25 not converged (global=2.0024 > 0.8), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4066 global=2.0915\n",
            "  step 10: local=0.4239 global=1.9508\n",
            "  step 20: local=0.4119 global=2.0304\n",
            "  step 30: local=0.4109 global=2.1068\n",
            "  step 40: local=0.3976 global=1.9453\n",
            "  step 50: local=0.4001 global=1.9234\n",
            "  step 60: local=0.4041 global=1.9833\n",
            "  step 70: local=0.4102 global=1.9237\n",
            "  step 80: local=0.4177 global=1.9669\n",
            "  step 90: local=0.4175 global=2.0833\n",
            "  [WARN] Layer 25 did not converge after 20 repeats (global=1.9215)\n",
            "  [TIME] Layer 25 took 443.2s | Total: 254.0min | ETA: 16.4min (2 layers left)\n",
            "\n",
            "--- Layer 26/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5146 global=2.0569\n",
            "  step 10: local=0.5027 global=2.0993\n",
            "  step 20: local=0.5051 global=2.1868\n",
            "  step 30: local=0.5067 global=2.1743\n",
            "  step 40: local=0.4858 global=2.0271\n",
            "  step 50: local=0.4816 global=1.9659\n",
            "  step 60: local=0.4832 global=2.1133\n",
            "  step 70: local=0.4936 global=2.0465\n",
            "  step 80: local=0.4763 global=1.9903\n",
            "  step 90: local=0.4670 global=1.9743\n",
            "  Layer 26 not converged (global=2.0996 > 0.8), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4710 global=2.0658\n",
            "  step 10: local=0.4807 global=2.1647\n",
            "  step 20: local=0.4658 global=2.0741\n",
            "  step 30: local=0.4440 global=2.0119\n",
            "  step 40: local=0.4566 global=2.0176\n",
            "  step 50: local=0.4487 global=2.1240\n",
            "  step 60: local=0.4670 global=2.1968\n",
            "  step 70: local=0.4497 global=2.1145\n",
            "  step 80: local=0.4479 global=1.9403\n",
            "  step 90: local=0.4398 global=1.9737\n",
            "  Layer 26 not converged (global=2.0525 > 0.8), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4371 global=2.0578\n",
            "  step 10: local=0.4227 global=2.0712\n",
            "  step 20: local=0.4097 global=2.0226\n",
            "  step 30: local=0.4188 global=2.0483\n",
            "  step 40: local=0.4504 global=2.0903\n",
            "  step 50: local=0.4254 global=1.9680\n",
            "  step 60: local=0.4399 global=2.0158\n",
            "  step 70: local=0.4223 global=2.1100\n",
            "  step 80: local=0.4053 global=2.0533\n",
            "  step 90: local=0.4059 global=2.0984\n",
            "  Layer 26 not converged (global=1.9930 > 0.8), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4058 global=1.9909\n",
            "  step 10: local=0.4037 global=2.0976\n",
            "  step 20: local=0.3980 global=1.8860\n",
            "  step 30: local=0.4272 global=1.9852\n",
            "  step 40: local=0.4004 global=2.0255\n",
            "  step 50: local=0.3937 global=2.1056\n",
            "  step 60: local=0.4073 global=2.0483\n",
            "  step 70: local=0.3894 global=1.9185\n",
            "  step 80: local=0.4124 global=1.8802\n",
            "  step 90: local=0.3841 global=2.0158\n",
            "  Layer 26 not converged (global=1.9730 > 0.8), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3996 global=1.9334\n",
            "  step 10: local=0.3898 global=2.0269\n",
            "  step 20: local=0.3914 global=1.9737\n",
            "  step 30: local=0.4042 global=2.1090\n",
            "  step 40: local=0.3884 global=1.9914\n",
            "  step 50: local=0.3802 global=2.0416\n",
            "  step 60: local=0.3825 global=2.0420\n",
            "  step 70: local=0.3868 global=2.0366\n",
            "  step 80: local=0.3827 global=1.9065\n",
            "  step 90: local=0.3833 global=2.0715\n",
            "  Layer 26 not converged (global=2.0614 > 0.8), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3994 global=2.0787\n",
            "  step 10: local=0.3746 global=1.8614\n",
            "  step 20: local=0.3751 global=1.9477\n",
            "  step 30: local=0.3724 global=2.0472\n",
            "  step 40: local=0.3745 global=1.8917\n",
            "  step 50: local=0.3841 global=2.0007\n",
            "  step 60: local=0.3821 global=2.1026\n",
            "  step 70: local=0.3874 global=1.9796\n",
            "  step 80: local=0.3684 global=1.9715\n",
            "  step 90: local=0.3767 global=2.0017\n",
            "  Layer 26 not converged (global=2.0873 > 0.8), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3858 global=1.9901\n",
            "  step 10: local=0.3689 global=1.9812\n",
            "  step 20: local=0.3715 global=2.1902\n",
            "  step 30: local=0.3786 global=2.0453\n",
            "  step 40: local=0.3754 global=1.9568\n",
            "  step 50: local=0.3726 global=2.1438\n",
            "  step 60: local=0.3703 global=1.9264\n",
            "  step 70: local=0.3794 global=1.9830\n",
            "  step 80: local=0.3720 global=2.1492\n",
            "  step 90: local=0.3743 global=2.1131\n",
            "  Layer 26 not converged (global=2.0184 > 0.8), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3789 global=2.0456\n",
            "  step 10: local=0.3676 global=1.9194\n",
            "  step 20: local=0.3677 global=2.0590\n",
            "  step 30: local=0.3637 global=2.0105\n",
            "  step 40: local=0.3627 global=1.9753\n",
            "  step 50: local=0.3638 global=2.0205\n",
            "  step 60: local=0.3541 global=1.9144\n",
            "  step 70: local=0.3750 global=2.1449\n",
            "  step 80: local=0.3692 global=2.0474\n",
            "  step 90: local=0.3716 global=1.9960\n",
            "  Layer 26 not converged (global=2.0185 > 0.8), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3669 global=2.0098\n",
            "  step 10: local=0.3734 global=2.0991\n",
            "  step 20: local=0.3672 global=2.1091\n",
            "  step 30: local=0.3596 global=2.0507\n",
            "  step 40: local=0.3518 global=2.0531\n",
            "  step 50: local=0.3595 global=1.9500\n",
            "  step 60: local=0.3709 global=1.9709\n",
            "  step 70: local=0.3552 global=1.9075\n",
            "  step 80: local=0.3549 global=2.1461\n",
            "  step 90: local=0.3609 global=2.1663\n",
            "  Layer 26 not converged (global=2.0072 > 0.8), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3654 global=1.9392\n",
            "  step 10: local=0.3654 global=1.9186\n",
            "  step 20: local=0.3738 global=1.9047\n",
            "  step 30: local=0.3604 global=1.9319\n",
            "  step 40: local=0.3571 global=2.1062\n",
            "  step 50: local=0.3640 global=1.9722\n",
            "  step 60: local=0.3620 global=2.0470\n",
            "  step 70: local=0.3848 global=2.1253\n",
            "  step 80: local=0.3559 global=1.9597\n",
            "  step 90: local=0.3512 global=1.9408\n",
            "  Layer 26 not converged (global=1.9488 > 0.8), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3532 global=1.9993\n",
            "  step 10: local=0.3580 global=1.9385\n",
            "  step 20: local=0.3781 global=1.9843\n",
            "  step 30: local=0.3552 global=2.1109\n",
            "  step 40: local=0.3814 global=1.9632\n",
            "  step 50: local=0.3535 global=2.0169\n",
            "  step 60: local=0.3578 global=2.1100\n",
            "  step 70: local=0.3730 global=2.0990\n",
            "  step 80: local=0.3501 global=1.9537\n",
            "  step 90: local=0.3513 global=1.8955\n",
            "  Layer 26 not converged (global=2.1123 > 0.8), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3587 global=2.0385\n",
            "  step 10: local=0.3727 global=1.9854\n",
            "  step 20: local=0.3494 global=1.9340\n",
            "  step 30: local=0.3513 global=1.9103\n",
            "  step 40: local=0.3770 global=2.0047\n",
            "  step 50: local=0.3665 global=2.1116\n",
            "  step 60: local=0.3705 global=2.0207\n",
            "  step 70: local=0.3532 global=1.9602\n",
            "  step 80: local=0.3447 global=1.9695\n",
            "  step 90: local=0.3641 global=2.0758\n",
            "  Layer 26 not converged (global=1.9578 > 0.8), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3610 global=2.1505\n",
            "  step 10: local=0.3650 global=2.0743\n",
            "  step 20: local=0.3551 global=1.8994\n",
            "  step 30: local=0.3571 global=1.9372\n",
            "  step 40: local=0.3642 global=2.0195\n",
            "  step 50: local=0.3668 global=2.0316\n",
            "  step 60: local=0.3403 global=1.9872\n",
            "  step 70: local=0.3594 global=2.0140\n",
            "  step 80: local=0.3671 global=2.0583\n",
            "  step 90: local=0.3455 global=1.9376\n",
            "  Layer 26 not converged (global=1.9547 > 0.8), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3486 global=1.9870\n",
            "  step 10: local=0.3477 global=2.0812\n",
            "  step 20: local=0.3477 global=2.0259\n",
            "  step 30: local=0.3453 global=2.0737\n",
            "  step 40: local=0.3646 global=1.9658\n",
            "  step 50: local=0.3620 global=2.0738\n",
            "  step 60: local=0.3445 global=1.8634\n",
            "  step 70: local=0.3678 global=1.9625\n",
            "  step 80: local=0.3414 global=2.0054\n",
            "  step 90: local=0.3640 global=2.0847\n",
            "  Layer 26 not converged (global=2.0552 > 0.8), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3655 global=2.0275\n",
            "  step 10: local=0.3634 global=1.9019\n",
            "  step 20: local=0.3370 global=1.8640\n",
            "  step 30: local=0.3541 global=1.9960\n",
            "  step 40: local=0.3526 global=1.9182\n",
            "  step 50: local=0.3551 global=2.0104\n",
            "  step 60: local=0.3639 global=1.9568\n",
            "  step 70: local=0.3555 global=2.0932\n",
            "  step 80: local=0.3635 global=1.9747\n",
            "  step 90: local=0.3530 global=2.0277\n",
            "  Layer 26 not converged (global=1.9914 > 0.8), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3466 global=2.0256\n",
            "  step 10: local=0.3600 global=2.0224\n",
            "  step 20: local=0.3469 global=1.8944\n",
            "  step 30: local=0.3612 global=2.0577\n",
            "  step 40: local=0.3602 global=2.0641\n",
            "  step 50: local=0.3364 global=1.8493\n",
            "  step 60: local=0.3499 global=1.9341\n",
            "  step 70: local=0.3615 global=2.0331\n",
            "  step 80: local=0.3573 global=1.8812\n",
            "  step 90: local=0.3571 global=1.9886\n",
            "  Layer 26 not converged (global=2.0629 > 0.8), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3594 global=2.0907\n",
            "  step 10: local=0.3400 global=1.9685\n",
            "  step 20: local=0.3466 global=1.9595\n",
            "  step 30: local=0.3474 global=1.9915\n",
            "  step 40: local=0.3428 global=1.9804\n",
            "  step 50: local=0.3640 global=1.9702\n",
            "  step 60: local=0.3523 global=2.1783\n",
            "  step 70: local=0.3546 global=2.0361\n",
            "  step 80: local=0.3419 global=1.9476\n",
            "  step 90: local=0.3639 global=2.1347\n",
            "  Layer 26 not converged (global=1.9683 > 0.8), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3538 global=1.9158\n",
            "  step 10: local=0.3704 global=1.9745\n",
            "  step 20: local=0.3562 global=2.1403\n",
            "  step 30: local=0.3555 global=2.1041\n",
            "  step 40: local=0.3444 global=2.0362\n",
            "  step 50: local=0.3438 global=1.9096\n",
            "  step 60: local=0.3515 global=2.0501\n",
            "  step 70: local=0.3493 global=2.0040\n",
            "  step 80: local=0.3375 global=1.9664\n",
            "  step 90: local=0.3665 global=2.0122\n",
            "  Layer 26 not converged (global=1.9293 > 0.8), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3435 global=1.9055\n",
            "  step 10: local=0.3562 global=2.1364\n",
            "  step 20: local=0.3474 global=2.0388\n",
            "  step 30: local=0.3490 global=1.9887\n",
            "  step 40: local=0.3461 global=2.0020\n",
            "  step 50: local=0.3478 global=2.0913\n",
            "  step 60: local=0.3595 global=2.1021\n",
            "  step 70: local=0.3458 global=2.0434\n",
            "  step 80: local=0.3419 global=2.0462\n",
            "  step 90: local=0.3443 global=1.9421\n",
            "  Layer 26 not converged (global=2.0788 > 0.8), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3558 global=1.9640\n",
            "  step 10: local=0.3586 global=1.9031\n",
            "  step 20: local=0.3522 global=2.1387\n",
            "  step 30: local=0.3594 global=2.1586\n",
            "  step 40: local=0.3356 global=1.9315\n",
            "  step 50: local=0.3554 global=1.9122\n",
            "  step 60: local=0.3410 global=1.8985\n",
            "  step 70: local=0.3552 global=1.9261\n",
            "  step 80: local=0.3380 global=2.1001\n",
            "  step 90: local=0.3527 global=1.9661\n",
            "  [WARN] Layer 26 did not converge after 20 repeats (global=1.9882)\n",
            "  [TIME] Layer 26 took 428.9s | Total: 261.2min | ETA: 8.2min (1 layers left)\n",
            "\n",
            "--- Layer 27/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5342 global=2.8379\n",
            "  step 10: local=0.5976 global=2.8405\n",
            "  step 20: local=0.5950 global=2.6527\n",
            "  step 30: local=0.4870 global=2.5952\n",
            "  step 40: local=0.5783 global=2.5826\n",
            "  step 50: local=0.5914 global=2.5121\n",
            "  step 60: local=0.6083 global=2.5073\n",
            "  step 70: local=0.5840 global=2.6375\n",
            "  step 80: local=0.5649 global=2.4237\n",
            "  step 90: local=0.5468 global=2.4971\n",
            "  Layer 27 not converged (global=2.5725 > 0.8), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5498 global=2.5594\n",
            "  step 10: local=0.5881 global=2.5267\n",
            "  step 20: local=0.4447 global=2.3878\n",
            "  step 30: local=0.5488 global=2.2841\n",
            "  step 40: local=0.4405 global=2.4168\n",
            "  step 50: local=0.4382 global=2.3521\n",
            "  step 60: local=0.4237 global=2.2863\n",
            "  step 70: local=0.5177 global=2.2230\n",
            "  step 80: local=0.5314 global=2.3063\n",
            "  step 90: local=0.5114 global=2.4231\n",
            "  Layer 27 not converged (global=2.3441 > 0.8), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4252 global=2.3126\n",
            "  step 10: local=0.4091 global=2.2403\n",
            "  step 20: local=0.4284 global=2.2487\n",
            "  step 30: local=0.4055 global=2.3457\n",
            "  step 40: local=0.4230 global=2.4345\n",
            "  step 50: local=0.4008 global=2.3367\n",
            "  step 60: local=0.5048 global=2.1440\n",
            "  step 70: local=0.4947 global=2.1756\n",
            "  step 80: local=0.3944 global=2.2415\n",
            "  step 90: local=0.4829 global=2.2656\n",
            "  Layer 27 not converged (global=2.1985 > 0.8), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4082 global=2.2004\n",
            "  step 10: local=0.4871 global=2.2263\n",
            "  step 20: local=0.4872 global=2.2776\n",
            "  step 30: local=0.4880 global=2.1516\n",
            "  step 40: local=0.4863 global=2.1833\n",
            "  step 50: local=0.3834 global=2.2731\n",
            "  step 60: local=0.4504 global=2.2130\n",
            "  step 70: local=0.3403 global=2.2752\n",
            "  step 80: local=0.3604 global=2.1545\n",
            "  step 90: local=0.4515 global=2.2494\n",
            "  Layer 27 not converged (global=2.1766 > 0.8), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3500 global=2.0361\n",
            "  step 10: local=0.4375 global=2.1268\n",
            "  step 20: local=0.4351 global=2.1693\n",
            "  step 30: local=0.4273 global=2.2632\n",
            "  step 40: local=0.3236 global=2.1861\n",
            "  step 50: local=0.4140 global=2.0652\n",
            "  step 60: local=0.4395 global=2.0247\n",
            "  step 70: local=0.3348 global=2.1360\n",
            "  step 80: local=0.3498 global=2.0670\n",
            "  step 90: local=0.3378 global=2.1565\n",
            "  Layer 27 not converged (global=2.2097 > 0.8), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4526 global=2.1163\n",
            "  step 10: local=0.4256 global=2.2640\n",
            "  step 20: local=0.3397 global=2.1416\n",
            "  step 30: local=0.4116 global=2.1764\n",
            "  step 40: local=0.4444 global=2.1686\n",
            "  step 50: local=0.4446 global=2.1615\n",
            "  step 60: local=0.4184 global=2.0422\n",
            "  step 70: local=0.3342 global=2.1970\n",
            "  step 80: local=0.3141 global=2.2243\n",
            "  step 90: local=0.4354 global=1.9863\n",
            "  Layer 27 not converged (global=2.2033 > 0.8), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4194 global=2.0621\n",
            "  step 10: local=0.4133 global=2.1706\n",
            "  step 20: local=0.3219 global=2.0097\n",
            "  step 30: local=0.3310 global=2.1341\n",
            "  step 40: local=0.3257 global=2.2371\n",
            "  step 50: local=0.3265 global=2.0910\n",
            "  step 60: local=0.4262 global=2.1092\n",
            "  step 70: local=0.3961 global=2.1251\n",
            "  step 80: local=0.3951 global=2.1102\n",
            "  step 90: local=0.3021 global=2.1057\n",
            "  Layer 27 not converged (global=2.0829 > 0.8), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.2972 global=2.3213\n",
            "  step 10: local=0.4244 global=2.1899\n",
            "  step 20: local=0.3024 global=2.0879\n",
            "  step 30: local=0.3277 global=2.2775\n",
            "  step 40: local=0.4251 global=2.0505\n",
            "  step 50: local=0.3115 global=2.1059\n",
            "  step 60: local=0.3136 global=2.2867\n",
            "  step 70: local=0.3041 global=2.2460\n",
            "  step 80: local=0.3214 global=2.1686\n",
            "  step 90: local=0.3214 global=2.0481\n",
            "  Layer 27 not converged (global=2.2144 > 0.8), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3237 global=2.1884\n",
            "  step 10: local=0.3179 global=2.1353\n",
            "  step 20: local=0.3390 global=2.1019\n",
            "  step 30: local=0.2930 global=2.1446\n",
            "  step 40: local=0.3312 global=2.0382\n",
            "  step 50: local=0.4496 global=2.2909\n",
            "  step 60: local=0.3812 global=2.1716\n",
            "  step 70: local=0.4023 global=2.1205\n",
            "  step 80: local=0.3862 global=2.1336\n",
            "  step 90: local=0.4284 global=2.2218\n",
            "  Layer 27 not converged (global=2.0598 > 0.8), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4108 global=2.2284\n",
            "  step 10: local=0.3963 global=2.1770\n",
            "  step 20: local=0.3003 global=2.1784\n",
            "  step 30: local=0.3886 global=2.0695\n",
            "  step 40: local=0.2873 global=2.0931\n",
            "  step 50: local=0.4131 global=2.0173\n",
            "  step 60: local=0.3764 global=2.2772\n",
            "  step 70: local=0.4127 global=2.3075\n",
            "  step 80: local=0.3190 global=2.0615\n",
            "  step 90: local=0.3919 global=2.0498\n",
            "  Layer 27 not converged (global=2.1321 > 0.8), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3942 global=2.0107\n",
            "  step 10: local=0.4202 global=2.0450\n",
            "  step 20: local=0.4095 global=2.2394\n",
            "  step 30: local=0.4068 global=2.1061\n",
            "  step 40: local=0.3937 global=2.1757\n",
            "  step 50: local=0.4296 global=2.2488\n",
            "  step 60: local=0.3242 global=2.0799\n",
            "  step 70: local=0.2833 global=2.0534\n",
            "  step 80: local=0.3237 global=2.1246\n",
            "  step 90: local=0.3270 global=2.0497\n",
            "  Layer 27 not converged (global=2.1048 > 0.8), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4040 global=2.0994\n",
            "  step 10: local=0.3873 global=2.2373\n",
            "  step 20: local=0.3170 global=2.0674\n",
            "  step 30: local=0.4059 global=2.1549\n",
            "  step 40: local=0.3987 global=2.2364\n",
            "  step 50: local=0.4222 global=2.2237\n",
            "  step 60: local=0.4230 global=2.0855\n",
            "  step 70: local=0.4234 global=2.0177\n",
            "  step 80: local=0.3378 global=2.1650\n",
            "  step 90: local=0.4069 global=2.1189\n",
            "  Layer 27 not converged (global=2.1707 > 0.8), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3024 global=2.0602\n",
            "  step 10: local=0.4209 global=2.0157\n",
            "  step 20: local=0.2915 global=2.1174\n",
            "  step 30: local=0.4081 global=2.2406\n",
            "  step 40: local=0.4150 global=2.1325\n",
            "  step 50: local=0.3145 global=2.0762\n",
            "  step 60: local=0.4359 global=2.0919\n",
            "  step 70: local=0.4026 global=2.1897\n",
            "  step 80: local=0.4153 global=2.2938\n",
            "  step 90: local=0.4142 global=2.1972\n",
            "  Layer 27 not converged (global=2.1615 > 0.8), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4142 global=2.0221\n",
            "  step 10: local=0.4097 global=2.0618\n",
            "  step 20: local=0.2996 global=2.1280\n",
            "  step 30: local=0.4126 global=2.1484\n",
            "  step 40: local=0.3197 global=2.0999\n",
            "  step 50: local=0.3180 global=2.1405\n",
            "  step 60: local=0.3945 global=2.1884\n",
            "  step 70: local=0.3238 global=2.0600\n",
            "  step 80: local=0.4142 global=2.1068\n",
            "  step 90: local=0.3957 global=2.2029\n",
            "  Layer 27 not converged (global=2.1075 > 0.8), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3991 global=2.1427\n",
            "  step 10: local=0.3116 global=2.1955\n",
            "  step 20: local=0.4210 global=2.0901\n",
            "  step 30: local=0.4109 global=2.1797\n",
            "  step 40: local=0.4316 global=1.9803\n",
            "  step 50: local=0.2951 global=2.0694\n",
            "  step 60: local=0.2996 global=2.1192\n",
            "  step 70: local=0.4022 global=2.2063\n",
            "  step 80: local=0.3839 global=2.1453\n",
            "  step 90: local=0.3853 global=2.0241\n",
            "  Layer 27 not converged (global=2.1640 > 0.8), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3060 global=1.9825\n",
            "  step 10: local=0.3110 global=2.0962\n",
            "  step 20: local=0.3854 global=2.0332\n",
            "  step 30: local=0.3941 global=2.1228\n",
            "  step 40: local=0.4038 global=2.0743\n",
            "  step 50: local=0.3990 global=2.2259\n",
            "  step 60: local=0.4013 global=2.1069\n",
            "  step 70: local=0.2913 global=2.1496\n",
            "  step 80: local=0.4009 global=2.1380\n",
            "  step 90: local=0.3027 global=2.1377\n",
            "  Layer 27 not converged (global=2.3043 > 0.8), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3969 global=2.0131\n",
            "  step 10: local=0.3933 global=2.1735\n",
            "  step 20: local=0.2844 global=2.1969\n",
            "  step 30: local=0.3023 global=1.9633\n",
            "  step 40: local=0.4078 global=2.0411\n",
            "  step 50: local=0.3151 global=2.1498\n",
            "  step 60: local=0.2912 global=1.9877\n",
            "  step 70: local=0.3893 global=2.1189\n",
            "  step 80: local=0.4022 global=2.2182\n",
            "  step 90: local=0.4113 global=2.0743\n",
            "  Layer 27 not converged (global=2.2569 > 0.8), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3078 global=2.0920\n",
            "  step 10: local=0.2854 global=2.1023\n",
            "  step 20: local=0.4022 global=2.0956\n",
            "  step 30: local=0.4114 global=2.0854\n",
            "  step 40: local=0.4075 global=2.3044\n",
            "  step 50: local=0.3911 global=2.1702\n",
            "  step 60: local=0.4001 global=2.0725\n",
            "  step 70: local=0.3918 global=2.2582\n",
            "  step 80: local=0.3864 global=2.0328\n",
            "  step 90: local=0.3007 global=2.0911\n",
            "  Layer 27 not converged (global=2.1656 > 0.8), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.2997 global=2.2703\n",
            "  step 10: local=0.4055 global=2.2310\n",
            "  step 20: local=0.3833 global=2.1491\n",
            "  step 30: local=0.3977 global=2.0370\n",
            "  step 40: local=0.3949 global=2.1749\n",
            "  step 50: local=0.3071 global=2.1234\n",
            "  step 60: local=0.3885 global=2.0889\n",
            "  step 70: local=0.3825 global=2.1281\n",
            "  step 80: local=0.2858 global=2.0253\n",
            "  step 90: local=0.4146 global=2.2745\n",
            "  Layer 27 not converged (global=2.1311 > 0.8), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3040 global=2.1599\n",
            "  step 10: local=0.3172 global=2.1099\n",
            "  step 20: local=0.3977 global=2.1211\n",
            "  step 30: local=0.2950 global=2.2142\n",
            "  step 40: local=0.3020 global=2.2199\n",
            "  step 50: local=0.3082 global=2.1644\n",
            "  step 60: local=0.3096 global=2.1687\n",
            "  step 70: local=0.3077 global=2.0582\n",
            "  step 80: local=0.3828 global=2.0786\n",
            "  step 90: local=0.3753 global=2.0024\n",
            "  [WARN] Layer 27 did not converge after 20 repeats (global=2.0538)\n",
            "  [TIME] Layer 27 took 415.2s | Total: 268.1min | ETA: 0.0min (0 layers left)\n",
            "\n",
            "============================================================\n",
            "PASS 4: E2E Quantizer Tuning (f-only)\n",
            "============================================================\n",
            "  Trainable f parameters: 196\n",
            "  step 0: loss=13.8585\n",
            "  step 10: loss=14.1392\n",
            "  step 20: loss=13.7723\n",
            "  step 30: loss=13.5313\n",
            "  step 40: loss=13.2892\n",
            "  step 50: loss=13.8754\n",
            "  step 60: loss=14.0866\n",
            "  step 70: loss=13.4581\n",
            "  step 80: loss=13.9901\n",
            "  step 90: loss=14.3224\n",
            "  step 100: loss=13.7996\n",
            "  step 110: loss=14.1575\n",
            "  step 120: loss=13.5934\n",
            "  step 130: loss=13.9860\n",
            "  step 140: loss=13.8243\n",
            "  step 150: loss=14.1434\n",
            "  step 160: loss=14.0033\n",
            "  step 170: loss=13.9047\n",
            "  step 180: loss=13.7617\n",
            "  step 190: loss=14.0978\n",
            "  step 200: loss=13.6756\n",
            "  step 210: loss=14.0106\n",
            "  step 220: loss=14.0806\n",
            "  step 230: loss=13.8628\n",
            "  step 240: loss=13.9593\n",
            "  step 250: loss=13.9317\n",
            "  step 260: loss=13.9365\n",
            "  step 270: loss=14.2030\n",
            "  step 280: loss=14.3387\n",
            "  step 290: loss=13.8889\n",
            "  step 300: loss=14.1323\n",
            "  step 310: loss=13.9441\n",
            "  step 320: loss=14.2947\n",
            "  step 330: loss=13.5379\n",
            "  step 340: loss=14.0777\n",
            "  step 350: loss=13.9251\n",
            "  step 360: loss=13.6097\n",
            "  step 370: loss=13.7077\n",
            "  step 380: loss=14.0281\n",
            "  step 390: loss=13.6584\n",
            "  step 400: loss=13.9530\n",
            "  step 410: loss=13.8431\n",
            "  step 420: loss=13.7604\n",
            "  step 430: loss=14.0212\n",
            "  step 440: loss=14.1611\n",
            "  step 450: loss=13.6913\n",
            "  step 460: loss=13.7694\n",
            "  step 470: loss=13.7918\n",
            "  step 480: loss=13.5926\n",
            "  step 490: loss=13.3990\n",
            "  [TIME] E2E pass took 4.8min\n",
            "\n",
            "============================================================\n",
            "Saving outputs\n",
            "============================================================\n",
            "  Model saved to: runs/progressive_qat_q2_v1_fresh/qat_state_dict.pt\n",
            "  Loss log saved to: runs/progressive_qat_q2_v1_fresh/loss_per_layer.csv\n",
            "\n",
            "Done! Total training time: 273.1min (4.55h)\n"
          ]
        }
      ],
      "source": [
        "# Full Progressive: MLP pass + E2E f-only (skip attention/refinement for v1)\n",
        "# For full 3-pass training, remove --skip_attention_pass and --skip_mlp_refinement\n",
        "# Version of Not Starting from 4-bit checkpoint for 2-bit training\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/train_qat_progressive.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --output_dir {RUN_DIR_PROGRESSIVE_FRESH} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --steps_per_layer_mlp {STEPS_PER_LAYER_MLP} \\\n",
        "  --e2e_steps {E2E_STEPS} \\\n",
        "  --local_weight {LOCAL_WEIGHT} \\\n",
        "  --global_weight {GLOBAL_WEIGHT} \\\n",
        "  --local_token_samples {LOCAL_TOKEN_SAMPLES} \\\n",
        "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
        "  --learning_rate {LR_PROGRESSIVE} \\\n",
        "  --e2e_learning_rate {LR_E2E} \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --skip_attention_pass \\\n",
        "  --max_layer_repeats 20 \\\n",
        "  --max_backtrack 5 \\\n",
        "  --layer_converge_threshold 0.8 \\\n",
        "  --skip_mlp_refinement"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SAVE RUN"
      ],
      "metadata": {
        "id": "A9MSJDvwhdnV"
      },
      "id": "A9MSJDvwhdnV"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# SAVE PROGRESSIVE QAT CHECKPOINT TO GOOGLE DRIVE\n",
        "# ============================================================\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive if not already mounted\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Source directory (matches RUN_DIR_PROGRESSIVE from config)\n",
        "RUN_NAME = \"progressive_qat_q2_v1_fresh\"\n",
        "RUN_DIR = f\"runs/{RUN_NAME}\"\n",
        "\n",
        "# Destination on Google Drive\n",
        "DEST_DIR_GD = \"/content/drive/MyDrive/qwen3_runs/\"\n",
        "!mkdir -p {DEST_DIR_GD}\n",
        "\n",
        "# Check if run directory exists and has content\n",
        "if os.path.isdir(RUN_DIR) and os.listdir(RUN_DIR):\n",
        "    # Compress the run directory\n",
        "    print(f\"[archive] Compressing {RUN_DIR}...\")\n",
        "    !tar -zcvf {RUN_NAME}.tgz -C runs {RUN_NAME}\n",
        "\n",
        "    # Copy to Google Drive\n",
        "    print(f\"[save] Copying {RUN_NAME}.tgz to Google Drive...\")\n",
        "    !rsync -ah --info=progress2 {RUN_NAME}.tgz {DEST_DIR_GD}\n",
        "\n",
        "    # Verify\n",
        "    gd_size = os.path.getsize(f\"{DEST_DIR_GD}/{RUN_NAME}.tgz\")\n",
        "    print(f\"[save] Saved to Google Drive: {gd_size / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(f\"[save] ERROR: {RUN_DIR} is empty or doesn't exist\")\n",
        "\n",
        "# got to \"Stage 3 resume\" to continue distill treaining\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVSFT6W9cffG",
        "outputId": "f726bfa1-55d6-49ab-89a2-14f9f8e8186b"
      },
      "id": "DVSFT6W9cffG",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[archive] Compressing runs/progressive_qat_q2_v1_fresh...\n",
            "progressive_qat_q2_v1_fresh/\n",
            "progressive_qat_q2_v1_fresh/loss_per_layer.csv\n",
            "progressive_qat_q2_v1_fresh/training_args.json\n",
            "progressive_qat_q2_v1_fresh/qat_state_dict.pt\n",
            "[save] Copying progressive_qat_q2_v1_fresh.tgz to Google Drive...\n",
            "        946.26M 100%  428.69MB/s    0:00:02 (xfr#1, to-chk=0/1)\n",
            "[save] Saved to Google Drive: 0.88 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wsla5Ua1SJrH"
      },
      "id": "wsla5Ua1SJrH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (!!!!)Stage 3 resume KD-QAT with\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "No2hsdevSKIB"
      },
      "id": "No2hsdevSKIB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afa02860-d593-411a-ce05-c6c2eb2ecd57",
        "id": "5VA-L82bSKIB"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-24 23:29:24.155008: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-24 23:29:24.171198: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766618964.192763   82536 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766618964.199201   82536 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766618964.215547   82536 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766618964.215573   82536 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766618964.215576   82536 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766618964.215579   82536 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 23:29:24.220382: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[device] cuda | amp_dtype=torch.bfloat16 | param_dtype=torch.bfloat16\n",
            "[qat] weight_bits=2\n",
            "[init] loading model state from runs/progressive_qat_q2_v1/qat_state_dict.pt\n",
            "[init] model weights loaded.\n",
            "[kd-cache] cache topk=32\n",
            "[kd-cache] dir=caches/alpaca_chat_think_both_L128_K32_R256 | weight=1.0 | T=2.0 | hard_top1=0.0 | hard_full_top1=0.0005\n",
            "opt_step: 100% 3000/3000 [53:54<00:00,  1.08s/step, loss=1.1946, lr=0.00e+00]\n",
            "Done. QAT checkpoint saved to: runs/qwen3_kdqat_cache_q2_3/qat_state_dict.pt\n"
          ]
        }
      ],
      "source": [
        "# Stage 3 resume KD-QAT with unfrozen attention and relaxed hard-top/full!\n",
        "\n",
        "CACHE_DIR_CHAT = 'caches/alpaca_chat_think_both_L128_K32_R256'\n",
        "CACHE_DIR = CACHE_DIR_CHAT  # Use config variable\n",
        "\n",
        "INIT_DIR_CACHE =  \"runs/progressive_qat_q2_v1_fresh\"\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_3_fresh\"\n",
        "\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_DIR_CACHE}/qat_state_dict.pt \\\n",
        "  --output_dir {RUN_DIR_CACHE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 128 \\\n",
        "  --gradient_accumulation_steps 1 \\\n",
        "  --learning_rate 5e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 3000 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.00 \\\n",
        "  --hard-full-top1-weight 0.0005\n"
      ],
      "id": "5VA-L82bSKIB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#save run\n"
      ],
      "metadata": {
        "id": "GLzqSxfRSKIB"
      },
      "id": "GLzqSxfRSKIB"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# SAVE PROGRESSIVE QAT CHECKPOINT TO GOOGLE DRIVE\n",
        "# ============================================================\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive if not already mounted\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Source directory (matches RUN_DIR_PROGRESSIVE from config)\n",
        "RUN_NAME = \"qwen3_kdqat_cache_q2_3\"\n",
        "RUN_DIR = f\"runs/{RUN_NAME}\"\n",
        "\n",
        "# Destination on Google Drive\n",
        "DEST_DIR_GD = \"/content/drive/MyDrive/qwen3_runs/\"\n",
        "!mkdir -p {DEST_DIR_GD}\n",
        "\n",
        "# Check if run directory exists and has content\n",
        "if os.path.isdir(RUN_DIR) and os.listdir(RUN_DIR):\n",
        "    # Compress the run directory\n",
        "    print(f\"[archive] Compressing {RUN_DIR}...\")\n",
        "    !tar -zcvf {RUN_NAME}.tgz -C runs {RUN_NAME}\n",
        "\n",
        "    # Copy to Google Drive\n",
        "    print(f\"[save] Copying {RUN_NAME}.tgz to Google Drive...\")\n",
        "    !rsync -ah --info=progress2 {RUN_NAME}.tgz {DEST_DIR_GD}\n",
        "\n",
        "    # Verify\n",
        "    gd_size = os.path.getsize(f\"{DEST_DIR_GD}/{RUN_NAME}.tgz\")\n",
        "    print(f\"[save] Saved to Google Drive: {gd_size / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(f\"[save] ERROR: {RUN_DIR} is empty or doesn't exist\")\n",
        "\n",
        "# got to \"Stage 3 resume\" to continue distill treaining"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e0433c4-8709-4eda-8aec-d27443c92fc6",
        "id": "BgPHyPvSSKIC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[archive] Compressing runs/qwen3_kdqat_cache_q2_3...\n",
            "qwen3_kdqat_cache_q2_3/\n",
            "qwen3_kdqat_cache_q2_3/special_tokens_map.json\n",
            "qwen3_kdqat_cache_q2_3/loss.csv\n",
            "qwen3_kdqat_cache_q2_3/added_tokens.json\n",
            "qwen3_kdqat_cache_q2_3/tokenizer_config.json\n",
            "qwen3_kdqat_cache_q2_3/run_state.json\n",
            "qwen3_kdqat_cache_q2_3/merges.txt\n",
            "qwen3_kdqat_cache_q2_3/training_args.json\n",
            "qwen3_kdqat_cache_q2_3/chat_template.jinja\n",
            "qwen3_kdqat_cache_q2_3/vocab.json\n",
            "qwen3_kdqat_cache_q2_3/final_state_dict.pt\n",
            "qwen3_kdqat_cache_q2_3/qat_state_dict.pt\n",
            "qwen3_kdqat_cache_q2_3/checkpoint_step3000.pt\n",
            "qwen3_kdqat_cache_q2_3/checkpoint_last.pt\n",
            "qwen3_kdqat_cache_q2_3/tokenizer.json\n",
            "[save] Copying qwen3_kdqat_cache_q2_3.tgz to Google Drive...\n",
            "          7.46G 100%  332.14MB/s    0:00:21 (xfr#1, to-chk=0/1)\n",
            "[save] Saved to Google Drive: 6.95 GB\n"
          ]
        }
      ],
      "id": "BgPHyPvSSKIC"
    },
    {
      "cell_type": "code",
      "source": [
        "# v3: Full 3-pass progressive training\n",
        "# MLP -> Attention -> MLP refinement -> E2E f-only\n",
        "# WARNING: May show instability at later layers for 2-bit\n",
        "# Starting from 4-bit checkpoint for 2-bit training\n",
        "\n",
        "RUN_NAME = \"progressive_qat_q2_v1_fresh\"\n",
        "\n",
        "RUN_DIR_PROGRESSIVE_V3 = \"runs/progressive_qat_q2_v3\"\n",
        "\n",
        "!python scripts/train_qat_progressive.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_CHECKPOINT} \\\n",
        "  --output_dir {RUN_DIR_PROGRESSIVE_V3} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --steps_per_layer_mlp {STEPS_PER_LAYER_MLP} \\\n",
        "  --steps_per_layer_attn {STEPS_PER_LAYER_ATTN} \\\n",
        "  --e2e_steps {E2E_STEPS} \\\n",
        "  --local_weight {LOCAL_WEIGHT} \\\n",
        "  --global_weight {GLOBAL_WEIGHT} \\\n",
        "  --local_token_samples {LOCAL_TOKEN_SAMPLES} \\\n",
        "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
        "  --learning_rate {LR_PROGRESSIVE} \\\n",
        "  --e2e_learning_rate {LR_E2E} \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head"
      ],
      "metadata": {
        "id": "_qK8LP8WrbSB"
      },
      "id": "_qK8LP8WrbSB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### SAVE PROGRESSIVE QAT CHECKPOINT TO GOOGLE DRIVE\n",
        "\n"
      ],
      "metadata": {
        "id": "tWaNjPdqRy0D"
      },
      "id": "tWaNjPdqRy0D"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dBFCVCZjwBo_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBFCVCZjwBo_",
        "outputId": "8eaba02b-c9ea-4930-df9b-69a5f75e180d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[archive] Compressing runs/qwen3_kdqat_cache_q2_3...\n",
            "qwen3_kdqat_cache_q2_3/\n",
            "qwen3_kdqat_cache_q2_3/special_tokens_map.json\n",
            "qwen3_kdqat_cache_q2_3/loss.csv\n",
            "qwen3_kdqat_cache_q2_3/added_tokens.json\n",
            "qwen3_kdqat_cache_q2_3/tokenizer_config.json\n",
            "qwen3_kdqat_cache_q2_3/run_state.json\n",
            "qwen3_kdqat_cache_q2_3/merges.txt\n",
            "qwen3_kdqat_cache_q2_3/training_args.json\n",
            "qwen3_kdqat_cache_q2_3/chat_template.jinja\n",
            "qwen3_kdqat_cache_q2_3/vocab.json\n",
            "qwen3_kdqat_cache_q2_3/final_state_dict.pt\n",
            "qwen3_kdqat_cache_q2_3/qat_state_dict.pt\n",
            "qwen3_kdqat_cache_q2_3/tokenizer.json\n",
            "[save] Copying qwen3_kdqat_cache_q2_3.tgz to Google Drive...\n",
            "          1.90G 100%  430.50MB/s    0:00:04 (xfr#1, to-chk=0/1)\n",
            "[save] Saved to Google Drive: 1.77 GB\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# SAVE PROGRESSIVE QAT CHECKPOINT TO GOOGLE DRIVE\n",
        "# ============================================================\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "\n",
        "# Mount Google Drive if not already mounted\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Source directory\n",
        "#RUN_NAME = \"progressive_qat_v1\"\n",
        "RUN_NAME = \"progressive_qat_q2_v3\"\n",
        "\n",
        "RUN_DIR = f\"runs/{RUN_NAME}\"\n",
        "\n",
        "# Destination on Google Drive\n",
        "DEST_DIR_GD = \"/content/drive/MyDrive/qwen3_runs/\"\n",
        "!mkdir -p {DEST_DIR_GD}\n",
        "\n",
        "# Check if run directory exists and has content\n",
        "if os.path.isdir(RUN_DIR) and os.listdir(RUN_DIR):\n",
        "    # Compress the run directory\n",
        "    print(f\"[archive] Compressing {RUN_DIR}...\")\n",
        "    !tar -zcvf {RUN_NAME}.tgz -C runs {RUN_NAME}\n",
        "\n",
        "    # Copy to Google Drive\n",
        "    print(f\"[save] Copying {RUN_NAME}.tgz to Google Drive...\")\n",
        "    !rsync -ah --info=progress2 {RUN_NAME}.tgz {DEST_DIR_GD}\n",
        "\n",
        "    # Verify\n",
        "    gd_size = os.path.getsize(f\"{DEST_DIR_GD}/{RUN_NAME}.tgz\")\n",
        "    print(f\"[save] Saved to Google Drive: {gd_size / (1024**3):.2f} GB\")\n",
        "\n",
        "    # Cleanup local archive (optional)\n",
        "    # !rm {RUN_NAME}.tgz\n",
        "else:\n",
        "    print(f\"[save] ERROR: {RUN_DIR} is empty or doesn't exist\")\n",
        "    print(\"[save] Run progressive training first\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### v3: Full 3-pass progressive training"
      ],
      "metadata": {
        "id": "VR08ZbGER2c2"
      },
      "id": "VR08ZbGER2c2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qiaq60t466e",
      "metadata": {
        "id": "qiaq60t466e"
      },
      "outputs": [],
      "source": [
        "# v3: Full 3-pass progressive training\n",
        "# MLP -> Attention -> MLP refinement -> E2E f-only\n",
        "# WARNING: May show instability at later layers for 2-bit\n",
        "\n",
        "RUN_DIR_PROGRESSIVE_V3 = \"runs/progressive_qat_v3\"\n",
        "\n",
        "!python scripts/train_qat_progressive.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --output_dir {RUN_DIR_PROGRESSIVE_V3} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --steps_per_layer_mlp {STEPS_PER_LAYER_MLP} \\\n",
        "  --steps_per_layer_attn {STEPS_PER_LAYER_ATTN} \\\n",
        "  --e2e_steps {E2E_STEPS} \\\n",
        "  --local_weight {LOCAL_WEIGHT} \\\n",
        "  --global_weight {GLOBAL_WEIGHT} \\\n",
        "  --local_token_samples {LOCAL_TOKEN_SAMPLES} \\\n",
        "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
        "  --learning_rate {LR_PROGRESSIVE} \\\n",
        "  --e2e_learning_rate {LR_E2E} \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ix3xjm2h0xq",
      "metadata": {
        "id": "ix3xjm2h0xq"
      },
      "outputs": [],
      "source": [
        "# Plot per-layer training progress\n",
        "# Change PLOT_RUN to visualize different runs\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Choose which run to visualize\n",
        "PLOT_RUN = RUN_DIR_E2E_FONLY  # or RUN_DIR_PROGRESSIVE, RUN_DIR_PROGRESSIVE_V3\n",
        "\n",
        "csv_path = f\"{PLOT_RUN}/loss_per_layer.csv\"\n",
        "if not os.path.exists(csv_path):\n",
        "    print(f\"Loss CSV not found at {csv_path}\")\n",
        "    print(\"Run training first or check the path.\")\n",
        "else:\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "    # Pass 1: MLP training (local loss)\n",
        "    mlp_df = df[(df['pass'] == 1) & (df['component'] == 'mlp')]\n",
        "    if not mlp_df.empty and 'local' in mlp_df.columns:\n",
        "        for layer in mlp_df['layer'].unique():\n",
        "            layer_df = mlp_df[mlp_df['layer'] == layer]\n",
        "            axes[0, 0].plot(layer_df['step'], layer_df['local'], label=f'L{layer}', alpha=0.7)\n",
        "        axes[0, 0].set_title('Pass 1: MLP Local Loss per Layer')\n",
        "        axes[0, 0].set_xlabel('Step')\n",
        "        axes[0, 0].set_ylabel('Loss')\n",
        "        axes[0, 0].legend(ncol=4, fontsize=6)\n",
        "    else:\n",
        "        axes[0, 0].set_title('Pass 1: MLP Local Loss (skipped or no local loss)')\n",
        "\n",
        "    # Pass 1: MLP global loss\n",
        "    if not mlp_df.empty and 'global' in mlp_df.columns:\n",
        "        for layer in mlp_df['layer'].unique():\n",
        "            layer_df = mlp_df[mlp_df['layer'] == layer]\n",
        "            axes[0, 1].plot(layer_df['step'], layer_df['global'], label=f'L{layer}', alpha=0.7)\n",
        "        axes[0, 1].set_title('Pass 1: MLP Global KD Loss per Layer')\n",
        "        axes[0, 1].set_xlabel('Step')\n",
        "        axes[0, 1].set_ylabel('Loss')\n",
        "        axes[0, 1].legend(ncol=4, fontsize=6)\n",
        "    else:\n",
        "        axes[0, 1].set_title('Pass 1: MLP Global Loss (skipped)')\n",
        "\n",
        "    # Pass 2: Attention training\n",
        "    attn_df = df[(df['pass'] == 2) & (df['component'] == 'attn')]\n",
        "    if not attn_df.empty and 'global' in attn_df.columns:\n",
        "        for layer in attn_df['layer'].unique():\n",
        "            layer_df = attn_df[attn_df['layer'] == layer]\n",
        "            axes[1, 0].plot(layer_df['step'], layer_df['global'], label=f'L{layer}', alpha=0.7)\n",
        "        axes[1, 0].set_title('Pass 2: Attention Global KD Loss per Layer')\n",
        "        axes[1, 0].set_xlabel('Step')\n",
        "        axes[1, 0].set_ylabel('Loss')\n",
        "        axes[1, 0].legend(ncol=4, fontsize=6)\n",
        "    else:\n",
        "        axes[1, 0].set_title('Pass 2: Attention (skipped)')\n",
        "        axes[1, 0].text(0.5, 0.5, 'Not run', ha='center', va='center', transform=axes[1, 0].transAxes)\n",
        "\n",
        "    # Pass 4: E2E f-only tuning\n",
        "    e2e_df = df[(df['pass'] == 4)]\n",
        "    if not e2e_df.empty and 'global' in e2e_df.columns:\n",
        "        axes[1, 1].plot(e2e_df['step'], e2e_df['global'], 'b-', linewidth=2)\n",
        "        axes[1, 1].set_title('Pass 4: E2E f-only Tuning')\n",
        "        axes[1, 1].set_xlabel('Step')\n",
        "        axes[1, 1].set_ylabel('Global KD Loss')\n",
        "    else:\n",
        "        axes[1, 1].set_title('Pass 4: E2E (not yet run)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{PLOT_RUN}/loss_per_layer.png\", dpi=150)\n",
        "    plt.show()\n",
        "    print(f\"Saved to {PLOT_RUN}/loss_per_layer.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qgtw9rd3cr",
      "metadata": {
        "id": "qgtw9rd3cr"
      },
      "source": [
        "### Inference Check: Progressive QAT Results\n",
        "\n",
        "Test the progressive QAT checkpoint with a quick inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4coakmebsik",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4coakmebsik",
        "outputId": "5ab1ae51-ec40-4a29-c9d0-92df9f482839"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-25 03:57:41.932578: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-25 03:57:41.952444: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766635061.977425    7043 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766635061.982748    7043 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766635061.996415    7043 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766635061.996438    7043 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766635061.996441    7043 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766635061.996444    7043 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-25 03:57:42.000573: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "user\n",
            "What is Apple Neural Engine?\n",
            "assistant\n",
            "<think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Apple Neural Engine (NLP) is a powerful and powerful artificial intelligence system that mimues human behavior, learning, and decision-making. It is a popular machine learning algorithm that is widely used for various AI applications, such as image recognition, language understanding, and language generation. It is also used in finance, finance, and finance, making it a important tool for financial analysts and analysts.\n"
          ]
        }
      ],
      "source": [
        "# Test inference with progressive QAT checkpoint\n",
        "# Change RUN_DIR to test different runs:\n",
        "#   RUN_DIR_E2E_FONLY, RUN_DIR_PROGRESSIVE_FONLY, RUN_DIR_PROGRESSIVE\n",
        "# progressive_qat_v1/qat_state_dict.pt\n",
        "\n",
        "#TEST_RUN = RUN_DIR_E2E_FONLY  # Change this to test other runs\n",
        "\n",
        "# Use the path where the checkpoint was unzipped\n",
        "TEST_RUN =  \"runs/qwen3_kdqat_cache_q2_4\"\n",
        "TEST_RUN =  \"runs/qwen3_kdqat_cache_q2_3\"\n",
        "\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {TEST_RUN}/qat_state_dict.pt \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --prompt \"What is Apple Neural Engine?\" \\\n",
        "  --do_sample true \\\n",
        "  --max_new_tokens 128"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yq8GHX0o3c2y",
      "metadata": {
        "id": "Yq8GHX0o3c2y"
      },
      "source": [
        "## 6) Stage B: LoRA recovery\n",
        "\n",
        "Two options:\n",
        "- **SFT LoRA** (Alpaca-style instruction tuning)\n",
        "- **Cached KD-LoRA** (preserve teacher distribution; no new “skills”)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B1yimj8aS11y",
      "metadata": {
        "id": "B1yimj8aS11y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LcGFKyxO3c2y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcGFKyxO3c2y",
        "outputId": "979e3378-150b-44fc-83ef-762801383bb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "[device] cuda | amp_dtype=torch.bfloat16 | param_dtype=torch.bfloat16\n",
            "2025-12-25 03:59:07.550602: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-25 03:59:07.570769: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766635147.595924    7465 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766635147.601384    7465 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766635147.615579    7465 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766635147.615602    7465 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766635147.615605    7465 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766635147.615608    7465 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-25 03:59:07.619894: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[qat] weight_bits=2\n",
            "Loaded QAT checkpoint. missing=0 unexpected=0\n",
            "Enabled LoRA on (196, 20185088) layers. Trainable params: 20,185,088\n",
            "[kd_cache] Note: cache max_length=128 (you passed --max_length=1024). --max_length is ignored in cache mode.\n",
            "[kd_cache] cache topk=32\n",
            "[kd_cache] Enabled cached KD-LoRA. T=2.0 weight=1.0 hard_top1=0.02 hard_full_top1=0.01\n",
            "opt_step: 100% 1000/1000 [09:49<00:00,  1.70step/s, loss=1.2426, lr=0.00e+00]\n",
            "Done. Saved LoRA adapter to: runs/qwen3_lora_recovery_cached_r32/lora_only_state_dict.pt\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# STAGE B: LoRA Recovery (Cached KD-LoRA)\n",
        "# ============================================================\n",
        "# Train LoRA adapters on top of QAT checkpoint\n",
        "\n",
        "CACHE_DIR = CACHE_DIR_CHAT\n",
        "#RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2\"\n",
        "RUN_DIR_CACHE  = \"runs/progressive_qat_v1\"\n",
        "RUN_DIR_CACHE  = \"runs/qwen3_kdqat_cache_q2_4\"\n",
        "RUN_DIR_CACHE =  \"runs/qwen3_kdqat_cache_q2_3\"\n",
        "\n",
        "\n",
        "LORA_DIM = 32\n",
        "LORA_RUN_KD = f\"runs/qwen3_lora_recovery_cached_r{LORA_DIM}\"\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/train_lora_recovery.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR_CACHE}/qat_state_dict.pt \\\n",
        "  --output_dir {LORA_RUN_KD} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --gradient_accumulation_steps 2 \\\n",
        "  --learning_rate 1e-5 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 1000 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 2 \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} \\\n",
        "  --lora_alpha {LORA_DIM} \\\n",
        "  --lora_dropout 0.0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.02 \\\n",
        "  --hard-full-top1-weight 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Defule6L3c2y",
      "metadata": {
        "id": "Defule6L3c2y"
      },
      "source": [
        "## 7) Plot loss\n",
        "\n",
        "In Colab, use `--no_show` + `--save` then display the PNG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QM1RNfDh3c2y",
      "metadata": {
        "id": "QM1RNfDh3c2y"
      },
      "outputs": [],
      "source": [
        "!python scripts/plot_loss.py --run_dir {RUN_DIR} --source csv --no_show --save {RUN_DIR}/loss.png\n",
        "from PIL import Image\n",
        "display(Image.open(f\"{RUN_DIR}/loss.png\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p3bOUiMt3c2y",
      "metadata": {
        "id": "p3bOUiMt3c2y"
      },
      "source": [
        "## 8) Inference sanity checks\n",
        "\n",
        "Greedy decode (`--do_sample false`) and keep outputs short (`--max_new_tokens 16`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aA_cJHvT3c2y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aA_cJHvT3c2y",
        "outputId": "cc468368-92bd-4981-e9f4-e2e894eca16b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "2025-12-25 04:11:49.105856: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-25 04:11:49.126275: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766635909.151365   10948 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766635909.156770   10948 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766635909.170813   10948 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766635909.170836   10948 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766635909.170839   10948 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766635909.170842   10948 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-25 04:11:49.175005: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[lora] loaded adapters for 196 layers from runs/qwen3_lora_recovery_cached_r32/lora_only_state_dict.pt\n",
            "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "user\n",
            "What is Machine Learning?\n",
            "assistant\n",
            "<think>\n",
            "<think>\n",
            "</think>\n",
            "\n",
            "Machine Learning is a type of machine learning that involves combining data sets to predict outcomes based on patterns. It is used in various fields, such as finance, finance, and social media. Machine learning is a powerful field that combines data to make decisions based on patterns. It is used in various fields, such as finance, finance, and social media. Machine learning is a important field that is growing in many industries, including finance, finance, and social media.\n"
          ]
        }
      ],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_3\"\n",
        "#RUN_DIR = \"runs/progressive_qat_v1\"\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --lora_checkpoint \"runs/qwen3_lora_recovery_cached_r32/lora_only_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
        "  --prompt \"What is Machine Learning?\" \\\n",
        "  --do_sample false \\\n",
        "  --enable_thinking true \\\n",
        "  --max_new_tokens 256\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EJlu4sOZCNpY",
      "metadata": {
        "id": "EJlu4sOZCNpY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cD985HdXlm0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cD985HdXlm0",
        "outputId": "65053f76-705a-419c-fddb-8ae1cff1fda1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-24 10:41:54.709024: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-24 10:41:54.729231: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766572914.754556  155967 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766572914.759941  155967 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766572914.773878  155967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572914.773902  155967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572914.773905  155967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572914.773908  155967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 10:41:54.777961: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[lora] loaded adapters for 196 layers from runs/qwen3_lora_recovery_cached_r32/lora_only_state_dict.pt\n",
            "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "user\n",
            "Explain how neural networks learn in simple terms\n",
            "assistant\n",
            "<think>\n",
            "<think>\n",
            "Okay, let's start with the basics. Neural networks are like a brain in a computer, right? They process information in layers, just like a human brain does. Each layer has some connections between neurons, and each neuron can process a single input value. \n",
            "\n",
            "So, if we have a simple example, like a simple neural network that takes a single input and outputs a single output, how does it work? Let's imagine a simple example. Let's say the input is a number, say 10. The first layer is the input layer, and the second layer is the output layer. The input layer has a single neuron, and the output layer has a single neuron. \n",
            "\n",
            "The input layer takes the input number and processes it. Then, the output layer takes the result and passes it back. So, the process is: input → output. \n",
            "\n",
            "But how does this work in practice? It's like a simple machine learning model. The input is the data, the output is the result, and the model learns by adjusting the weights and biases to minimize the error. \n",
            "\n",
            "So, in simple terms, neural networks work by taking a set of inputs, applying some operations to them, and then producing a set of outputs. The process is iterative, and the model learns over time to make better predictions or decisions. \n",
            "\n",
            "I think that's a good explanation. Let me make sure it's simple and not too technical. The key is that the neural network is like a computer, and the process of learning is similar to how a human brain learns. \n",
            "\n",
            "I think that's a good explanation. Let me check if I'm missing anything. No, I think I've covered the basics. If there's anything else, I can add more details, but I'm already confident in this explanation.\n",
            "</think>\n",
            "\n",
            "**Explanation of How Neural Networks Learn in Simple Terms:**\n",
            "\n",
            "**1. Introduction:**\n",
            "- **Neural Networks (NNs):** They are like computers in the brain, where each neuron processes information.\n",
            "- **Input Layer:** Takes the input data and processes it.\n",
            "- **Output Layer:** Produces the output, which is the result of the processing.\n",
            "\n",
            "**2. Simple Example:**\n",
            "- **Input:** A single input value, say 10.\n",
            "- **First Layer (Input Layer):** Processes the input, maybe by adding some weights and biases.\n",
            "- **Output Layer (Output Layer):** Passes the result back, with the final output being the result of the processing.\n",
            "\n",
            "**3. How It Works:**\n",
            "- **Processing:** Each neuron in a layer takes the input and processes it.\n",
            "- **Learning:** The network learns to adjust its weights and biases over time to minimize the error.\n",
            "- **Iteration:** The process is repeated over time, and the model improves its performance.\n",
            "\n",
            "**4. Simple Explanation in Words:**\n",
            "- Neural networks are like a computer that learns by doing tasks. The input is the data, the output is the result, and the model learns to make better predictions.\n",
            "\n",
            "**5. Conclusion:**\n",
            "- Neural networks are a type of machine learning model that processes information in layers, just like a human brain does. They are simple, can be used for various tasks, and are very effective in learning.\n",
            "\n",
            "This explanation is simple, visual, and easy to understand.\n"
          ]
        }
      ],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_4\"\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --lora_checkpoint \"runs/qwen3_lora_recovery_cached_r32/lora_only_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
        "  --prompt \"Explain how neural networks learn in simple terms\" \\\n",
        "  --do_sample false \\\n",
        "  --enable_thinking true \\\n",
        "  --max_new_tokens 1024\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xZ6aSDAa3c2y",
      "metadata": {
        "id": "xZ6aSDAa3c2y"
      },
      "outputs": [],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --lora_checkpoint \"runs/qwen3_lora_recovery_cached_r64/lora_only_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
        "  --prompt \"2+2=\" \\\n",
        "  --do_sample false \\\n",
        "  --enable_thinking true \\\n",
        "  --max_new_tokens 90\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZOIo97VlyAbh",
      "metadata": {
        "id": "ZOIo97VlyAbh"
      },
      "outputs": [],
      "source": [
        "LORA_DIM = 64\n",
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2\"\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
        "  --prompt \"What is capital of France?\" \\\n",
        "  --do_sample true \\\n",
        "  --max_new_tokens 64\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ef_Q8P0i3c2y",
      "metadata": {
        "id": "Ef_Q8P0i3c2y"
      },
      "source": [
        "## 9) Optional: snap weights to the exact grid\n",
        "\n",
        "This produces a float checkpoint with weights snapped to the N-bit codebook (not bitpacked)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jk_qSZIs3c2y",
      "metadata": {
        "id": "jk_qSZIs3c2y"
      },
      "outputs": [],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "!python scripts/hard_quantize_checkpoint.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}/checkpoint_last.pt \\\n",
        "  --output_path {RUN_DIR}/hard_quant_full_state_dict.pt \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7HnE7IFS3dK",
      "metadata": {
        "id": "f7HnE7IFS3dK"
      },
      "outputs": [],
      "source": [
        "%cd qwen3_apple_style_2bit_qat_lora\n",
        "%ls -l runs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beQD2eyDWWnb",
      "metadata": {
        "id": "beQD2eyDWWnb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}