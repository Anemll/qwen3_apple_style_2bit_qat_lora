{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SR-012: LUT Candidate Analysis & Evaluation\n",
    "\n",
    "**Version:** 1.1.0 | **Build:** 2026-01-17\n",
    "\n",
    "Analyze Q/LUT distributions, generate LUT candidates, and evaluate via fast perplexity screening.\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "| Step | Script | Device | Time |\n",
    "|------|--------|--------|------|\n",
    "| 1 | `analyze_q_lut_stats.py` | CPU | ~30s |\n",
    "| 2 | `apply_lut_candidates.py` | CPU | ~1-2min |\n",
    "| 3 | `eval_lut_candidates.py` | GPU/CPU | ~30s-15min per candidate |\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Trained V2 QAT checkpoint** (from SR-008, SR-011, etc.)\n",
    "- GPU recommended for PPL evaluation (optional - can skip)\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. Run **Section 1** (Setup)\n",
    "2. Run **Section 2** to find your checkpoints\n",
    "3. Fill in `V2_CHECKPOINT` in **Section 3**\n",
    "4. Run remaining sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1.1 Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"Google Drive mounted at /content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1.2 Clone Repository\n",
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/Anemll/qwen3_apple_style_2bit_qat_lora.git\"  #@param {type:\"string\"}\n",
    "REPO_DIR = \"/content/repo\"\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "    print(f\"Cloned to {REPO_DIR}\")\n",
    "else:\n",
    "    print(f\"Repository already exists at {REPO_DIR}\")\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "!git pull\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1.3 Install Dependencies\n",
    "!pip install -q transformers accelerate datasets sentencepiece protobuf torch\n",
    "print(\"Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1.4 Check Device\n",
    "import torch\n",
    "\n",
    "print(\"Device Detection:\")\n",
    "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    HAS_GPU = True\n",
    "else:\n",
    "    print(\"  No GPU detected\")\n",
    "    print(\"  Steps 1-2 (stats, candidates) will work fine\")\n",
    "    print(\"  Step 3 (PPL eval) will be slow - consider skipping\")\n",
    "    HAS_GPU = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Discover Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2.1 Find Checkpoints on Google Drive\n",
    "#@markdown Run this cell to discover available V2 checkpoints.\n",
    "#@markdown Copy the path you want to use into Section 3.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "SEARCH_PATHS = [\n",
    "    \"/content/drive/MyDrive/qat_runs\",\n",
    "    \"/content/drive/MyDrive/qwen3_runs\",\n",
    "    \"/content/drive/MyDrive\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SEARCHING FOR V2 CHECKPOINTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "found_any = False\n",
    "for search_path in SEARCH_PATHS:\n",
    "    p = Path(search_path)\n",
    "    if not p.exists():\n",
    "        continue\n",
    "    \n",
    "    # Look for .pt files\n",
    "    checkpoints = list(p.glob(\"**/*v2*.pt\")) + list(p.glob(\"**/*fp16*.pt\"))\n",
    "    checkpoints = sorted(set(checkpoints))[:15]  # Dedupe and limit\n",
    "    \n",
    "    if checkpoints:\n",
    "        found_any = True\n",
    "        print(f\"\\nFound in {search_path}:\")\n",
    "        for ckpt in checkpoints:\n",
    "            size_mb = ckpt.stat().st_size / 1e6\n",
    "            print(f\"  {ckpt} ({size_mb:.1f} MB)\")\n",
    "\n",
    "if not found_any:\n",
    "    print(\"\\nNo V2 checkpoints found.\")\n",
    "    print(\"Upload a trained checkpoint to Google Drive first.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Copy one of the paths above into V2_CHECKPOINT in Section 3\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Configuration\n",
    "\n",
    "**IMPORTANT:** Update `V2_CHECKPOINT` below with your checkpoint path from Section 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3.1 Main Configuration\n",
    "#@markdown ### Checkpoint Path (REQUIRED)\n",
    "V2_CHECKPOINT = \"\"  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ### Model Settings\n",
    "MODEL_ID = \"Qwen/Qwen3-0.6B\"  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ### LUT Candidate Options\n",
    "LUT_FAMILIES = \"A,B,C,D\"  #@param [\"A\", \"B\", \"C\", \"D\", \"A,B\", \"A,B,C,D\"]\n",
    "MAX_ABS = 1.0  #@param {type:\"number\"}\n",
    "SCOPE = \"all\"  #@param [\"all\", \"mlp\", \"attn\"]\n",
    "\n",
    "#@markdown ### PPL Evaluation Options\n",
    "SKIP_PPL_EVAL = False  #@param {type:\"boolean\"}\n",
    "MAX_CHUNKS_FAST = 20  #@param {type:\"integer\"}\n",
    "TOP_K = 3  #@param {type:\"integer\"}\n",
    "RUN_FULL_PPL = True  #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ### Output Paths\n",
    "OUTPUT_DIR = \"/content/lut_candidates\"  #@param {type:\"string\"}\n",
    "GDRIVE_OUTPUT = \"/content/drive/MyDrive/qat_runs/SR-012_lut_analysis\"  #@param {type:\"string\"}\n",
    "\n",
    "# Validation\n",
    "print(\"=\" * 60)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "if not V2_CHECKPOINT:\n",
    "    print(\"ERROR: V2_CHECKPOINT is empty!\")\n",
    "    print(\"       Run Section 2 to find checkpoints, then paste the path here.\")\n",
    "else:\n",
    "    print(f\"Checkpoint:    {V2_CHECKPOINT}\")\n",
    "    print(f\"Model:         {MODEL_ID}\")\n",
    "    print(f\"LUT Families:  {LUT_FAMILIES}\")\n",
    "    print(f\"Max Abs:       {MAX_ABS}\")\n",
    "    print(f\"Scope:         {SCOPE}\")\n",
    "    print(f\"Skip PPL:      {SKIP_PPL_EVAL}\")\n",
    "    print(f\"Output:        {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3.2 Verify Checkpoint & Sync to Local\n",
    "#@markdown Copies checkpoint to local storage for faster processing.\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "os.chdir(\"/content/repo\")\n",
    "\n",
    "if not V2_CHECKPOINT:\n",
    "    raise ValueError(\"V2_CHECKPOINT is not set! Go back to Section 3.1\")\n",
    "\n",
    "if not os.path.exists(V2_CHECKPOINT):\n",
    "    raise FileNotFoundError(f\"Checkpoint not found: {V2_CHECKPOINT}\")\n",
    "\n",
    "size_mb = os.path.getsize(V2_CHECKPOINT) / 1e6\n",
    "print(f\"Checkpoint found: {V2_CHECKPOINT}\")\n",
    "print(f\"Size: {size_mb:.1f} MB\")\n",
    "\n",
    "# Sync to local storage\n",
    "LOCAL_DIR = \"/content/checkpoints\"\n",
    "os.makedirs(LOCAL_DIR, exist_ok=True)\n",
    "local_ckpt = Path(LOCAL_DIR) / Path(V2_CHECKPOINT).name\n",
    "\n",
    "if not local_ckpt.exists():\n",
    "    print(f\"\\nCopying to local storage...\")\n",
    "    shutil.copy(V2_CHECKPOINT, local_ckpt)\n",
    "    print(f\"  -> {local_ckpt}\")\n",
    "    \n",
    "    # Copy config.json if exists\n",
    "    config_src = Path(V2_CHECKPOINT).parent / \"config.json\"\n",
    "    if config_src.exists():\n",
    "        shutil.copy(config_src, Path(LOCAL_DIR) / \"config.json\")\n",
    "        print(f\"  -> config.json\")\n",
    "else:\n",
    "    print(f\"\\nLocal copy exists: {local_ckpt}\")\n",
    "\n",
    "CHECKPOINT_PATH = str(local_ckpt)\n",
    "print(f\"\\nUsing: {CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Analyze Q/LUT Statistics (CPU)\n",
    "\n",
    "Computes per-layer distribution metrics to guide LUT selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.1 Run Q/LUT Statistics Analysis\n",
    "#@markdown Takes ~30 seconds on CPU.\n",
    "\n",
    "import os\n",
    "os.chdir(\"/content/repo\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "STATS_OUTPUT = f\"{OUTPUT_DIR}/q_lut_stats.json\"\n",
    "\n",
    "!python scripts/analyze_q_lut_stats.py \"{CHECKPOINT_PATH}\" \\\n",
    "    --scope {SCOPE} \\\n",
    "    --output \"{STATS_OUTPUT}\" \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4.2 Display Statistics Summary\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "stats_file = Path(STATS_OUTPUT)\n",
    "if stats_file.exists():\n",
    "    with open(stats_file) as f:\n",
    "        stats = json.load(f)\n",
    "    \n",
    "    summary = stats.get('summary', {})\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Q/LUT STATISTICS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total layers:              {summary.get('total_layers', 'N/A')}\")\n",
    "    print(f\"  MLP:                     {summary.get('mlp_layers', 'N/A')}\")\n",
    "    print(f\"  Attention:               {summary.get('attn_layers', 'N/A')}\")\n",
    "    print()\n",
    "    print(f\"p999/max_abs (max):        {summary.get('p999_over_maxabs_max', 'N/A')}\")\n",
    "    print(f\"p999/max_abs (avg):        {summary.get('p999_over_maxabs_avg', 'N/A')}\")\n",
    "    print(f\"Layers need wider max_abs: {summary.get('layers_need_widen_maxabs', 'N/A')}\")\n",
    "    print()\n",
    "    \n",
    "    # Recommendation\n",
    "    if summary.get('p999_over_maxabs_max', 0) > 1.0:\n",
    "        suggested = round(MAX_ABS * 1.5, 1)\n",
    "        print(f\"RECOMMENDATION: Increase MAX_ABS to {suggested} or higher\")\n",
    "    else:\n",
    "        print(f\"max_abs={MAX_ABS} looks adequate\")\n",
    "else:\n",
    "    print(f\"Stats file not found: {stats_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Generate LUT Candidates (CPU)\n",
    "\n",
    "Creates checkpoint variants with different LUT families:\n",
    "\n",
    "| Family | Description |\n",
    "|--------|-------------|\n",
    "| **A** | Uniform (linspace) |\n",
    "| **B** | Dense-center (more values near 0) |\n",
    "| **C** | Heavy-tail (more values at extremes) |\n",
    "| **D** | Quantile (data-driven) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5.1 Generate LUT Candidates\n",
    "#@markdown Takes ~1-2 minutes on CPU.\n",
    "\n",
    "import os\n",
    "os.chdir(\"/content/repo\")\n",
    "\n",
    "!python scripts/apply_lut_candidates.py \"{CHECKPOINT_PATH}\" \\\n",
    "    --output-dir \"{OUTPUT_DIR}\" \\\n",
    "    --families {LUT_FAMILIES} \\\n",
    "    --max-abs {MAX_ABS} \\\n",
    "    --scope {SCOPE} \\\n",
    "    --save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5.2 List Generated Candidates\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "summary_file = Path(OUTPUT_DIR) / \"candidates_summary.json\"\n",
    "\n",
    "if summary_file.exists():\n",
    "    with open(summary_file) as f:\n",
    "        summary = json.load(f)\n",
    "    \n",
    "    candidates = summary.get('candidates', [])\n",
    "    candidates.sort(key=lambda x: x.get('avg_mae', float('inf')))\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"GENERATED {len(candidates)} CANDIDATES\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Rank':<5} {'Name':<35} {'Avg MAE':<12} {'Max MAE':<12}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for i, c in enumerate(candidates, 1):\n",
    "        print(f\"{i:<5} {c['name']:<35} {c['avg_mae']:.6f}     {c['max_mae']:.6f}\")\n",
    "    \n",
    "    print()\n",
    "    print(f\"Best by MAE: {candidates[0]['name']}\")\n",
    "else:\n",
    "    print(f\"Summary not found: {summary_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Evaluate Candidates (GPU/CPU)\n",
    "\n",
    "**Optional:** Skip if no GPU and you don't want to wait.\n",
    "\n",
    "Uses perplexity to rank candidates. Set `SKIP_PPL_EVAL=True` in Section 3 to skip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6.1 Run PPL Evaluation\n",
    "#@markdown - GPU: ~30s per candidate (fast), ~3min (full)\n",
    "#@markdown - CPU: ~10-15min per candidate\n",
    "\n",
    "import os\n",
    "os.chdir(\"/content/repo\")\n",
    "\n",
    "EVAL_OUTPUT = f\"{OUTPUT_DIR}/eval_results.json\"\n",
    "\n",
    "if SKIP_PPL_EVAL:\n",
    "    print(\"PPL evaluation SKIPPED (SKIP_PPL_EVAL=True)\")\n",
    "    print(\"Use MAE ranking from Step 5 instead.\")\n",
    "else:\n",
    "    cmd = f'''python scripts/eval_lut_candidates.py \"{OUTPUT_DIR}\" \\\n",
    "        --max-chunks {MAX_CHUNKS_FAST} \\\n",
    "        --top-k {TOP_K} \\\n",
    "        --model-id {MODEL_ID} \\\n",
    "        --device auto \\\n",
    "        --dtype auto \\\n",
    "        --output \"{EVAL_OUTPUT}\"'''\n",
    "    \n",
    "    if RUN_FULL_PPL:\n",
    "        cmd += \" --full-ppl\"\n",
    "    \n",
    "    print(\"Running PPL evaluation...\")\n",
    "    !{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6.2 Display Evaluation Results\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "eval_file = Path(EVAL_OUTPUT) if 'EVAL_OUTPUT' in dir() else Path(f\"{OUTPUT_DIR}/eval_results.json\")\n",
    "\n",
    "if eval_file.exists():\n",
    "    with open(eval_file) as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    entries = results.get('results', [])\n",
    "    entries.sort(key=lambda x: x.get('ppl_fast', float('inf')))\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Rank':<5} {'Candidate':<30} {'Fast PPL':<10} {'Full PPL':<10} {'MAE':<10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i, r in enumerate(entries, 1):\n",
    "        fast = f\"{r['ppl_fast']:.2f}\" if r.get('ppl_fast') else \"N/A\"\n",
    "        full = f\"{r['ppl_full']:.2f}\" if r.get('ppl_full') else \"N/A\"\n",
    "        mae = f\"{r['avg_mae']:.6f}\" if r.get('avg_mae') else \"N/A\"\n",
    "        print(f\"{i:<5} {r['name']:<30} {fast:<10} {full:<10} {mae:<10}\")\n",
    "    \n",
    "    if entries:\n",
    "        print()\n",
    "        print(f\"BEST: {entries[0]['name']}\")\n",
    "elif SKIP_PPL_EVAL:\n",
    "    print(\"PPL evaluation was skipped.\")\n",
    "else:\n",
    "    print(f\"Results not found: {eval_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Get Best Candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7.1 Determine Best Candidate\n",
    "#@markdown Uses PPL if available, otherwise MAE.\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "best_checkpoint = None\n",
    "best_name = None\n",
    "\n",
    "# Try PPL results\n",
    "eval_file = Path(f\"{OUTPUT_DIR}/eval_results.json\")\n",
    "if eval_file.exists():\n",
    "    with open(eval_file) as f:\n",
    "        results = json.load(f)\n",
    "    entries = results.get('results', [])\n",
    "    if entries:\n",
    "        entries.sort(key=lambda x: x.get('ppl_fast', float('inf')))\n",
    "        best_checkpoint = entries[0]['checkpoint_path']\n",
    "        best_name = entries[0]['name']\n",
    "        print(f\"Best by PPL: {best_name}\")\n",
    "\n",
    "# Fallback to MAE\n",
    "if not best_checkpoint:\n",
    "    summary_file = Path(f\"{OUTPUT_DIR}/candidates_summary.json\")\n",
    "    if summary_file.exists():\n",
    "        with open(summary_file) as f:\n",
    "            summary = json.load(f)\n",
    "        candidates = summary.get('candidates', [])\n",
    "        if candidates:\n",
    "            candidates.sort(key=lambda x: x.get('avg_mae', float('inf')))\n",
    "            best_checkpoint = candidates[0]['checkpoint_path']\n",
    "            best_name = candidates[0]['name']\n",
    "            print(f\"Best by MAE: {best_name}\")\n",
    "\n",
    "if best_checkpoint:\n",
    "    print(f\"Checkpoint: {best_checkpoint}\")\n",
    "else:\n",
    "    print(\"No candidates found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7.2 Test Inference (Optional)\n",
    "RUN_INFERENCE = True  #@param {type:\"boolean\"}\n",
    "TEST_PROMPT = \"What is the capital of France?\"  #@param {type:\"string\"}\n",
    "\n",
    "if RUN_INFERENCE and best_checkpoint:\n",
    "    !python scripts/test_inference.py \"{best_checkpoint}\" \\\n",
    "        --prompt \"{TEST_PROMPT}\" \\\n",
    "        --max-tokens 256\n",
    "elif not best_checkpoint:\n",
    "    print(\"No checkpoint available\")\n",
    "else:\n",
    "    print(\"Inference skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Save Results to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8.1 Copy Results to Google Drive\n",
    "COPY_BEST_CHECKPOINT = True  #@param {type:\"boolean\"}\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "os.makedirs(GDRIVE_OUTPUT, exist_ok=True)\n",
    "\n",
    "# Copy JSON files\n",
    "output_path = Path(OUTPUT_DIR)\n",
    "for f in output_path.glob(\"*.json\"):\n",
    "    dest = Path(GDRIVE_OUTPUT) / f.name\n",
    "    shutil.copy(f, dest)\n",
    "    print(f\"Copied: {f.name}\")\n",
    "\n",
    "# Copy best checkpoint\n",
    "if COPY_BEST_CHECKPOINT and best_checkpoint:\n",
    "    ckpt = Path(best_checkpoint)\n",
    "    if ckpt.exists():\n",
    "        dest = Path(GDRIVE_OUTPUT) / ckpt.name\n",
    "        shutil.copy(ckpt, dest)\n",
    "        print(f\"Copied: {ckpt.name}\")\n",
    "\n",
    "print(f\"\\nSaved to: {GDRIVE_OUTPUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Decision Guide\n",
    "\n",
    "| Observation | Recommended Action |\n",
    "|-------------|--------------------|\n",
    "| `p999_over_maxabs > 1.0` | Increase `MAX_ABS` (1.5x-2x) |\n",
    "| `pct_Qeff_outside > 0.5%` | Increase `MAX_ABS` |\n",
    "| High `tail_ratio` (p99/p90 > 2) | Try Family C (Heavy-tail) |\n",
    "| Low `center_ratio` (p50/p90 < 0.3) | Try Family B (Dense-center) |\n",
    "| All metrics benign | Family A (Uniform) is fine |\n",
    "| Want optimal fit | Family D (Quantile) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Alternative: Single Family with Wider Range\n",
    "\n",
    "If stats suggest widening `max_abs`, run this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 10.1 Generate Single Family Candidates\n",
    "SINGLE_FAMILY = \"C\"  #@param [\"A\", \"B\", \"C\", \"D\"]\n",
    "WIDER_MAX_ABS = 1.5  #@param {type:\"number\"}\n",
    "\n",
    "import os\n",
    "os.chdir(\"/content/repo\")\n",
    "\n",
    "SINGLE_OUTPUT = f\"{OUTPUT_DIR}_family_{SINGLE_FAMILY}\"\n",
    "\n",
    "!python scripts/apply_lut_candidates.py \"{CHECKPOINT_PATH}\" \\\n",
    "    --output-dir \"{SINGLE_OUTPUT}\" \\\n",
    "    --families {SINGLE_FAMILY} \\\n",
    "    --max-abs {WIDER_MAX_ABS} \\\n",
    "    --scope {SCOPE} \\\n",
    "    --save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 10.2 Evaluate Single Family (Optional)\n",
    "if not SKIP_PPL_EVAL:\n",
    "    !python scripts/eval_lut_candidates.py \"{SINGLE_OUTPUT}\" \\\n",
    "        --max-chunks {MAX_CHUNKS_FAST} \\\n",
    "        --top-k 3 \\\n",
    "        --model-id {MODEL_ID} \\\n",
    "        --device auto \\\n",
    "        --full-ppl \\\n",
    "        --output \"{SINGLE_OUTPUT}/eval_results.json\"\n",
    "else:\n",
    "    print(\"PPL skipped. Check candidates_summary.json for MAE ranking.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Notes\n",
    "\n",
    "**Scripts:**\n",
    "- `analyze_q_lut_stats.py` - Per-layer Q/LUT statistics\n",
    "- `apply_lut_candidates.py` - Generate LUT candidate checkpoints\n",
    "- `eval_lut_candidates.py` - Fast PPL screening\n",
    "\n",
    "**Memory:**\n",
    "- Stats + candidates: ~4-6 GB RAM\n",
    "- PPL evaluation: ~6-8 GB VRAM\n",
    "\n",
    "**CPU-only mode:**\n",
    "- Set `SKIP_PPL_EVAL = True`\n",
    "- Use MAE ranking from Step 5"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
