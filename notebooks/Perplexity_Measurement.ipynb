{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfqNhwHgMgm0"
   },
   "source": [
    "# Perplexity Measurement\n",
    "\n",
    "**Version:** 1.0 | **Build:** 2026-01-10\n",
    "\n",
    "Measure perplexity of QAT checkpoints and compare with baseline Qwen model.\n",
    "\n",
    "**Perplexity** = exp(cross-entropy loss) on next-token prediction.\n",
    "- Lower is better\n",
    "- WikiText-2 baselines: GPT-2 ~22, good LLMs ~5-10\n",
    "\n",
    "## Setup (Colab)\n",
    "\n",
    "Run the setup cells below to clone the repository and mount Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Clone repository (run once)\n",
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/anemll/qwen3_apple_style_2bit_qat_lora.git\"  #@param {type:\"string\"}\n",
    "REPO_DIR = \"qwen3_apple_style_2bit_qat_lora\"\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone {REPO_URL}\n",
    "    print(f\"✓ Cloned to {REPO_DIR}\")\n",
    "else:\n",
    "    print(f\"✓ Repository already exists at {REPO_DIR}\")\n",
    "\n",
    "# Change to repo directory\n",
    "os.chdir(REPO_DIR)\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "!git pull"
   ],
   "metadata": {
    "id": "YEUb0M_zMgm0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Mount Google Drive (for checkpoints)\n",
    "MOUNT_DRIVE = True  #@param {type:\"boolean\"}\n",
    "\n",
    "if MOUNT_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"✓ Google Drive mounted at /content/drive\")\n",
    "    print(\"  Use paths like: /content/drive/MyDrive/qat_checkpoints/model.pt\")"
   ],
   "metadata": {
    "id": "JWsJeJoJMgm1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#@title Download perplexity results from Google Drive (optional)\n# Downloads existing results/perplexity.json from Google Drive to merge with local results\nimport os\nimport json\nimport subprocess\nfrom pathlib import Path\n\n# Ensure we're in the correct working directory\nREPO_ROOT = \"/content/qwen3_apple_style_2bit_qat_lora\"\nif os.path.exists(REPO_ROOT):\n    os.chdir(REPO_ROOT)\nprint(f\"Working directory: {os.getcwd()}\")\n\nGDRIVE_RESULTS = \"/content/drive/MyDrive/qat_runs/perplexity.json\"\nLOCAL_RESULTS = \"results/perplexity.json\"\n\n# Create results directory\nos.makedirs(\"results\", exist_ok=True)\n\n# Use subprocess to check if file exists (more reliable with GDrive FUSE)\nresult = subprocess.run([\"test\", \"-f\", GDRIVE_RESULTS], capture_output=True)\ngdrive_exists = result.returncode == 0\n\nif gdrive_exists:\n    if os.path.exists(LOCAL_RESULTS):\n        # Merge: GDrive results take precedence for same keys (they're the \"master\")\n        with open(GDRIVE_RESULTS, 'r') as f:\n            gdrive_data = json.load(f)\n        with open(LOCAL_RESULTS, 'r') as f:\n            local_data = json.load(f)\n        merged = {**local_data, **gdrive_data}\n        with open(LOCAL_RESULTS, 'w') as f:\n            json.dump(merged, f, indent=2)\n        print(f\"✓ Merged {len(gdrive_data)} GDrive + {len(local_data)} local = {len(merged)} results\")\n    else:\n        # Use shell cp (more reliable with GDrive FUSE)\n        subprocess.run([\"cp\", GDRIVE_RESULTS, LOCAL_RESULTS], check=True)\n        with open(LOCAL_RESULTS, 'r') as f:\n            data = json.load(f)\n        print(f\"✓ Downloaded {len(data)} results from Google Drive\")\nelse:\n    print(f\"No results on Google Drive yet: {GDRIVE_RESULTS}\")\n    print(\"  Results will be created after first measurement.\")\n\nprint(f\"Runs folder exists: {os.path.exists('runs')}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "O3B9UU_FMgm1"
   },
   "outputs": [],
   "source": "#@title Config\n# Checkpoint path - supports multiple formats:\n#   \"runs/SR-011_foo/v2_fp16_20260110.pt\"  (full local path)\n#   \"SR-011_foo/v2_fp16_20260110.pt\"       (run/file - auto-downloads from GDrive)\n#   \"SR-011_foo\"                           (run only - downloads all FP16 files)\n#\n# The script will auto-download from Google Drive if file not found locally.\n\nCHECKPOINT = \"SR-011_L1024_attn_touchup_sparse2/v2_q4a4_r32_fp16_20260110_180453.pt\"  #@param {type:\"string\"}\nMODEL_NAME = \"Qwen/Qwen3-0.6B\"  #@param {type:\"string\"}\n\n# LoRA rank - AUTO-DETECTED from config.json by default (set to 0)\n# The script reads lora_r from config.json, so you usually don't need to change this.\n# Only set manually if you want to override the config or config.json is missing.\nLORA_R = 0  #@param {type:\"integer\"}\n\n# Evaluation settings\nMAX_LENGTH = 1024  #@param {type:\"integer\"}\nSTRIDE = 512  #@param {type:\"integer\"}\nVERBOSE = True  #@param {type:\"boolean\"}\nUSE_FP16 = True  #@param {type:\"boolean\"}"
  },
  {
   "cell_type": "markdown",
   "source": "## Download Checkpoint from Google Drive\n\nUse `gdrive_sync.py` to download specific checkpoint files from Google Drive.\n\n**Note:** FP16 checkpoints are recommended for perplexity measurement:\n- Smaller file size (~1.2GB vs ~2.4GB for FP32)\n- Required `--dtype fp16` flag for correct loading\n- Same numerical results as FP32 for inference",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#@title Download checkpoint from Google Drive (if needed)\n# Handles flexible path formats:\n#   \"runs/SR-011_foo/checkpoint.pt\" -> extracts run name + file\n#   \"SR-011_foo/checkpoint.pt\" -> run name + file\n#   \"SR-011_foo\" -> run name only (downloads all matching files)\n#\n# Also downloads config.json (required for correct quantization params)\n\nimport os\nfrom pathlib import Path\n\n# Ensure we're in the correct working directory\nREPO_ROOT = \"/content/qwen3_apple_style_2bit_qat_lora\"\nif os.path.exists(REPO_ROOT):\n    os.chdir(REPO_ROOT)\nprint(f\"Working directory: {os.getcwd()}\")\nprint(f\"Runs folder exists: {os.path.exists('runs')}\")\n\n# Parse checkpoint path\nckpt_path = CHECKPOINT.strip()\n\n# Remove \"runs/\" prefix if present\nif ckpt_path.startswith(\"runs/\"):\n    ckpt_path = ckpt_path[5:]\n\n# Split into run_name and file_name\nparts = ckpt_path.split(\"/\")\nif len(parts) >= 2:\n    # Format: \"run_name/checkpoint.pt\"\n    run_name = parts[0]\n    file_pattern = parts[-1]  # Last part is the file\nelse:\n    # Format: \"run_name\" only\n    run_name = parts[0]\n    file_pattern = \"*fp16*.pt\" if USE_FP16 else \"*.pt\"\n\nprint(f\"Run name: {run_name}\")\nprint(f\"File pattern: {file_pattern}\")\n\n# Build local path\nlocal_run_dir = Path(f\"runs/{run_name}\")\nlocal_checkpoint = local_run_dir / file_pattern if \"*\" not in file_pattern else None\n\ndef download_config_if_needed(run_name, local_run_dir):\n    \"\"\"Download config.json if not present locally.\"\"\"\n    config_files = [\"config.json\", \"v2_config.json\"]\n    for config_name in config_files:\n        local_config = local_run_dir / config_name\n        if local_config.exists():\n            print(f\"✓ Config found: {local_config}\")\n            return True\n    \n    # Try to download config.json\n    print(f\"Downloading config.json...\")\n    !python scripts/gdrive_sync.py down {run_name} --only \"config.json\"\n    \n    # Check if downloaded\n    for config_name in config_files:\n        local_config = local_run_dir / config_name\n        if local_config.exists():\n            print(f\"✓ Config downloaded: {local_config}\")\n            return True\n    \n    # Try v2_config.json\n    !python scripts/gdrive_sync.py down {run_name} --only \"v2_config.json\"\n    for config_name in config_files:\n        local_config = local_run_dir / config_name\n        if local_config.exists():\n            print(f\"✓ Config downloaded: {local_config}\")\n            return True\n    \n    print(f\"⚠ No config.json found (will use defaults)\")\n    return False\n\n# Check if already exists locally\nif local_checkpoint and local_checkpoint.exists():\n    CHECKPOINT = str(local_checkpoint)\n    print(f\"✓ Found locally: {CHECKPOINT}\")\n    print(f\"  Size: {local_checkpoint.stat().st_size / 1024 / 1024:.1f} MB\")\n    # Still check for config\n    download_config_if_needed(run_name, local_run_dir)\nelif local_run_dir.exists() and \"*\" not in file_pattern:\n    # Directory exists but file doesn't - try to find it\n    matches = list(local_run_dir.glob(file_pattern))\n    if matches:\n        CHECKPOINT = str(matches[0])\n        print(f\"✓ Found locally: {CHECKPOINT}\")\n        download_config_if_needed(run_name, local_run_dir)\n    else:\n        print(f\"File not found locally, downloading from Google Drive...\")\n        !python scripts/gdrive_sync.py down {run_name} --only \"{file_pattern}\"\n        # Also download config\n        download_config_if_needed(run_name, local_run_dir)\n        matches = list(local_run_dir.glob(file_pattern))\n        if matches:\n            CHECKPOINT = str(matches[0])\n            print(f\"✓ Downloaded: {CHECKPOINT}\")\n        else:\n            print(f\"⚠ Download failed. Check Google Drive.\")\nelse:\n    # Download from Google Drive\n    print(f\"Downloading from Google Drive...\")\n    print(f\"  Run: {run_name}\")\n    print(f\"  Pattern: {file_pattern}\")\n    !python scripts/gdrive_sync.py down {run_name} --only \"{file_pattern}\"\n    \n    # Also download config.json\n    download_config_if_needed(run_name, local_run_dir)\n    \n    # Find downloaded file\n    if local_run_dir.exists():\n        if \"*\" in file_pattern:\n            matches = list(local_run_dir.glob(file_pattern))\n        else:\n            matches = [local_run_dir / file_pattern] if (local_run_dir / file_pattern).exists() else []\n        if matches:\n            CHECKPOINT = str(matches[0])\n            print(f\"✓ Downloaded: {CHECKPOINT}\")\n            print(f\"  Size: {matches[0].stat().st_size / 1024 / 1024:.1f} MB\")\n        else:\n            print(f\"⚠ Download may have failed.\")\n            print(f\"  Check: python scripts/gdrive_sync.py list\")\n    else:\n        print(f\"⚠ Run directory not created. Check if Google Drive is mounted.\")\n\nprint(f\"\\nCheckpoint: {CHECKPOINT}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfj13clyMgm1"
   },
   "outputs": [],
   "source": [
    "#@title Install dependencies (run once)\n",
    "!pip install -q datasets transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PmjjEzXMgm1"
   },
   "outputs": [],
   "source": [
    "#@title Device setup\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Auto-detect device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "    DTYPE = torch.bfloat16\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "    DTYPE = torch.float32\n",
    "else:\n",
    "    try:\n",
    "        import torch_xla.core.xla_model as xm\n",
    "        DEVICE = 'tpu'\n",
    "        DTYPE = torch.bfloat16\n",
    "    except ImportError:\n",
    "        DEVICE = 'cpu'\n",
    "        DTYPE = torch.float32\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Dtype: {DTYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCsn9jN8Mgm1"
   },
   "source": [
    "## 1. Measure Baseline Model Perplexity\n",
    "\n",
    "Measure the original Qwen model (no QAT) to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Wn6zlFz2Mgm1"
   },
   "outputs": [],
   "source": [
    "#@title Measure baseline perplexity\n",
    "!python scripts/measure_perplexity.py --baseline \\\n",
    "    --model {MODEL_NAME} \\\n",
    "    --max-length {MAX_LENGTH} \\\n",
    "    --stride {STRIDE} \\\n",
    "    --device {DEVICE} \\\n",
    "    {'--verbose' if VERBOSE else ''}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q83bAIbzMgm1"
   },
   "source": [
    "## 2. Measure QAT Checkpoint Perplexity\n",
    "\n",
    "Measure the quantized model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ny1xvFpbMgm1"
   },
   "outputs": [],
   "source": [
    "#@title Measure checkpoint perplexity\n",
    "new_path = '/content/qwen3_apple_style_2bit_qat_lora/'\n",
    "os.chdir(new_path)\n",
    "!ls -ltr {CHECKPOINT}\n",
    "!git pull\n",
    "lora_flag = f\"--lora-r {LORA_R}\" if LORA_R > 0 else \"\"\n",
    "verbose_flag = \"--verbose\" if VERBOSE else \"\"\n",
    "#!python scripts/scripts/gdrive_sync.py  {CHECKPOINT}\"\n",
    "!python scripts/measure_perplexity.py \"{CHECKPOINT}\" \\\n",
    "    --model {MODEL_NAME} \\\n",
    "    --max-length {MAX_LENGTH} \\\n",
    "    --stride {STRIDE} \\\n",
    "    --device {DEVICE} \\\n",
    "    {lora_flag} \\\n",
    "    {verbose_flag} --dtype fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNDg0ZLuMgm1"
   },
   "source": [
    "## 3. Compare Multiple Checkpoints (Optional)\n",
    "\n",
    "Compare perplexity across training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wk0o7RShMgm1"
   },
   "outputs": [],
   "source": [
    "#@title List available checkpoints\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "checkpoint_dir = Path(CHECKPOINT).parent\n",
    "if checkpoint_dir.exists():\n",
    "    checkpoints = sorted(checkpoint_dir.glob(\"*.pt\"))\n",
    "    print(f\"Found {len(checkpoints)} checkpoints in {checkpoint_dir}:\")\n",
    "    for ckpt in checkpoints[-10:]:  # Show last 10\n",
    "        size_mb = ckpt.stat().st_size / 1024 / 1024\n",
    "        print(f\"  {ckpt.name:<50} {size_mb:.1f} MB\")\n",
    "else:\n",
    "    print(f\"Directory not found: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NKIg2bwRMgm1"
   },
   "outputs": [],
   "source": [
    "#@title Batch measure multiple checkpoints\n",
    "CHECKPOINTS_TO_MEASURE = [\n",
    "    # Add checkpoint paths here\n",
    "    # \"runs/SR-011/checkpoint_step1000.pt\",\n",
    "    # \"runs/SR-011/checkpoint_step2000.pt\",\n",
    "]\n",
    "\n",
    "results = []\n",
    "for ckpt in CHECKPOINTS_TO_MEASURE:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Measuring: {ckpt}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    !python scripts/measure_perplexity.py \"{ckpt}\" \\\n",
    "        --model {MODEL_NAME} \\\n",
    "        --max-length {MAX_LENGTH} \\\n",
    "        --stride {STRIDE} \\\n",
    "        --device {DEVICE}"
   ]
  },
  {
   "cell_type": "code",
   "source": "#@title Upload perplexity results to Google Drive\n# Saves results/perplexity.json to Google Drive for persistence across sessions\nimport os\nimport json\nimport subprocess\nfrom pathlib import Path\n\n# Ensure we're in the correct working directory\nREPO_ROOT = \"/content/qwen3_apple_style_2bit_qat_lora\"\nif os.path.exists(REPO_ROOT):\n    os.chdir(REPO_ROOT)\nprint(f\"Working directory: {os.getcwd()}\")\n\nGDRIVE_RESULTS = \"/content/drive/MyDrive/qat_runs/perplexity.json\"\nLOCAL_RESULTS = \"results/perplexity.json\"\n\nif os.path.exists(LOCAL_RESULTS):\n    # Create GDrive directory if needed\n    os.makedirs(os.path.dirname(GDRIVE_RESULTS), exist_ok=True)\n    \n    # Use subprocess to check if GDrive file exists (more reliable with FUSE)\n    result = subprocess.run([\"test\", \"-f\", GDRIVE_RESULTS], capture_output=True)\n    gdrive_exists = result.returncode == 0\n    \n    if gdrive_exists:\n        # Merge: local results take precedence (they're newer)\n        with open(GDRIVE_RESULTS, 'r') as f:\n            gdrive_data = json.load(f)\n        with open(LOCAL_RESULTS, 'r') as f:\n            local_data = json.load(f)\n        # Local results overwrite GDrive for same keys\n        merged = {**gdrive_data, **local_data}\n        with open(GDRIVE_RESULTS, 'w') as f:\n            json.dump(merged, f, indent=2)\n        print(f\"✓ Merged and uploaded: {len(merged)} results\")\n        print(f\"  (GDrive: {len(gdrive_data)}, Local: {len(local_data)})\")\n    else:\n        # Use shell cp (more reliable with GDrive FUSE)\n        subprocess.run([\"cp\", LOCAL_RESULTS, GDRIVE_RESULTS], check=True)\n        print(f\"✓ Uploaded: {LOCAL_RESULTS}\")\n    \n    print(f\"  -> {GDRIVE_RESULTS}\")\n    \n    # Show summary\n    with open(GDRIVE_RESULTS, 'r') as f:\n        data = json.load(f)\n    print(f\"  Total results on GDrive: {len(data)}\")\nelse:\n    print(f\"No local results to upload: {LOCAL_RESULTS}\")\n    print(\"  Run measurements first.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#@title Show all saved perplexity results\n# Results are automatically saved to results/perplexity.json after each measurement\n!python scripts/measure_perplexity.py --list",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXU0XJGxMgm1"
   },
   "source": [
    "## 4. Using Cache Data (Alternative)\n",
    "\n",
    "If WikiText-2 download fails, use existing KD cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S0IZ3LQzMgm1"
   },
   "outputs": [],
   "source": [
    "#@title Measure with KD cache\n",
    "CACHE_DIR = \"caches/alpaca_chat_think_both_L128_K128\"  #@param {type:\"string\"}\n",
    "NUM_SAMPLES = 100  #@param {type:\"integer\"}\n",
    "\n",
    "!python scripts/measure_perplexity.py \"{CHECKPOINT}\" \\\n",
    "    --cache-dir \"{CACHE_DIR}\" \\\n",
    "    --num-samples {NUM_SAMPLES} \\\n",
    "    --model {MODEL_NAME} \\\n",
    "    --max-length {MAX_LENGTH} \\\n",
    "    --stride {STRIDE} \\\n",
    "    --device {DEVICE}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "V5E1"
  },
  "accelerator": "TPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}