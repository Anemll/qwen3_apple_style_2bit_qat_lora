{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "rRgC0uK43c2v",
      "metadata": {
        "id": "rRgC0uK43c2v"
      },
      "source": [
        "# Qwen3-0.6B — Apple-style QAT (2-bit / 4-bit) + KD + LoRA recovery\n",
        "\n",
        "This notebook mirrors the structure of common “phone deployment” notebooks, but uses **this repo’s** pipeline:\n",
        "\n",
        "- **Stage A (recommended default):** KD-QAT on plain text (C4 streaming) or KD-cache QAT\n",
        "- **Stage B:** LoRA recovery (either SFT or cached KD-LoRA)\n",
        "- Plot `loss.csv`\n",
        "- Run inference sanity checks\n",
        "\n",
        "Notes:\n",
        "- Qwen3 requires `transformers>=4.51.0`.\n",
        "- For disk usage: C4 is huge; prefer `--streaming` unless you explicitly want to download.\n",
        "- Bitwidth: use `-q 2` (default) or `-q 4` (less aggressive). Checkpoints persist the bitwidth per layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i6Pb9Kki3c2w",
      "metadata": {
        "id": "i6Pb9Kki3c2w"
      },
      "source": [
        "## 0) Setup (Colab / local)\n",
        "\n",
        "If you’re in Colab, clone the repo. If you’re already in the repo directory locally, you can skip this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0cfd00c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Config (edit these) ----\n",
        "MODEL_NAME = 'Qwen/Qwen3-0.6B'\n",
        "TEACHER_NAME = MODEL_NAME\n",
        "QUANT_BITS = 4  # 2 or 4\n",
        "DEVICE = 'auto'\n",
        "AMP_DTYPE = 'auto'\n",
        "PARAM_DTYPE = 'auto'\n",
        "DTYPE = 'auto'\n",
        "\n",
        "# Cache dirs\n",
        "CACHE_DIR_CHAT = 'caches/alpaca_chat_think_both_L128_K32_R256'\n",
        "CACHE_DIR_TEXT = 'caches/c4_qwen3_L64_K32_R256'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u2vonfu23c2w",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u2vonfu23c2w",
        "outputId": "b965d1e3-c14a-46e3-c2ee-24ccd229b369"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'qwen3_apple_style_2bit_qat_lora'...\n",
            "remote: Enumerating objects: 131, done.\u001b[K\n",
            "remote: Counting objects: 100% (131/131), done.\u001b[K\n",
            "remote: Compressing objects: 100% (82/82), done.\u001b[K\n",
            "remote: Total 131 (delta 68), reused 109 (delta 46), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (131/131), 99.37 KiB | 16.56 MiB/s, done.\n",
            "Resolving deltas: 100% (68/68), done.\n",
            "/content/qwen3_apple_style_2bit_qat_lora\n"
          ]
        }
      ],
      "source": [
        "# Colab-only:\n",
        "!git clone https://github.com/Anemll/qwen3_apple_style_2bit_qat_lora\n",
        "%cd qwen3_apple_style_2bit_qat_lora\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZUmiISSL3c2w",
      "metadata": {
        "id": "ZUmiISSL3c2w"
      },
      "source": [
        "## 1) Install dependencies (uv)\n",
        "\n",
        "This repo is set up to work with `uv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ACRjVuVa3c2x",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACRjVuVa3c2x",
        "outputId": "ec4f6742-fbc4-4e05-da92-74246fc64cab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/22.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/22.2 MB\u001b[0m \u001b[31m167.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/22.2 MB\u001b[0m \u001b[31m195.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m21.2/22.2 MB\u001b[0m \u001b[31m264.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m250.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m119.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m7 packages\u001b[0m \u001b[2min 134ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m62 packages\u001b[0m \u001b[2min 361ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 798ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 0.87ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mqat-lora\u001b[0m\u001b[2m==0.0.0 (from file:///content/qwen3_apple_style_2bit_qat_lora)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip -q install uv\n",
        "!uv pip install -r requirements.txt\n",
        "!uv pip install -e .\n",
        "# plotting\n",
        "!uv pip install -q matplotlib\n",
        "!uv pip install -q plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yfoLPbBk3c2x",
      "metadata": {
        "id": "yfoLPbBk3c2x"
      },
      "source": [
        "## 2) Optional: Hugging Face login\n",
        "\n",
        "If you hit gated model/dataset errors, log in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wLMWX7E23c2x",
      "metadata": {
        "id": "wLMWX7E23c2x"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import login\n",
        "# login()  # paste token when prompted\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fA37ilf33c2x",
      "metadata": {
        "id": "fA37ilf33c2x"
      },
      "source": [
        "## 3) Quick environment check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VUCXehkU3c2x",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUCXehkU3c2x",
        "outputId": "8e7ed691-bb60-414d-9a2a-65a69cfa3a49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch 2.9.0+cu126\n",
            "transformers 4.57.3\n",
            "cuda True\n",
            "mps False\n"
          ]
        }
      ],
      "source": [
        "import torch, transformers\n",
        "print('torch', torch.__version__)\n",
        "print('transformers', transformers.__version__)\n",
        "print('cuda', torch.cuda.is_available())\n",
        "print('mps', torch.backends.mps.is_available())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GCXUYtVC3c2x",
      "metadata": {
        "id": "GCXUYtVC3c2x"
      },
      "source": [
        "## 4) Stage A ((Optional)): KD-QAT on streaming C4\n",
        "\n",
        "This preserves the base model’s behavior under low-bit fake-quant weights.\n",
        "\n",
        "Tips:\n",
        "- Start with a small run (`--max_steps 50`) to validate the pipeline.\n",
        "- Use `-q 4` if 2-bit is too unstable; 4-bit is less aggressive.\n",
        "- On MPS, prefer `--ema_decay 0` for KD-QAT.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jERGktjwjz29",
      "metadata": {
        "id": "jERGktjwjz29"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TpT02cskrs6D",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TpT02cskrs6D",
        "outputId": "3994d5e7-2ee7-47b6-ecbf-fabe4f8e90d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[device] cuda | dtype=torch.bfloat16\n",
            "2025-12-17 23:40:12.337234: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-17 23:40:12.357384: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766014812.382692   39984 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766014812.388150   39984 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766014812.402070   39984 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766014812.402094   39984 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766014812.402097   39984 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766014812.402099   39984 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-17 23:40:12.406247: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "README.md: 7.47kB [00:00, 26.7MB/s]\n",
            "data/train-00000-of-00001-a09b74b3ef9c3b(…): 100% 24.2M/24.2M [00:01<00:00, 19.0MB/s]\n",
            "Generating train split: 100% 52002/52002 [00:00<00:00, 409867.46 examples/s]\n",
            "[write] shard_00000.pt | N=512\n",
            "[write] shard_00001.pt | N=512\n",
            "[write] shard_00002.pt | N=512\n",
            "[write] shard_00003.pt | N=512\n",
            "[write] shard_00004.pt | N=512\n",
            "[write] shard_00005.pt | N=512\n",
            "[write] shard_00006.pt | N=512\n",
            "[write] shard_00007.pt | N=512\n",
            "[write] shard_00008.pt | N=512\n",
            "[write] shard_00009.pt | N=512\n",
            "[write] shard_00010.pt | N=512\n",
            "[write] shard_00011.pt | N=512\n",
            "[write] shard_00012.pt | N=512\n",
            "[write] shard_00013.pt | N=512\n",
            "[write] shard_00014.pt | N=512\n",
            "[write] shard_00015.pt | N=512\n",
            "[write] shard_00016.pt | N=512\n",
            "[write] shard_00017.pt | N=512\n",
            "[write] shard_00018.pt | N=512\n",
            "[write] shard_00019.pt | N=512\n",
            "[write] shard_00020.pt | N=512\n",
            "[write] shard_00021.pt | N=512\n",
            "[write] shard_00022.pt | N=512\n",
            "[write] shard_00023.pt | N=512\n",
            "[write] shard_00024.pt | N=512\n",
            "[write] shard_00025.pt | N=512\n",
            "[write] shard_00026.pt | N=512\n",
            "[write] shard_00027.pt | N=512\n",
            "[write] shard_00028.pt | N=512\n",
            "[write] shard_00029.pt | N=512\n",
            "[write] shard_00030.pt | N=512\n",
            "[write] shard_00031.pt | N=512\n",
            "[write] shard_00032.pt | N=512\n",
            "[write] shard_00033.pt | N=512\n",
            "[write] shard_00034.pt | N=512\n",
            "[write] shard_00035.pt | N=512\n",
            "[write] shard_00036.pt | N=512\n",
            "[write] shard_00037.pt | N=512\n",
            "[write] shard_00038.pt | N=512\n",
            "[write] shard_00039.pt | N=32\n",
            "Done. Cached sequences=20000 | out_dir=caches/alpaca_chat_think_both_L128_K32_R256\n"
          ]
        }
      ],
      "source": [
        "# generate thinking dataset, SKIP if you copy from GD\n",
        "!python scripts/precompute_teacher_topk.py \\\n",
        "  --teacher_model_name_or_path {MODEL_NAME} \\\n",
        "  --dataset_name tatsu-lab/alpaca \\\n",
        "  --dataset_split train \\\n",
        "  --dataset_format alpaca_chat \\\n",
        "  --enable_thinking both \\\n",
        "  --max_length 128 \\\n",
        "  --topk 32 \\\n",
        "  --rand_neg 256 \\\n",
        "  --num_sequences 20000 \\\n",
        "  --batch_size 1 \\\n",
        "  --shard_size 512 \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  --output_dir {CACHE_DIR_CHAT}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rlxJ3X8a3c2x",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rlxJ3X8a3c2x",
        "outputId": "a3a21f5c-ef21-4629-d2c1-2015ee274ee4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[device] cuda | dtype=torch.bfloat16\n",
            "2025-12-18 02:00:07.257740: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-18 02:00:07.277713: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766023207.303615   12696 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766023207.309183   12696 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766023207.323190   12696 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766023207.323214   12696 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766023207.323217   12696 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766023207.323220   12696 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-18 02:00:07.327471: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "README.md: 41.1kB [00:00, 96.2MB/s]\n",
            "Resolving data files: 100% 1024/1024 [00:03<00:00, 302.24it/s]\n",
            "Resolving data files: 100% 1024/1024 [00:00<00:00, 28014.37it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/qwen3_apple_style_2bit_qat_lora/scripts/precompute_teacher_topk.py\", line 378, in main\n",
            "    out_obj = teacher(input_ids=input_ids, attention_mask=attn, use_cache=False, return_dict=True)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 918, in wrapper\n",
            "    output = func(self, *args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 480, in forward\n",
            "    outputs: BaseModelOutputWithPast = self.model(\n",
            "                                       ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 1072, in wrapper\n",
            "    outputs = func(self, *args, **kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 410, in forward\n",
            "    hidden_states = decoder_layer(\n",
            "                    ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\", line 94, in __call__\n",
            "    return super().__call__(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 260, in forward\n",
            "    hidden_states, _ = self.self_attn(\n",
            "                       ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\", line 229, in forward\n",
            "    attn_output = self.o_proj(attn_output)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\", line 134, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/qwen3_apple_style_2bit_qat_lora/scripts/precompute_teacher_topk.py\", line 439, in <module>\n",
            "    main()\n",
            "  File \"/content/qwen3_apple_style_2bit_qat_lora/scripts/precompute_teacher_topk.py\", line 367, in main\n",
            "    with torch.inference_mode():\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/autograd/grad_mode.py\", line 288, in __exit__\n",
            "    self._inference_mode_context.__exit__(exc_type, exc_value, traceback)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "# generate standard dataset, SKIP if you COPY from Google Drive!\n",
        "import os\n",
        "\n",
        "CACHE_DIR = CACHE_DIR_TEXT\n",
        "\n",
        "if not os.path.isdir(CACHE_DIR):\n",
        "    print(f\"[cache] {CACHE_DIR} not found → generating cache\")\n",
        "\n",
        "    !python scripts/precompute_teacher_topk.py \\\n",
        "      --teacher_model_name_or_path {MODEL_NAME} \\\n",
        "      --dataset_name allenai/c4 \\\n",
        "      --dataset_config_name en \\\n",
        "      --dataset_split train \\\n",
        "      --dataset_text_field text \\\n",
        "      --streaming \\\n",
        "      --shuffle_buffer 10000 \\\n",
        "      --max_length 64 \\\n",
        "      --topk 32 \\\n",
        "      --rand_neg 256 \\\n",
        "      --num_sequences 2000 \\\n",
        "      --batch_size 1 \\\n",
        "      --shard_size 512 \\\n",
        "      --device {DEVICE} \\\n",
        "      --dtype {DTYPE} \\\n",
        "      --output_dir {CACHE_DIR}\n",
        "\n",
        "else:\n",
        "    print(f\"[cache] {CACHE_DIR} already exists → skipping generation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vlhuS4N9GbN4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlhuS4N9GbN4",
        "outputId": "f2690e35-43d5-4a73-a87a-eeb5550e290a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "        218.38M 100%   10.69MB/s    0:00:19 (xfr#5, to-chk=0/6)\n",
            "          4.40G 100%   11.82MB/s    0:05:55 (xfr#41, to-chk=0/42)\n"
          ]
        }
      ],
      "source": [
        "#moutn GD and copy training sets!\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#copy back from drive\n",
        "!mkdir -p caches\n",
        "\n",
        "!rsync -ah --info=progress2 \\\n",
        "  /content/drive/MyDrive/qwen3_caches/c4_qwen3_L64_K32_R256/ \\\n",
        "  caches/c4_qwen3_L64_K32_R256/\n",
        "\n",
        "!rsync -ah --info=progress2 \\\n",
        "  /content/drive/MyDrive/qwen3_caches/alpaca_chat_think_both_L128_K32_R256/ \\\n",
        "  caches/alpaca_chat_think_both_L128_K32_R256\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IIIWEkllwGEA",
      "metadata": {
        "collapsed": true,
        "id": "IIIWEkllwGEA"
      },
      "outputs": [],
      "source": [
        "#copy training sets to GD\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir -p /content/drive/MyDrive/qwen3_caches\n",
        "#!cp -rv caches/c4_qwen3_L64_K32_R256 /content/drive/MyDrive/qwen3_caches/\n",
        "#!cp -rv caches/c4_qwen3_L64_K32_R256 /content/drive/MyDrive/qwen3_caches/\n",
        "\n",
        "\n",
        "#!cp -rv caches/c4_qwen3_L64_K32_R256 /content/drive/MyDrive/qwen3_caches/\n",
        "#!cp -rv caches/alpaca_chat_think_both_L128_K32_R256  /content/drive/MyDrive/qwen3_caches/\n",
        "\n",
        "!rsync -ah --progress caches/c4_qwen3_L64_K32_R256 /content/drive/MyDrive/qwen3_caches/\n",
        "!rsync -ah --progress caches/alpaca_chat_think_both_L128_K32_R256 /content/drive/MyDrive/qwen3_caches/\n",
        "\n",
        "#cp -r caches/alpaca_chat_think_both_L128_K32_R256 \\\n",
        "#      /content/drive/MyDrive/qwen3_caches/\n",
        "#RUN_DIR = \"runs/qwen3_kdqat_stream_q2\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A7UPbdfK3c2x",
      "metadata": {
        "id": "A7UPbdfK3c2x"
      },
      "outputs": [],
      "source": [
        "# DISABLED --- NOTE used! see #5 for first QAT step\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --teacher_model_name_or_path {MODEL_NAME} \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --dataset_name allenai/c4 \\\n",
        "  --dataset_config_name en \\\n",
        "  --dataset_split train \\\n",
        "  --dataset_format text \\\n",
        "  --dataset_text_field text \\\n",
        "  --streaming \\\n",
        "  --shuffle_buffer 10000 \\\n",
        "  --output_dir {RUN_DIR} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 1 \\\n",
        "  --gradient_accumulation_steps 16 \\\n",
        "  --learning_rate 5e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 50 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --logging_steps 10 \\\n",
        "  --save_steps 50\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UmgbC-RI3c2x",
      "metadata": {
        "id": "UmgbC-RI3c2x"
      },
      "source": [
        "### (Optional) Resume\n",
        "\n",
        "`--resume_from_checkpoint auto` resolves to `checkpoint_last.pt` if it exists in the output directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mJqdmXSA3c2x",
      "metadata": {
        "id": "mJqdmXSA3c2x"
      },
      "outputs": [],
      "source": [
        "# !python scripts/train_qat.py ... --output_dir {RUN_DIR} --max_steps 500 --resume_from_checkpoint auto\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R7UZBhau3c2x",
      "metadata": {
        "id": "R7UZBhau3c2x"
      },
      "source": [
        "## 5)  KD-cache: precompute teacher top-k + negatives\n",
        "\n",
        "Cache mode is MPS-friendly:\n",
        "- no teacher model during training\n",
        "- no full-vocab logits\n",
        "\n",
        "If you see good KD loss but bad greedy decoding, increase negative coverage (`--rand_neg`) and/or add hard top-1 terms:\n",
        "- `--hard-top1-weight 0.05`\n",
        "- `--hard-full-top1-weight 0.02`–`0.05`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g4s_PzQW3c2x",
      "metadata": {
        "id": "g4s_PzQW3c2x"
      },
      "source": [
        "### KD-cache QAT training\n",
        "\n",
        "This uses cached teacher signals + candidate softmax."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06LaYj0vPIE7",
      "metadata": {
        "id": "06LaYj0vPIE7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1Li8Aysa3c2x",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Li8Aysa3c2x",
        "outputId": "7d789931-4a50-4ef3-d3c6-0c900221a525"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-18 14:36:50.092192: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-18 14:36:50.109207: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766068610.130796    5337 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766068610.137293    5337 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766068610.153610    5337 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766068610.153638    5337 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766068610.153641    5337 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766068610.153643    5337 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-18 14:36:50.158497: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[device] cuda | amp_dtype=torch.bfloat16 | param_dtype=torch.bfloat16\n",
            "tokenizer_config.json: 9.73kB [00:00, 36.7MB/s]\n",
            "vocab.json: 2.78MB [00:00, 118MB/s]\n",
            "merges.txt: 1.67MB [00:00, 137MB/s]\n",
            "tokenizer.json: 100% 11.4M/11.4M [00:01<00:00, 6.97MB/s]\n",
            "config.json: 100% 726/726 [00:00<00:00, 8.41MB/s]\n",
            "model.safetensors: 100% 1.50G/1.50G [00:03<00:00, 407MB/s]\n",
            "generation_config.json: 100% 239/239 [00:00<00:00, 2.74MB/s]\n",
            "[qat] weight_bits=2\n",
            "[kd-cache] cache topk=32\n",
            "[kd-cache] dir=caches/alpaca_chat_think_both_L128_K32_R256 | weight=1.0 | T=2.0 | hard_top1=0.05 | hard_full_top1=0.03\n",
            "[freeze] frozen_elements=97517568 | trainable_params=447/506\n",
            "opt_step: 100% 1000/1000 [18:28<00:00,  1.11s/step, loss=3.1581, lr=0.00e+00]\n",
            "Done. QAT checkpoint saved to: runs/qwen3_kdqat_cache_q2/qat_state_dict.pt\n"
          ]
        }
      ],
      "source": [
        "#stage 1 - conservative run freeze MLP and Attention\n",
        "#CACHE_DIR = CACHE_DIR_TEXT\n",
        "CACHE_DIR = \"caches/alpaca_chat_think_both_L128_K32_R256\"\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2\"\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --output_dir {RUN_DIR_CACHE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 128 \\\n",
        "  --gradient_accumulation_steps 1 \\\n",
        "  --learning_rate 5e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 1000 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 5 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.05 \\\n",
        "  --hard-full-top1-weight 0.03 \\\n",
        "  --ov-freeze \\\n",
        "  --freeze-last-mlp \\\n",
        "  --freeze-last-mlp-layers 1\n",
        "  #  --save_steps 500 \\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JUvQIIDeRUF6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUvQIIDeRUF6",
        "outputId": "1525d264-e6c6-426d-fb8e-0ff57347322e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-18 14:56:30.687661: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-18 14:56:30.704782: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766069790.726044   10483 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766069790.732532   10483 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766069790.748388   10483 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766069790.748417   10483 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766069790.748420   10483 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766069790.748423   10483 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-18 14:56:30.753248: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[device] cuda | amp_dtype=torch.bfloat16 | param_dtype=torch.bfloat16\n",
            "[qat] weight_bits=2\n",
            "[init] loading model state from runs/qwen3_kdqat_cache_q2/qat_state_dict.pt\n",
            "[init] model weights loaded.\n",
            "[kd-cache] cache topk=32\n",
            "[kd-cache] dir=caches/alpaca_chat_think_both_L128_K32_R256 | weight=1.0 | T=2.0 | hard_top1=0.02 | hard_full_top1=0.01\n",
            "[freeze] frozen_elements=88080384 | trainable_params=450/506\n",
            "opt_step: 100% 1000/1000 [18:26<00:00,  1.11s/step, loss=2.2086, lr=0.00e+00]\n",
            "Done. QAT checkpoint saved to: runs/qwen3_kdqat_cache_q2_2/qat_state_dict.pt\n"
          ]
        }
      ],
      "source": [
        "# Starge 2 resume KD-QAT with unfrozen layers\n",
        "CACHE_DIR = \"caches/alpaca_chat_think_both_L128_K32_R256\"\n",
        "INIT_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2\"\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_DIR_CACHE}/qat_state_dict.pt \\\n",
        "  --output_dir {RUN_DIR_CACHE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 128 \\\n",
        "  --gradient_accumulation_steps 1 \\\n",
        "  --learning_rate 5e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 1000 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --ov-freeze \\\n",
        "  --hard-top1-weight 0.02 \\\n",
        "  --hard-full-top1-weight 0.01\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KXQUTgEJfzl4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXQUTgEJfzl4",
        "outputId": "828b7fad-a610-4afe-8df2-7fd2177279b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-18 15:15:24.480459: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-18 15:15:24.498705: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766070924.519620   15385 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766070924.525944   15385 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766070924.542169   15385 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766070924.542196   15385 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766070924.542199   15385 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766070924.542201   15385 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-18 15:15:24.546889: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[device] cuda | amp_dtype=torch.bfloat16 | param_dtype=torch.bfloat16\n",
            "[qat] weight_bits=2\n",
            "[init] loading model state from runs/qwen3_kdqat_cache_q2_2/qat_state_dict.pt\n",
            "[init] model weights loaded.\n",
            "[kd-cache] cache topk=32\n",
            "[kd-cache] dir=caches/alpaca_chat_think_both_L128_K32_R256 | weight=1.0 | T=2.0 | hard_top1=0.0 | hard_full_top1=0.0005\n",
            "opt_step: 100% 1000/1000 [18:33<00:00,  1.11s/step, loss=1.6925, lr=0.00e+00]\n",
            "Done. QAT checkpoint saved to: runs/qwen3_kdqat_cache_q2_3/qat_state_dict.pt\n"
          ]
        }
      ],
      "source": [
        "# Stage 3 resume KD-QAT with unfrozen attention and relaxed hard-top/full!\n",
        "\n",
        "CACHE_DIR = \"caches/alpaca_chat_think_both_L128_K32_R256\"\n",
        "INIT_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_3\"\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_DIR_CACHE}/qat_state_dict.pt \\\n",
        "  --output_dir {RUN_DIR_CACHE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 128 \\\n",
        "  --gradient_accumulation_steps 1 \\\n",
        "  --learning_rate 5e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 1000 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.00 \\\n",
        "  --hard-full-top1-weight 0.0005\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q1Zjl_WnHZ4O",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1Zjl_WnHZ4O",
        "outputId": "70760464-2645-4b89-8a8f-d4467e95a477"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-18 15:40:54.295253: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-18 15:40:54.313779: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766072454.335347   22166 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766072454.341994   22166 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766072454.358756   22166 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766072454.358784   22166 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766072454.358788   22166 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766072454.358790   22166 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-18 15:40:54.363703: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[device] cuda | amp_dtype=torch.bfloat16 | param_dtype=torch.bfloat16\n",
            "[qat] weight_bits=2\n",
            "[init] loading model state from runs/qwen3_kdqat_cache_q2_3/qat_state_dict.pt\n",
            "[init] model weights loaded.\n",
            "[kd-cache] cache topk=32\n",
            "[kd-cache] dir=caches/alpaca_chat_think_both_L128_K32_R256 | weight=1.0 | T=2.0 | hard_top1=0.0 | hard_full_top1=0.0\n",
            "opt_step: 100% 500/500 [10:54<00:00,  1.31s/step, loss=1.6272, lr=0.00e+00]\n",
            "Done. QAT checkpoint saved to: runs/qwen3_kdqat_cache_q2_4/qat_state_dict.pt\n"
          ]
        }
      ],
      "source": [
        "# Stage 4 resume KD-QAT with unfrozen attention and relaxed hard-top/full!\n",
        "#   --hard-full-top1-weight 0.0000\n",
        "#   learning_rate 2e-6\n",
        "CACHE_DIR = \"caches/alpaca_chat_think_both_L128_K32_R256\"\n",
        "INIT_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_3\"\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_4\"\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_DIR_CACHE}/qat_state_dict.pt \\\n",
        "  --output_dir {RUN_DIR_CACHE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 160 \\\n",
        "  --gradient_accumulation_steps 1 \\\n",
        "  --learning_rate 2e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 500 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 5 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.00 \\\n",
        "  --hard-full-top1-weight 0.0000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yq8GHX0o3c2y",
      "metadata": {
        "id": "Yq8GHX0o3c2y"
      },
      "source": [
        "## 6) Stage B: LoRA recovery\n",
        "\n",
        "Two options:\n",
        "- **SFT LoRA** (Alpaca-style instruction tuning)\n",
        "- **Cached KD-LoRA** (preserve teacher distribution; no new “skills”)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B1yimj8aS11y",
      "metadata": {
        "id": "B1yimj8aS11y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LcGFKyxO3c2y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcGFKyxO3c2y",
        "outputId": "3b1dfd05-f7e2-4f74-dca7-07a35f9bafdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[device] cuda | amp_dtype=torch.bfloat16 | param_dtype=torch.bfloat16\n",
            "2025-12-18 16:59:07.388152: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-18 16:59:07.408019: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766077147.432804   43555 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766077147.438097   43555 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766077147.451543   43555 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766077147.451566   43555 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766077147.451569   43555 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766077147.451572   43555 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-18 16:59:07.455677: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[qat] weight_bits=2\n",
            "Loaded QAT checkpoint. missing=0 unexpected=0\n",
            "Enabled LoRA on (196, 40370176) layers. Trainable params: 40,370,176\n",
            "[kd_cache] Note: cache max_length=128 (you passed --max_length=1024). --max_length is ignored in cache mode.\n",
            "[kd_cache] cache topk=32\n",
            "[kd_cache] Enabled cached KD-LoRA. T=2.0 weight=1.0 hard_top1=0.02 hard_full_top1=0.01\n",
            "[loss.csv] Rotated existing loss.csv (last_step=44) -> loss_prev_20251218_165918.csv\n",
            "opt_step: 100% 1500/1500 [19:59<00:00,  1.25step/s, loss=1.8993, lr=0.00e+00]\n",
            "Done. Saved LoRA adapter to: runs/qwen3_lora_recovery_cached_r64/lora_only_state_dict.pt\n"
          ]
        }
      ],
      "source": [
        "CACHE_DIR = \"caches/alpaca_chat_think_both_L128_K32_R256\"\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "LORA_DIM = 64\n",
        "LORA_RUN_KD = f\"runs/qwen3_lora_recovery_cached_r{LORA_DIM}\" # Define LORA_RUN_KD as well\n",
        "\n",
        "#   --max_steps 438 \\\n",
        "!python scripts/train_lora_recovery.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR_CACHE}/final_state_dict.pt \\\n",
        "  --output_dir {LORA_RUN_KD} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --per_device_train_batch_size 128 \\\n",
        "  --gradient_accumulation_steps 1 \\\n",
        "  --learning_rate 1e-5 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 1500 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 2 \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} \\\n",
        "  --lora_alpha {LORA_DIM} \\\n",
        "  --lora_dropout 0.0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.02 \\\n",
        "  --hard-full-top1-weight 0.01\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Defule6L3c2y",
      "metadata": {
        "id": "Defule6L3c2y"
      },
      "source": [
        "## 7) Plot loss\n",
        "\n",
        "In Colab, use `--no_show` + `--save` then display the PNG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QM1RNfDh3c2y",
      "metadata": {
        "id": "QM1RNfDh3c2y"
      },
      "outputs": [],
      "source": [
        "!python scripts/plot_loss.py --run_dir {RUN_DIR} --source csv --no_show --save {RUN_DIR}/loss.png\n",
        "from PIL import Image\n",
        "display(Image.open(f\"{RUN_DIR}/loss.png\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p3bOUiMt3c2y",
      "metadata": {
        "id": "p3bOUiMt3c2y"
      },
      "source": [
        "## 8) Inference sanity checks\n",
        "\n",
        "Greedy decode (`--do_sample false`) and keep outputs short (`--max_new_tokens 16`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aA_cJHvT3c2y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aA_cJHvT3c2y",
        "outputId": "8e2f4d38-621e-44d2-fa3d-6f7a62867c84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-18 17:21:38.241432: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-18 17:21:38.261766: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766078498.286912   49388 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766078498.292409   49388 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766078498.306387   49388 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766078498.306410   49388 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766078498.306413   49388 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766078498.306416   49388 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-18 17:21:38.310559: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[lora] loaded adapters for 196 layers from runs/qwen3_lora_recovery_cached_r64/lora_only_state_dict.pt\n",
            "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "user\n",
            "What is capital of France?\n",
            "assistant\n",
            "<think>\n",
            "</think>\n",
            "\n",
            "The capital of France is approximately $10000000000000000000000000000\n"
          ]
        }
      ],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --lora_checkpoint \"runs/qwen3_lora_recovery_cached_r64/lora_only_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
        "  --prompt \"What is capital of France?\" \\\n",
        "  --do_sample false \\\n",
        "  --enable_thinking true \\\n",
        "  --max_new_tokens 40\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cD985HdXlm0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cD985HdXlm0",
        "outputId": "927291c4-1309-42a5-b857-f9b5608a862a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-18 17:26:48.407223: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-18 17:26:48.427401: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766078808.452492   50954 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766078808.457837   50954 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766078808.471491   50954 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766078808.471516   50954 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766078808.471519   50954 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766078808.471522   50954 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-18 17:26:48.475694: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[lora] loaded adapters for 196 layers from runs/qwen3_lora_recovery_cached_r64/lora_only_state_dict.pt\n",
            "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "user\n",
            "What is Apple Neural Engine?\n",
            "assistant\n",
            "<think>\n",
            "</think>\n",
            "\n",
            "The **Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla Tesla\n"
          ]
        }
      ],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --lora_checkpoint \"runs/qwen3_lora_recovery_cached_r64/lora_only_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
        "  --prompt \"What is Apple Neural Engine?\" \\\n",
        "  --do_sample false \\\n",
        "  --enable_thinking true \\\n",
        "  --max_new_tokens 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xZ6aSDAa3c2y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZ6aSDAa3c2y",
        "outputId": "c1a67a91-e1a6-4553-b0f9-17d5fb5f7949"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-18 17:26:15.784397: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-18 17:26:15.804707: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766078775.830117   50751 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766078775.835578   50751 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766078775.849485   50751 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766078775.849509   50751 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766078775.849513   50751 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766078775.849515   50751 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-18 17:26:15.853760: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[lora] loaded adapters for 196 layers from runs/qwen3_lora_recovery_cached_r64/lora_only_state_dict.pt\n",
            "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "user\n",
            "2+2=\n",
            "assistant\n",
            "<think>\n",
            "Okay, I can't't't be too too to be a good source of energy.\n"
          ]
        }
      ],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --lora_checkpoint \"runs/qwen3_lora_recovery_cached_r64/lora_only_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
        "  --prompt \"2+2=\" \\\n",
        "  --do_sample false \\\n",
        "  --enable_thinking true \\\n",
        "  --max_new_tokens 90\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZOIo97VlyAbh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOIo97VlyAbh",
        "outputId": "59872cb3-0827-44a0-df78-f53d1ac048a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-18 15:56:05.695278: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-18 15:56:05.715142: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766073365.739846   26345 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766073365.745393   26345 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766073365.759608   26345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766073365.759637   26345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766073365.759641   26345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766073365.759645   26345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-18 15:56:05.763775: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "user\n",
            "What is capital of France?\n",
            "assistant\n",
            "<think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Digitalization of agriculture is a essential important process in which understanding of agriculture and agriculture is a crucial process in which which is essential for understanding of agriculture and agriculture, and it is essential to analyze and understanding of agriculture and agriculture.\n"
          ]
        }
      ],
      "source": [
        "LORA_DIM = 64\n",
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2\"\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
        "  --prompt \"What is capital of France?\" \\\n",
        "  --do_sample true \\\n",
        "  --max_new_tokens 64\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d03aadb",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "Ef_Q8P0i3c2y",
      "metadata": {
        "id": "Ef_Q8P0i3c2y"
      },
      "source": [
        "## 9) Optional: snap weights to the exact grid\n",
        "\n",
        "This produces a float checkpoint with weights snapped to the N-bit codebook (not bitpacked)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jk_qSZIs3c2y",
      "metadata": {
        "id": "jk_qSZIs3c2y"
      },
      "outputs": [],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "!python scripts/hard_quantize_checkpoint.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}/checkpoint_last.pt \\\n",
        "  --output_path {RUN_DIR}/hard_quant_full_state_dict.pt \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7HnE7IFS3dK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7HnE7IFS3dK",
        "outputId": "2a19fa5f-28b0-4db8-f3a9-12e39aa64150"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'qwen3_apple_style_2bit_qat_lora'\n",
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Dec 17 21:49 \u001b[0m\u001b[01;34mqwen3_kdqat_cache_q2\u001b[0m/\n",
            "drwxr-xr-x 2 root root 4096 Dec 17 22:01 \u001b[01;34mqwen3_lora_recovery_cached\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "%cd qwen3_apple_style_2bit_qat_lora\n",
        "%ls -l runs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beQD2eyDWWnb",
      "metadata": {
        "id": "beQD2eyDWWnb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
