{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "rRgC0uK43c2v",
      "metadata": {
        "id": "rRgC0uK43c2v"
      },
      "source": [
        "# Qwen3-0.6B — Apple-style QAT (2-bit / 4-bit) + KD + LoRA recovery\n",
        "\n",
        "This notebook mirrors the structure of common “phone deployment” notebooks, but uses **this repo’s** pipeline:\n",
        "\n",
        "- **Stage A (recommended default):** KD-QAT on plain text (C4 streaming) or KD-cache QAT\n",
        "- **Stage B:** LoRA recovery (either SFT or cached KD-LoRA)\n",
        "- Plot `loss.csv`\n",
        "- Run inference sanity checks\n",
        "\n",
        "Notes:\n",
        "- Qwen3 requires `transformers>=4.51.0`.\n",
        "- For disk usage: C4 is huge; prefer `--streaming` unless you explicitly want to download.\n",
        "- Bitwidth: use `-q 2` (default) or `-q 4` (less aggressive). Checkpoints persist the bitwidth per layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i6Pb9Kki3c2w",
      "metadata": {
        "id": "i6Pb9Kki3c2w"
      },
      "source": [
        "## 0) Setup (Colab / local)\n",
        "\n",
        "If you’re in Colab, clone the repo. If you’re already in the repo directory locally, you can skip this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "UXlLPdtkGM35",
      "metadata": {
        "id": "UXlLPdtkGM35"
      },
      "outputs": [],
      "source": [
        "# ---- Config (edit these) ----\n",
        "#MODEL_NAME = 'Qwen/Qwen3-4B-Thinking-2507'\n",
        "MODEL_NAME = 'Qwen/Qwen3-0.6B'\n",
        "TEACHER_NAME = MODEL_NAME\n",
        "QUANT_BITS = 2  # 2 or 4\n",
        "DEVICE = 'auto'\n",
        "AMP_DTYPE = 'auto'\n",
        "PARAM_DTYPE = 'auto'\n",
        "DTYPE = 'auto'\n",
        "\n",
        "# Cache dirs\n",
        "CACHE_DIR_CHAT = 'caches/alpaca_chat_think_both_L128_K32_R256'\n",
        "CACHE_DIR_TEXT = 'caches/c4_qwen3_L64_K32_R256'\n",
        "#CACHE_DIR_CHAT = 'caches/Q4B_alpaca_chat_think_L128_K32_R256'\n",
        "#CACHE_DIR_TEXT = 'caches/Q4B_c4_qwen3_L64_K32_R256'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "u2vonfu23c2w",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u2vonfu23c2w",
        "outputId": "55d95536-f8e3-4307-8693-05cec37b09db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'qwen3_apple_style_2bit_qat_lora' already exists and is not an empty directory.\n",
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "Already up to date.\n",
            "HEAD is now at a4fd586 Add timing metrics to progressive QAT training\n"
          ]
        }
      ],
      "source": [
        "# Colab-only:\n",
        "%cd /content/\n",
        "!git clone https://github.com/Anemll/qwen3_apple_style_2bit_qat_lora\n",
        "%cd qwen3_apple_style_2bit_qat_lora\n",
        "!git fetch\n",
        "!git pull\n",
        "!git reset --hard HEAD\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZUmiISSL3c2w",
      "metadata": {
        "id": "ZUmiISSL3c2w"
      },
      "source": [
        "## 1) Install dependencies (uv)\n",
        "\n",
        "This repo is set up to work with `uv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ACRjVuVa3c2x",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACRjVuVa3c2x",
        "outputId": "8a59c6dc-87a1-4be7-9b52-61df3af5cf4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/22.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/22.2 MB\u001b[0m \u001b[31m205.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/22.2 MB\u001b[0m \u001b[31m234.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m246.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m246.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m114.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m7 packages\u001b[0m \u001b[2min 137ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m62 packages\u001b[0m \u001b[2min 342ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 797ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 0.98ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mqat-lora\u001b[0m\u001b[2m==0.0.0 (from file:///content/qwen3_apple_style_2bit_qat_lora)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip -q install uv\n",
        "!uv pip install -r requirements.txt\n",
        "!uv pip install -e .\n",
        "# plotting\n",
        "!uv pip install -q matplotlib\n",
        "!uv pip install -q plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yfoLPbBk3c2x",
      "metadata": {
        "id": "yfoLPbBk3c2x"
      },
      "source": [
        "## 2) Optional: Hugging Face login\n",
        "\n",
        "If you hit gated model/dataset errors, log in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wLMWX7E23c2x",
      "metadata": {
        "id": "wLMWX7E23c2x"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()  # paste token when prompted\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fA37ilf33c2x",
      "metadata": {
        "id": "fA37ilf33c2x"
      },
      "source": [
        "## 3) Quick environment check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "VUCXehkU3c2x",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUCXehkU3c2x",
        "outputId": "da600363-4122-4d54-ba48-d6f8f2c73913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch 2.9.0+cu126\n",
            "transformers 4.57.3\n",
            "cuda True\n",
            "mps False\n"
          ]
        }
      ],
      "source": [
        "import torch, transformers\n",
        "print('torch', torch.__version__)\n",
        "print('transformers', transformers.__version__)\n",
        "print('cuda', torch.cuda.is_available())\n",
        "print('mps', torch.backends.mps.is_available())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GCXUYtVC3c2x",
      "metadata": {
        "id": "GCXUYtVC3c2x"
      },
      "source": [
        "## 4) Stage A ((Optional)): KD-QAT on streaming C4\n",
        "\n",
        "This preserves the base model’s behavior under low-bit fake-quant weights.\n",
        "\n",
        "Tips:\n",
        "- Start with a small run (`--max_steps 50`) to validate the pipeline.\n",
        "- Use `-q 4` if 2-bit is too unstable; 4-bit is less aggressive.\n",
        "- On MPS, prefer `--ema_decay 0` for KD-QAT.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jERGktjwjz29",
      "metadata": {
        "id": "jERGktjwjz29"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TpT02cskrs6D",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TpT02cskrs6D",
        "outputId": "5e5648b6-a34d-4ada-85c9-2292e31b316f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[device] cuda | dtype=torch.bfloat16\n",
            "tokenizer_config.json: 9.73kB [00:00, 39.0MB/s]\n",
            "vocab.json: 2.78MB [00:00, 65.7MB/s]\n",
            "merges.txt: 1.67MB [00:00, 132MB/s]\n",
            "tokenizer.json: 100% 11.4M/11.4M [00:00<00:00, 20.4MB/s]\n",
            "config.json: 100% 726/726 [00:00<00:00, 6.09MB/s]\n",
            "2025-12-24 01:22:39.394668: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-24 01:22:39.417464: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766539359.443242    8009 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766539359.449385    8009 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766539359.463433    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766539359.463456    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766539359.463459    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766539359.463461    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 01:22:39.467717: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors: 100% 1.50G/1.50G [00:01<00:00, 935MB/s] \n",
            "generation_config.json: 100% 239/239 [00:00<00:00, 2.03MB/s]\n",
            "README.md: 7.47kB [00:00, 30.6MB/s]\n",
            "data/train-00000-of-00001-a09b74b3ef9c3b(…): 100% 24.2M/24.2M [00:00<00:00, 34.7MB/s]\n",
            "Generating train split: 100% 52002/52002 [00:00<00:00, 336912.65 examples/s]\n",
            "[write] shard_00000.pt | N=512\n",
            "[write] shard_00001.pt | N=512\n",
            "[write] shard_00002.pt | N=512\n",
            "[write] shard_00003.pt | N=512\n",
            "[write] shard_00004.pt | N=512\n",
            "[write] shard_00005.pt | N=512\n",
            "[write] shard_00006.pt | N=512\n",
            "[write] shard_00007.pt | N=512\n",
            "[write] shard_00008.pt | N=512\n",
            "[write] shard_00009.pt | N=512\n",
            "[write] shard_00010.pt | N=512\n",
            "[write] shard_00011.pt | N=512\n",
            "[write] shard_00012.pt | N=512\n",
            "[write] shard_00013.pt | N=512\n",
            "[write] shard_00014.pt | N=512\n",
            "[write] shard_00015.pt | N=512\n",
            "[write] shard_00016.pt | N=512\n",
            "[write] shard_00017.pt | N=512\n",
            "[write] shard_00018.pt | N=512\n",
            "[write] shard_00019.pt | N=512\n",
            "[write] shard_00020.pt | N=512\n",
            "[write] shard_00021.pt | N=512\n",
            "[write] shard_00022.pt | N=512\n",
            "[write] shard_00023.pt | N=512\n",
            "[write] shard_00024.pt | N=512\n",
            "[write] shard_00025.pt | N=512\n",
            "[write] shard_00026.pt | N=512\n",
            "[write] shard_00027.pt | N=512\n",
            "[write] shard_00028.pt | N=512\n",
            "[write] shard_00029.pt | N=512\n",
            "[write] shard_00030.pt | N=512\n",
            "[write] shard_00031.pt | N=512\n",
            "[write] shard_00032.pt | N=512\n",
            "[write] shard_00033.pt | N=512\n",
            "[write] shard_00034.pt | N=512\n",
            "[write] shard_00035.pt | N=512\n",
            "[write] shard_00036.pt | N=512\n",
            "[write] shard_00037.pt | N=512\n",
            "[write] shard_00038.pt | N=512\n",
            "[write] shard_00039.pt | N=32\n",
            "Done. Cached sequences=20000 | out_dir=caches/alpaca_chat_think_L128_K32_R256\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# GENERATE THINKING DATASET (Alpaca chat format)\n",
        "# ============================================================\n",
        "# SKIP THIS CELL if you already have the cache on Google Drive!\n",
        "# Use the \"LOAD FROM GOOGLE DRIVE\" cell instead.\n",
        "\n",
        "!python scripts/precompute_teacher_topk.py \\\n",
        "  --teacher_model_name_or_path {MODEL_NAME} \\\n",
        "  --dataset_name tatsu-lab/alpaca \\\n",
        "  --dataset_split train \\\n",
        "  --dataset_format alpaca_chat \\\n",
        "  --enable_thinking true \\\n",
        "  --max_length 128 \\\n",
        "  --topk 32 \\\n",
        "  --rand_neg 256 \\\n",
        "  --num_sequences 20000 \\\n",
        "  --batch_size 1 \\\n",
        "  --shard_size 512 \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  --output_dir {CACHE_DIR_CHAT}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rlxJ3X8a3c2x",
      "metadata": {
        "id": "rlxJ3X8a3c2x"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# GENERATE TEXT DATASET (C4 streaming)\n",
        "# ============================================================\n",
        "# SKIP THIS CELL if you already have the cache on Google Drive!\n",
        "# Use the \"LOAD FROM GOOGLE DRIVE\" cell instead.\n",
        "\n",
        "import os\n",
        "\n",
        "CACHE_DIR = CACHE_DIR_TEXT\n",
        "\n",
        "if not os.path.isdir(CACHE_DIR):\n",
        "    print(f\"[cache] {CACHE_DIR} not found -> generating cache...\")\n",
        "\n",
        "    !python scripts/precompute_teacher_topk.py \\\n",
        "      --teacher_model_name_or_path {MODEL_NAME} \\\n",
        "      --dataset_name allenai/c4 \\\n",
        "      --dataset_config_name en \\\n",
        "      --dataset_split train \\\n",
        "      --dataset_text_field text \\\n",
        "      --streaming \\\n",
        "      --shuffle_buffer 10000 \\\n",
        "      --max_length 64 \\\n",
        "      --topk 32 \\\n",
        "      --rand_neg 256 \\\n",
        "      --num_sequences 2000 \\\n",
        "      --batch_size 1 \\\n",
        "      --shard_size 512 \\\n",
        "      --device {DEVICE} \\\n",
        "      --dtype {DTYPE} \\\n",
        "      --output_dir {CACHE_DIR}\n",
        "\n",
        "else:\n",
        "    print(f\"[cache] {CACHE_DIR} already exists -> skipping generation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Nt45qTnlKO7a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt45qTnlKO7a",
        "outputId": "be3ace25-b2da-42ec-df47-30b79fc479a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[gzip] Compressing caches/alpaca_chat_think_L128_K32_R256...\n",
            "caches/alpaca_chat_think_L128_K32_R256/\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00018.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00011.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00033.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00031.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00036.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00038.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00030.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00000.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00032.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00010.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00039.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00008.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00005.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00025.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/meta.json\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00015.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00028.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00016.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00009.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00021.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00037.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00001.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00035.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00029.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00003.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00006.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00014.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00007.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00002.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00013.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00024.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00034.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00022.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00020.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00004.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00012.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00019.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00027.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00026.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00017.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00023.pt\n",
            "[gzip] Done: 2.84 GB\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# COMPRESS CHAT CACHE (for Google Drive upload)\n",
        "# ============================================================\n",
        "# SKIP if cache is already compressed or loaded from Google Drive\n",
        "\n",
        "import os\n",
        "\n",
        "if os.path.isdir(CACHE_DIR_CHAT):\n",
        "    print(f\"[gzip] Compressing {CACHE_DIR_CHAT}...\")\n",
        "    !tar -zcvf {CACHE_DIR_CHAT}.tgz {CACHE_DIR_CHAT}\n",
        "    compressed_size = os.path.getsize(f\"{CACHE_DIR_CHAT}.tgz\")\n",
        "    print(f\"[gzip] Done: {compressed_size / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(f\"[gzip] Directory {CACHE_DIR_CHAT} not found. Skipping.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IIIWEkllwGEA",
      "metadata": {
        "collapsed": true,
        "id": "IIIWEkllwGEA"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SAVE CACHED KD DATA TO GOOGLE DRIVE (run after generating cache)\n",
        "# ============================================================\n",
        "# This saves the generated cache to Google Drive for future sessions\n",
        "# Only run this AFTER you've generated the cache with precompute_teacher_topk.py\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create destination directory\n",
        "!mkdir -p /content/drive/MyDrive/qwen3_caches\n",
        "\n",
        "# Choose which cache to save (should match what you generated)\n",
        "CACHE_NAME = \"alpaca_chat_think_both_L128_K32_R256\"\n",
        "\n",
        "import os\n",
        "\n",
        "# Check if cache exists - copy folder directly (no compression needed)\n",
        "if os.path.isdir(f\"caches/{CACHE_NAME}\"):\n",
        "    # Copy folder to Google Drive\n",
        "    print(f\"[save] Copying {CACHE_NAME} to Google Drive...\")\n",
        "    !rsync -ah --info=progress2 caches/{CACHE_NAME}/ /content/drive/MyDrive/qwen3_caches/{CACHE_NAME}/\n",
        "\n",
        "    # Verify\n",
        "    num_shards = len([f for f in os.listdir(f\"/content/drive/MyDrive/qwen3_caches/{CACHE_NAME}\") if f.startswith(\"shard_\")])\n",
        "    print(f\"[save] Saved to Google Drive: {num_shards} shards\")\n",
        "else:\n",
        "    print(f\"[save] ERROR: Cache directory caches/{CACHE_NAME} not found\")\n",
        "    print(\"[save] Run precompute_teacher_topk.py first to generate the cache\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n4snxtz2q6q",
      "metadata": {
        "id": "n4snxtz2q6q"
      },
      "source": [
        "## 4.5) Google Drive Cache Management\n",
        "\n",
        "**Workflow for KD Cache:**\n",
        "\n",
        "1. **First time setup** (slow):\n",
        "   - Run `precompute_teacher_topk.py` to generate cache\n",
        "   - Run \"SAVE TO GOOGLE DRIVE\" cell to persist\n",
        "   \n",
        "2. **Subsequent sessions** (fast):\n",
        "   - Run \"LOAD FROM GOOGLE DRIVE\" cell to restore cache\n",
        "   - Skip cache generation step\n",
        "\n",
        "The cached KD data (~2-3 GB compressed) contains precomputed teacher logits for knowledge distillation training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7745eb2",
      "metadata": {
        "id": "d7745eb2"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# COMPRESS TEXT CACHE (for Google Drive upload)\n",
        "# ============================================================\n",
        "# SKIP if cache is already compressed or loaded from Google Drive\n",
        "\n",
        "import os\n",
        "\n",
        "if os.path.isdir(CACHE_DIR_TEXT):\n",
        "    print(f\"[gzip] Compressing {CACHE_DIR_TEXT}...\")\n",
        "    !tar -zcvf {CACHE_DIR_TEXT}.tgz {CACHE_DIR_TEXT}\n",
        "    compressed_size = os.path.getsize(f\"{CACHE_DIR_TEXT}.tgz\")\n",
        "    print(f\"[gzip] Done: {compressed_size / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(f\"[gzip] Directory {CACHE_DIR_TEXT} not found. Skipping.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (!!) LOAD CACHED KD DATA FROM GOOGLE DRIVE\n"
      ],
      "metadata": {
        "id": "JhOpTWn8hthM"
      },
      "id": "JhOpTWn8hthM"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "vlhuS4N9GbN4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlhuS4N9GbN4",
        "outputId": "23b9b92b-d27e-48b5-95aa-912ef9964cf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[cache] Copying alpaca_chat_think_both_L128_K32_R256 from Google Drive...\n",
            "          4.40G 100%   12.01MB/s    0:05:49 (xfr#41, to-chk=0/42)\n",
            "[cache] Successfully loaded alpaca_chat_think_both_L128_K32_R256 with 40 shards\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# LOAD CACHED KD DATA FROM GOOGLE DRIVE (run this cell first!)\n",
        "# ============================================================\n",
        "# Mount Google Drive and copy cached KD data back to local storage\n",
        "# This avoids regenerating the cache every session\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create local cache directory\n",
        "!mkdir -p caches\n",
        "\n",
        "# Cache folder to load (copy folder directly, no .tgz)\n",
        "CACHE_NAME = \"alpaca_chat_think_both_L128_K32_R256\"\n",
        "\n",
        "# Copy folder directly from Google Drive\n",
        "SRC_PATH = f\"/content/drive/MyDrive/qwen3_caches/{CACHE_NAME}\"\n",
        "DST_PATH = f\"caches/{CACHE_NAME}\"\n",
        "\n",
        "print(f\"[cache] Copying {CACHE_NAME} from Google Drive...\")\n",
        "!rsync -ah --info=progress2 {SRC_PATH}/ {DST_PATH}/\n",
        "\n",
        "# Verify copy\n",
        "import os\n",
        "if os.path.isdir(DST_PATH):\n",
        "    num_shards = len([f for f in os.listdir(DST_PATH) if f.startswith(\"shard_\")])\n",
        "    print(f\"[cache] Successfully loaded {CACHE_NAME} with {num_shards} shards\")\n",
        "else:\n",
        "    print(f\"[cache] ERROR: Failed to copy {CACHE_NAME}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NKhv4eaYs4qR",
      "metadata": {
        "id": "NKhv4eaYs4qR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a28b3d7b-eb60-4800-88c2-8459cec2acbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[checkpoint] Copying qwen3_kdqat_cache_q2_4.tgz from Google Drive...\n",
            "\r              0   0%    0.00kB/s    0:00:00 (xfr#0, to-chk=0/1)\n",
            "[checkpoint] Checking tarball structure...\n",
            "qwen3_kdqat_cache_q2_4/\n",
            "qwen3_kdqat_cache_q2_4/special_tokens_map.json\n",
            "qwen3_kdqat_cache_q2_4/loss.csv\n",
            "qwen3_kdqat_cache_q2_4/added_tokens.json\n",
            "qwen3_kdqat_cache_q2_4/tokenizer_config.json\n",
            "[checkpoint] Extracting qwen3_kdqat_cache_q2_4.tgz...\n",
            "[checkpoint] Successfully loaded qwen3_kdqat_cache_q2_4 with 12 files:\n",
            "  - added_tokens.json\n",
            "  - chat_template.jinja\n",
            "  - final_state_dict.pt\n",
            "  - loss.csv\n",
            "  - merges.txt\n",
            "  ... and 7 more\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# LOAD 4-BIT CHECKPOINT FROM GOOGLE DRIVE (for 2-bit initialization)\n",
        "# ============================================================\n",
        "# Copy the 4-bit trained checkpoint to use as starting point for 2-bit training\n",
        "\n",
        "import os\n",
        "\n",
        "# Create runs directory\n",
        "!mkdir -p runs\n",
        "\n",
        "# 4-bit checkpoint to load (best result from 4-bit training)\n",
        "CHECKPOINT_NAME = \"qwen3_kdqat_cache_q2_4\"\n",
        "SRC_PATH = f\"/content/drive/MyDrive/qwen3_runs/{CHECKPOINT_NAME}.tgz\"\n",
        "DST_PATH = f\"runs/{CHECKPOINT_NAME}.tgz\"\n",
        "\n",
        "# Copy from Google Drive\n",
        "print(f\"[checkpoint] Copying {CHECKPOINT_NAME}.tgz from Google Drive...\")\n",
        "!rsync -ah --info=progress2 {SRC_PATH} {DST_PATH}\n",
        "\n",
        "# Check tarball structure first\n",
        "print(f\"[checkpoint] Checking tarball structure...\")\n",
        "!tar -tzf {DST_PATH} | head -5\n",
        "\n",
        "# Extract to runs/ directory (tarball contains folder without runs/ prefix)\n",
        "print(f\"[checkpoint] Extracting {CHECKPOINT_NAME}.tgz...\")\n",
        "!tar -xzf {DST_PATH} -C runs/\n",
        "\n",
        "# Verify extraction\n",
        "if os.path.isdir(f\"runs/{CHECKPOINT_NAME}\"):\n",
        "    files = os.listdir(f\"runs/{CHECKPOINT_NAME}\")\n",
        "    print(f\"[checkpoint] Successfully loaded {CHECKPOINT_NAME} with {len(files)} files:\")\n",
        "    for f in sorted(files)[:5]:\n",
        "        print(f\"  - {f}\")\n",
        "    if len(files) > 5:\n",
        "        print(f\"  ... and {len(files)-5} more\")\n",
        "else:\n",
        "    # Try to find where it extracted\n",
        "    print(f\"[checkpoint] Checking runs/ directory...\")\n",
        "    !ls -la runs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AHSsmdhzsBTo",
      "metadata": {
        "id": "AHSsmdhzsBTo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A7UPbdfK3c2x",
      "metadata": {
        "id": "A7UPbdfK3c2x"
      },
      "outputs": [],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_stream_q2\"\n",
        "\n",
        "# DISABLED --- NOTE used! see #5 for first QAT step\n",
        "# Construct the command string in Python to ensure variable interpolation\n",
        "command_str = f\"\"\"python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --teacher_model_name_or_path {MODEL_NAME} \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --dataset_name allenai/c4 \\\n",
        "  --dataset_config_name en \\\n",
        "  --dataset_split train \\\n",
        "  --dataset_format text \\\n",
        "  --dataset_text_field text \\\n",
        "  --streaming \\\n",
        "  --shuffle_buffer 10000 \\\n",
        "  --output_dir {RUN_DIR} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 1 \\\n",
        "  --gradient_accumulation_steps 16 \\\n",
        "  --learning_rate 5e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 50 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --logging_steps 10 \\\n",
        "  --save_steps 50\"\"\"\n",
        "\n",
        "# Execute the constructed command string\n",
        "!{command_str}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UmgbC-RI3c2x",
      "metadata": {
        "id": "UmgbC-RI3c2x"
      },
      "source": [
        "### (Optional) Resume\n",
        "\n",
        "`--resume_from_checkpoint auto` resolves to `checkpoint_last.pt` if it exists in the output directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mJqdmXSA3c2x",
      "metadata": {
        "id": "mJqdmXSA3c2x"
      },
      "outputs": [],
      "source": [
        "# !python scripts/train_qat.py ... --output_dir {RUN_DIR} --max_steps 500 --resume_from_checkpoint auto\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R7UZBhau3c2x",
      "metadata": {
        "id": "R7UZBhau3c2x"
      },
      "source": [
        "## 5)  KD-cache: precompute teacher top-k + negatives\n",
        "\n",
        "Cache mode is MPS-friendly:\n",
        "- no teacher model during training\n",
        "- no full-vocab logits\n",
        "\n",
        "If you see good KD loss but bad greedy decoding, increase negative coverage (`--rand_neg`) and/or add hard top-1 terms:\n",
        "- `--hard-top1-weight 0.05`\n",
        "- `--hard-full-top1-weight 0.02`–`0.05`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g4s_PzQW3c2x",
      "metadata": {
        "id": "g4s_PzQW3c2x"
      },
      "source": [
        "### KD-cache QAT training\n",
        "\n",
        "This uses cached teacher signals + candidate softmax."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m7q7F2_jgN6Z"
      },
      "id": "m7q7F2_jgN6Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1Li8Aysa3c2x",
      "metadata": {
        "id": "1Li8Aysa3c2x"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STAGE 1: KD-QAT (Conservative - freeze MLP/Attention)\n",
        "# ============================================================\n",
        "# First training stage with frozen output layers for stability\n",
        "\n",
        "%pwd\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "CACHE_DIR = CACHE_DIR_CHAT\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2\"\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --output_dir {RUN_DIR_CACHE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 1 \\\n",
        "  --gradient_accumulation_steps 4 \\\n",
        "  --learning_rate 5e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 1000 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 5 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.05 \\\n",
        "  --hard-full-top1-weight 0.03 \\\n",
        "  --ov-freeze \\\n",
        "  --freeze-last-mlp \\\n",
        "  --freeze-last-mlp-layers 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UKgDA-7OuA8m",
      "metadata": {
        "id": "UKgDA-7OuA8m"
      },
      "outputs": [],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --prompt \"What capital city of France is?\" \\\n",
        "  --do_sample true \\\n",
        "  --max_new_tokens 64\n",
        "\n",
        "#  --prompt \"What is Capital of france?\" \\\n",
        "#   --prompt \"What is Apple Neural Engine?\" \\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sexJNkyJyMgv",
      "metadata": {
        "id": "sexJNkyJyMgv"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb19fb17",
      "metadata": {
        "id": "eb19fb17"
      },
      "outputs": [],
      "source": [
        "# Define source and destination paths\n",
        "SOURCE_FILE = \"runs/qwen3_kdqat_cache_q2_2/qat_state_dict.pt\"\n",
        "DEST_DIR_GD = \"/content/drive/MyDrive/runs/Q4B/q2_2/\"\n",
        "\n",
        "# Ensure the destination directory exists on Google Drive\n",
        "!mkdir -p {DEST_DIR_GD}\n",
        "\n",
        "# Copy the file to Google Drive\n",
        "!cp -v {SOURCE_FILE} {DEST_DIR_GD}\n",
        "print(f\"Copied {SOURCE_FILE} to {DEST_DIR_GD}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hbfDaYP5yN6-",
      "metadata": {
        "id": "hbfDaYP5yN6-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JUvQIIDeRUF6",
      "metadata": {
        "id": "JUvQIIDeRUF6"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STAGE 2: KD-QAT (Unfrozen layers, resume from Stage 1)\n",
        "# ============================================================\n",
        "# Continue training with all layers unfrozen\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "CACHE_DIR = CACHE_DIR_CHAT\n",
        "INIT_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2\"\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_DIR_CACHE}/qat_state_dict.pt \\\n",
        "  --output_dir {RUN_DIR_CACHE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 1 \\\n",
        "  --gradient_accumulation_steps 4 \\\n",
        "  --learning_rate 5e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 1000 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --ov-freeze \\\n",
        "  --hard-top1-weight 0.02 \\\n",
        "  --hard-full-top1-weight 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (!!!!)Stage 3 resume KD-QAT with\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "GJ7WaTFCgRHN"
      },
      "id": "GJ7WaTFCgRHN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KXQUTgEJfzl4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXQUTgEJfzl4",
        "outputId": "afa02860-d593-411a-ce05-c6c2eb2ecd57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-24 23:29:24.155008: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-24 23:29:24.171198: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766618964.192763   82536 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766618964.199201   82536 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766618964.215547   82536 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766618964.215573   82536 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766618964.215576   82536 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766618964.215579   82536 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 23:29:24.220382: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[device] cuda | amp_dtype=torch.bfloat16 | param_dtype=torch.bfloat16\n",
            "[qat] weight_bits=2\n",
            "[init] loading model state from runs/progressive_qat_q2_v1/qat_state_dict.pt\n",
            "[init] model weights loaded.\n",
            "[kd-cache] cache topk=32\n",
            "[kd-cache] dir=caches/alpaca_chat_think_both_L128_K32_R256 | weight=1.0 | T=2.0 | hard_top1=0.0 | hard_full_top1=0.0005\n",
            "opt_step: 100% 3000/3000 [53:54<00:00,  1.08s/step, loss=1.1946, lr=0.00e+00]\n",
            "Done. QAT checkpoint saved to: runs/qwen3_kdqat_cache_q2_3/qat_state_dict.pt\n"
          ]
        }
      ],
      "source": [
        "# Stage 3 resume KD-QAT with unfrozen attention and relaxed hard-top/full!\n",
        "\n",
        "CACHE_DIR = CACHE_DIR_CHAT  # Use config variable\n",
        "#INIT_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "INIT_DIR_CACHE =  \"runs/progressive_qat_q2_v1\"\n",
        "\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_3\"\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_DIR_CACHE}/qat_state_dict.pt \\\n",
        "  --output_dir {RUN_DIR_CACHE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 128 \\\n",
        "  --gradient_accumulation_steps 1 \\\n",
        "  --learning_rate 5e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 3000 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.00 \\\n",
        "  --hard-full-top1-weight 0.0005\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#save run\n"
      ],
      "metadata": {
        "id": "ESr5L_EStT6y"
      },
      "id": "ESr5L_EStT6y"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# SAVE PROGRESSIVE QAT CHECKPOINT TO GOOGLE DRIVE\n",
        "# ============================================================\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive if not already mounted\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Source directory (matches RUN_DIR_PROGRESSIVE from config)\n",
        "RUN_NAME = \"qwen3_kdqat_cache_q2_3\"\n",
        "RUN_DIR = f\"runs/{RUN_NAME}\"\n",
        "\n",
        "# Destination on Google Drive\n",
        "DEST_DIR_GD = \"/content/drive/MyDrive/qwen3_runs/\"\n",
        "!mkdir -p {DEST_DIR_GD}\n",
        "\n",
        "# Check if run directory exists and has content\n",
        "if os.path.isdir(RUN_DIR) and os.listdir(RUN_DIR):\n",
        "    # Compress the run directory\n",
        "    print(f\"[archive] Compressing {RUN_DIR}...\")\n",
        "    !tar -zcvf {RUN_NAME}.tgz -C runs {RUN_NAME}\n",
        "\n",
        "    # Copy to Google Drive\n",
        "    print(f\"[save] Copying {RUN_NAME}.tgz to Google Drive...\")\n",
        "    !rsync -ah --info=progress2 {RUN_NAME}.tgz {DEST_DIR_GD}\n",
        "\n",
        "    # Verify\n",
        "    gd_size = os.path.getsize(f\"{DEST_DIR_GD}/{RUN_NAME}.tgz\")\n",
        "    print(f\"[save] Saved to Google Drive: {gd_size / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(f\"[save] ERROR: {RUN_DIR} is empty or doesn't exist\")\n",
        "\n",
        "# got to \"Stage 3 resume\" to continue distill treaining"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPk8pkD3tUeu",
        "outputId": "6e0433c4-8709-4eda-8aec-d27443c92fc6"
      },
      "id": "RPk8pkD3tUeu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[archive] Compressing runs/qwen3_kdqat_cache_q2_3...\n",
            "qwen3_kdqat_cache_q2_3/\n",
            "qwen3_kdqat_cache_q2_3/special_tokens_map.json\n",
            "qwen3_kdqat_cache_q2_3/loss.csv\n",
            "qwen3_kdqat_cache_q2_3/added_tokens.json\n",
            "qwen3_kdqat_cache_q2_3/tokenizer_config.json\n",
            "qwen3_kdqat_cache_q2_3/run_state.json\n",
            "qwen3_kdqat_cache_q2_3/merges.txt\n",
            "qwen3_kdqat_cache_q2_3/training_args.json\n",
            "qwen3_kdqat_cache_q2_3/chat_template.jinja\n",
            "qwen3_kdqat_cache_q2_3/vocab.json\n",
            "qwen3_kdqat_cache_q2_3/final_state_dict.pt\n",
            "qwen3_kdqat_cache_q2_3/qat_state_dict.pt\n",
            "qwen3_kdqat_cache_q2_3/checkpoint_step3000.pt\n",
            "qwen3_kdqat_cache_q2_3/checkpoint_last.pt\n",
            "qwen3_kdqat_cache_q2_3/tokenizer.json\n",
            "[save] Copying qwen3_kdqat_cache_q2_3.tgz to Google Drive...\n",
            "          7.46G 100%  332.14MB/s    0:00:21 (xfr#1, to-chk=0/1)\n",
            "[save] Saved to Google Drive: 6.95 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "U5THSkU3oo2V"
      },
      "id": "U5THSkU3oo2V"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28cb12c7"
      },
      "source": [
        "#### Pull and Unzip Progressive QAT Checkpoint"
      ],
      "id": "28cb12c7"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the checkpoint name to pull\n",
        "RUN_NAME = \"qwen3_kdqat_cache_q2_3\"\n",
        "\n",
        "# Source path on Google Drive\n",
        "SRC_PATH_GD = f\"/content/drive/MyDrive/qwen3_runs/{RUN_NAME}.tgz\"\n",
        "# Destination path locally\n",
        "DST_PATH_LOCAL = f\"{RUN_NAME}.tgz\"\n",
        "\n",
        "# Create runs directory if it doesn't exist\n",
        "!mkdir -p runs\n",
        "\n",
        "print(f\"[pull] Copying {RUN_NAME}.tgz from Google Drive...\")\n",
        "!rsync -ah --info=progress2 {SRC_PATH_GD} {DST_PATH_LOCAL}\n",
        "\n",
        "# Check if the tarball was copied successfully\n",
        "if os.path.exists(DST_PATH_LOCAL):\n",
        "    print(f\"[pull] Extracting {RUN_NAME}.tgz...\")\n",
        "    !tar -xzf {DST_PATH_LOCAL} -C runs/\n",
        "    print(f\"[pull] Successfully extracted to runs/{RUN_NAME}\")\n",
        "\n",
        "    # Optionally, remove the tarball after extraction to save space\n",
        "    # !rm {DST_PATH_LOCAL}\n",
        "else:\n",
        "    print(f\"[pull] ERROR: {RUN_NAME}.tgz not found on Google Drive. Make sure it was saved correctly.\")\n",
        "\n",
        "# Use the path where the checkpoint was unzipped to the inference check code cell\n",
        "#(4coakmebsik). This comment clarifies that the TEST_RUN variable should point to the d\n",
        "#irectory where the QAT checkpoint was extracted for inference."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVx16qzAodl2",
        "outputId": "ae861164-dfef-41b1-aec9-24e6d1005dcd"
      },
      "id": "uVx16qzAodl2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[pull] Copying qwen3_kdqat_cache_q2_3.tgz from Google Drive...\n",
            "          7.46G 100%  136.13MB/s    0:00:52 (xfr#1, to-chk=0/1)\n",
            "[pull] Extracting qwen3_kdqat_cache_q2_3.tgz...\n",
            "[pull] Successfully extracted to runs/qwen3_kdqat_cache_q2_3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q1Zjl_WnHZ4O",
      "metadata": {
        "id": "Q1Zjl_WnHZ4O"
      },
      "outputs": [],
      "source": [
        "# Stage 4 resume KD-QAT with unfrozen attention and relaxed hard-top/full!\n",
        "#   --hard-full-top1-weight 0.0000\n",
        "#   learning_rate 2e-6\n",
        "CACHE_DIR = CACHE_DIR_CHAT  # Use config variable\n",
        "INIT_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_3\"\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_4\"\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_DIR_CACHE}/qat_state_dict.pt \\\n",
        "  --output_dir {RUN_DIR_CACHE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 160 \\\n",
        "  --gradient_accumulation_steps 1 \\\n",
        "  --learning_rate 2e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 500 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 5 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.00 \\\n",
        "  --hard-full-top1-weight 0.0000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acak6adahpf",
      "metadata": {
        "id": "acak6adahpf"
      },
      "source": [
        "## 5.5) Progressive Layer-by-Layer QAT (Experimental)\n",
        "\n",
        "This approach trains one layer at a time with:\n",
        "- **Local reconstruction loss**: MSE between quantized and fp MLP outputs\n",
        "- **Global KD loss**: Cached teacher logits\n",
        "- **Prefix quantized / suffix fp**: Earlier layers stay quantized, later layers use full precision\n",
        "\n",
        "### Recommended Training Order (most stable first):\n",
        "\n",
        "1. **E2E f-only** (Option 1): Train ALL f parameters at once\n",
        "   - Skip progressive passes, just run Pass 4\n",
        "   - Most stable, fastest validation\n",
        "   \n",
        "2. **Progressive f-only** (Option 2): Layer-by-layer f-param training\n",
        "   - Uses `--train_f_only` flag\n",
        "   - Disable local loss with `--local_weight 0.0`\n",
        "   \n",
        "3. **Full progressive** (Option 3): Train weights + f per layer\n",
        "   - Most aggressive, may show instability at later layers\n",
        "\n",
        "### GPU Configuration:\n",
        "\n",
        "| GPU | Recommended batch_size |\n",
        "|-----|------------------------|\n",
        "| T4 (15GB) | 2-4 |\n",
        "| V100 (32GB) | 4-8 |\n",
        "| A100 (40GB) | 8-16 |\n",
        "| A100 (80GB) / H100 | 16-32 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "scxwylalctd",
      "metadata": {
        "id": "scxwylalctd"
      },
      "outputs": [],
      "source": [
        "# ---- Progressive QAT Config (2-bit from 4-bit checkpoint) ----\n",
        "# Starting from 4-bit trained checkpoint for better 2-bit initialization\n",
        "\n",
        "# 4-bit checkpoint as initialization (loaded from Google Drive)\n",
        "#INIT_CHECKPOINT = \"runs/qwen3_kdqat_cache_q2_4/qat_state_dict.pt\"\n",
        "\n",
        "# Adjust batch_size for your GPU (A100: 8-16, V100: 4-8, T4: 2-4)\n",
        "BATCH_SIZE = 96                # Increase for faster instances (A100/H100)\n",
        "STEPS_PER_LAYER_MLP = 100      # Steps per MLP layer (Pass 1 + Pass 3)\n",
        "STEPS_PER_LAYER_ATTN = 30      # Steps per attention layer (Pass 2)\n",
        "E2E_STEPS = 500                # E2E quantizer tuning steps (Pass 4)\n",
        "LOCAL_WEIGHT = 0.3             # Local reconstruction loss weight\n",
        "GLOBAL_WEIGHT = 1.0            # Global KD loss weight\n",
        "LOCAL_TOKEN_SAMPLES = 128      # Tokens to sample for local loss\n",
        "MAX_GRAD_NORM = 1.0            # Gradient clipping (important for 2-bit)\n",
        "\n",
        "# Learning rates (lower for 2-bit stability)\n",
        "LR_PROGRESSIVE = 2e-6          # Learning rate for progressive passes\n",
        "LR_E2E = 5e-5                  # Learning rate for E2E f-only tuning\n",
        "\n",
        "# Output directories (2-bit versions)\n",
        "RUN_DIR_E2E_FONLY = \"runs/e2e_f_only_q2\"\n",
        "RUN_DIR_PROGRESSIVE_FONLY = \"runs/progressive_f_only_q2\"\n",
        "RUN_DIR_PROGRESSIVE = \"runs/progressive_qat_q2_v1\"\n",
        "RUN_DIR_PROGRESSIVE_FRESH = \"runs/progressive_qat_q2_v1_fresh\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q9f32hx0qo",
      "metadata": {
        "id": "q9f32hx0qo"
      },
      "source": [
        "### Option 1: E2E f-only Training (Recommended First)\n",
        "\n",
        "**Most stable approach** - trains ALL `_f_param` (quantization scales) simultaneously.\n",
        "Skip all progressive layer-by-layer passes and go straight to Pass 4.\n",
        "\n",
        "This is recommended when:\n",
        "- Progressive layer-by-layer shows instability (local loss hitting 10.0)\n",
        "- You want to validate the infrastructure works before trying progressive\n",
        "- You have limited time and want the fastest path to a working checkpoint\n",
        "\n",
        "The `f` parameter is the learnable quantization scale from Apple-style quantization:\n",
        "- Actual scale `s = softplus(f)` ensures positivity\n",
        "- Training only `f` keeps weights frozen - more stable for ultra-low-bit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vh3iifkgpj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vh3iifkgpj",
        "outputId": "c0ebe461-dfaa-44e1-a38e-6c5fdb2c9a87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "[device] cuda | amp_dtype=torch.bfloat16 | param_dtype=torch.bfloat16\n",
            "tokenizer_config.json: 9.73kB [00:00, 38.5MB/s]\n",
            "vocab.json: 2.78MB [00:00, 52.2MB/s]\n",
            "merges.txt: 1.67MB [00:00, 139MB/s]\n",
            "tokenizer.json: 100% 11.4M/11.4M [00:01<00:00, 10.2MB/s]\n",
            "[model] Loading Qwen/Qwen3-0.6B\n",
            "config.json: 100% 726/726 [00:00<00:00, 8.23MB/s]\n",
            "2025-12-24 18:47:22.719539: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-24 18:47:22.740144: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766602042.765073    9837 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766602042.770583    9837 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766602042.784760    9837 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766602042.784783    9837 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766602042.784786    9837 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766602042.784789    9837 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 18:47:22.789000: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors: 100% 1.50G/1.50G [00:02<00:00, 570MB/s]\n",
            "generation_config.json: 100% 239/239 [00:00<00:00, 2.04MB/s]\n",
            "[init] Loading model state from runs/qwen3_kdqat_cache_q2_4/qat_state_dict.pt\n",
            "[qat] weight_bits=2\n",
            "[kd-cache] Loading from caches/alpaca_chat_think_both_L128_K32_R256\n",
            "[model] 28 transformer layers\n",
            "[training] batch_size=96 steps_per_mlp=50 e2e_steps=500\n",
            "\n",
            "============================================================\n",
            "PASS 4: E2E Quantizer Tuning (f-only)\n",
            "============================================================\n",
            "  Trainable f parameters: 196\n",
            "  step 0: loss=12.1407\n",
            "  step 10: loss=11.5678\n",
            "  step 20: loss=11.8305\n",
            "  step 30: loss=11.9946\n",
            "  step 40: loss=11.7304\n",
            "  step 50: loss=11.8770\n",
            "  step 60: loss=12.0282\n",
            "  step 70: loss=11.6242\n",
            "  step 80: loss=11.5261\n",
            "  step 90: loss=11.6720\n",
            "  step 100: loss=12.3927\n",
            "  step 110: loss=11.5418\n",
            "  step 120: loss=12.0243\n",
            "  step 130: loss=11.7296\n",
            "  step 140: loss=12.1050\n",
            "  step 150: loss=11.6462\n",
            "  step 160: loss=11.9153\n",
            "  step 170: loss=11.9587\n",
            "  step 180: loss=11.9907\n",
            "  step 190: loss=11.9216\n",
            "  step 200: loss=12.2080\n",
            "  step 210: loss=12.0106\n",
            "  step 220: loss=11.6917\n",
            "  step 230: loss=12.0309\n",
            "  step 240: loss=11.9478\n",
            "  step 250: loss=11.7766\n",
            "  step 260: loss=11.9367\n",
            "  step 270: loss=11.8300\n",
            "  step 280: loss=12.2159\n",
            "  step 290: loss=11.9723\n",
            "  step 300: loss=11.9617\n",
            "  step 310: loss=11.8662\n",
            "  step 320: loss=11.9585\n",
            "  step 330: loss=11.6636\n",
            "  step 340: loss=11.9637\n",
            "  step 350: loss=11.8636\n",
            "  step 360: loss=11.8626\n",
            "  step 370: loss=12.0558\n",
            "  step 380: loss=11.7337\n",
            "  step 390: loss=11.9441\n",
            "  step 400: loss=11.9119\n",
            "  step 410: loss=12.0105\n",
            "  step 420: loss=11.6374\n",
            "  step 430: loss=11.8217\n",
            "  step 440: loss=11.6547\n",
            "  step 450: loss=11.7303\n",
            "  step 460: loss=11.8293\n",
            "  step 470: loss=11.6669\n",
            "  step 480: loss=11.8896\n",
            "  step 490: loss=11.7591\n",
            "\n",
            "============================================================\n",
            "Saving outputs\n",
            "============================================================\n",
            "  Model saved to: runs/e2e_f_only_q2/qat_state_dict.pt\n",
            "  Loss log saved to: runs/e2e_f_only_q2/loss_per_layer.csv\n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# E2E f-only: Skip ALL progressive passes, train all f parameters at once\n",
        "# This is the simplest and most stable approach\n",
        "# Starting from 4-bit checkpoint for 2-bit training\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/train_qat_progressive.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_CHECKPOINT} \\\n",
        "  --output_dir {RUN_DIR_E2E_FONLY} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --skip_mlp_pass \\\n",
        "  --skip_attention_pass \\\n",
        "  --skip_mlp_refinement \\\n",
        "  --e2e_steps {E2E_STEPS} \\\n",
        "  --e2e_learning_rate {LR_E2E} \\\n",
        "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vab9x1zlqhs",
      "metadata": {
        "id": "vab9x1zlqhs"
      },
      "source": [
        "### Option 2: Progressive f-only Training\n",
        "\n",
        "Layer-by-layer training but only trains `_f_param` (quantization scales), not weights.\n",
        "More stable than full progressive training, but may still see instability at later layers.\n",
        "\n",
        "Use `--train_f_only` flag to freeze weights and only train quantization scales per layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oqxrx3c2fe",
      "metadata": {
        "id": "oqxrx3c2fe"
      },
      "outputs": [],
      "source": [
        "# Progressive f-only: Layer-by-layer, but only train quantization scales\n",
        "# Use --train_f_only for more stable training\n",
        "# Starting from 4-bit checkpoint for 2-bit training\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/train_qat_progressive.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_CHECKPOINT} \\\n",
        "  --output_dir {RUN_DIR_PROGRESSIVE_FONLY} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --train_f_only \\\n",
        "  --steps_per_layer_mlp {STEPS_PER_LAYER_MLP} \\\n",
        "  --steps_per_layer_attn {STEPS_PER_LAYER_ATTN} \\\n",
        "  --e2e_steps {E2E_STEPS} \\\n",
        "  --local_weight 0.0 \\\n",
        "  --global_weight {GLOBAL_WEIGHT} \\\n",
        "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
        "  --learning_rate {LR_PROGRESSIVE} \\\n",
        "  --e2e_learning_rate {LR_E2E} \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --skip_mlp_refinement"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e0gsxkcc4",
      "metadata": {
        "id": "8e0gsxkcc4"
      },
      "source": [
        "### (!!!) Option 3: Full Progressive Training (weights + f)\n",
        "\n",
        "Full layer-by-layer training with weights and quantization scales.\n",
        "Most aggressive but potentially unstable for ultra-low-bit (2-bit).\n",
        "\n",
        "**Training Order (3-pass v3):**\n",
        "1. **Pass 1**: Train MLP layers (local reconstruction + global KD)\n",
        "2. **Pass 2**: Train attention layers (global KD only)\n",
        "3. **Pass 3**: MLP refinement (addresses MLP-attention coupling)\n",
        "4. **Pass 4**: E2E quantizer-only tuning (f-param only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7qzyw0rd18e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qzyw0rd18e",
        "outputId": "2d5d0b37-0270-4378-9c7b-b632edf1de47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "[device] cuda | amp_dtype=torch.bfloat16 | param_dtype=torch.bfloat16\n",
            "[model] Loading Qwen/Qwen3-0.6B\n",
            "2025-12-25 16:08:15.961412: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-25 16:08:15.982074: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766678896.008043    7272 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766678896.013538    7272 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766678896.027397    7272 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766678896.027420    7272 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766678896.027423    7272 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766678896.027426    7272 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-25 16:08:16.031445: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[qat] weight_bits=2\n",
            "[kd-cache] Loading from caches/alpaca_chat_think_both_L128_K32_R256\n",
            "[model] 28 transformer layers\n",
            "[training] batch_size=96 steps_per_mlp=100 e2e_steps=500\n",
            "\n",
            "============================================================\n",
            "PASS 1: Progressive MLP Training\n",
            "============================================================\n",
            "\n",
            "--- Layer 0/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.3494 global=0.3103\n",
            "  step 10: local=0.3382 global=0.2855\n",
            "  step 20: local=0.3492 global=0.2434\n",
            "  step 30: local=0.3432 global=0.1991\n",
            "  step 40: local=0.3397 global=0.1870\n",
            "  step 50: local=0.3383 global=0.1507\n",
            "  step 60: local=0.3525 global=0.1538\n",
            "  step 70: local=0.3396 global=0.1509\n",
            "  step 80: local=0.3415 global=0.1342\n",
            "  step 90: local=0.3258 global=0.1289\n",
            "  [TIME] Layer 0 took 42.1s | Total: 0.7min | ETA: 19.0min (27 layers left)\n",
            "\n",
            "--- Layer 1/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5552 global=0.9723\n",
            "  step 10: local=0.5542 global=0.8242\n",
            "  step 20: local=0.5590 global=0.7217\n",
            "  step 30: local=0.5291 global=0.6767\n",
            "  step 40: local=0.5580 global=0.5802\n",
            "  step 50: local=0.5594 global=0.5204\n",
            "  step 60: local=0.5432 global=0.5274\n",
            "  step 70: local=0.5154 global=0.4829\n",
            "  step 80: local=0.5483 global=0.4389\n",
            "  step 90: local=0.5385 global=0.3880\n",
            "  [TIME] Layer 1 took 40.9s | Total: 1.4min | ETA: 18.0min (26 layers left)\n",
            "\n",
            "--- Layer 2/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "\n",
            "  [EARLY BACKTRACK] Layer 2 started at 9.8619 (>3.8628 = 0.3863 * 10.0)\n",
            "  [EARLY BACKTRACK] Aborting layer 2, going back to retrain layer 1 (backtrack 1/5)\n",
            "\n",
            "--- Layer 1/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.5026 global=0.3701\n",
            "  step 10: local=0.5516 global=0.3547\n",
            "  step 20: local=0.5257 global=0.3187\n",
            "  step 30: local=0.5143 global=0.3523\n",
            "  step 40: local=0.5003 global=0.2853\n",
            "  step 50: local=0.5210 global=0.3050\n",
            "  step 60: local=0.5151 global=0.3019\n",
            "  step 70: local=0.4838 global=0.2737\n",
            "  step 80: local=0.4819 global=0.3013\n",
            "  step 90: local=0.4883 global=0.2556\n",
            "  [TIME] Layer 1 took 40.9s | Total: 2.1min | ETA: 17.9min (26 layers left)\n",
            "\n",
            "--- Layer 2/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "\n",
            "  [EARLY BACKTRACK] Layer 2 started at 9.5895 (>2.5595 = 0.2559 * 10.0)\n",
            "  [EARLY BACKTRACK] Aborting layer 2, going back to retrain layer 1 (backtrack 2/5)\n",
            "\n",
            "--- Layer 1/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4931 global=0.2627\n",
            "  step 10: local=0.4839 global=0.2435\n",
            "  step 20: local=0.4714 global=0.2438\n",
            "  step 30: local=0.4780 global=0.2394\n",
            "  step 40: local=0.4638 global=0.2249\n",
            "  step 50: local=0.4975 global=0.2144\n",
            "  step 60: local=0.4760 global=0.2138\n",
            "  step 70: local=0.4688 global=0.2126\n",
            "  step 80: local=0.4712 global=0.2085\n",
            "  step 90: local=0.4540 global=0.2280\n",
            "  [TIME] Layer 1 took 40.9s | Total: 2.8min | ETA: 17.9min (26 layers left)\n",
            "\n",
            "--- Layer 2/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "\n",
            "  [EARLY BACKTRACK] Layer 2 started at 9.1461 (>2.3695 = 0.2370 * 10.0)\n",
            "  [EARLY BACKTRACK] Aborting layer 2, going back to retrain layer 1 (backtrack 3/5)\n",
            "\n",
            "--- Layer 1/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4648 global=0.2176\n",
            "  step 10: local=0.4686 global=0.1998\n",
            "  step 20: local=0.4660 global=0.1966\n",
            "  step 30: local=0.4614 global=0.2047\n",
            "  step 40: local=0.4364 global=0.2090\n",
            "  step 50: local=0.4428 global=0.1881\n",
            "  step 60: local=0.4195 global=0.2141\n",
            "  step 70: local=0.4515 global=0.1930\n",
            "  step 80: local=0.4393 global=0.1980\n",
            "  step 90: local=0.4487 global=0.1988\n",
            "  [TIME] Layer 1 took 40.9s | Total: 3.5min | ETA: 17.8min (26 layers left)\n",
            "\n",
            "--- Layer 2/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "\n",
            "  [EARLY BACKTRACK] Layer 2 started at 8.8606 (>1.9363 = 0.1936 * 10.0)\n",
            "  [EARLY BACKTRACK] Aborting layer 2, going back to retrain layer 1 (backtrack 4/5)\n",
            "\n",
            "--- Layer 1/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4274 global=0.1938\n",
            "  step 10: local=0.4317 global=0.1816\n",
            "  step 20: local=0.4209 global=0.1864\n",
            "  step 30: local=0.4164 global=0.1810\n",
            "  step 40: local=0.4130 global=0.1955\n",
            "  step 50: local=0.4067 global=0.1664\n",
            "  step 60: local=0.3807 global=0.1756\n",
            "  step 70: local=0.4200 global=0.1698\n",
            "  step 80: local=0.3879 global=0.1706\n",
            "  step 90: local=0.4168 global=0.1777\n",
            "  [TIME] Layer 1 took 40.9s | Total: 4.1min | ETA: 17.8min (26 layers left)\n",
            "\n",
            "--- Layer 2/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "\n",
            "  [EARLY BACKTRACK] Layer 2 started at 8.5920 (>1.6888 = 0.1689 * 10.0)\n",
            "  [EARLY BACKTRACK] Aborting layer 2, going back to retrain layer 1 (backtrack 5/5)\n",
            "\n",
            "--- Layer 1/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.4312 global=0.1872\n",
            "  step 10: local=0.3930 global=0.1695\n",
            "  step 20: local=0.4096 global=0.1715\n",
            "  step 30: local=0.3989 global=0.1969\n",
            "  step 40: local=0.3898 global=0.1810\n",
            "  step 50: local=0.3866 global=0.1889\n"
          ]
        }
      ],
      "source": [
        "# Full Progressive: MLP pass + E2E f-only (skip attention/refinement for v1)\n",
        "# For full 3-pass training, remove --skip_attention_pass and --skip_mlp_refinement\n",
        "# Version of Not Starting from 4-bit checkpoint for 2-bit training\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/train_qat_progressive.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --output_dir {RUN_DIR_PROGRESSIVE_FRESH} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --steps_per_layer_mlp {STEPS_PER_LAYER_MLP} \\\n",
        "  --e2e_steps {E2E_STEPS} \\\n",
        "  --local_weight {LOCAL_WEIGHT} \\\n",
        "  --global_weight {GLOBAL_WEIGHT} \\\n",
        "  --local_token_samples {LOCAL_TOKEN_SAMPLES} \\\n",
        "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
        "  --learning_rate {LR_PROGRESSIVE} \\\n",
        "  --e2e_learning_rate {LR_E2E} \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --skip_attention_pass \\\n",
        "  --max_layer_repeats 20 \\\n",
        "  --max_backtrack 5 \\\n",
        "  --layer_converge_threshold 0.8 \\\n",
        "  --skip_mlp_refinement"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SAVE RUN"
      ],
      "metadata": {
        "id": "A9MSJDvwhdnV"
      },
      "id": "A9MSJDvwhdnV"
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# SAVE PROGRESSIVE QAT CHECKPOINT TO GOOGLE DRIVE\n",
        "# ============================================================\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive if not already mounted\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Source directory (matches RUN_DIR_PROGRESSIVE from config)\n",
        "RUN_NAME = \"progressive_qat_q2_v1\"\n",
        "RUN_DIR = f\"runs/{RUN_NAME}\"\n",
        "\n",
        "# Destination on Google Drive\n",
        "DEST_DIR_GD = \"/content/drive/MyDrive/qwen3_runs/\"\n",
        "!mkdir -p {DEST_DIR_GD}\n",
        "\n",
        "# Check if run directory exists and has content\n",
        "if os.path.isdir(RUN_DIR) and os.listdir(RUN_DIR):\n",
        "    # Compress the run directory\n",
        "    print(f\"[archive] Compressing {RUN_DIR}...\")\n",
        "    !tar -zcvf {RUN_NAME}.tgz -C runs {RUN_NAME}\n",
        "\n",
        "    # Copy to Google Drive\n",
        "    print(f\"[save] Copying {RUN_NAME}.tgz to Google Drive...\")\n",
        "    !rsync -ah --info=progress2 {RUN_NAME}.tgz {DEST_DIR_GD}\n",
        "\n",
        "    # Verify\n",
        "    gd_size = os.path.getsize(f\"{DEST_DIR_GD}/{RUN_NAME}.tgz\")\n",
        "    print(f\"[save] Saved to Google Drive: {gd_size / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(f\"[save] ERROR: {RUN_DIR} is empty or doesn't exist\")\n",
        "\n",
        "# got to \"Stage 3 resume\" to continue distill treaining\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVSFT6W9cffG",
        "outputId": "2b2a34d7-1473-44a9-f1eb-eddffcee3dd0"
      },
      "id": "DVSFT6W9cffG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[archive] Compressing runs/progressive_qat_q2_v1...\n",
            "progressive_qat_q2_v1/\n",
            "progressive_qat_q2_v1/loss_per_layer.csv\n",
            "progressive_qat_q2_v1/training_args.json\n",
            "progressive_qat_q2_v1/qat_state_dict.pt\n",
            "[save] Copying progressive_qat_q2_v1.tgz to Google Drive...\n",
            "        945.50M 100%  442.21MB/s    0:00:02 (xfr#1, to-chk=0/1)\n",
            "[save] Saved to Google Drive: 0.88 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# v3: Full 3-pass progressive training\n",
        "# MLP -> Attention -> MLP refinement -> E2E f-only\n",
        "# WARNING: May show instability at later layers for 2-bit\n",
        "# Starting from 4-bit checkpoint for 2-bit training\n",
        "\n",
        "RUN_DIR_PROGRESSIVE_V3 = \"runs/progressive_qat_q2_v3\"\n",
        "\n",
        "!python scripts/train_qat_progressive.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_CHECKPOINT} \\\n",
        "  --output_dir {RUN_DIR_PROGRESSIVE_V3} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --steps_per_layer_mlp {STEPS_PER_LAYER_MLP} \\\n",
        "  --steps_per_layer_attn {STEPS_PER_LAYER_ATTN} \\\n",
        "  --e2e_steps {E2E_STEPS} \\\n",
        "  --local_weight {LOCAL_WEIGHT} \\\n",
        "  --global_weight {GLOBAL_WEIGHT} \\\n",
        "  --local_token_samples {LOCAL_TOKEN_SAMPLES} \\\n",
        "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
        "  --learning_rate {LR_PROGRESSIVE} \\\n",
        "  --e2e_learning_rate {LR_E2E} \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head"
      ],
      "metadata": {
        "id": "_qK8LP8WrbSB"
      },
      "id": "_qK8LP8WrbSB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dBFCVCZjwBo_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBFCVCZjwBo_",
        "outputId": "8eaba02b-c9ea-4930-df9b-69a5f75e180d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[archive] Compressing runs/qwen3_kdqat_cache_q2_3...\n",
            "qwen3_kdqat_cache_q2_3/\n",
            "qwen3_kdqat_cache_q2_3/special_tokens_map.json\n",
            "qwen3_kdqat_cache_q2_3/loss.csv\n",
            "qwen3_kdqat_cache_q2_3/added_tokens.json\n",
            "qwen3_kdqat_cache_q2_3/tokenizer_config.json\n",
            "qwen3_kdqat_cache_q2_3/run_state.json\n",
            "qwen3_kdqat_cache_q2_3/merges.txt\n",
            "qwen3_kdqat_cache_q2_3/training_args.json\n",
            "qwen3_kdqat_cache_q2_3/chat_template.jinja\n",
            "qwen3_kdqat_cache_q2_3/vocab.json\n",
            "qwen3_kdqat_cache_q2_3/final_state_dict.pt\n",
            "qwen3_kdqat_cache_q2_3/qat_state_dict.pt\n",
            "qwen3_kdqat_cache_q2_3/tokenizer.json\n",
            "[save] Copying qwen3_kdqat_cache_q2_3.tgz to Google Drive...\n",
            "          1.90G 100%  430.50MB/s    0:00:04 (xfr#1, to-chk=0/1)\n",
            "[save] Saved to Google Drive: 1.77 GB\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# SAVE PROGRESSIVE QAT CHECKPOINT TO GOOGLE DRIVE\n",
        "# ============================================================\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "\n",
        "# Mount Google Drive if not already mounted\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Source directory\n",
        "#RUN_NAME = \"progressive_qat_v1\"\n",
        "RUN_NAME = \"progressive_qat_q2_v3\"\n",
        "\n",
        "RUN_DIR = f\"runs/{RUN_NAME}\"\n",
        "\n",
        "# Destination on Google Drive\n",
        "DEST_DIR_GD = \"/content/drive/MyDrive/qwen3_runs/\"\n",
        "!mkdir -p {DEST_DIR_GD}\n",
        "\n",
        "# Check if run directory exists and has content\n",
        "if os.path.isdir(RUN_DIR) and os.listdir(RUN_DIR):\n",
        "    # Compress the run directory\n",
        "    print(f\"[archive] Compressing {RUN_DIR}...\")\n",
        "    !tar -zcvf {RUN_NAME}.tgz -C runs {RUN_NAME}\n",
        "\n",
        "    # Copy to Google Drive\n",
        "    print(f\"[save] Copying {RUN_NAME}.tgz to Google Drive...\")\n",
        "    !rsync -ah --info=progress2 {RUN_NAME}.tgz {DEST_DIR_GD}\n",
        "\n",
        "    # Verify\n",
        "    gd_size = os.path.getsize(f\"{DEST_DIR_GD}/{RUN_NAME}.tgz\")\n",
        "    print(f\"[save] Saved to Google Drive: {gd_size / (1024**3):.2f} GB\")\n",
        "\n",
        "    # Cleanup local archive (optional)\n",
        "    # !rm {RUN_NAME}.tgz\n",
        "else:\n",
        "    print(f\"[save] ERROR: {RUN_DIR} is empty or doesn't exist\")\n",
        "    print(\"[save] Run progressive training first\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qiaq60t466e",
      "metadata": {
        "id": "qiaq60t466e"
      },
      "outputs": [],
      "source": [
        "# v3: Full 3-pass progressive training\n",
        "# MLP -> Attention -> MLP refinement -> E2E f-only\n",
        "# WARNING: May show instability at later layers for 2-bit\n",
        "\n",
        "RUN_DIR_PROGRESSIVE_V3 = \"runs/progressive_qat_v3\"\n",
        "\n",
        "!python scripts/train_qat_progressive.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --output_dir {RUN_DIR_PROGRESSIVE_V3} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --steps_per_layer_mlp {STEPS_PER_LAYER_MLP} \\\n",
        "  --steps_per_layer_attn {STEPS_PER_LAYER_ATTN} \\\n",
        "  --e2e_steps {E2E_STEPS} \\\n",
        "  --local_weight {LOCAL_WEIGHT} \\\n",
        "  --global_weight {GLOBAL_WEIGHT} \\\n",
        "  --local_token_samples {LOCAL_TOKEN_SAMPLES} \\\n",
        "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
        "  --learning_rate {LR_PROGRESSIVE} \\\n",
        "  --e2e_learning_rate {LR_E2E} \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ix3xjm2h0xq",
      "metadata": {
        "id": "ix3xjm2h0xq"
      },
      "outputs": [],
      "source": [
        "# Plot per-layer training progress\n",
        "# Change PLOT_RUN to visualize different runs\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Choose which run to visualize\n",
        "PLOT_RUN = RUN_DIR_E2E_FONLY  # or RUN_DIR_PROGRESSIVE, RUN_DIR_PROGRESSIVE_V3\n",
        "\n",
        "csv_path = f\"{PLOT_RUN}/loss_per_layer.csv\"\n",
        "if not os.path.exists(csv_path):\n",
        "    print(f\"Loss CSV not found at {csv_path}\")\n",
        "    print(\"Run training first or check the path.\")\n",
        "else:\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "    # Pass 1: MLP training (local loss)\n",
        "    mlp_df = df[(df['pass'] == 1) & (df['component'] == 'mlp')]\n",
        "    if not mlp_df.empty and 'local' in mlp_df.columns:\n",
        "        for layer in mlp_df['layer'].unique():\n",
        "            layer_df = mlp_df[mlp_df['layer'] == layer]\n",
        "            axes[0, 0].plot(layer_df['step'], layer_df['local'], label=f'L{layer}', alpha=0.7)\n",
        "        axes[0, 0].set_title('Pass 1: MLP Local Loss per Layer')\n",
        "        axes[0, 0].set_xlabel('Step')\n",
        "        axes[0, 0].set_ylabel('Loss')\n",
        "        axes[0, 0].legend(ncol=4, fontsize=6)\n",
        "    else:\n",
        "        axes[0, 0].set_title('Pass 1: MLP Local Loss (skipped or no local loss)')\n",
        "\n",
        "    # Pass 1: MLP global loss\n",
        "    if not mlp_df.empty and 'global' in mlp_df.columns:\n",
        "        for layer in mlp_df['layer'].unique():\n",
        "            layer_df = mlp_df[mlp_df['layer'] == layer]\n",
        "            axes[0, 1].plot(layer_df['step'], layer_df['global'], label=f'L{layer}', alpha=0.7)\n",
        "        axes[0, 1].set_title('Pass 1: MLP Global KD Loss per Layer')\n",
        "        axes[0, 1].set_xlabel('Step')\n",
        "        axes[0, 1].set_ylabel('Loss')\n",
        "        axes[0, 1].legend(ncol=4, fontsize=6)\n",
        "    else:\n",
        "        axes[0, 1].set_title('Pass 1: MLP Global Loss (skipped)')\n",
        "\n",
        "    # Pass 2: Attention training\n",
        "    attn_df = df[(df['pass'] == 2) & (df['component'] == 'attn')]\n",
        "    if not attn_df.empty and 'global' in attn_df.columns:\n",
        "        for layer in attn_df['layer'].unique():\n",
        "            layer_df = attn_df[attn_df['layer'] == layer]\n",
        "            axes[1, 0].plot(layer_df['step'], layer_df['global'], label=f'L{layer}', alpha=0.7)\n",
        "        axes[1, 0].set_title('Pass 2: Attention Global KD Loss per Layer')\n",
        "        axes[1, 0].set_xlabel('Step')\n",
        "        axes[1, 0].set_ylabel('Loss')\n",
        "        axes[1, 0].legend(ncol=4, fontsize=6)\n",
        "    else:\n",
        "        axes[1, 0].set_title('Pass 2: Attention (skipped)')\n",
        "        axes[1, 0].text(0.5, 0.5, 'Not run', ha='center', va='center', transform=axes[1, 0].transAxes)\n",
        "\n",
        "    # Pass 4: E2E f-only tuning\n",
        "    e2e_df = df[(df['pass'] == 4)]\n",
        "    if not e2e_df.empty and 'global' in e2e_df.columns:\n",
        "        axes[1, 1].plot(e2e_df['step'], e2e_df['global'], 'b-', linewidth=2)\n",
        "        axes[1, 1].set_title('Pass 4: E2E f-only Tuning')\n",
        "        axes[1, 1].set_xlabel('Step')\n",
        "        axes[1, 1].set_ylabel('Global KD Loss')\n",
        "    else:\n",
        "        axes[1, 1].set_title('Pass 4: E2E (not yet run)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{PLOT_RUN}/loss_per_layer.png\", dpi=150)\n",
        "    plt.show()\n",
        "    print(f\"Saved to {PLOT_RUN}/loss_per_layer.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qgtw9rd3cr",
      "metadata": {
        "id": "qgtw9rd3cr"
      },
      "source": [
        "### Inference Check: Progressive QAT Results\n",
        "\n",
        "Test the progressive QAT checkpoint with a quick inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4coakmebsik",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4coakmebsik",
        "outputId": "5ab1ae51-ec40-4a29-c9d0-92df9f482839"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-25 03:57:41.932578: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-25 03:57:41.952444: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766635061.977425    7043 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766635061.982748    7043 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766635061.996415    7043 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766635061.996438    7043 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766635061.996441    7043 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766635061.996444    7043 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-25 03:57:42.000573: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "user\n",
            "What is Apple Neural Engine?\n",
            "assistant\n",
            "<think>\n",
            "\n",
            "</think>\n",
            "\n",
            "Apple Neural Engine (NLP) is a powerful and powerful artificial intelligence system that mimues human behavior, learning, and decision-making. It is a popular machine learning algorithm that is widely used for various AI applications, such as image recognition, language understanding, and language generation. It is also used in finance, finance, and finance, making it a important tool for financial analysts and analysts.\n"
          ]
        }
      ],
      "source": [
        "# Test inference with progressive QAT checkpoint\n",
        "# Change RUN_DIR to test different runs:\n",
        "#   RUN_DIR_E2E_FONLY, RUN_DIR_PROGRESSIVE_FONLY, RUN_DIR_PROGRESSIVE\n",
        "# progressive_qat_v1/qat_state_dict.pt\n",
        "\n",
        "#TEST_RUN = RUN_DIR_E2E_FONLY  # Change this to test other runs\n",
        "\n",
        "# Use the path where the checkpoint was unzipped\n",
        "TEST_RUN =  \"runs/qwen3_kdqat_cache_q2_4\"\n",
        "TEST_RUN =  \"runs/qwen3_kdqat_cache_q2_3\"\n",
        "\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {TEST_RUN}/qat_state_dict.pt \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --prompt \"What is Apple Neural Engine?\" \\\n",
        "  --do_sample true \\\n",
        "  --max_new_tokens 128"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yq8GHX0o3c2y",
      "metadata": {
        "id": "Yq8GHX0o3c2y"
      },
      "source": [
        "## 6) Stage B: LoRA recovery\n",
        "\n",
        "Two options:\n",
        "- **SFT LoRA** (Alpaca-style instruction tuning)\n",
        "- **Cached KD-LoRA** (preserve teacher distribution; no new “skills”)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B1yimj8aS11y",
      "metadata": {
        "id": "B1yimj8aS11y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LcGFKyxO3c2y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcGFKyxO3c2y",
        "outputId": "979e3378-150b-44fc-83ef-762801383bb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "[device] cuda | amp_dtype=torch.bfloat16 | param_dtype=torch.bfloat16\n",
            "2025-12-25 03:59:07.550602: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-25 03:59:07.570769: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766635147.595924    7465 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766635147.601384    7465 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766635147.615579    7465 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766635147.615602    7465 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766635147.615605    7465 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766635147.615608    7465 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-25 03:59:07.619894: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[qat] weight_bits=2\n",
            "Loaded QAT checkpoint. missing=0 unexpected=0\n",
            "Enabled LoRA on (196, 20185088) layers. Trainable params: 20,185,088\n",
            "[kd_cache] Note: cache max_length=128 (you passed --max_length=1024). --max_length is ignored in cache mode.\n",
            "[kd_cache] cache topk=32\n",
            "[kd_cache] Enabled cached KD-LoRA. T=2.0 weight=1.0 hard_top1=0.02 hard_full_top1=0.01\n",
            "opt_step: 100% 1000/1000 [09:49<00:00,  1.70step/s, loss=1.2426, lr=0.00e+00]\n",
            "Done. Saved LoRA adapter to: runs/qwen3_lora_recovery_cached_r32/lora_only_state_dict.pt\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# STAGE B: LoRA Recovery (Cached KD-LoRA)\n",
        "# ============================================================\n",
        "# Train LoRA adapters on top of QAT checkpoint\n",
        "\n",
        "CACHE_DIR = CACHE_DIR_CHAT\n",
        "#RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2\"\n",
        "RUN_DIR_CACHE  = \"runs/progressive_qat_v1\"\n",
        "RUN_DIR_CACHE  = \"runs/qwen3_kdqat_cache_q2_4\"\n",
        "RUN_DIR_CACHE =  \"runs/qwen3_kdqat_cache_q2_3\"\n",
        "\n",
        "\n",
        "LORA_DIM = 32\n",
        "LORA_RUN_KD = f\"runs/qwen3_lora_recovery_cached_r{LORA_DIM}\"\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/train_lora_recovery.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR_CACHE}/qat_state_dict.pt \\\n",
        "  --output_dir {LORA_RUN_KD} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --gradient_accumulation_steps 2 \\\n",
        "  --learning_rate 1e-5 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 1000 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 2 \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} \\\n",
        "  --lora_alpha {LORA_DIM} \\\n",
        "  --lora_dropout 0.0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.02 \\\n",
        "  --hard-full-top1-weight 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Defule6L3c2y",
      "metadata": {
        "id": "Defule6L3c2y"
      },
      "source": [
        "## 7) Plot loss\n",
        "\n",
        "In Colab, use `--no_show` + `--save` then display the PNG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QM1RNfDh3c2y",
      "metadata": {
        "id": "QM1RNfDh3c2y"
      },
      "outputs": [],
      "source": [
        "!python scripts/plot_loss.py --run_dir {RUN_DIR} --source csv --no_show --save {RUN_DIR}/loss.png\n",
        "from PIL import Image\n",
        "display(Image.open(f\"{RUN_DIR}/loss.png\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p3bOUiMt3c2y",
      "metadata": {
        "id": "p3bOUiMt3c2y"
      },
      "source": [
        "## 8) Inference sanity checks\n",
        "\n",
        "Greedy decode (`--do_sample false`) and keep outputs short (`--max_new_tokens 16`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aA_cJHvT3c2y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aA_cJHvT3c2y",
        "outputId": "cc468368-92bd-4981-e9f4-e2e894eca16b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "2025-12-25 04:11:49.105856: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-25 04:11:49.126275: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766635909.151365   10948 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766635909.156770   10948 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766635909.170813   10948 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766635909.170836   10948 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766635909.170839   10948 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766635909.170842   10948 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-25 04:11:49.175005: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[lora] loaded adapters for 196 layers from runs/qwen3_lora_recovery_cached_r32/lora_only_state_dict.pt\n",
            "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "user\n",
            "What is Machine Learning?\n",
            "assistant\n",
            "<think>\n",
            "<think>\n",
            "</think>\n",
            "\n",
            "Machine Learning is a type of machine learning that involves combining data sets to predict outcomes based on patterns. It is used in various fields, such as finance, finance, and social media. Machine learning is a powerful field that combines data to make decisions based on patterns. It is used in various fields, such as finance, finance, and social media. Machine learning is a important field that is growing in many industries, including finance, finance, and social media.\n"
          ]
        }
      ],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_3\"\n",
        "#RUN_DIR = \"runs/progressive_qat_v1\"\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --lora_checkpoint \"runs/qwen3_lora_recovery_cached_r32/lora_only_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
        "  --prompt \"What is Machine Learning?\" \\\n",
        "  --do_sample false \\\n",
        "  --enable_thinking true \\\n",
        "  --max_new_tokens 256\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EJlu4sOZCNpY",
      "metadata": {
        "id": "EJlu4sOZCNpY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cD985HdXlm0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cD985HdXlm0",
        "outputId": "65053f76-705a-419c-fddb-8ae1cff1fda1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-24 10:41:54.709024: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-24 10:41:54.729231: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766572914.754556  155967 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766572914.759941  155967 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766572914.773878  155967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572914.773902  155967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572914.773905  155967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572914.773908  155967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 10:41:54.777961: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[lora] loaded adapters for 196 layers from runs/qwen3_lora_recovery_cached_r32/lora_only_state_dict.pt\n",
            "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "user\n",
            "Explain how neural networks learn in simple terms\n",
            "assistant\n",
            "<think>\n",
            "<think>\n",
            "Okay, let's start with the basics. Neural networks are like a brain in a computer, right? They process information in layers, just like a human brain does. Each layer has some connections between neurons, and each neuron can process a single input value. \n",
            "\n",
            "So, if we have a simple example, like a simple neural network that takes a single input and outputs a single output, how does it work? Let's imagine a simple example. Let's say the input is a number, say 10. The first layer is the input layer, and the second layer is the output layer. The input layer has a single neuron, and the output layer has a single neuron. \n",
            "\n",
            "The input layer takes the input number and processes it. Then, the output layer takes the result and passes it back. So, the process is: input → output. \n",
            "\n",
            "But how does this work in practice? It's like a simple machine learning model. The input is the data, the output is the result, and the model learns by adjusting the weights and biases to minimize the error. \n",
            "\n",
            "So, in simple terms, neural networks work by taking a set of inputs, applying some operations to them, and then producing a set of outputs. The process is iterative, and the model learns over time to make better predictions or decisions. \n",
            "\n",
            "I think that's a good explanation. Let me make sure it's simple and not too technical. The key is that the neural network is like a computer, and the process of learning is similar to how a human brain learns. \n",
            "\n",
            "I think that's a good explanation. Let me check if I'm missing anything. No, I think I've covered the basics. If there's anything else, I can add more details, but I'm already confident in this explanation.\n",
            "</think>\n",
            "\n",
            "**Explanation of How Neural Networks Learn in Simple Terms:**\n",
            "\n",
            "**1. Introduction:**\n",
            "- **Neural Networks (NNs):** They are like computers in the brain, where each neuron processes information.\n",
            "- **Input Layer:** Takes the input data and processes it.\n",
            "- **Output Layer:** Produces the output, which is the result of the processing.\n",
            "\n",
            "**2. Simple Example:**\n",
            "- **Input:** A single input value, say 10.\n",
            "- **First Layer (Input Layer):** Processes the input, maybe by adding some weights and biases.\n",
            "- **Output Layer (Output Layer):** Passes the result back, with the final output being the result of the processing.\n",
            "\n",
            "**3. How It Works:**\n",
            "- **Processing:** Each neuron in a layer takes the input and processes it.\n",
            "- **Learning:** The network learns to adjust its weights and biases over time to minimize the error.\n",
            "- **Iteration:** The process is repeated over time, and the model improves its performance.\n",
            "\n",
            "**4. Simple Explanation in Words:**\n",
            "- Neural networks are like a computer that learns by doing tasks. The input is the data, the output is the result, and the model learns to make better predictions.\n",
            "\n",
            "**5. Conclusion:**\n",
            "- Neural networks are a type of machine learning model that processes information in layers, just like a human brain does. They are simple, can be used for various tasks, and are very effective in learning.\n",
            "\n",
            "This explanation is simple, visual, and easy to understand.\n"
          ]
        }
      ],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_4\"\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --lora_checkpoint \"runs/qwen3_lora_recovery_cached_r32/lora_only_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
        "  --prompt \"Explain how neural networks learn in simple terms\" \\\n",
        "  --do_sample false \\\n",
        "  --enable_thinking true \\\n",
        "  --max_new_tokens 1024\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xZ6aSDAa3c2y",
      "metadata": {
        "id": "xZ6aSDAa3c2y"
      },
      "outputs": [],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --lora_checkpoint \"runs/qwen3_lora_recovery_cached_r64/lora_only_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
        "  --prompt \"2+2=\" \\\n",
        "  --do_sample false \\\n",
        "  --enable_thinking true \\\n",
        "  --max_new_tokens 90\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZOIo97VlyAbh",
      "metadata": {
        "id": "ZOIo97VlyAbh"
      },
      "outputs": [],
      "source": [
        "LORA_DIM = 64\n",
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2\"\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
        "  --prompt \"What is capital of France?\" \\\n",
        "  --do_sample true \\\n",
        "  --max_new_tokens 64\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ef_Q8P0i3c2y",
      "metadata": {
        "id": "Ef_Q8P0i3c2y"
      },
      "source": [
        "## 9) Optional: snap weights to the exact grid\n",
        "\n",
        "This produces a float checkpoint with weights snapped to the N-bit codebook (not bitpacked)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jk_qSZIs3c2y",
      "metadata": {
        "id": "jk_qSZIs3c2y"
      },
      "outputs": [],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "!python scripts/hard_quantize_checkpoint.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}/checkpoint_last.pt \\\n",
        "  --output_path {RUN_DIR}/hard_quant_full_state_dict.pt \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7HnE7IFS3dK",
      "metadata": {
        "id": "f7HnE7IFS3dK"
      },
      "outputs": [],
      "source": [
        "%cd qwen3_apple_style_2bit_qat_lora\n",
        "%ls -l runs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beQD2eyDWWnb",
      "metadata": {
        "id": "beQD2eyDWWnb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}