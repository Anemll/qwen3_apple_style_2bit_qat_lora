{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anemll/qwen3_apple_style_2bit_qat_lora/blob/main/notebooks/Qwen3_QAT_KD_LoRA-per-layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rRgC0uK43c2v",
      "metadata": {
        "id": "rRgC0uK43c2v"
      },
      "source": [
        "# Qwen3-0.6B — Apple-style QAT (2-bit / 4-bit) + KD + LoRA recovery\n",
        "\n",
        "This notebook mirrors the structure of common “phone deployment” notebooks, but uses **this repo’s** pipeline:\n",
        "\n",
        "- **Stage A (recommended default):** KD-QAT on plain text (C4 streaming) or KD-cache QAT\n",
        "- **Stage B:** LoRA recovery (either SFT or cached KD-LoRA)\n",
        "- Plot `loss.csv`\n",
        "- Run inference sanity checks\n",
        "\n",
        "Notes:\n",
        "- Qwen3 requires `transformers>=4.51.0`.\n",
        "- For disk usage: C4 is huge; prefer `--streaming` unless you explicitly want to download.\n",
        "- Bitwidth: use `-q 2` (default) or `-q 4` (less aggressive). Checkpoints persist the bitwidth per layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i6Pb9Kki3c2w",
      "metadata": {
        "id": "i6Pb9Kki3c2w"
      },
      "source": [
        "## 0) Setup (Colab / local)\n",
        "\n",
        "If you’re in Colab, clone the repo. If you’re already in the repo directory locally, you can skip this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "UXlLPdtkGM35",
      "metadata": {
        "id": "UXlLPdtkGM35"
      },
      "outputs": [],
      "source": [
        "# ---- Config (edit these) ----\n",
        "#MODEL_NAME = 'Qwen/Qwen3-4B-Thinking-2507'\n",
        "MODEL_NAME = 'Qwen/Qwen3-0.6B'\n",
        "TEACHER_NAME = MODEL_NAME\n",
        "QUANT_BITS = 4  # 2 or 4\n",
        "DEVICE = 'auto'\n",
        "AMP_DTYPE = 'auto'\n",
        "PARAM_DTYPE = 'auto'\n",
        "DTYPE = 'auto'\n",
        "\n",
        "# Cache dirs\n",
        "CACHE_DIR_CHAT = 'caches/alpaca_chat_think_L128_K32_R256'\n",
        "CACHE_DIR_TEXT = 'caches/c4_qwen3_L64_K32_R256'\n",
        "#CACHE_DIR_CHAT = 'caches/Q4B_alpaca_chat_think_L128_K32_R256'\n",
        "#CACHE_DIR_TEXT = 'caches/Q4B_c4_qwen3_L64_K32_R256'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "u2vonfu23c2w",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u2vonfu23c2w",
        "outputId": "7439d3b5-ec70-49bc-a1dc-c47b4b2374ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'qwen3_apple_style_2bit_qat_lora' already exists and is not an empty directory.\n",
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (4/4), 565 bytes | 282.00 KiB/s, done.\n",
            "From https://github.com/Anemll/qwen3_apple_style_2bit_qat_lora\n",
            "   f1836c4..b9c2c0d  main       -> origin/main\n",
            "Updating f1836c4..b9c2c0d\n",
            "Fast-forward\n",
            " scripts/train_qat_progressive.py | 4 \u001b[32m++++\u001b[m\n",
            " 1 file changed, 4 insertions(+)\n",
            "HEAD is now at b9c2c0d Implement early backtrack cleanup in QAT training process\n"
          ]
        }
      ],
      "source": [
        "# Colab-only:\n",
        "%cd /content/\n",
        "!git clone https://github.com/Anemll/qwen3_apple_style_2bit_qat_lora\n",
        "%cd qwen3_apple_style_2bit_qat_lora\n",
        "!git fetch\n",
        "!git pull\n",
        "!git reset --hard HEAD\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZUmiISSL3c2w",
      "metadata": {
        "id": "ZUmiISSL3c2w"
      },
      "source": [
        "## 1) Install dependencies (uv)\n",
        "\n",
        "This repo is set up to work with `uv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "ACRjVuVa3c2x",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACRjVuVa3c2x",
        "outputId": "6ad351ae-0f68-4e6c-d45b-25a4b6a89d6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m7 packages\u001b[0m \u001b[2min 95ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m62 packages\u001b[0m \u001b[2min 120ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 738ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 0.37ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 0.80ms\u001b[0m\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mqat-lora\u001b[0m\u001b[2m==0.0.0 (from file:///content/qwen3_apple_style_2bit_qat_lora)\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mqat-lora\u001b[0m\u001b[2m==0.0.0 (from file:///content/qwen3_apple_style_2bit_qat_lora/qwen3_apple_style_2bit_qat_lora)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip -q install uv\n",
        "!uv pip install -r requirements.txt\n",
        "!uv pip install -e .\n",
        "# plotting\n",
        "!uv pip install -q matplotlib\n",
        "!uv pip install -q plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yfoLPbBk3c2x",
      "metadata": {
        "id": "yfoLPbBk3c2x"
      },
      "source": [
        "## 2) Optional: Hugging Face login\n",
        "\n",
        "If you hit gated model/dataset errors, log in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wLMWX7E23c2x",
      "metadata": {
        "id": "wLMWX7E23c2x"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()  # paste token when prompted\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fA37ilf33c2x",
      "metadata": {
        "id": "fA37ilf33c2x"
      },
      "source": [
        "## 3) Quick environment check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "VUCXehkU3c2x",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUCXehkU3c2x",
        "outputId": "0377d1f6-830e-429e-c7e4-65de61b795ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch 2.9.0+cu126\n",
            "transformers 4.57.3\n",
            "cuda True\n",
            "mps False\n"
          ]
        }
      ],
      "source": [
        "import torch, transformers\n",
        "print('torch', torch.__version__)\n",
        "print('transformers', transformers.__version__)\n",
        "print('cuda', torch.cuda.is_available())\n",
        "print('mps', torch.backends.mps.is_available())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GCXUYtVC3c2x",
      "metadata": {
        "id": "GCXUYtVC3c2x"
      },
      "source": [
        "## 4) Stage A ((Optional)): KD-QAT on streaming C4\n",
        "\n",
        "This preserves the base model’s behavior under low-bit fake-quant weights.\n",
        "\n",
        "Tips:\n",
        "- Start with a small run (`--max_steps 50`) to validate the pipeline.\n",
        "- Use `-q 4` if 2-bit is too unstable; 4-bit is less aggressive.\n",
        "- On MPS, prefer `--ema_decay 0` for KD-QAT.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jERGktjwjz29",
      "metadata": {
        "id": "jERGktjwjz29"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TpT02cskrs6D",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TpT02cskrs6D",
        "outputId": "5e5648b6-a34d-4ada-85c9-2292e31b316f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[device] cuda | dtype=torch.bfloat16\n",
            "tokenizer_config.json: 9.73kB [00:00, 39.0MB/s]\n",
            "vocab.json: 2.78MB [00:00, 65.7MB/s]\n",
            "merges.txt: 1.67MB [00:00, 132MB/s]\n",
            "tokenizer.json: 100% 11.4M/11.4M [00:00<00:00, 20.4MB/s]\n",
            "config.json: 100% 726/726 [00:00<00:00, 6.09MB/s]\n",
            "2025-12-24 01:22:39.394668: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-24 01:22:39.417464: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766539359.443242    8009 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766539359.449385    8009 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766539359.463433    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766539359.463456    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766539359.463459    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766539359.463461    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 01:22:39.467717: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors: 100% 1.50G/1.50G [00:01<00:00, 935MB/s] \n",
            "generation_config.json: 100% 239/239 [00:00<00:00, 2.03MB/s]\n",
            "README.md: 7.47kB [00:00, 30.6MB/s]\n",
            "data/train-00000-of-00001-a09b74b3ef9c3b(…): 100% 24.2M/24.2M [00:00<00:00, 34.7MB/s]\n",
            "Generating train split: 100% 52002/52002 [00:00<00:00, 336912.65 examples/s]\n",
            "[write] shard_00000.pt | N=512\n",
            "[write] shard_00001.pt | N=512\n",
            "[write] shard_00002.pt | N=512\n",
            "[write] shard_00003.pt | N=512\n",
            "[write] shard_00004.pt | N=512\n",
            "[write] shard_00005.pt | N=512\n",
            "[write] shard_00006.pt | N=512\n",
            "[write] shard_00007.pt | N=512\n",
            "[write] shard_00008.pt | N=512\n",
            "[write] shard_00009.pt | N=512\n",
            "[write] shard_00010.pt | N=512\n",
            "[write] shard_00011.pt | N=512\n",
            "[write] shard_00012.pt | N=512\n",
            "[write] shard_00013.pt | N=512\n",
            "[write] shard_00014.pt | N=512\n",
            "[write] shard_00015.pt | N=512\n",
            "[write] shard_00016.pt | N=512\n",
            "[write] shard_00017.pt | N=512\n",
            "[write] shard_00018.pt | N=512\n",
            "[write] shard_00019.pt | N=512\n",
            "[write] shard_00020.pt | N=512\n",
            "[write] shard_00021.pt | N=512\n",
            "[write] shard_00022.pt | N=512\n",
            "[write] shard_00023.pt | N=512\n",
            "[write] shard_00024.pt | N=512\n",
            "[write] shard_00025.pt | N=512\n",
            "[write] shard_00026.pt | N=512\n",
            "[write] shard_00027.pt | N=512\n",
            "[write] shard_00028.pt | N=512\n",
            "[write] shard_00029.pt | N=512\n",
            "[write] shard_00030.pt | N=512\n",
            "[write] shard_00031.pt | N=512\n",
            "[write] shard_00032.pt | N=512\n",
            "[write] shard_00033.pt | N=512\n",
            "[write] shard_00034.pt | N=512\n",
            "[write] shard_00035.pt | N=512\n",
            "[write] shard_00036.pt | N=512\n",
            "[write] shard_00037.pt | N=512\n",
            "[write] shard_00038.pt | N=512\n",
            "[write] shard_00039.pt | N=32\n",
            "Done. Cached sequences=20000 | out_dir=caches/alpaca_chat_think_L128_K32_R256\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# GENERATE THINKING DATASET (Alpaca chat format)\n",
        "# ============================================================\n",
        "# SKIP THIS CELL if you already have the cache on Google Drive!\n",
        "# Use the \"LOAD FROM GOOGLE DRIVE\" cell instead.\n",
        "\n",
        "!python scripts/precompute_teacher_topk.py \\\n",
        "  --teacher_model_name_or_path {MODEL_NAME} \\\n",
        "  --dataset_name tatsu-lab/alpaca \\\n",
        "  --dataset_split train \\\n",
        "  --dataset_format alpaca_chat \\\n",
        "  --enable_thinking true \\\n",
        "  --max_length 128 \\\n",
        "  --topk 32 \\\n",
        "  --rand_neg 256 \\\n",
        "  --num_sequences 20000 \\\n",
        "  --batch_size 1 \\\n",
        "  --shard_size 512 \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  --output_dir {CACHE_DIR_CHAT}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rlxJ3X8a3c2x",
      "metadata": {
        "id": "rlxJ3X8a3c2x"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# GENERATE TEXT DATASET (C4 streaming)\n",
        "# ============================================================\n",
        "# SKIP THIS CELL if you already have the cache on Google Drive!\n",
        "# Use the \"LOAD FROM GOOGLE DRIVE\" cell instead.\n",
        "\n",
        "import os\n",
        "\n",
        "CACHE_DIR = CACHE_DIR_TEXT\n",
        "\n",
        "if not os.path.isdir(CACHE_DIR):\n",
        "    print(f\"[cache] {CACHE_DIR} not found -> generating cache...\")\n",
        "\n",
        "    !python scripts/precompute_teacher_topk.py \\\n",
        "      --teacher_model_name_or_path {MODEL_NAME} \\\n",
        "      --dataset_name allenai/c4 \\\n",
        "      --dataset_config_name en \\\n",
        "      --dataset_split train \\\n",
        "      --dataset_text_field text \\\n",
        "      --streaming \\\n",
        "      --shuffle_buffer 10000 \\\n",
        "      --max_length 64 \\\n",
        "      --topk 32 \\\n",
        "      --rand_neg 256 \\\n",
        "      --num_sequences 2000 \\\n",
        "      --batch_size 1 \\\n",
        "      --shard_size 512 \\\n",
        "      --device {DEVICE} \\\n",
        "      --dtype {DTYPE} \\\n",
        "      --output_dir {CACHE_DIR}\n",
        "\n",
        "else:\n",
        "    print(f\"[cache] {CACHE_DIR} already exists -> skipping generation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Nt45qTnlKO7a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt45qTnlKO7a",
        "outputId": "be3ace25-b2da-42ec-df47-30b79fc479a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[gzip] Compressing caches/alpaca_chat_think_L128_K32_R256...\n",
            "caches/alpaca_chat_think_L128_K32_R256/\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00018.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00011.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00033.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00031.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00036.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00038.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00030.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00000.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00032.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00010.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00039.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00008.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00005.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00025.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/meta.json\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00015.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00028.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00016.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00009.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00021.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00037.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00001.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00035.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00029.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00003.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00006.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00014.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00007.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00002.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00013.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00024.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00034.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00022.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00020.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00004.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00012.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00019.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00027.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00026.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00017.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00023.pt\n",
            "[gzip] Done: 2.84 GB\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# COMPRESS CHAT CACHE (for Google Drive upload)\n",
        "# ============================================================\n",
        "# SKIP if cache is already compressed or loaded from Google Drive\n",
        "\n",
        "import os\n",
        "\n",
        "if os.path.isdir(CACHE_DIR_CHAT):\n",
        "    print(f\"[gzip] Compressing {CACHE_DIR_CHAT}...\")\n",
        "    !tar -zcvf {CACHE_DIR_CHAT}.tgz {CACHE_DIR_CHAT}\n",
        "    compressed_size = os.path.getsize(f\"{CACHE_DIR_CHAT}.tgz\")\n",
        "    print(f\"[gzip] Done: {compressed_size / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(f\"[gzip] Directory {CACHE_DIR_CHAT} not found. Skipping.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IIIWEkllwGEA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IIIWEkllwGEA",
        "outputId": "ef280f39-3d15-42f0-d0e1-41d6b31cb05b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[save] Copying alpaca_chat_think_L128_K32_R256.tgz to Google Drive...\n",
            "          3.05G 100%  427.75MB/s    0:00:06 (xfr#1, to-chk=0/1)\n",
            "[save] Saved to Google Drive: 2.84 GB\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# SAVE CACHED KD DATA TO GOOGLE DRIVE (run after generating cache)\n",
        "# ============================================================\n",
        "# This saves the generated cache to Google Drive for future sessions\n",
        "# Only run this AFTER you've generated the cache with precompute_teacher_topk.py\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create destination directory\n",
        "!mkdir -p /content/drive/MyDrive/qwen3_caches\n",
        "\n",
        "# Choose which cache to save (should match what you generated)\n",
        "CACHE_NAME = \"alpaca_chat_think_L128_K32_R256\"  # For Qwen3-4B thinking model\n",
        "#CACHE_NAME = \"Q4B_alpaca_chat_think_L128_K32_R256\"  # For Qwen3-4B thinking model\n",
        "\n",
        "import os\n",
        "\n",
        "# Check if cache exists and compress if needed\n",
        "if os.path.isdir(f\"caches/{CACHE_NAME}\"):\n",
        "    # Compress if not already compressed\n",
        "    if not os.path.exists(f\"caches/{CACHE_NAME}.tgz\"):\n",
        "        print(f\"[gzip] Compressing {CACHE_NAME}...\")\n",
        "        !tar -zcvf caches/{CACHE_NAME}.tgz -C caches {CACHE_NAME}\n",
        "\n",
        "    # Copy to Google Drive\n",
        "    print(f\"[save] Copying {CACHE_NAME}.tgz to Google Drive...\")\n",
        "    !rsync -ah --info=progress2 caches/{CACHE_NAME}.tgz /content/drive/MyDrive/qwen3_caches/\n",
        "\n",
        "    # Verify\n",
        "    gd_size = os.path.getsize(f\"/content/drive/MyDrive/qwen3_caches/{CACHE_NAME}.tgz\")\n",
        "    print(f\"[save] Saved to Google Drive: {gd_size / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(f\"[save] ERROR: Cache directory caches/{CACHE_NAME} not found\")\n",
        "    print(\"[save] Run precompute_teacher_topk.py first to generate the cache\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n4snxtz2q6q",
      "metadata": {
        "id": "n4snxtz2q6q"
      },
      "source": [
        "## 4.5) Google Drive Cache Management\n",
        "\n",
        "**Workflow for KD Cache:**\n",
        "\n",
        "1. **First time setup** (slow):\n",
        "   - Run `precompute_teacher_topk.py` to generate cache\n",
        "   - Run \"SAVE TO GOOGLE DRIVE\" cell to persist\n",
        "   \n",
        "2. **Subsequent sessions** (fast):\n",
        "   - Run \"LOAD FROM GOOGLE DRIVE\" cell to restore cache\n",
        "   - Skip cache generation step\n",
        "\n",
        "The cached KD data (~2-3 GB compressed) contains precomputed teacher logits for knowledge distillation training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7745eb2",
      "metadata": {
        "id": "d7745eb2"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# COMPRESS TEXT CACHE (for Google Drive upload)\n",
        "# ============================================================\n",
        "# SKIP if cache is already compressed or loaded from Google Drive\n",
        "\n",
        "import os\n",
        "\n",
        "if os.path.isdir(CACHE_DIR_TEXT):\n",
        "    print(f\"[gzip] Compressing {CACHE_DIR_TEXT}...\")\n",
        "    !tar -zcvf {CACHE_DIR_TEXT}.tgz {CACHE_DIR_TEXT}\n",
        "    compressed_size = os.path.getsize(f\"{CACHE_DIR_TEXT}.tgz\")\n",
        "    print(f\"[gzip] Done: {compressed_size / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(f\"[gzip] Directory {CACHE_DIR_TEXT} not found. Skipping.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vlhuS4N9GbN4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlhuS4N9GbN4",
        "outputId": "974e0da4-8e12-4c8c-f4c4-d62be4bae769"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[cache] Copying alpaca_chat_think_L128_K32_R256.tgz from Google Drive...\n",
            "rsync: [sender] link_stat \"/content/drive/MyDrive/qwen3_caches/alpaca_chat_think_L128_K32_R256.tgz\" failed: No such file or directory (2)\n",
            "              0 100%    0.00kB/s    0:00:00 (xfr#0, to-chk=0/0)\n",
            "rsync error: some files/attrs were not transferred (see previous errors) (code 23) at main.c(1338) [sender=3.2.7]\n",
            "[cache] Extracting alpaca_chat_think_L128_K32_R256.tgz...\n",
            "tar (child): caches/alpaca_chat_think_L128_K32_R256.tgz: Cannot open: No such file or directory\n",
            "tar (child): Error is not recoverable: exiting now\n",
            "tar: Child returned status 2\n",
            "tar: Error is not recoverable: exiting now\n",
            "[cache] ERROR: Failed to extract alpaca_chat_think_L128_K32_R256\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# LOAD CACHED KD DATA FROM GOOGLE DRIVE (run this cell first!)\n",
        "# ============================================================\n",
        "# Mount Google Drive and copy cached KD data back to local storage\n",
        "# This avoids regenerating the cache every session\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create local cache directory\n",
        "!mkdir -p caches\n",
        "\n",
        "# Choose which cache to load (uncomment the one you need)\n",
        "#CACHE_NAME = \"Q4B_alpaca_chat_think_L128_K32_R256\"  # For Qwen3-4B thinking model\n",
        "CACHE_NAME = \"alpaca_chat_think_L128_K32_R256\"  # For Qwen3-4B thinking model\n",
        "\n",
        "# Copy from Google Drive to local\n",
        "print(f\"[cache] Copying {CACHE_NAME}.tgz from Google Drive...\")\n",
        "!rsync -ah --info=progress2 \\\n",
        "  /content/drive/MyDrive/qwen3_caches/{CACHE_NAME}.tgz \\\n",
        "  caches/\n",
        "\n",
        "# Unzip the cache\n",
        "print(f\"[cache] Extracting {CACHE_NAME}.tgz...\")\n",
        "!tar -xzf caches/{CACHE_NAME}.tgz -C .\n",
        "\n",
        "# Verify extraction\n",
        "import os\n",
        "if os.path.isdir(f\"caches/{CACHE_NAME}\"):\n",
        "    num_shards = len([f for f in os.listdir(f\"caches/{CACHE_NAME}\") if f.startswith(\"shard_\")])\n",
        "    print(f\"[cache] Successfully loaded {CACHE_NAME} with {num_shards} shards\")\n",
        "else:\n",
        "    print(f\"[cache] ERROR: Failed to extract {CACHE_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NKhv4eaYs4qR",
      "metadata": {
        "id": "NKhv4eaYs4qR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AHSsmdhzsBTo",
      "metadata": {
        "id": "AHSsmdhzsBTo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A7UPbdfK3c2x",
      "metadata": {
        "id": "A7UPbdfK3c2x"
      },
      "outputs": [],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_stream_q2\"\n",
        "\n",
        "# DISABLED --- NOTE used! see #5 for first QAT step\n",
        "# Construct the command string in Python to ensure variable interpolation\n",
        "command_str = f\"\"\"python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --teacher_model_name_or_path {MODEL_NAME} \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --dataset_name allenai/c4 \\\n",
        "  --dataset_config_name en \\\n",
        "  --dataset_split train \\\n",
        "  --dataset_format text \\\n",
        "  --dataset_text_field text \\\n",
        "  --streaming \\\n",
        "  --shuffle_buffer 10000 \\\n",
        "  --output_dir {RUN_DIR} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 1 \\\n",
        "  --gradient_accumulation_steps 16 \\\n",
        "  --learning_rate 5e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 50 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --logging_steps 10 \\\n",
        "  --save_steps 50\"\"\"\n",
        "\n",
        "# Execute the constructed command string\n",
        "!{command_str}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UmgbC-RI3c2x",
      "metadata": {
        "id": "UmgbC-RI3c2x"
      },
      "source": [
        "### (Optional) Resume\n",
        "\n",
        "`--resume_from_checkpoint auto` resolves to `checkpoint_last.pt` if it exists in the output directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mJqdmXSA3c2x",
      "metadata": {
        "id": "mJqdmXSA3c2x"
      },
      "outputs": [],
      "source": [
        "# !python scripts/train_qat.py ... --output_dir {RUN_DIR} --max_steps 500 --resume_from_checkpoint auto\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R7UZBhau3c2x",
      "metadata": {
        "id": "R7UZBhau3c2x"
      },
      "source": [
        "## 5)  KD-cache: precompute teacher top-k + negatives\n",
        "\n",
        "Cache mode is MPS-friendly:\n",
        "- no teacher model during training\n",
        "- no full-vocab logits\n",
        "\n",
        "If you see good KD loss but bad greedy decoding, increase negative coverage (`--rand_neg`) and/or add hard top-1 terms:\n",
        "- `--hard-top1-weight 0.05`\n",
        "- `--hard-full-top1-weight 0.02`–`0.05`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g4s_PzQW3c2x",
      "metadata": {
        "id": "g4s_PzQW3c2x"
      },
      "source": [
        "### KD-cache QAT training\n",
        "\n",
        "This uses cached teacher signals + candidate softmax."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06LaYj0vPIE7",
      "metadata": {
        "id": "06LaYj0vPIE7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1Li8Aysa3c2x",
      "metadata": {
        "id": "1Li8Aysa3c2x"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STAGE 1: KD-QAT (Conservative - freeze MLP/Attention)\n",
        "# ============================================================\n",
        "# First training stage with frozen output layers for stability\n",
        "\n",
        "%pwd\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "CACHE_DIR = CACHE_DIR_CHAT\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2\"\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --output_dir {RUN_DIR_CACHE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 1 \\\n",
        "  --gradient_accumulation_steps 4 \\\n",
        "  --learning_rate 5e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 1000 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 5 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.05 \\\n",
        "  --hard-full-top1-weight 0.03 \\\n",
        "  --ov-freeze \\\n",
        "  --freeze-last-mlp \\\n",
        "  --freeze-last-mlp-layers 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UKgDA-7OuA8m",
      "metadata": {
        "id": "UKgDA-7OuA8m"
      },
      "outputs": [],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --prompt \"What capital city of France is?\" \\\n",
        "  --do_sample true \\\n",
        "  --max_new_tokens 64\n",
        "\n",
        "#  --prompt \"What is Capital of france?\" \\\n",
        "#   --prompt \"What is Apple Neural Engine?\" \\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sexJNkyJyMgv",
      "metadata": {
        "id": "sexJNkyJyMgv"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb19fb17",
      "metadata": {
        "id": "eb19fb17"
      },
      "outputs": [],
      "source": [
        "# Define source and destination paths\n",
        "SOURCE_FILE = \"runs/qwen3_kdqat_cache_q2_2/qat_state_dict.pt\"\n",
        "DEST_DIR_GD = \"/content/drive/MyDrive/runs/Q4B/q2_2/\"\n",
        "\n",
        "# Ensure the destination directory exists on Google Drive\n",
        "!mkdir -p {DEST_DIR_GD}\n",
        "\n",
        "# Copy the file to Google Drive\n",
        "!cp -v {SOURCE_FILE} {DEST_DIR_GD}\n",
        "print(f\"Copied {SOURCE_FILE} to {DEST_DIR_GD}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hbfDaYP5yN6-",
      "metadata": {
        "id": "hbfDaYP5yN6-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JUvQIIDeRUF6",
      "metadata": {
        "id": "JUvQIIDeRUF6"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STAGE 2: KD-QAT (Unfrozen layers, resume from Stage 1)\n",
        "# ============================================================\n",
        "# Continue training with all layers unfrozen\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "CACHE_DIR = CACHE_DIR_CHAT\n",
        "INIT_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2\"\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_DIR_CACHE}/qat_state_dict.pt \\\n",
        "  --output_dir {RUN_DIR_CACHE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 1 \\\n",
        "  --gradient_accumulation_steps 4 \\\n",
        "  --learning_rate 5e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 1000 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --ov-freeze \\\n",
        "  --hard-top1-weight 0.02 \\\n",
        "  --hard-full-top1-weight 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qsfHq1gvtpg-",
      "metadata": {
        "id": "qsfHq1gvtpg-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "KXQUTgEJfzl4",
      "metadata": {
        "id": "KXQUTgEJfzl4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7e3e013-2a2e-4cdb-97e5-57837e55ddeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-24 09:45:19.295044: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-24 09:45:19.313067: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766569519.334706  140859 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766569519.341210  140859 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766569519.357631  140859 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766569519.357675  140859 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766569519.357678  140859 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766569519.357681  140859 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 09:45:19.362606: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[device] cuda | amp_dtype=torch.bfloat16 | param_dtype=torch.bfloat16\n",
            "[qat] weight_bits=4\n",
            "[init] loading model state from runs/progressive_qat_v1/qat_state_dict.pt\n",
            "[init] model weights loaded.\n",
            "[kd-cache] cache topk=32\n",
            "[kd-cache] dir=caches/alpaca_chat_think_L128_K32_R256 | weight=1.0 | T=2.0 | hard_top1=0.0 | hard_full_top1=0.0005\n",
            "opt_step: 100% 1000/1000 [17:56<00:00,  1.08s/step, loss=0.5243, lr=0.00e+00]\n",
            "Done. QAT checkpoint saved to: runs/qwen3_kdqat_cache_q2_3/qat_state_dict.pt\n"
          ]
        }
      ],
      "source": [
        "# Stage 3 resume KD-QAT with unfrozen attention and relaxed hard-top/full!\n",
        "\n",
        "CACHE_DIR = \"caches/alpaca_chat_think_L128_K32_R256\"\n",
        "#INIT_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "INIT_DIR_CACHE =  \"runs/progressive_qat_v1\"\n",
        "\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_3\"\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_DIR_CACHE}/qat_state_dict.pt \\\n",
        "  --output_dir {RUN_DIR_CACHE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 128 \\\n",
        "  --gradient_accumulation_steps 1 \\\n",
        "  --learning_rate 5e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 1000 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.00 \\\n",
        "  --hard-full-top1-weight 0.0005\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "Q1Zjl_WnHZ4O",
      "metadata": {
        "id": "Q1Zjl_WnHZ4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2d58877-11f6-49af-b833-9b7e47c0611a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-24 10:15:35.150276: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-24 10:15:35.167601: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766571335.188876  148818 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766571335.195291  148818 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766571335.211259  148818 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766571335.211289  148818 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766571335.211291  148818 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766571335.211294  148818 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 10:15:35.216177: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[device] cuda | amp_dtype=torch.bfloat16 | param_dtype=torch.bfloat16\n",
            "[qat] weight_bits=4\n",
            "[init] loading model state from runs/qwen3_kdqat_cache_q2_3/qat_state_dict.pt\n",
            "[init] model weights loaded.\n",
            "[kd-cache] cache topk=32\n",
            "[kd-cache] dir=caches/alpaca_chat_think_L128_K32_R256 | weight=1.0 | T=2.0 | hard_top1=0.0 | hard_full_top1=0.0\n",
            "opt_step: 100% 500/500 [10:39<00:00,  1.28s/step, loss=0.5157, lr=0.00e+00]\n",
            "Done. QAT checkpoint saved to: runs/qwen3_kdqat_cache_q2_4/qat_state_dict.pt\n"
          ]
        }
      ],
      "source": [
        "# Stage 4 resume KD-QAT with unfrozen attention and relaxed hard-top/full!\n",
        "#   --hard-full-top1-weight 0.0000\n",
        "#   learning_rate 2e-6\n",
        "CACHE_DIR = \"caches/alpaca_chat_think_L128_K32_R256\"\n",
        "INIT_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_3\"\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_4\"\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_DIR_CACHE}/qat_state_dict.pt \\\n",
        "  --output_dir {RUN_DIR_CACHE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 160 \\\n",
        "  --gradient_accumulation_steps 1 \\\n",
        "  --learning_rate 2e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 500 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 5 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.00 \\\n",
        "  --hard-full-top1-weight 0.0000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acak6adahpf",
      "metadata": {
        "id": "acak6adahpf"
      },
      "source": [
        "## 5.5) Progressive Layer-by-Layer QAT (Experimental)\n",
        "\n",
        "This approach trains one layer at a time with:\n",
        "- **Local reconstruction loss**: MSE between quantized and fp MLP outputs\n",
        "- **Global KD loss**: Cached teacher logits\n",
        "- **Prefix quantized / suffix fp**: Earlier layers stay quantized, later layers use full precision\n",
        "\n",
        "### Recommended Training Order (most stable first):\n",
        "\n",
        "1. **E2E f-only** (Option 1): Train ALL f parameters at once\n",
        "   - Skip progressive passes, just run Pass 4\n",
        "   - Most stable, fastest validation\n",
        "   \n",
        "2. **Progressive f-only** (Option 2): Layer-by-layer f-param training\n",
        "   - Uses `--train_f_only` flag\n",
        "   - Disable local loss with `--local_weight 0.0`\n",
        "   \n",
        "3. **Full progressive** (Option 3): Train weights + f per layer\n",
        "   - Most aggressive, may show instability at later layers\n",
        "\n",
        "### GPU Configuration:\n",
        "\n",
        "| GPU | Recommended batch_size |\n",
        "|-----|------------------------|\n",
        "| T4 (15GB) | 2-4 |\n",
        "| V100 (32GB) | 4-8 |\n",
        "| A100 (40GB) | 8-16 |\n",
        "| A100 (80GB) / H100 | 16-32 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "scxwylalctd",
      "metadata": {
        "id": "scxwylalctd"
      },
      "outputs": [],
      "source": [
        "# ---- Progressive QAT Config ----\n",
        "# Adjust batch_size for your GPU (A100: 8-16, V100: 4-8, T4: 2-4)\n",
        "BATCH_SIZE = 96                # Increase for faster instances (A100/H100)\n",
        "STEPS_PER_LAYER_MLP = 100      # Steps per MLP layer (Pass 1 + Pass 3)\n",
        "STEPS_PER_LAYER_ATTN = 30     # Steps per attention layer (Pass 2)\n",
        "E2E_STEPS = 500               # E2E quantizer tuning steps (Pass 4)\n",
        "LOCAL_WEIGHT = 0.3            # Local reconstruction loss weight\n",
        "GLOBAL_WEIGHT = 1.0           # Global KD loss weight\n",
        "LOCAL_TOKEN_SAMPLES = 128     # Tokens to sample for local loss\n",
        "MAX_GRAD_NORM = 1.0           # Gradient clipping (important for 2-bit)\n",
        "\n",
        "# Learning rates\n",
        "LR_PROGRESSIVE = 5e-6         # Learning rate for progressive passes\n",
        "LR_E2E = 1e-4                 # Learning rate for E2E f-only tuning\n",
        "\n",
        "# Output directories\n",
        "RUN_DIR_E2E_FONLY = \"runs/e2e_f_only\"\n",
        "RUN_DIR_PROGRESSIVE_FONLY = \"runs/progressive_f_only\"\n",
        "RUN_DIR_PROGRESSIVE = \"runs/progressive_qat_v1\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q9f32hx0qo",
      "metadata": {
        "id": "q9f32hx0qo"
      },
      "source": [
        "### Option 1: E2E f-only Training (Recommended First)\n",
        "\n",
        "**Most stable approach** - trains ALL `_f_param` (quantization scales) simultaneously.\n",
        "Skip all progressive layer-by-layer passes and go straight to Pass 4.\n",
        "\n",
        "This is recommended when:\n",
        "- Progressive layer-by-layer shows instability (local loss hitting 10.0)\n",
        "- You want to validate the infrastructure works before trying progressive\n",
        "- You have limited time and want the fastest path to a working checkpoint\n",
        "\n",
        "The `f` parameter is the learnable quantization scale from Apple-style quantization:\n",
        "- Actual scale `s = softplus(f)` ensures positivity\n",
        "- Training only `f` keeps weights frozen - more stable for ultra-low-bit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vh3iifkgpj",
      "metadata": {
        "id": "vh3iifkgpj",
        "outputId": "4cc06f1d-23aa-417e-92b9-5aaea6dba2d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "[device] cuda | amp_dtype=torch.bfloat16 | param_dtype=torch.bfloat16\n",
            "[model] Loading Qwen/Qwen3-0.6B\n",
            "2025-12-24 01:53:17.534006: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-24 01:53:17.556130: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766541197.581856   16151 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766541197.587401   16151 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766541197.602015   16151 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766541197.602044   16151 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766541197.602048   16151 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766541197.602051   16151 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 01:53:17.606358: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[qat] weight_bits=4\n",
            "[kd-cache] Loading from caches/alpaca_chat_think_L128_K32_R256\n",
            "[model] 28 transformer layers\n",
            "\n",
            "============================================================\n",
            "PASS 4: E2E Quantizer Tuning (f-only)\n",
            "============================================================\n",
            "  Trainable f parameters: 196\n",
            "  step 0: loss=10.4815\n",
            "  step 10: loss=10.6626\n",
            "  step 20: loss=10.5577\n",
            "  step 30: loss=10.5951\n",
            "  step 40: loss=10.7329\n",
            "  step 50: loss=10.4800\n",
            "  step 60: loss=10.5936\n",
            "  step 70: loss=10.5929\n",
            "  step 80: loss=10.5984\n",
            "  step 90: loss=10.5669\n",
            "  step 100: loss=10.6184\n",
            "  step 110: loss=10.6343\n",
            "  step 120: loss=10.4301\n",
            "  step 130: loss=10.4968\n",
            "  step 140: loss=10.5011\n",
            "  step 150: loss=10.5783\n",
            "  step 160: loss=10.6029\n",
            "  step 170: loss=10.4627\n",
            "  step 180: loss=10.6370\n",
            "  step 190: loss=10.5549\n",
            "  step 200: loss=10.7009\n",
            "  step 210: loss=10.6632\n",
            "  step 220: loss=10.4980\n",
            "  step 230: loss=10.6916\n",
            "  step 240: loss=10.5702\n",
            "  step 250: loss=10.5969\n",
            "  step 260: loss=10.5019\n",
            "  step 270: loss=10.6430\n",
            "  step 280: loss=10.4538\n",
            "  step 290: loss=10.5254\n",
            "  step 300: loss=10.3753\n",
            "  step 310: loss=10.5633\n",
            "  step 320: loss=10.2545\n",
            "  step 330: loss=10.5237\n",
            "  step 340: loss=10.4550\n",
            "  step 350: loss=10.3413\n"
          ]
        }
      ],
      "source": [
        "# E2E f-only: Skip ALL progressive passes, train all f parameters at once\n",
        "# This is the simplest and most stable approach\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/train_qat_progressive.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --output_dir {RUN_DIR_E2E_FONLY} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --skip_mlp_pass \\\n",
        "  --skip_attention_pass \\\n",
        "  --skip_mlp_refinement \\\n",
        "  --e2e_steps {E2E_STEPS} \\\n",
        "  --e2e_learning_rate {LR_E2E} \\\n",
        "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vab9x1zlqhs",
      "metadata": {
        "id": "vab9x1zlqhs"
      },
      "source": [
        "### Option 2: Progressive f-only Training\n",
        "\n",
        "Layer-by-layer training but only trains `_f_param` (quantization scales), not weights.\n",
        "More stable than full progressive training, but may still see instability at later layers.\n",
        "\n",
        "Use `--train_f_only` flag to freeze weights and only train quantization scales per layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oqxrx3c2fe",
      "metadata": {
        "id": "oqxrx3c2fe"
      },
      "outputs": [],
      "source": [
        "# Progressive f-only: Layer-by-layer, but only train quantization scales\n",
        "# Use --train_f_only for more stable training\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/train_qat_progressive.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --output_dir {RUN_DIR_PROGRESSIVE_FONLY} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --train_f_only \\\n",
        "  --steps_per_layer_mlp {STEPS_PER_LAYER_MLP} \\\n",
        "  --steps_per_layer_attn {STEPS_PER_LAYER_ATTN} \\\n",
        "  --e2e_steps {E2E_STEPS} \\\n",
        "  --local_weight 0.0 \\\n",
        "  --global_weight {GLOBAL_WEIGHT} \\\n",
        "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
        "  --learning_rate {LR_PROGRESSIVE} \\\n",
        "  --e2e_learning_rate {LR_E2E} \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --skip_mlp_refinement"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e0gsxkcc4",
      "metadata": {
        "id": "8e0gsxkcc4"
      },
      "source": [
        "**bold text**### Option 3: Full Progressive Training (weights + f)\n",
        "\n",
        "Full layer-by-layer training with weights and quantization scales.\n",
        "Most aggressive but potentially unstable for ultra-low-bit (2-bit).\n",
        "\n",
        "**Training Order (3-pass v3):**\n",
        "1. **Pass 1**: Train MLP layers (local reconstruction + global KD)\n",
        "2. **Pass 2**: Train attention layers (global KD only)\n",
        "3. **Pass 3**: MLP refinement (addresses MLP-attention coupling)\n",
        "4. **Pass 4**: E2E quantizer-only tuning (f-param only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "7qzyw0rd18e",
      "metadata": {
        "id": "7qzyw0rd18e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74d666d4-059a-4d7c-d488-a64ca476a24d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  step 20: local=0.0812 global=0.6635\n",
            "  step 30: local=0.0819 global=0.6992\n",
            "  step 40: local=0.0810 global=0.7120\n",
            "  step 50: local=0.0845 global=0.6668\n",
            "  step 60: local=0.0818 global=0.6478\n",
            "  step 70: local=0.0853 global=0.6801\n",
            "  step 80: local=0.0810 global=0.6688\n",
            "  step 90: local=0.0860 global=0.7088\n",
            "  Layer 10 not converged (global=0.7042 > 0.4), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0820 global=0.6838\n",
            "  step 10: local=0.0811 global=0.7135\n",
            "  step 20: local=0.0861 global=0.6800\n",
            "  step 30: local=0.0830 global=0.6718\n",
            "  step 40: local=0.0811 global=0.6880\n",
            "  step 50: local=0.0815 global=0.6670\n",
            "  step 60: local=0.0834 global=0.6685\n",
            "  step 70: local=0.0827 global=0.6548\n",
            "  step 80: local=0.0831 global=0.6848\n",
            "  step 90: local=0.0803 global=0.7109\n",
            "  Layer 10 not converged (global=0.6552 > 0.4), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0855 global=0.7128\n",
            "  step 10: local=0.0811 global=0.6635\n",
            "  step 20: local=0.0841 global=0.7079\n",
            "  step 30: local=0.0818 global=0.7036\n",
            "  step 40: local=0.0809 global=0.6847\n",
            "  step 50: local=0.0819 global=0.6855\n",
            "  step 60: local=0.0819 global=0.6747\n",
            "  step 70: local=0.0817 global=0.6682\n",
            "  step 80: local=0.0833 global=0.7038\n",
            "  step 90: local=0.0821 global=0.6916\n",
            "  Layer 10 not converged (global=0.7044 > 0.4), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0836 global=0.6854\n",
            "  step 10: local=0.0810 global=0.6457\n",
            "  step 20: local=0.0875 global=0.6724\n",
            "  step 30: local=0.0866 global=0.7175\n",
            "  step 40: local=0.0830 global=0.6753\n",
            "  step 50: local=0.0786 global=0.6727\n",
            "  step 60: local=0.0817 global=0.6887\n",
            "  step 70: local=0.0819 global=0.6766\n",
            "  step 80: local=0.0824 global=0.6503\n",
            "  step 90: local=0.0840 global=0.6424\n",
            "  Layer 10 not converged (global=0.6624 > 0.4), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0816 global=0.6923\n",
            "  step 10: local=0.0833 global=0.6869\n",
            "  step 20: local=0.0796 global=0.6584\n",
            "  step 30: local=0.0813 global=0.7337\n",
            "  step 40: local=0.0832 global=0.6709\n",
            "  step 50: local=0.0821 global=0.6612\n",
            "  step 60: local=0.0829 global=0.6970\n",
            "  step 70: local=0.0850 global=0.6838\n",
            "  step 80: local=0.0825 global=0.7211\n",
            "  step 90: local=0.0847 global=0.6916\n",
            "  Layer 10 not converged (global=0.6492 > 0.4), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0824 global=0.6786\n",
            "  step 10: local=0.0827 global=0.6755\n",
            "  step 20: local=0.0788 global=0.6710\n",
            "  step 30: local=0.0853 global=0.7072\n",
            "  step 40: local=0.0823 global=0.6572\n",
            "  step 50: local=0.0800 global=0.7002\n",
            "  step 60: local=0.0845 global=0.6431\n",
            "  step 70: local=0.0813 global=0.7138\n",
            "  step 80: local=0.0837 global=0.6646\n",
            "  step 90: local=0.0843 global=0.7151\n",
            "  Layer 10 not converged (global=0.7063 > 0.4), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0814 global=0.6761\n",
            "  step 10: local=0.0799 global=0.7014\n",
            "  step 20: local=0.0857 global=0.7392\n",
            "  step 30: local=0.0806 global=0.7028\n",
            "  step 40: local=0.0840 global=0.6687\n",
            "  step 50: local=0.0830 global=0.6942\n",
            "  step 60: local=0.0828 global=0.7326\n",
            "  step 70: local=0.0855 global=0.6808\n",
            "  step 80: local=0.0801 global=0.6930\n",
            "  step 90: local=0.0846 global=0.7099\n",
            "  Layer 10 not converged (global=0.6367 > 0.4), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0806 global=0.6504\n",
            "  step 10: local=0.0843 global=0.7096\n",
            "  step 20: local=0.0823 global=0.7304\n",
            "  step 30: local=0.0857 global=0.7136\n",
            "  step 40: local=0.0801 global=0.7142\n",
            "  step 50: local=0.0823 global=0.6683\n",
            "  step 60: local=0.0850 global=0.6655\n",
            "  step 70: local=0.0817 global=0.6944\n",
            "  step 80: local=0.0844 global=0.6669\n",
            "  step 90: local=0.0819 global=0.7120\n",
            "  Layer 10 not converged (global=0.6735 > 0.4), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0796 global=0.6965\n",
            "  step 10: local=0.0820 global=0.6824\n",
            "  step 20: local=0.0804 global=0.6749\n",
            "  step 30: local=0.0821 global=0.6455\n",
            "  step 40: local=0.0834 global=0.6596\n",
            "  step 50: local=0.0824 global=0.6689\n",
            "  step 60: local=0.0834 global=0.6847\n",
            "  step 70: local=0.0843 global=0.6832\n",
            "  step 80: local=0.0861 global=0.7026\n",
            "  step 90: local=0.0841 global=0.6822\n",
            "  Layer 10 not converged (global=0.6791 > 0.4), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0837 global=0.6563\n",
            "  step 10: local=0.0811 global=0.6889\n",
            "  step 20: local=0.0821 global=0.6352\n",
            "  step 30: local=0.0820 global=0.6987\n",
            "  step 40: local=0.0799 global=0.6858\n",
            "  step 50: local=0.0835 global=0.6376\n",
            "  step 60: local=0.0810 global=0.6925\n",
            "  step 70: local=0.0797 global=0.6993\n",
            "  step 80: local=0.0851 global=0.7040\n",
            "  step 90: local=0.0868 global=0.6466\n",
            "  Layer 10 not converged (global=0.6625 > 0.4), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0836 global=0.6748\n",
            "  step 10: local=0.0804 global=0.6854\n",
            "  step 20: local=0.0821 global=0.6933\n",
            "  step 30: local=0.0846 global=0.6525\n",
            "  step 40: local=0.0803 global=0.6513\n",
            "  step 50: local=0.0834 global=0.6749\n",
            "  step 60: local=0.0818 global=0.6566\n",
            "  step 70: local=0.0824 global=0.6931\n",
            "  step 80: local=0.0840 global=0.7052\n",
            "  step 90: local=0.0858 global=0.6598\n",
            "  Layer 10 not converged (global=0.6656 > 0.4), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0828 global=0.6415\n",
            "  step 10: local=0.0809 global=0.6742\n",
            "  step 20: local=0.0838 global=0.6634\n",
            "  step 30: local=0.0811 global=0.7031\n",
            "  step 40: local=0.0812 global=0.6766\n",
            "  step 50: local=0.0844 global=0.7065\n",
            "  step 60: local=0.0841 global=0.6740\n",
            "  step 70: local=0.0852 global=0.6654\n",
            "  step 80: local=0.0831 global=0.6822\n",
            "  step 90: local=0.0833 global=0.6623\n",
            "  Layer 10 not converged (global=0.7050 > 0.4), repeating...\n",
            "\n",
            "--- Layer 10/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0837 global=0.6638\n",
            "  step 10: local=0.0825 global=0.6495\n",
            "  step 20: local=0.0839 global=0.6787\n",
            "  step 30: local=0.0810 global=0.7057\n",
            "  step 40: local=0.0852 global=0.7069\n",
            "  step 50: local=0.0829 global=0.6584\n",
            "  step 60: local=0.0819 global=0.7029\n",
            "  step 70: local=0.0844 global=0.6990\n",
            "  step 80: local=0.0834 global=0.6794\n",
            "  step 90: local=0.0797 global=0.6808\n",
            "  [WARN] Layer 10 did not converge after 20 repeats (global=0.6175)\n",
            "\n",
            "--- Layer 11/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0951 global=0.6802\n",
            "  step 10: local=0.0912 global=0.6700\n",
            "  step 20: local=0.0885 global=0.7046\n",
            "  step 30: local=0.0930 global=0.6921\n",
            "  step 40: local=0.0862 global=0.6847\n",
            "  step 50: local=0.0851 global=0.6450\n",
            "  step 60: local=0.0873 global=0.6717\n",
            "  step 70: local=0.0810 global=0.7144\n",
            "  step 80: local=0.0824 global=0.6720\n",
            "  step 90: local=0.0857 global=0.6703\n",
            "  Layer 11 not converged (global=0.7677 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0898 global=0.6864\n",
            "  step 10: local=0.0872 global=0.6736\n",
            "  step 20: local=0.0868 global=0.6447\n",
            "  step 30: local=0.0838 global=0.6399\n",
            "  step 40: local=0.0786 global=0.6858\n",
            "  step 50: local=0.0788 global=0.6821\n",
            "  step 60: local=0.0782 global=0.6526\n",
            "  step 70: local=0.0803 global=0.7264\n",
            "  step 80: local=0.0791 global=0.6668\n",
            "  step 90: local=0.0783 global=0.6530\n",
            "  Layer 11 not converged (global=0.6810 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0772 global=0.6895\n",
            "  step 10: local=0.0819 global=0.6768\n",
            "  step 20: local=0.0753 global=0.7120\n",
            "  step 30: local=0.0768 global=0.6838\n",
            "  step 40: local=0.0753 global=0.6693\n",
            "  step 50: local=0.0755 global=0.6667\n",
            "  step 60: local=0.0741 global=0.6607\n",
            "  step 70: local=0.0713 global=0.6977\n",
            "  step 80: local=0.0729 global=0.6463\n",
            "  step 90: local=0.0683 global=0.6890\n",
            "  Layer 11 not converged (global=0.6490 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0731 global=0.6329\n",
            "  step 10: local=0.0731 global=0.7004\n",
            "  step 20: local=0.0713 global=0.6518\n",
            "  step 30: local=0.0682 global=0.7037\n",
            "  step 40: local=0.0710 global=0.6634\n",
            "  step 50: local=0.0711 global=0.6892\n",
            "  step 60: local=0.0694 global=0.7261\n",
            "  step 70: local=0.0719 global=0.6902\n",
            "  step 80: local=0.0727 global=0.6541\n",
            "  step 90: local=0.0711 global=0.6804\n",
            "  Layer 11 not converged (global=0.6811 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0684 global=0.7169\n",
            "  step 10: local=0.0725 global=0.6643\n",
            "  step 20: local=0.0694 global=0.6772\n",
            "  step 30: local=0.0686 global=0.6947\n",
            "  step 40: local=0.0694 global=0.6350\n",
            "  step 50: local=0.0679 global=0.6945\n",
            "  step 60: local=0.0718 global=0.7145\n",
            "  step 70: local=0.0695 global=0.6988\n",
            "  step 80: local=0.0710 global=0.6962\n",
            "  step 90: local=0.0673 global=0.6532\n",
            "  Layer 11 not converged (global=0.6535 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0684 global=0.6508\n",
            "  step 10: local=0.0715 global=0.6780\n",
            "  step 20: local=0.0707 global=0.6508\n",
            "  step 30: local=0.0692 global=0.6952\n",
            "  step 40: local=0.0721 global=0.6797\n",
            "  step 50: local=0.0666 global=0.6657\n",
            "  step 60: local=0.0676 global=0.6578\n",
            "  step 70: local=0.0672 global=0.6274\n",
            "  step 80: local=0.0715 global=0.6417\n",
            "  step 90: local=0.0649 global=0.6527\n",
            "  Layer 11 not converged (global=0.6591 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0661 global=0.6653\n",
            "  step 10: local=0.0697 global=0.6639\n",
            "  step 20: local=0.0677 global=0.6837\n",
            "  step 30: local=0.0663 global=0.6640\n",
            "  step 40: local=0.0702 global=0.6404\n",
            "  step 50: local=0.0653 global=0.6700\n",
            "  step 60: local=0.0668 global=0.6184\n",
            "  step 70: local=0.0663 global=0.6799\n",
            "  step 80: local=0.0690 global=0.6664\n",
            "  step 90: local=0.0670 global=0.6201\n",
            "  Layer 11 not converged (global=0.6628 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0682 global=0.6718\n",
            "  step 10: local=0.0680 global=0.6793\n",
            "  step 20: local=0.0677 global=0.6835\n",
            "  step 30: local=0.0663 global=0.6265\n",
            "  step 40: local=0.0667 global=0.6530\n",
            "  step 50: local=0.0645 global=0.6657\n",
            "  step 60: local=0.0681 global=0.6736\n",
            "  step 70: local=0.0679 global=0.6325\n",
            "  step 80: local=0.0682 global=0.6317\n",
            "  step 90: local=0.0661 global=0.6536\n",
            "  Layer 11 not converged (global=0.6386 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0668 global=0.6379\n",
            "  step 10: local=0.0648 global=0.6732\n",
            "  step 20: local=0.0665 global=0.6847\n",
            "  step 30: local=0.0661 global=0.6387\n",
            "  step 40: local=0.0659 global=0.6219\n",
            "  step 50: local=0.0699 global=0.6524\n",
            "  step 60: local=0.0646 global=0.6423\n",
            "  step 70: local=0.0663 global=0.6813\n",
            "  step 80: local=0.0673 global=0.6554\n",
            "  step 90: local=0.0672 global=0.6849\n",
            "  Layer 11 not converged (global=0.6572 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0675 global=0.6539\n",
            "  step 10: local=0.0663 global=0.6460\n",
            "  step 20: local=0.0658 global=0.6622\n",
            "  step 30: local=0.0715 global=0.6425\n",
            "  step 40: local=0.0675 global=0.6423\n",
            "  step 50: local=0.0697 global=0.6295\n",
            "  step 60: local=0.0650 global=0.6565\n",
            "  step 70: local=0.0634 global=0.6821\n",
            "  step 80: local=0.0651 global=0.6857\n",
            "  step 90: local=0.0677 global=0.6385\n",
            "  Layer 11 not converged (global=0.6418 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0663 global=0.6815\n",
            "  step 10: local=0.0676 global=0.6780\n",
            "  step 20: local=0.0669 global=0.6558\n",
            "  step 30: local=0.0655 global=0.6573\n",
            "  step 40: local=0.0643 global=0.6493\n",
            "  step 50: local=0.0658 global=0.6404\n",
            "  step 60: local=0.0690 global=0.6756\n",
            "  step 70: local=0.0677 global=0.6639\n",
            "  step 80: local=0.0660 global=0.6567\n",
            "  step 90: local=0.0662 global=0.6196\n",
            "  Layer 11 not converged (global=0.6683 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0697 global=0.6441\n",
            "  step 10: local=0.0686 global=0.6896\n",
            "  step 20: local=0.0628 global=0.6474\n",
            "  step 30: local=0.0643 global=0.6451\n",
            "  step 40: local=0.0685 global=0.6603\n",
            "  step 50: local=0.0674 global=0.6507\n",
            "  step 60: local=0.0678 global=0.6234\n",
            "  step 70: local=0.0644 global=0.6180\n",
            "  step 80: local=0.0668 global=0.6634\n",
            "  step 90: local=0.0675 global=0.6613\n",
            "  Layer 11 not converged (global=0.6241 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0654 global=0.6321\n",
            "  step 10: local=0.0705 global=0.7050\n",
            "  step 20: local=0.0653 global=0.6467\n",
            "  step 30: local=0.0638 global=0.6346\n",
            "  step 40: local=0.0662 global=0.6682\n",
            "  step 50: local=0.0723 global=0.6581\n",
            "  step 60: local=0.0669 global=0.6923\n",
            "  step 70: local=0.0671 global=0.6657\n",
            "  step 80: local=0.0689 global=0.6506\n",
            "  step 90: local=0.0652 global=0.6500\n",
            "  Layer 11 not converged (global=0.6372 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0674 global=0.6446\n",
            "  step 10: local=0.0643 global=0.6821\n",
            "  step 20: local=0.0600 global=0.6302\n",
            "  step 30: local=0.0655 global=0.6721\n",
            "  step 40: local=0.0654 global=0.6175\n",
            "  step 50: local=0.0651 global=0.6854\n",
            "  step 60: local=0.0658 global=0.6377\n",
            "  step 70: local=0.0657 global=0.6887\n",
            "  step 80: local=0.0691 global=0.6503\n",
            "  step 90: local=0.0657 global=0.6756\n",
            "  Layer 11 not converged (global=0.6507 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0655 global=0.7146\n",
            "  step 10: local=0.0676 global=0.6765\n",
            "  step 20: local=0.0666 global=0.6428\n",
            "  step 30: local=0.0664 global=0.6679\n",
            "  step 40: local=0.0649 global=0.7032\n",
            "  step 50: local=0.0654 global=0.6525\n",
            "  step 60: local=0.0652 global=0.6660\n",
            "  step 70: local=0.0677 global=0.6844\n",
            "  step 80: local=0.0667 global=0.6258\n",
            "  step 90: local=0.0658 global=0.6837\n",
            "  Layer 11 not converged (global=0.6387 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0677 global=0.7043\n",
            "  step 10: local=0.0658 global=0.6891\n",
            "  step 20: local=0.0641 global=0.6861\n",
            "  step 30: local=0.0653 global=0.6443\n",
            "  step 40: local=0.0660 global=0.6409\n",
            "  step 50: local=0.0667 global=0.6686\n",
            "  step 60: local=0.0657 global=0.6418\n",
            "  step 70: local=0.0670 global=0.6850\n",
            "  step 80: local=0.0665 global=0.6707\n",
            "  step 90: local=0.0658 global=0.6569\n",
            "  Layer 11 not converged (global=0.6579 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0675 global=0.6492\n",
            "  step 10: local=0.0676 global=0.6199\n",
            "  step 20: local=0.0657 global=0.6333\n",
            "  step 30: local=0.0664 global=0.6443\n",
            "  step 40: local=0.0667 global=0.6567\n",
            "  step 50: local=0.0654 global=0.6564\n",
            "  step 60: local=0.0667 global=0.6766\n",
            "  step 70: local=0.0644 global=0.6561\n",
            "  step 80: local=0.0670 global=0.6330\n",
            "  step 90: local=0.0677 global=0.6629\n",
            "  Layer 11 not converged (global=0.6340 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0680 global=0.6121\n",
            "  step 10: local=0.0681 global=0.6730\n",
            "  step 20: local=0.0675 global=0.6603\n",
            "  step 30: local=0.0681 global=0.6143\n",
            "  step 40: local=0.0687 global=0.6648\n",
            "  step 50: local=0.0640 global=0.6723\n",
            "  step 60: local=0.0664 global=0.6773\n",
            "  step 70: local=0.0633 global=0.6209\n",
            "  step 80: local=0.0639 global=0.6475\n",
            "  step 90: local=0.0649 global=0.6598\n",
            "  Layer 11 not converged (global=0.6492 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0647 global=0.6684\n",
            "  step 10: local=0.0681 global=0.6270\n",
            "  step 20: local=0.0656 global=0.6262\n",
            "  step 30: local=0.0680 global=0.6485\n",
            "  step 40: local=0.0645 global=0.6314\n",
            "  step 50: local=0.0661 global=0.6673\n",
            "  step 60: local=0.0672 global=0.6795\n",
            "  step 70: local=0.0657 global=0.6343\n",
            "  step 80: local=0.0622 global=0.6176\n",
            "  step 90: local=0.0632 global=0.6473\n",
            "  Layer 11 not converged (global=0.6862 > 0.4), repeating...\n",
            "\n",
            "--- Layer 11/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0645 global=0.6383\n",
            "  step 10: local=0.0694 global=0.6764\n",
            "  step 20: local=0.0653 global=0.6516\n",
            "  step 30: local=0.0671 global=0.6802\n",
            "  step 40: local=0.0670 global=0.6479\n",
            "  step 50: local=0.0665 global=0.6416\n",
            "  step 60: local=0.0645 global=0.6575\n",
            "  step 70: local=0.0633 global=0.6373\n",
            "  step 80: local=0.0647 global=0.6379\n",
            "  step 90: local=0.0649 global=0.6256\n",
            "  [WARN] Layer 11 did not converge after 20 repeats (global=0.7015)\n",
            "\n",
            "--- Layer 12/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0772 global=0.6636\n",
            "  step 10: local=0.0766 global=0.6863\n",
            "  step 20: local=0.0730 global=0.6880\n",
            "  step 30: local=0.0718 global=0.6407\n",
            "  step 40: local=0.0744 global=0.6825\n",
            "  step 50: local=0.0759 global=0.6791\n",
            "  step 60: local=0.0719 global=0.6564\n",
            "  step 70: local=0.0694 global=0.6581\n",
            "  step 80: local=0.0739 global=0.6483\n",
            "  step 90: local=0.0681 global=0.6395\n",
            "  Layer 12 not converged (global=0.6307 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0708 global=0.6745\n",
            "  step 10: local=0.0688 global=0.6635\n",
            "  step 20: local=0.0679 global=0.6526\n",
            "  step 30: local=0.0714 global=0.6166\n",
            "  step 40: local=0.0685 global=0.6404\n",
            "  step 50: local=0.0649 global=0.6847\n",
            "  step 60: local=0.0648 global=0.6425\n",
            "  step 70: local=0.0636 global=0.6398\n",
            "  step 80: local=0.0615 global=0.6569\n",
            "  step 90: local=0.0628 global=0.6451\n",
            "  Layer 12 not converged (global=0.6493 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0659 global=0.6190\n",
            "  step 10: local=0.0633 global=0.6121\n",
            "  step 20: local=0.0640 global=0.6578\n",
            "  step 30: local=0.0677 global=0.6547\n",
            "  step 40: local=0.0642 global=0.6253\n",
            "  step 50: local=0.0618 global=0.6974\n",
            "  step 60: local=0.0632 global=0.6386\n",
            "  step 70: local=0.0660 global=0.6278\n",
            "  step 80: local=0.0648 global=0.6589\n",
            "  step 90: local=0.0640 global=0.6482\n",
            "  Layer 12 not converged (global=0.6367 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0627 global=0.6826\n",
            "  step 10: local=0.0662 global=0.6550\n",
            "  step 20: local=0.0625 global=0.6387\n",
            "  step 30: local=0.0627 global=0.6405\n",
            "  step 40: local=0.0640 global=0.6328\n",
            "  step 50: local=0.0613 global=0.6708\n",
            "  step 60: local=0.0638 global=0.6173\n",
            "  step 70: local=0.0608 global=0.6586\n",
            "  step 80: local=0.0606 global=0.6045\n",
            "  step 90: local=0.0610 global=0.6716\n",
            "  Layer 12 not converged (global=0.6638 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0620 global=0.6256\n",
            "  step 10: local=0.0633 global=0.6733\n",
            "  step 20: local=0.0633 global=0.6365\n",
            "  step 30: local=0.0618 global=0.6617\n",
            "  step 40: local=0.0610 global=0.7002\n",
            "  step 50: local=0.0624 global=0.6614\n",
            "  step 60: local=0.0628 global=0.6290\n",
            "  step 70: local=0.0618 global=0.6521\n",
            "  step 80: local=0.0632 global=0.6879\n",
            "  step 90: local=0.0603 global=0.6379\n",
            "  Layer 12 not converged (global=0.6435 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0571 global=0.6505\n",
            "  step 10: local=0.0601 global=0.6680\n",
            "  step 20: local=0.0625 global=0.6105\n",
            "  step 30: local=0.0574 global=0.6679\n",
            "  step 40: local=0.0632 global=0.6877\n",
            "  step 50: local=0.0589 global=0.6732\n",
            "  step 60: local=0.0606 global=0.6704\n",
            "  step 70: local=0.0618 global=0.6288\n",
            "  step 80: local=0.0619 global=0.6250\n",
            "  step 90: local=0.0642 global=0.6507\n",
            "  Layer 12 not converged (global=0.6505 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0616 global=0.6254\n",
            "  step 10: local=0.0563 global=0.6685\n",
            "  step 20: local=0.0595 global=0.6551\n",
            "  step 30: local=0.0603 global=0.6409\n",
            "  step 40: local=0.0628 global=0.6304\n",
            "  step 50: local=0.0582 global=0.6033\n",
            "  step 60: local=0.0594 global=0.6163\n",
            "  step 70: local=0.0579 global=0.6291\n",
            "  step 80: local=0.0608 global=0.6391\n",
            "  step 90: local=0.0599 global=0.6393\n",
            "  Layer 12 not converged (global=0.6259 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0613 global=0.6590\n",
            "  step 10: local=0.0613 global=0.6387\n",
            "  step 20: local=0.0625 global=0.6178\n",
            "  step 30: local=0.0606 global=0.6472\n",
            "  step 40: local=0.0593 global=0.5956\n",
            "  step 50: local=0.0633 global=0.6554\n",
            "  step 60: local=0.0586 global=0.6445\n",
            "  step 70: local=0.0653 global=0.5974\n",
            "  step 80: local=0.0613 global=0.6474\n",
            "  step 90: local=0.0587 global=0.6546\n",
            "  Layer 12 not converged (global=0.6668 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0605 global=0.6592\n",
            "  step 10: local=0.0569 global=0.6049\n",
            "  step 20: local=0.0592 global=0.6294\n",
            "  step 30: local=0.0588 global=0.6437\n",
            "  step 40: local=0.0583 global=0.6516\n",
            "  step 50: local=0.0590 global=0.6093\n",
            "  step 60: local=0.0582 global=0.6087\n",
            "  step 70: local=0.0590 global=0.6303\n",
            "  step 80: local=0.0585 global=0.6125\n",
            "  step 90: local=0.0590 global=0.6500\n",
            "  Layer 12 not converged (global=0.6986 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0623 global=0.6618\n",
            "  step 10: local=0.0601 global=0.6160\n",
            "  step 20: local=0.0594 global=0.6011\n",
            "  step 30: local=0.0611 global=0.6287\n",
            "  step 40: local=0.0603 global=0.6196\n",
            "  step 50: local=0.0611 global=0.6583\n",
            "  step 60: local=0.0616 global=0.6338\n",
            "  step 70: local=0.0584 global=0.6622\n",
            "  step 80: local=0.0616 global=0.6292\n",
            "  step 90: local=0.0619 global=0.6243\n",
            "  Layer 12 not converged (global=0.6332 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0606 global=0.6390\n",
            "  step 10: local=0.0604 global=0.6188\n",
            "  step 20: local=0.0593 global=0.6197\n",
            "  step 30: local=0.0631 global=0.6076\n",
            "  step 40: local=0.0592 global=0.6343\n",
            "  step 50: local=0.0616 global=0.6589\n",
            "  step 60: local=0.0629 global=0.6613\n",
            "  step 70: local=0.0572 global=0.6177\n",
            "  step 80: local=0.0599 global=0.6585\n",
            "  step 90: local=0.0580 global=0.6548\n",
            "  Layer 12 not converged (global=0.6611 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0622 global=0.6330\n",
            "  step 10: local=0.0588 global=0.6346\n",
            "  step 20: local=0.0598 global=0.6264\n",
            "  step 30: local=0.0582 global=0.6180\n",
            "  step 40: local=0.0615 global=0.6512\n",
            "  step 50: local=0.0581 global=0.6422\n",
            "  step 60: local=0.0589 global=0.6327\n",
            "  step 70: local=0.0601 global=0.5966\n",
            "  step 80: local=0.0606 global=0.6206\n",
            "  step 90: local=0.0600 global=0.6659\n",
            "  Layer 12 not converged (global=0.6331 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0588 global=0.6243\n",
            "  step 10: local=0.0585 global=0.6212\n",
            "  step 20: local=0.0605 global=0.6381\n",
            "  step 30: local=0.0571 global=0.6281\n",
            "  step 40: local=0.0592 global=0.6015\n",
            "  step 50: local=0.0615 global=0.5961\n",
            "  step 60: local=0.0603 global=0.6398\n",
            "  step 70: local=0.0597 global=0.6388\n",
            "  step 80: local=0.0560 global=0.6106\n",
            "  step 90: local=0.0598 global=0.6807\n",
            "  Layer 12 not converged (global=0.6216 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0608 global=0.6244\n",
            "  step 10: local=0.0595 global=0.6137\n",
            "  step 20: local=0.0568 global=0.6439\n",
            "  step 30: local=0.0617 global=0.6362\n",
            "  step 40: local=0.0573 global=0.6683\n",
            "  step 50: local=0.0606 global=0.6428\n",
            "  step 60: local=0.0606 global=0.6264\n",
            "  step 70: local=0.0608 global=0.6285\n",
            "  step 80: local=0.0583 global=0.6219\n",
            "  step 90: local=0.0580 global=0.6597\n",
            "  Layer 12 not converged (global=0.6469 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0596 global=0.6070\n",
            "  step 10: local=0.0573 global=0.6477\n",
            "  step 20: local=0.0596 global=0.5949\n",
            "  step 30: local=0.0568 global=0.6626\n",
            "  step 40: local=0.0582 global=0.6154\n",
            "  step 50: local=0.0590 global=0.6642\n",
            "  step 60: local=0.0570 global=0.6268\n",
            "  step 70: local=0.0586 global=0.6515\n",
            "  step 80: local=0.0586 global=0.6909\n",
            "  step 90: local=0.0577 global=0.6532\n",
            "  Layer 12 not converged (global=0.6320 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0592 global=0.6202\n",
            "  step 10: local=0.0614 global=0.6438\n",
            "  step 20: local=0.0615 global=0.6802\n",
            "  step 30: local=0.0632 global=0.6301\n",
            "  step 40: local=0.0561 global=0.6421\n",
            "  step 50: local=0.0614 global=0.6602\n",
            "  step 60: local=0.0587 global=0.6035\n",
            "  step 70: local=0.0588 global=0.6613\n",
            "  step 80: local=0.0618 global=0.6805\n",
            "  step 90: local=0.0577 global=0.6656\n",
            "  Layer 12 not converged (global=0.6158 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0585 global=0.6632\n",
            "  step 10: local=0.0612 global=0.6219\n",
            "  step 20: local=0.0605 global=0.6181\n",
            "  step 30: local=0.0608 global=0.6447\n",
            "  step 40: local=0.0589 global=0.6182\n",
            "  step 50: local=0.0603 global=0.6615\n",
            "  step 60: local=0.0590 global=0.6488\n",
            "  step 70: local=0.0623 global=0.6355\n",
            "  step 80: local=0.0586 global=0.6246\n",
            "  step 90: local=0.0599 global=0.5974\n",
            "  Layer 12 not converged (global=0.6826 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0595 global=0.6107\n",
            "  step 10: local=0.0569 global=0.6236\n",
            "  step 20: local=0.0567 global=0.6330\n",
            "  step 30: local=0.0592 global=0.6352\n",
            "  step 40: local=0.0608 global=0.6527\n",
            "  step 50: local=0.0595 global=0.6333\n",
            "  step 60: local=0.0584 global=0.6123\n",
            "  step 70: local=0.0583 global=0.6418\n",
            "  step 80: local=0.0567 global=0.5903\n",
            "  step 90: local=0.0614 global=0.6511\n",
            "  Layer 12 not converged (global=0.6758 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0612 global=0.6399\n",
            "  step 10: local=0.0599 global=0.5930\n",
            "  step 20: local=0.0571 global=0.6426\n",
            "  step 30: local=0.0591 global=0.6501\n",
            "  step 40: local=0.0618 global=0.6533\n",
            "  step 50: local=0.0586 global=0.6004\n",
            "  step 60: local=0.0607 global=0.6253\n",
            "  step 70: local=0.0567 global=0.6387\n",
            "  step 80: local=0.0596 global=0.6474\n",
            "  step 90: local=0.0602 global=0.6050\n",
            "  Layer 12 not converged (global=0.6510 > 0.4), repeating...\n",
            "\n",
            "--- Layer 12/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0582 global=0.6047\n",
            "  step 10: local=0.0612 global=0.6266\n",
            "  step 20: local=0.0595 global=0.6087\n",
            "  step 30: local=0.0561 global=0.6463\n",
            "  step 40: local=0.0594 global=0.6575\n",
            "  step 50: local=0.0571 global=0.6118\n",
            "  step 60: local=0.0611 global=0.5972\n",
            "  step 70: local=0.0588 global=0.6245\n",
            "  step 80: local=0.0613 global=0.6161\n",
            "  step 90: local=0.0590 global=0.6547\n",
            "  [WARN] Layer 12 did not converge after 20 repeats (global=0.6508)\n",
            "\n",
            "--- Layer 13/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0739 global=0.6383\n",
            "  step 10: local=0.0705 global=0.6649\n",
            "  step 20: local=0.0712 global=0.6302\n",
            "  step 30: local=0.0692 global=0.6237\n",
            "  step 40: local=0.0712 global=0.6403\n",
            "  step 50: local=0.0665 global=0.6190\n",
            "  step 60: local=0.0669 global=0.6197\n",
            "  step 70: local=0.0669 global=0.6054\n",
            "  step 80: local=0.0642 global=0.6314\n",
            "  step 90: local=0.0626 global=0.6573\n",
            "  Layer 13 not converged (global=0.6042 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0678 global=0.6596\n",
            "  step 10: local=0.0661 global=0.6149\n",
            "  step 20: local=0.0652 global=0.6548\n",
            "  step 30: local=0.0650 global=0.6510\n",
            "  step 40: local=0.0672 global=0.6265\n",
            "  step 50: local=0.0627 global=0.6305\n",
            "  step 60: local=0.0650 global=0.6227\n",
            "  step 70: local=0.0616 global=0.6122\n",
            "  step 80: local=0.0639 global=0.6434\n",
            "  step 90: local=0.0590 global=0.6352\n",
            "  Layer 13 not converged (global=0.6454 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0620 global=0.6258\n",
            "  step 10: local=0.0628 global=0.5908\n",
            "  step 20: local=0.0612 global=0.6118\n",
            "  step 30: local=0.0613 global=0.6587\n",
            "  step 40: local=0.0636 global=0.6163\n",
            "  step 50: local=0.0635 global=0.6118\n",
            "  step 60: local=0.0598 global=0.6289\n",
            "  step 70: local=0.0595 global=0.6186\n",
            "  step 80: local=0.0592 global=0.5931\n",
            "  step 90: local=0.0568 global=0.5882\n",
            "  Layer 13 not converged (global=0.6077 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0611 global=0.6297\n",
            "  step 10: local=0.0586 global=0.6294\n",
            "  step 20: local=0.0601 global=0.6005\n",
            "  step 30: local=0.0597 global=0.6695\n",
            "  step 40: local=0.0608 global=0.6133\n",
            "  step 50: local=0.0627 global=0.6023\n",
            "  step 60: local=0.0581 global=0.6319\n",
            "  step 70: local=0.0607 global=0.6243\n",
            "  step 80: local=0.0592 global=0.6566\n",
            "  step 90: local=0.0591 global=0.6288\n",
            "  Layer 13 not converged (global=0.5888 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0601 global=0.6138\n",
            "  step 10: local=0.0611 global=0.6166\n",
            "  step 20: local=0.0603 global=0.6113\n",
            "  step 30: local=0.0598 global=0.6476\n",
            "  step 40: local=0.0590 global=0.5944\n",
            "  step 50: local=0.0617 global=0.6334\n",
            "  step 60: local=0.0622 global=0.5810\n",
            "  step 70: local=0.0564 global=0.6471\n",
            "  step 80: local=0.0602 global=0.6019\n",
            "  step 90: local=0.0599 global=0.6500\n",
            "  Layer 13 not converged (global=0.6434 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0614 global=0.6125\n",
            "  step 10: local=0.0604 global=0.6365\n",
            "  step 20: local=0.0601 global=0.6773\n",
            "  step 30: local=0.0615 global=0.6385\n",
            "  step 40: local=0.0586 global=0.6053\n",
            "  step 50: local=0.0576 global=0.6287\n",
            "  step 60: local=0.0591 global=0.6642\n",
            "  step 70: local=0.0580 global=0.6132\n",
            "  step 80: local=0.0578 global=0.6260\n",
            "  step 90: local=0.0598 global=0.6450\n",
            "  Layer 13 not converged (global=0.5796 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0584 global=0.5877\n",
            "  step 10: local=0.0587 global=0.6462\n",
            "  step 20: local=0.0610 global=0.6624\n",
            "  step 30: local=0.0570 global=0.6492\n",
            "  step 40: local=0.0577 global=0.6459\n",
            "  step 50: local=0.0604 global=0.6051\n",
            "  step 60: local=0.0591 global=0.6030\n",
            "  step 70: local=0.0579 global=0.6280\n",
            "  step 80: local=0.0554 global=0.6003\n",
            "  step 90: local=0.0568 global=0.6437\n",
            "  Layer 13 not converged (global=0.6128 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0587 global=0.6336\n",
            "  step 10: local=0.0573 global=0.6171\n",
            "  step 20: local=0.0584 global=0.6079\n",
            "  step 30: local=0.0594 global=0.5822\n",
            "  step 40: local=0.0580 global=0.5937\n",
            "  step 50: local=0.0587 global=0.6069\n",
            "  step 60: local=0.0586 global=0.6156\n",
            "  step 70: local=0.0588 global=0.6184\n",
            "  step 80: local=0.0557 global=0.6347\n",
            "  step 90: local=0.0565 global=0.6153\n",
            "  Layer 13 not converged (global=0.6167 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0566 global=0.5965\n",
            "  step 10: local=0.0552 global=0.6242\n",
            "  step 20: local=0.0555 global=0.5735\n",
            "  step 30: local=0.0581 global=0.6331\n",
            "  step 40: local=0.0577 global=0.6226\n",
            "  step 50: local=0.0575 global=0.5761\n",
            "  step 60: local=0.0589 global=0.6246\n",
            "  step 70: local=0.0593 global=0.6324\n",
            "  step 80: local=0.0573 global=0.6357\n",
            "  step 90: local=0.0561 global=0.5836\n",
            "  Layer 13 not converged (global=0.5968 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0564 global=0.6086\n",
            "  step 10: local=0.0556 global=0.6229\n",
            "  step 20: local=0.0605 global=0.6305\n",
            "  step 30: local=0.0596 global=0.5887\n",
            "  step 40: local=0.0571 global=0.5867\n",
            "  step 50: local=0.0587 global=0.6089\n",
            "  step 60: local=0.0580 global=0.5915\n",
            "  step 70: local=0.0582 global=0.6289\n",
            "  step 80: local=0.0570 global=0.6398\n",
            "  step 90: local=0.0563 global=0.5931\n",
            "  Layer 13 not converged (global=0.6048 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0563 global=0.5816\n",
            "  step 10: local=0.0581 global=0.6064\n",
            "  step 20: local=0.0580 global=0.5995\n",
            "  step 30: local=0.0577 global=0.6352\n",
            "  step 40: local=0.0584 global=0.6124\n",
            "  step 50: local=0.0576 global=0.6394\n",
            "  step 60: local=0.0604 global=0.6065\n",
            "  step 70: local=0.0581 global=0.6017\n",
            "  step 80: local=0.0589 global=0.6186\n",
            "  step 90: local=0.0570 global=0.5970\n",
            "  Layer 13 not converged (global=0.6376 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0588 global=0.5994\n",
            "  step 10: local=0.0623 global=0.5863\n",
            "  step 20: local=0.0575 global=0.6122\n",
            "  step 30: local=0.0581 global=0.6377\n",
            "  step 40: local=0.0585 global=0.6395\n",
            "  step 50: local=0.0554 global=0.5968\n",
            "  step 60: local=0.0573 global=0.6373\n",
            "  step 70: local=0.0581 global=0.6331\n",
            "  step 80: local=0.0622 global=0.6092\n",
            "  step 90: local=0.0573 global=0.6134\n",
            "  Layer 13 not converged (global=0.5546 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0583 global=0.6062\n",
            "  step 10: local=0.0570 global=0.5965\n",
            "  step 20: local=0.0562 global=0.6276\n",
            "  step 30: local=0.0572 global=0.6198\n",
            "  step 40: local=0.0576 global=0.6100\n",
            "  step 50: local=0.0584 global=0.5764\n",
            "  step 60: local=0.0575 global=0.5972\n",
            "  step 70: local=0.0573 global=0.6448\n",
            "  step 80: local=0.0571 global=0.6023\n",
            "  step 90: local=0.0583 global=0.5981\n",
            "  Layer 13 not converged (global=0.6925 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0574 global=0.6150\n",
            "  step 10: local=0.0582 global=0.6061\n",
            "  step 20: local=0.0589 global=0.5814\n",
            "  step 30: local=0.0596 global=0.5759\n",
            "  step 40: local=0.0593 global=0.6163\n",
            "  step 50: local=0.0567 global=0.6172\n",
            "  step 60: local=0.0594 global=0.5900\n",
            "  step 70: local=0.0584 global=0.6574\n",
            "  step 80: local=0.0571 global=0.6029\n",
            "  step 90: local=0.0582 global=0.5920\n",
            "  Layer 13 not converged (global=0.6170 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0548 global=0.6210\n",
            "  step 10: local=0.0573 global=0.6131\n",
            "  step 20: local=0.0573 global=0.6463\n",
            "  step 30: local=0.0585 global=0.6198\n",
            "  step 40: local=0.0571 global=0.6033\n",
            "  step 50: local=0.0547 global=0.6075\n",
            "  step 60: local=0.0589 global=0.6031\n",
            "  step 70: local=0.0574 global=0.6393\n",
            "  step 80: local=0.0573 global=0.5860\n",
            "  step 90: local=0.0586 global=0.6247\n",
            "  Layer 13 not converged (global=0.5881 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0570 global=0.5734\n",
            "  step 10: local=0.0554 global=0.6396\n",
            "  step 20: local=0.0565 global=0.5943\n",
            "  step 30: local=0.0597 global=0.6422\n",
            "  step 40: local=0.0589 global=0.6049\n",
            "  step 50: local=0.0574 global=0.6297\n",
            "  step 60: local=0.0571 global=0.6701\n",
            "  step 70: local=0.0559 global=0.6315\n",
            "  step 80: local=0.0563 global=0.5986\n",
            "  step 90: local=0.0565 global=0.6222\n",
            "  Layer 13 not converged (global=0.6231 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0546 global=0.6581\n",
            "  step 10: local=0.0587 global=0.6070\n",
            "  step 20: local=0.0561 global=0.6196\n",
            "  step 30: local=0.0589 global=0.6396\n",
            "  step 40: local=0.0574 global=0.5811\n",
            "  step 50: local=0.0560 global=0.6397\n",
            "  step 60: local=0.0602 global=0.6562\n",
            "  step 70: local=0.0567 global=0.6435\n",
            "  step 80: local=0.0583 global=0.6396\n",
            "  step 90: local=0.0582 global=0.6007\n",
            "  Layer 13 not converged (global=0.5999 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0576 global=0.5980\n",
            "  step 10: local=0.0568 global=0.6229\n",
            "  step 20: local=0.0547 global=0.5959\n",
            "  step 30: local=0.0567 global=0.6382\n",
            "  step 40: local=0.0582 global=0.6282\n",
            "  step 50: local=0.0572 global=0.6128\n",
            "  step 60: local=0.0563 global=0.6030\n",
            "  step 70: local=0.0586 global=0.5772\n",
            "  step 80: local=0.0554 global=0.5890\n",
            "  step 90: local=0.0564 global=0.6026\n",
            "  Layer 13 not converged (global=0.6063 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0572 global=0.6106\n",
            "  step 10: local=0.0588 global=0.6147\n",
            "  step 20: local=0.0548 global=0.6302\n",
            "  step 30: local=0.0552 global=0.6106\n",
            "  step 40: local=0.0551 global=0.5917\n",
            "  step 50: local=0.0567 global=0.6197\n",
            "  step 60: local=0.0554 global=0.5696\n",
            "  step 70: local=0.0576 global=0.6298\n",
            "  step 80: local=0.0587 global=0.6190\n",
            "  step 90: local=0.0564 global=0.5727\n",
            "  Layer 13 not converged (global=0.6126 > 0.4), repeating...\n",
            "\n",
            "--- Layer 13/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0563 global=0.6204\n",
            "  step 10: local=0.0602 global=0.6282\n",
            "  step 20: local=0.0590 global=0.6322\n",
            "  step 30: local=0.0539 global=0.5796\n",
            "  step 40: local=0.0584 global=0.6039\n",
            "  step 50: local=0.0563 global=0.6186\n",
            "  step 60: local=0.0592 global=0.6263\n",
            "  step 70: local=0.0576 global=0.5845\n",
            "  step 80: local=0.0563 global=0.5831\n",
            "  step 90: local=0.0591 global=0.6059\n",
            "  [WARN] Layer 13 did not converge after 20 repeats (global=0.5907)\n",
            "\n",
            "--- Layer 14/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0782 global=0.5991\n",
            "  step 10: local=0.0793 global=0.6323\n",
            "  step 20: local=0.0761 global=0.6436\n",
            "  step 30: local=0.0773 global=0.5960\n",
            "  step 40: local=0.0727 global=0.5828\n",
            "  step 50: local=0.0741 global=0.6082\n",
            "  step 60: local=0.0736 global=0.6003\n",
            "  step 70: local=0.0745 global=0.6367\n",
            "  step 80: local=0.0756 global=0.6146\n",
            "  step 90: local=0.0704 global=0.6392\n",
            "  Layer 14 not converged (global=0.6154 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0750 global=0.6077\n",
            "  step 10: local=0.0728 global=0.5999\n",
            "  step 20: local=0.0710 global=0.6165\n",
            "  step 30: local=0.0734 global=0.5960\n",
            "  step 40: local=0.0721 global=0.5970\n",
            "  step 50: local=0.0699 global=0.5838\n",
            "  step 60: local=0.0681 global=0.6107\n",
            "  step 70: local=0.0713 global=0.6321\n",
            "  step 80: local=0.0677 global=0.6350\n",
            "  step 90: local=0.0687 global=0.5932\n",
            "  Layer 14 not converged (global=0.5936 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0714 global=0.6325\n",
            "  step 10: local=0.0702 global=0.6284\n",
            "  step 20: local=0.0710 global=0.6025\n",
            "  step 30: local=0.0665 global=0.6088\n",
            "  step 40: local=0.0671 global=0.5981\n",
            "  step 50: local=0.0663 global=0.5905\n",
            "  step 60: local=0.0675 global=0.6217\n",
            "  step 70: local=0.0672 global=0.6116\n",
            "  step 80: local=0.0657 global=0.6026\n",
            "  step 90: local=0.0644 global=0.5691\n",
            "  Layer 14 not converged (global=0.6102 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0659 global=0.5897\n",
            "  step 10: local=0.0649 global=0.6354\n",
            "  step 20: local=0.0650 global=0.5928\n",
            "  step 30: local=0.0652 global=0.5881\n",
            "  step 40: local=0.0650 global=0.6046\n",
            "  step 50: local=0.0645 global=0.5955\n",
            "  step 60: local=0.0652 global=0.5708\n",
            "  step 70: local=0.0643 global=0.5656\n",
            "  step 80: local=0.0670 global=0.6053\n",
            "  step 90: local=0.0635 global=0.6067\n",
            "  Layer 14 not converged (global=0.5718 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0671 global=0.5806\n",
            "  step 10: local=0.0634 global=0.6442\n",
            "  step 20: local=0.0651 global=0.5926\n",
            "  step 30: local=0.0614 global=0.5809\n",
            "  step 40: local=0.0662 global=0.6066\n",
            "  step 50: local=0.0638 global=0.6008\n",
            "  step 60: local=0.0652 global=0.6336\n",
            "  step 70: local=0.0660 global=0.6063\n",
            "  step 80: local=0.0651 global=0.5899\n",
            "  step 90: local=0.0622 global=0.5958\n",
            "  Layer 14 not converged (global=0.5811 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0680 global=0.5902\n",
            "  step 10: local=0.0654 global=0.6252\n",
            "  step 20: local=0.0622 global=0.5724\n",
            "  step 30: local=0.0645 global=0.6103\n",
            "  step 40: local=0.0651 global=0.5604\n",
            "  step 50: local=0.0655 global=0.6258\n",
            "  step 60: local=0.0607 global=0.5789\n",
            "  step 70: local=0.0659 global=0.6280\n",
            "  step 80: local=0.0635 global=0.5904\n",
            "  step 90: local=0.0631 global=0.6144\n",
            "  Layer 14 not converged (global=0.5908 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0637 global=0.6551\n",
            "  step 10: local=0.0638 global=0.6158\n",
            "  step 20: local=0.0649 global=0.5819\n",
            "  step 30: local=0.0642 global=0.6065\n",
            "  step 40: local=0.0622 global=0.6390\n",
            "  step 50: local=0.0629 global=0.5912\n",
            "  step 60: local=0.0636 global=0.6025\n",
            "  step 70: local=0.0657 global=0.6233\n",
            "  step 80: local=0.0645 global=0.5660\n",
            "  step 90: local=0.0624 global=0.6250\n",
            "  Layer 14 not converged (global=0.5795 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0636 global=0.6386\n",
            "  step 10: local=0.0638 global=0.6273\n",
            "  step 20: local=0.0643 global=0.6236\n",
            "  step 30: local=0.0621 global=0.5842\n",
            "  step 40: local=0.0647 global=0.5829\n",
            "  step 50: local=0.0649 global=0.6054\n",
            "  step 60: local=0.0629 global=0.5791\n",
            "  step 70: local=0.0631 global=0.6210\n",
            "  step 80: local=0.0649 global=0.6108\n",
            "  step 90: local=0.0620 global=0.5943\n",
            "  Layer 14 not converged (global=0.5962 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0622 global=0.5859\n",
            "  step 10: local=0.0631 global=0.5611\n",
            "  step 20: local=0.0634 global=0.5709\n",
            "  step 30: local=0.0619 global=0.5856\n",
            "  step 40: local=0.0634 global=0.5915\n",
            "  step 50: local=0.0629 global=0.5976\n",
            "  step 60: local=0.0613 global=0.6139\n",
            "  step 70: local=0.0623 global=0.5940\n",
            "  step 80: local=0.0618 global=0.5749\n",
            "  step 90: local=0.0600 global=0.6015\n",
            "  Layer 14 not converged (global=0.5711 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0622 global=0.5532\n",
            "  step 10: local=0.0615 global=0.6120\n",
            "  step 20: local=0.0627 global=0.6005\n",
            "  step 30: local=0.0629 global=0.5563\n",
            "  step 40: local=0.0635 global=0.6024\n",
            "  step 50: local=0.0621 global=0.6090\n",
            "  step 60: local=0.0648 global=0.6132\n",
            "  step 70: local=0.0624 global=0.5622\n",
            "  step 80: local=0.0644 global=0.5875\n",
            "  step 90: local=0.0605 global=0.5999\n",
            "  Layer 14 not converged (global=0.5861 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0617 global=0.6094\n",
            "  step 10: local=0.0623 global=0.5673\n",
            "  step 20: local=0.0600 global=0.5637\n",
            "  step 30: local=0.0614 global=0.5870\n",
            "  step 40: local=0.0603 global=0.5716\n",
            "  step 50: local=0.0605 global=0.6071\n",
            "  step 60: local=0.0618 global=0.6183\n",
            "  step 70: local=0.0627 global=0.5722\n",
            "  step 80: local=0.0602 global=0.5605\n",
            "  step 90: local=0.0623 global=0.5847\n",
            "  Layer 14 not converged (global=0.6202 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0635 global=0.5776\n",
            "  step 10: local=0.0612 global=0.6149\n",
            "  step 20: local=0.0644 global=0.5929\n",
            "  step 30: local=0.0634 global=0.6179\n",
            "  step 40: local=0.0646 global=0.5849\n",
            "  step 50: local=0.0636 global=0.5798\n",
            "  step 60: local=0.0633 global=0.5957\n",
            "  step 70: local=0.0619 global=0.5775\n",
            "  step 80: local=0.0623 global=0.5786\n",
            "  step 90: local=0.0630 global=0.5652\n",
            "  Layer 14 not converged (global=0.6364 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0629 global=0.5919\n",
            "  step 10: local=0.0632 global=0.6136\n",
            "  step 20: local=0.0632 global=0.6161\n",
            "  step 30: local=0.0606 global=0.5767\n",
            "  step 40: local=0.0633 global=0.6157\n",
            "  step 50: local=0.0623 global=0.6118\n",
            "  step 60: local=0.0636 global=0.5873\n",
            "  step 70: local=0.0585 global=0.5931\n",
            "  step 80: local=0.0609 global=0.5828\n",
            "  step 90: local=0.0624 global=0.5762\n",
            "  Layer 14 not converged (global=0.5664 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0633 global=0.6076\n",
            "  step 10: local=0.0622 global=0.5975\n",
            "  step 20: local=0.0603 global=0.5888\n",
            "  step 30: local=0.0606 global=0.5555\n",
            "  step 40: local=0.0636 global=0.5761\n",
            "  step 50: local=0.0611 global=0.6224\n",
            "  step 60: local=0.0644 global=0.5808\n",
            "  step 70: local=0.0622 global=0.5758\n",
            "  step 80: local=0.0606 global=0.5921\n",
            "  step 90: local=0.0608 global=0.5851\n",
            "  Layer 14 not converged (global=0.5898 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0603 global=0.5606\n",
            "  step 10: local=0.0613 global=0.5555\n",
            "  step 20: local=0.0616 global=0.5941\n",
            "  step 30: local=0.0621 global=0.5962\n",
            "  step 40: local=0.0617 global=0.5694\n",
            "  step 50: local=0.0635 global=0.6340\n",
            "  step 60: local=0.0611 global=0.5823\n",
            "  step 70: local=0.0625 global=0.5709\n",
            "  step 80: local=0.0610 global=0.5972\n",
            "  step 90: local=0.0594 global=0.5915\n",
            "  Layer 14 not converged (global=0.5808 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0622 global=0.6244\n",
            "  step 10: local=0.0612 global=0.5971\n",
            "  step 20: local=0.0636 global=0.5804\n",
            "  step 30: local=0.0634 global=0.5872\n",
            "  step 40: local=0.0620 global=0.5812\n",
            "  step 50: local=0.0633 global=0.6179\n",
            "  step 60: local=0.0616 global=0.5648\n",
            "  step 70: local=0.0619 global=0.6023\n",
            "  step 80: local=0.0614 global=0.5524\n",
            "  step 90: local=0.0597 global=0.6180\n",
            "  Layer 14 not converged (global=0.6082 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0617 global=0.5724\n",
            "  step 10: local=0.0626 global=0.6214\n",
            "  step 20: local=0.0610 global=0.5833\n",
            "  step 30: local=0.0610 global=0.6073\n",
            "  step 40: local=0.0632 global=0.6479\n",
            "  step 50: local=0.0605 global=0.6097\n",
            "  step 60: local=0.0603 global=0.5754\n",
            "  step 70: local=0.0630 global=0.5998\n",
            "  step 80: local=0.0608 global=0.6334\n",
            "  step 90: local=0.0613 global=0.5858\n",
            "  Layer 14 not converged (global=0.5931 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0613 global=0.5974\n",
            "  step 10: local=0.0622 global=0.6184\n",
            "  step 20: local=0.0613 global=0.5606\n",
            "  step 30: local=0.0618 global=0.6197\n",
            "  step 40: local=0.0629 global=0.6326\n",
            "  step 50: local=0.0614 global=0.6217\n",
            "  step 60: local=0.0639 global=0.6181\n",
            "  step 70: local=0.0616 global=0.5791\n",
            "  step 80: local=0.0629 global=0.5781\n",
            "  step 90: local=0.0623 global=0.6001\n",
            "  Layer 14 not converged (global=0.6037 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0602 global=0.5745\n",
            "  step 10: local=0.0623 global=0.6162\n",
            "  step 20: local=0.0613 global=0.6062\n",
            "  step 30: local=0.0596 global=0.5901\n",
            "  step 40: local=0.0606 global=0.5810\n",
            "  step 50: local=0.0614 global=0.5566\n",
            "  step 60: local=0.0595 global=0.5663\n",
            "  step 70: local=0.0606 global=0.5810\n",
            "  step 80: local=0.0645 global=0.5870\n",
            "  step 90: local=0.0635 global=0.5938\n",
            "  Layer 14 not converged (global=0.5780 > 0.4), repeating...\n",
            "\n",
            "--- Layer 14/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0589 global=0.6097\n",
            "  step 10: local=0.0624 global=0.5898\n",
            "  step 20: local=0.0631 global=0.5716\n",
            "  step 30: local=0.0585 global=0.5978\n",
            "  step 40: local=0.0606 global=0.5491\n",
            "  step 50: local=0.0618 global=0.6073\n",
            "  step 60: local=0.0612 global=0.5967\n",
            "  step 70: local=0.0610 global=0.5530\n",
            "  step 80: local=0.0631 global=0.5978\n",
            "  step 90: local=0.0628 global=0.6050\n",
            "  [WARN] Layer 14 did not converge after 20 repeats (global=0.6135)\n",
            "\n",
            "--- Layer 15/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0949 global=0.6241\n",
            "  step 10: local=0.0937 global=0.5697\n",
            "  step 20: local=0.0910 global=0.5937\n",
            "  step 30: local=0.0956 global=0.6041\n",
            "  step 40: local=0.0854 global=0.6122\n",
            "  step 50: local=0.0862 global=0.5691\n",
            "  step 60: local=0.0821 global=0.5669\n",
            "  step 70: local=0.0813 global=0.5889\n",
            "  step 80: local=0.0787 global=0.5721\n",
            "  step 90: local=0.0808 global=0.6081\n",
            "  Layer 15 not converged (global=0.6537 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0830 global=0.6183\n",
            "  step 10: local=0.0845 global=0.5713\n",
            "  step 20: local=0.0819 global=0.5571\n",
            "  step 30: local=0.0805 global=0.5819\n",
            "  step 40: local=0.0764 global=0.5759\n",
            "  step 50: local=0.0766 global=0.6114\n",
            "  step 60: local=0.0782 global=0.5880\n",
            "  step 70: local=0.0761 global=0.6134\n",
            "  step 80: local=0.0752 global=0.5809\n",
            "  step 90: local=0.0726 global=0.5752\n",
            "  Layer 15 not converged (global=0.5829 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0738 global=0.5904\n",
            "  step 10: local=0.0755 global=0.5721\n",
            "  step 20: local=0.0773 global=0.5735\n",
            "  step 30: local=0.0761 global=0.5578\n",
            "  step 40: local=0.0728 global=0.5855\n",
            "  step 50: local=0.0707 global=0.6070\n",
            "  step 60: local=0.0725 global=0.6080\n",
            "  step 70: local=0.0697 global=0.5696\n",
            "  step 80: local=0.0753 global=0.6049\n",
            "  step 90: local=0.0728 global=0.6019\n",
            "  Layer 15 not converged (global=0.6105 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0725 global=0.5777\n",
            "  step 10: local=0.0716 global=0.5817\n",
            "  step 20: local=0.0710 global=0.5724\n",
            "  step 30: local=0.0686 global=0.5660\n",
            "  step 40: local=0.0707 global=0.5964\n",
            "  step 50: local=0.0677 global=0.5875\n",
            "  step 60: local=0.0671 global=0.5775\n",
            "  step 70: local=0.0702 global=0.5431\n",
            "  step 80: local=0.0686 global=0.5640\n",
            "  step 90: local=0.0726 global=0.6083\n",
            "  Layer 15 not converged (global=0.5763 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0692 global=0.5674\n",
            "  step 10: local=0.0688 global=0.5618\n",
            "  step 20: local=0.0696 global=0.5796\n",
            "  step 30: local=0.0696 global=0.5738\n",
            "  step 40: local=0.0710 global=0.5478\n",
            "  step 50: local=0.0690 global=0.5421\n",
            "  step 60: local=0.0701 global=0.5795\n",
            "  step 70: local=0.0675 global=0.5814\n",
            "  step 80: local=0.0673 global=0.5569\n",
            "  step 90: local=0.0683 global=0.6180\n",
            "  Layer 15 not converged (global=0.5642 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0696 global=0.5680\n",
            "  step 10: local=0.0719 global=0.5567\n",
            "  step 20: local=0.0654 global=0.5811\n",
            "  step 30: local=0.0680 global=0.5762\n",
            "  step 40: local=0.0684 global=0.6076\n",
            "  step 50: local=0.0683 global=0.5815\n",
            "  step 60: local=0.0685 global=0.5636\n",
            "  step 70: local=0.0672 global=0.5718\n",
            "  step 80: local=0.0686 global=0.5670\n",
            "  step 90: local=0.0649 global=0.6018\n",
            "  Layer 15 not converged (global=0.5869 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0667 global=0.5486\n",
            "  step 10: local=0.0677 global=0.5858\n",
            "  step 20: local=0.0664 global=0.5351\n",
            "  step 30: local=0.0689 global=0.6014\n",
            "  step 40: local=0.0685 global=0.5566\n",
            "  step 50: local=0.0662 global=0.6035\n",
            "  step 60: local=0.0689 global=0.5667\n",
            "  step 70: local=0.0696 global=0.5920\n",
            "  step 80: local=0.0682 global=0.6306\n",
            "  step 90: local=0.0670 global=0.5915\n",
            "  Layer 15 not converged (global=0.5702 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0666 global=0.5581\n",
            "  step 10: local=0.0689 global=0.5822\n",
            "  step 20: local=0.0687 global=0.6154\n",
            "  step 30: local=0.0684 global=0.5666\n",
            "  step 40: local=0.0678 global=0.5792\n",
            "  step 50: local=0.0676 global=0.5993\n",
            "  step 60: local=0.0667 global=0.5439\n",
            "  step 70: local=0.0695 global=0.6016\n",
            "  step 80: local=0.0697 global=0.6141\n",
            "  step 90: local=0.0671 global=0.6045\n",
            "  Layer 15 not converged (global=0.5508 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0684 global=0.5978\n",
            "  step 10: local=0.0698 global=0.5607\n",
            "  step 20: local=0.0676 global=0.5597\n",
            "  step 30: local=0.0692 global=0.5812\n",
            "  step 40: local=0.0682 global=0.5547\n",
            "  step 50: local=0.0654 global=0.5982\n",
            "  step 60: local=0.0700 global=0.5879\n",
            "  step 70: local=0.0706 global=0.5706\n",
            "  step 80: local=0.0641 global=0.5616\n",
            "  step 90: local=0.0639 global=0.5391\n",
            "  Layer 15 not converged (global=0.6201 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0677 global=0.5473\n",
            "  step 10: local=0.0680 global=0.5630\n",
            "  step 20: local=0.0686 global=0.5686\n",
            "  step 30: local=0.0694 global=0.5761\n",
            "  step 40: local=0.0671 global=0.5895\n",
            "  step 50: local=0.0698 global=0.5705\n",
            "  step 60: local=0.0683 global=0.5533\n",
            "  step 70: local=0.0624 global=0.5796\n",
            "  step 80: local=0.0682 global=0.5306\n",
            "  step 90: local=0.0664 global=0.5892\n",
            "  Layer 15 not converged (global=0.6082 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0661 global=0.5777\n",
            "  step 10: local=0.0669 global=0.5352\n",
            "  step 20: local=0.0681 global=0.5772\n",
            "  step 30: local=0.0673 global=0.5834\n",
            "  step 40: local=0.0680 global=0.5894\n",
            "  step 50: local=0.0658 global=0.5414\n",
            "  step 60: local=0.0672 global=0.5660\n",
            "  step 70: local=0.0665 global=0.5765\n",
            "  step 80: local=0.0670 global=0.5873\n",
            "  step 90: local=0.0662 global=0.5460\n",
            "  Layer 15 not converged (global=0.5845 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0650 global=0.5416\n",
            "  step 10: local=0.0659 global=0.5654\n",
            "  step 20: local=0.0671 global=0.5489\n",
            "  step 30: local=0.0678 global=0.5851\n",
            "  step 40: local=0.0676 global=0.5953\n",
            "  step 50: local=0.0688 global=0.5497\n",
            "  step 60: local=0.0643 global=0.5384\n",
            "  step 70: local=0.0671 global=0.5612\n",
            "  step 80: local=0.0672 global=0.5552\n",
            "  step 90: local=0.0670 global=0.5903\n",
            "  Layer 15 not converged (global=0.5857 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0654 global=0.5696\n",
            "  step 10: local=0.0679 global=0.5936\n",
            "  step 20: local=0.0690 global=0.5625\n",
            "  step 30: local=0.0658 global=0.5581\n",
            "  step 40: local=0.0646 global=0.5721\n",
            "  step 50: local=0.0663 global=0.5557\n",
            "  step 60: local=0.0665 global=0.5574\n",
            "  step 70: local=0.0681 global=0.5420\n",
            "  step 80: local=0.0646 global=0.5709\n",
            "  step 90: local=0.0668 global=0.5916\n",
            "  Layer 15 not converged (global=0.5433 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0664 global=0.5923\n",
            "  step 10: local=0.0660 global=0.5559\n",
            "  step 20: local=0.0651 global=0.5912\n",
            "  step 30: local=0.0661 global=0.5881\n",
            "  step 40: local=0.0679 global=0.5627\n",
            "  step 50: local=0.0660 global=0.5688\n",
            "  step 60: local=0.0657 global=0.5596\n",
            "  step 70: local=0.0649 global=0.5531\n",
            "  step 80: local=0.0663 global=0.5829\n",
            "  step 90: local=0.0659 global=0.5759\n",
            "  Layer 15 not converged (global=0.5828 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0632 global=0.5654\n",
            "  step 10: local=0.0685 global=0.5318\n",
            "  step 20: local=0.0677 global=0.5532\n",
            "  step 30: local=0.0684 global=0.5980\n",
            "  step 40: local=0.0674 global=0.5566\n",
            "  step 50: local=0.0651 global=0.5516\n",
            "  step 60: local=0.0651 global=0.5694\n",
            "  step 70: local=0.0652 global=0.5646\n",
            "  step 80: local=0.0680 global=0.5399\n",
            "  step 90: local=0.0643 global=0.5336\n",
            "  Layer 15 not converged (global=0.5553 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0666 global=0.5714\n",
            "  step 10: local=0.0632 global=0.5730\n",
            "  step 20: local=0.0656 global=0.5489\n",
            "  step 30: local=0.0654 global=0.6100\n",
            "  step 40: local=0.0677 global=0.5600\n",
            "  step 50: local=0.0655 global=0.5489\n",
            "  step 60: local=0.0645 global=0.5730\n",
            "  step 70: local=0.0674 global=0.5690\n",
            "  step 80: local=0.0636 global=0.5998\n",
            "  step 90: local=0.0654 global=0.5745\n",
            "  Layer 15 not converged (global=0.5374 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0668 global=0.5566\n",
            "  step 10: local=0.0668 global=0.5655\n",
            "  step 20: local=0.0690 global=0.5596\n",
            "  step 30: local=0.0666 global=0.5949\n",
            "  step 40: local=0.0638 global=0.5416\n",
            "  step 50: local=0.0651 global=0.5790\n",
            "  step 60: local=0.0650 global=0.5290\n",
            "  step 70: local=0.0666 global=0.5951\n",
            "  step 80: local=0.0658 global=0.5513\n",
            "  step 90: local=0.0673 global=0.5972\n",
            "  Layer 15 not converged (global=0.5918 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0664 global=0.5614\n",
            "  step 10: local=0.0679 global=0.5863\n",
            "  step 20: local=0.0676 global=0.6251\n",
            "  step 30: local=0.0663 global=0.5857\n",
            "  step 40: local=0.0647 global=0.5516\n",
            "  step 50: local=0.0638 global=0.5767\n",
            "  step 60: local=0.0676 global=0.6089\n",
            "  step 70: local=0.0669 global=0.5615\n",
            "  step 80: local=0.0639 global=0.5735\n",
            "  step 90: local=0.0664 global=0.5942\n",
            "  Layer 15 not converged (global=0.5357 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0665 global=0.5391\n",
            "  step 10: local=0.0659 global=0.5970\n",
            "  step 20: local=0.0657 global=0.6091\n",
            "  step 30: local=0.0643 global=0.5992\n",
            "  step 40: local=0.0658 global=0.5920\n",
            "  step 50: local=0.0652 global=0.5560\n",
            "  step 60: local=0.0681 global=0.5565\n",
            "  step 70: local=0.0657 global=0.5758\n",
            "  step 80: local=0.0670 global=0.5504\n",
            "  step 90: local=0.0651 global=0.5935\n",
            "  Layer 15 not converged (global=0.5645 > 0.4), repeating...\n",
            "\n",
            "--- Layer 15/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0668 global=0.5839\n",
            "  step 10: local=0.0683 global=0.5666\n",
            "  step 20: local=0.0650 global=0.5577\n",
            "  step 30: local=0.0682 global=0.5347\n",
            "  step 40: local=0.0679 global=0.5427\n",
            "  step 50: local=0.0643 global=0.5592\n",
            "  step 60: local=0.0642 global=0.5645\n",
            "  step 70: local=0.0680 global=0.5722\n",
            "  step 80: local=0.0645 global=0.5855\n",
            "  step 90: local=0.0658 global=0.5669\n",
            "  [WARN] Layer 15 did not converge after 20 repeats (global=0.5678)\n",
            "\n",
            "--- Layer 16/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1395 global=0.6030\n",
            "  step 10: local=0.1386 global=0.6233\n",
            "  step 20: local=0.1385 global=0.5679\n",
            "  step 30: local=0.1325 global=0.6284\n",
            "  step 40: local=0.1336 global=0.6132\n",
            "  step 50: local=0.1328 global=0.5637\n",
            "  step 60: local=0.1302 global=0.6083\n",
            "  step 70: local=0.1307 global=0.6159\n",
            "  step 80: local=0.1324 global=0.6229\n",
            "  step 90: local=0.1302 global=0.5675\n",
            "  Layer 16 not converged (global=0.5810 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1353 global=0.5965\n",
            "  step 10: local=0.1326 global=0.6064\n",
            "  step 20: local=0.1333 global=0.6088\n",
            "  step 30: local=0.1283 global=0.5680\n",
            "  step 40: local=0.1286 global=0.5622\n",
            "  step 50: local=0.1292 global=0.5870\n",
            "  step 60: local=0.1276 global=0.5681\n",
            "  step 70: local=0.1234 global=0.6075\n",
            "  step 80: local=0.1254 global=0.6139\n",
            "  step 90: local=0.1271 global=0.5682\n",
            "  Layer 16 not converged (global=0.5804 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1233 global=0.5521\n",
            "  step 10: local=0.1211 global=0.5823\n",
            "  step 20: local=0.1240 global=0.5738\n",
            "  step 30: local=0.1295 global=0.6074\n",
            "  step 40: local=0.1265 global=0.5825\n",
            "  step 50: local=0.1228 global=0.6059\n",
            "  step 60: local=0.1239 global=0.5721\n",
            "  step 70: local=0.1212 global=0.5675\n",
            "  step 80: local=0.1194 global=0.5841\n",
            "  step 90: local=0.1225 global=0.5668\n",
            "  Layer 16 not converged (global=0.6076 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1181 global=0.5672\n",
            "  step 10: local=0.1214 global=0.5514\n",
            "  step 20: local=0.1173 global=0.5807\n",
            "  step 30: local=0.1221 global=0.5984\n",
            "  step 40: local=0.1223 global=0.6013\n",
            "  step 50: local=0.1205 global=0.5617\n",
            "  step 60: local=0.1212 global=0.5985\n",
            "  step 70: local=0.1252 global=0.5967\n",
            "  step 80: local=0.1184 global=0.5686\n",
            "  step 90: local=0.1192 global=0.5751\n",
            "  Layer 16 not converged (global=0.5197 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1226 global=0.5665\n",
            "  step 10: local=0.1199 global=0.5546\n",
            "  step 20: local=0.1188 global=0.5861\n",
            "  step 30: local=0.1189 global=0.5779\n",
            "  step 40: local=0.1177 global=0.5695\n",
            "  step 50: local=0.1166 global=0.5362\n",
            "  step 60: local=0.1126 global=0.5566\n",
            "  step 70: local=0.1138 global=0.6013\n",
            "  step 80: local=0.1168 global=0.5580\n",
            "  step 90: local=0.1113 global=0.5541\n",
            "  Layer 16 not converged (global=0.6458 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1160 global=0.5750\n",
            "  step 10: local=0.1133 global=0.5686\n",
            "  step 20: local=0.1180 global=0.5429\n",
            "  step 30: local=0.1113 global=0.5363\n",
            "  step 40: local=0.1172 global=0.5715\n",
            "  step 50: local=0.1165 global=0.5747\n",
            "  step 60: local=0.1192 global=0.5472\n",
            "  step 70: local=0.1185 global=0.6108\n",
            "  step 80: local=0.1190 global=0.5616\n",
            "  step 90: local=0.1142 global=0.5495\n",
            "  Layer 16 not converged (global=0.5730 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1171 global=0.5735\n",
            "  step 10: local=0.1200 global=0.5719\n",
            "  step 20: local=0.1146 global=0.5976\n",
            "  step 30: local=0.1189 global=0.5715\n",
            "  step 40: local=0.1143 global=0.5551\n",
            "  step 50: local=0.1156 global=0.5643\n",
            "  step 60: local=0.1109 global=0.5582\n",
            "  step 70: local=0.1116 global=0.5979\n",
            "  step 80: local=0.1146 global=0.5437\n",
            "  step 90: local=0.1167 global=0.5808\n",
            "  Layer 16 not converged (global=0.5417 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1174 global=0.5257\n",
            "  step 10: local=0.1155 global=0.5945\n",
            "  step 20: local=0.1098 global=0.5503\n",
            "  step 30: local=0.1220 global=0.5967\n",
            "  step 40: local=0.1218 global=0.5568\n",
            "  step 50: local=0.1158 global=0.5838\n",
            "  step 60: local=0.1146 global=0.6272\n",
            "  step 70: local=0.1175 global=0.5846\n",
            "  step 80: local=0.1157 global=0.5502\n",
            "  step 90: local=0.1097 global=0.5737\n",
            "  Layer 16 not converged (global=0.5748 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1117 global=0.6067\n",
            "  step 10: local=0.1130 global=0.5594\n",
            "  step 20: local=0.1120 global=0.5685\n",
            "  step 30: local=0.1175 global=0.5896\n",
            "  step 40: local=0.1143 global=0.5361\n",
            "  step 50: local=0.1164 global=0.5937\n",
            "  step 60: local=0.1214 global=0.6088\n",
            "  step 70: local=0.1176 global=0.5988\n",
            "  step 80: local=0.1121 global=0.5871\n",
            "  step 90: local=0.1184 global=0.5506\n",
            "  Layer 16 not converged (global=0.5520 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1153 global=0.5527\n",
            "  step 10: local=0.1164 global=0.5733\n",
            "  step 20: local=0.1092 global=0.5475\n",
            "  step 30: local=0.1182 global=0.5891\n",
            "  step 40: local=0.1179 global=0.5810\n",
            "  step 50: local=0.1162 global=0.5640\n",
            "  step 60: local=0.1161 global=0.5543\n",
            "  step 70: local=0.1138 global=0.5291\n",
            "  step 80: local=0.1138 global=0.5383\n",
            "  step 90: local=0.1135 global=0.5564\n",
            "  Layer 16 not converged (global=0.5585 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1139 global=0.5610\n",
            "  step 10: local=0.1207 global=0.5710\n",
            "  step 20: local=0.1055 global=0.5790\n",
            "  step 30: local=0.1102 global=0.5662\n",
            "  step 40: local=0.1125 global=0.5465\n",
            "  step 50: local=0.1139 global=0.5725\n",
            "  step 60: local=0.1154 global=0.5229\n",
            "  step 70: local=0.1177 global=0.5825\n",
            "  step 80: local=0.1144 global=0.5714\n",
            "  step 90: local=0.1136 global=0.5268\n",
            "  Layer 16 not converged (global=0.5612 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1159 global=0.5690\n",
            "  step 10: local=0.1202 global=0.5776\n",
            "  step 20: local=0.1163 global=0.5867\n",
            "  step 30: local=0.1165 global=0.5340\n",
            "  step 40: local=0.1121 global=0.5623\n",
            "  step 50: local=0.1118 global=0.5722\n",
            "  step 60: local=0.1163 global=0.5785\n",
            "  step 70: local=0.1146 global=0.5393\n",
            "  step 80: local=0.1149 global=0.5329\n",
            "  step 90: local=0.1149 global=0.5576\n",
            "  Layer 16 not converged (global=0.5451 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1120 global=0.5420\n",
            "  step 10: local=0.1175 global=0.5807\n",
            "  step 20: local=0.1147 global=0.5900\n",
            "  step 30: local=0.1140 global=0.5427\n",
            "  step 40: local=0.1123 global=0.5290\n",
            "  step 50: local=0.1124 global=0.5576\n",
            "  step 60: local=0.1144 global=0.5496\n",
            "  step 70: local=0.1183 global=0.5838\n",
            "  step 80: local=0.1184 global=0.5613\n",
            "  step 90: local=0.1111 global=0.5845\n",
            "  Layer 16 not converged (global=0.5652 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1209 global=0.5524\n",
            "  step 10: local=0.1148 global=0.5478\n",
            "  step 20: local=0.1127 global=0.5643\n",
            "  step 30: local=0.1135 global=0.5487\n",
            "  step 40: local=0.1193 global=0.5492\n",
            "  step 50: local=0.1114 global=0.5355\n",
            "  step 60: local=0.1130 global=0.5651\n",
            "  step 70: local=0.1167 global=0.5817\n",
            "  step 80: local=0.1149 global=0.5854\n",
            "  step 90: local=0.1125 global=0.5475\n",
            "  Layer 16 not converged (global=0.5483 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1171 global=0.5833\n",
            "  step 10: local=0.1154 global=0.5821\n",
            "  step 20: local=0.1160 global=0.5550\n",
            "  step 30: local=0.1142 global=0.5615\n",
            "  step 40: local=0.1155 global=0.5525\n",
            "  step 50: local=0.1116 global=0.5428\n",
            "  step 60: local=0.1144 global=0.5736\n",
            "  step 70: local=0.1141 global=0.5667\n",
            "  step 80: local=0.1120 global=0.5584\n",
            "  step 90: local=0.1087 global=0.5244\n",
            "  Layer 16 not converged (global=0.5648 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1145 global=0.5444\n",
            "  step 10: local=0.1174 global=0.5910\n",
            "  step 20: local=0.1162 global=0.5475\n",
            "  step 30: local=0.1111 global=0.5442\n",
            "  step 40: local=0.1126 global=0.5632\n",
            "  step 50: local=0.1161 global=0.5579\n",
            "  step 60: local=0.1117 global=0.5344\n",
            "  step 70: local=0.1125 global=0.5272\n",
            "  step 80: local=0.1156 global=0.5618\n",
            "  step 90: local=0.1148 global=0.5643\n",
            "  Layer 16 not converged (global=0.5320 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1153 global=0.5392\n",
            "  step 10: local=0.1106 global=0.6025\n",
            "  step 20: local=0.1115 global=0.5542\n",
            "  step 30: local=0.1161 global=0.5415\n",
            "  step 40: local=0.1126 global=0.5648\n",
            "  step 50: local=0.1150 global=0.5643\n",
            "  step 60: local=0.1174 global=0.5889\n",
            "  step 70: local=0.1171 global=0.5634\n",
            "  step 80: local=0.1155 global=0.5472\n",
            "  step 90: local=0.1118 global=0.5567\n",
            "  Layer 16 not converged (global=0.5442 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1152 global=0.5522\n",
            "  step 10: local=0.1207 global=0.5897\n",
            "  step 20: local=0.1119 global=0.5370\n",
            "  step 30: local=0.1118 global=0.5724\n",
            "  step 40: local=0.1089 global=0.5194\n",
            "  step 50: local=0.1122 global=0.5870\n",
            "  step 60: local=0.1158 global=0.5436\n",
            "  step 70: local=0.1176 global=0.5915\n",
            "  step 80: local=0.1050 global=0.5510\n",
            "  step 90: local=0.1176 global=0.5761\n",
            "  Layer 16 not converged (global=0.5549 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1145 global=0.6197\n",
            "  step 10: local=0.1174 global=0.5779\n",
            "  step 20: local=0.1179 global=0.5437\n",
            "  step 30: local=0.1140 global=0.5679\n",
            "  step 40: local=0.1135 global=0.6001\n",
            "  step 50: local=0.1175 global=0.5524\n",
            "  step 60: local=0.1118 global=0.5626\n",
            "  step 70: local=0.1163 global=0.5830\n",
            "  step 80: local=0.1111 global=0.5305\n",
            "  step 90: local=0.1078 global=0.5887\n",
            "  Layer 16 not converged (global=0.5447 > 0.4), repeating...\n",
            "\n",
            "--- Layer 16/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1212 global=0.6023\n",
            "  step 10: local=0.1158 global=0.5937\n",
            "  step 20: local=0.1112 global=0.5810\n",
            "  step 30: local=0.1086 global=0.5459\n",
            "  step 40: local=0.1147 global=0.5477\n",
            "  step 50: local=0.1107 global=0.5690\n",
            "  step 60: local=0.1132 global=0.5427\n",
            "  step 70: local=0.1125 global=0.5838\n",
            "  step 80: local=0.1187 global=0.5772\n",
            "  step 90: local=0.1147 global=0.5595\n",
            "  [WARN] Layer 16 did not converge after 20 repeats (global=0.5570)\n",
            "\n",
            "--- Layer 17/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0746 global=0.5674\n",
            "  step 10: local=0.0767 global=0.5395\n",
            "  step 20: local=0.0720 global=0.5483\n",
            "  step 30: local=0.0704 global=0.5674\n",
            "  step 40: local=0.0712 global=0.5669\n",
            "  step 50: local=0.0717 global=0.5767\n",
            "  step 60: local=0.0695 global=0.5860\n",
            "  step 70: local=0.0678 global=0.5701\n",
            "  step 80: local=0.0665 global=0.5524\n",
            "  step 90: local=0.0666 global=0.5776\n",
            "  Layer 17 not converged (global=0.5424 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0683 global=0.5271\n",
            "  step 10: local=0.0671 global=0.5855\n",
            "  step 20: local=0.0665 global=0.5716\n",
            "  step 30: local=0.0678 global=0.5288\n",
            "  step 40: local=0.0695 global=0.5700\n",
            "  step 50: local=0.0651 global=0.5807\n",
            "  step 60: local=0.0674 global=0.5890\n",
            "  step 70: local=0.0640 global=0.5347\n",
            "  step 80: local=0.0647 global=0.5628\n",
            "  step 90: local=0.0633 global=0.5713\n",
            "  Layer 17 not converged (global=0.5555 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0660 global=0.5756\n",
            "  step 10: local=0.0658 global=0.5354\n",
            "  step 20: local=0.0625 global=0.5310\n",
            "  step 30: local=0.0647 global=0.5538\n",
            "  step 40: local=0.0615 global=0.5367\n",
            "  step 50: local=0.0622 global=0.5768\n",
            "  step 60: local=0.0621 global=0.5840\n",
            "  step 70: local=0.0609 global=0.5356\n",
            "  step 80: local=0.0632 global=0.5241\n",
            "  step 90: local=0.0606 global=0.5508\n",
            "  Layer 17 not converged (global=0.5833 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0615 global=0.5426\n",
            "  step 10: local=0.0627 global=0.5753\n",
            "  step 20: local=0.0638 global=0.5531\n",
            "  step 30: local=0.0617 global=0.5762\n",
            "  step 40: local=0.0631 global=0.5419\n",
            "  step 50: local=0.0611 global=0.5394\n",
            "  step 60: local=0.0627 global=0.5559\n",
            "  step 70: local=0.0574 global=0.5384\n",
            "  step 80: local=0.0618 global=0.5407\n",
            "  step 90: local=0.0635 global=0.5255\n",
            "  Layer 17 not converged (global=0.5951 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0614 global=0.5536\n",
            "  step 10: local=0.0598 global=0.5712\n",
            "  step 20: local=0.0622 global=0.5751\n",
            "  step 30: local=0.0575 global=0.5357\n",
            "  step 40: local=0.0607 global=0.5704\n",
            "  step 50: local=0.0612 global=0.5682\n",
            "  step 60: local=0.0630 global=0.5409\n",
            "  step 70: local=0.0619 global=0.5486\n",
            "  step 80: local=0.0589 global=0.5398\n",
            "  step 90: local=0.0580 global=0.5300\n",
            "  Layer 17 not converged (global=0.5245 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0626 global=0.5604\n",
            "  step 10: local=0.0627 global=0.5541\n",
            "  step 20: local=0.0627 global=0.5442\n",
            "  step 30: local=0.0580 global=0.5108\n",
            "  step 40: local=0.0610 global=0.5293\n",
            "  step 50: local=0.0604 global=0.5757\n",
            "  step 60: local=0.0597 global=0.5330\n",
            "  step 70: local=0.0580 global=0.5283\n",
            "  step 80: local=0.0585 global=0.5478\n",
            "  step 90: local=0.0607 global=0.5431\n",
            "  Layer 17 not converged (global=0.5428 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0583 global=0.5195\n",
            "  step 10: local=0.0588 global=0.5130\n",
            "  step 20: local=0.0610 global=0.5453\n",
            "  step 30: local=0.0585 global=0.5473\n",
            "  step 40: local=0.0576 global=0.5231\n",
            "  step 50: local=0.0621 global=0.5880\n",
            "  step 60: local=0.0586 global=0.5380\n",
            "  step 70: local=0.0603 global=0.5249\n",
            "  step 80: local=0.0609 global=0.5481\n",
            "  step 90: local=0.0581 global=0.5477\n",
            "  Layer 17 not converged (global=0.5352 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0607 global=0.5711\n",
            "  step 10: local=0.0610 global=0.5475\n",
            "  step 20: local=0.0630 global=0.5317\n",
            "  step 30: local=0.0577 global=0.5406\n",
            "  step 40: local=0.0606 global=0.5349\n",
            "  step 50: local=0.0584 global=0.5739\n",
            "  step 60: local=0.0637 global=0.5216\n",
            "  step 70: local=0.0590 global=0.5565\n",
            "  step 80: local=0.0617 global=0.5011\n",
            "  step 90: local=0.0584 global=0.5691\n",
            "  Layer 17 not converged (global=0.5600 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0596 global=0.5285\n",
            "  step 10: local=0.0590 global=0.5748\n",
            "  step 20: local=0.0576 global=0.5328\n",
            "  step 30: local=0.0591 global=0.5598\n",
            "  step 40: local=0.0584 global=0.6025\n",
            "  step 50: local=0.0567 global=0.5605\n",
            "  step 60: local=0.0627 global=0.5262\n",
            "  step 70: local=0.0606 global=0.5506\n",
            "  step 80: local=0.0587 global=0.5802\n",
            "  step 90: local=0.0587 global=0.5361\n",
            "  Layer 17 not converged (global=0.5445 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0581 global=0.5441\n",
            "  step 10: local=0.0600 global=0.5642\n",
            "  step 20: local=0.0589 global=0.5139\n",
            "  step 30: local=0.0581 global=0.5713\n",
            "  step 40: local=0.0594 global=0.5823\n",
            "  step 50: local=0.0581 global=0.5764\n",
            "  step 60: local=0.0580 global=0.5628\n",
            "  step 70: local=0.0581 global=0.5291\n",
            "  step 80: local=0.0603 global=0.5298\n",
            "  step 90: local=0.0569 global=0.5504\n",
            "  Layer 17 not converged (global=0.5556 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0610 global=0.5251\n",
            "  step 10: local=0.0613 global=0.5626\n",
            "  step 20: local=0.0619 global=0.5582\n",
            "  step 30: local=0.0613 global=0.5408\n",
            "  step 40: local=0.0603 global=0.5313\n",
            "  step 50: local=0.0618 global=0.5074\n",
            "  step 60: local=0.0589 global=0.5153\n",
            "  step 70: local=0.0609 global=0.5339\n",
            "  step 80: local=0.0588 global=0.5372\n",
            "  step 90: local=0.0582 global=0.5470\n",
            "  Layer 17 not converged (global=0.5289 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0594 global=0.5566\n",
            "  step 10: local=0.0594 global=0.5401\n",
            "  step 20: local=0.0606 global=0.5249\n",
            "  step 30: local=0.0590 global=0.5497\n",
            "  step 40: local=0.0598 global=0.5016\n",
            "  step 50: local=0.0559 global=0.5596\n",
            "  step 60: local=0.0590 global=0.5464\n",
            "  step 70: local=0.0595 global=0.5054\n",
            "  step 80: local=0.0623 global=0.5440\n",
            "  step 90: local=0.0576 global=0.5542\n",
            "  Layer 17 not converged (global=0.5595 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0588 global=0.5638\n",
            "  step 10: local=0.0586 global=0.5124\n",
            "  step 20: local=0.0585 global=0.5403\n",
            "  step 30: local=0.0581 global=0.5499\n",
            "  step 40: local=0.0590 global=0.5550\n",
            "  step 50: local=0.0607 global=0.5162\n",
            "  step 60: local=0.0597 global=0.5109\n",
            "  step 70: local=0.0562 global=0.5345\n",
            "  step 80: local=0.0576 global=0.5188\n",
            "  step 90: local=0.0614 global=0.5583\n",
            "  Layer 17 not converged (global=0.6006 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0621 global=0.5665\n",
            "  step 10: local=0.0567 global=0.5196\n",
            "  step 20: local=0.0591 global=0.5084\n",
            "  step 30: local=0.0591 global=0.5344\n",
            "  step 40: local=0.0585 global=0.5256\n",
            "  step 50: local=0.0581 global=0.5597\n",
            "  step 60: local=0.0603 global=0.5389\n",
            "  step 70: local=0.0590 global=0.5613\n",
            "  step 80: local=0.0583 global=0.5281\n",
            "  step 90: local=0.0599 global=0.5266\n",
            "  Layer 17 not converged (global=0.5321 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0586 global=0.5428\n",
            "  step 10: local=0.0594 global=0.5256\n",
            "  step 20: local=0.0616 global=0.5290\n",
            "  step 30: local=0.0582 global=0.5135\n",
            "  step 40: local=0.0583 global=0.5417\n",
            "  step 50: local=0.0598 global=0.5588\n",
            "  step 60: local=0.0572 global=0.5630\n",
            "  step 70: local=0.0559 global=0.5263\n",
            "  step 80: local=0.0590 global=0.5597\n",
            "  step 90: local=0.0591 global=0.5578\n",
            "  Layer 17 not converged (global=0.5678 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0602 global=0.5309\n",
            "  step 10: local=0.0585 global=0.5392\n",
            "  step 20: local=0.0554 global=0.5308\n",
            "  step 30: local=0.0570 global=0.5211\n",
            "  step 40: local=0.0596 global=0.5498\n",
            "  step 50: local=0.0576 global=0.5448\n",
            "  step 60: local=0.0581 global=0.5350\n",
            "  step 70: local=0.0566 global=0.5018\n",
            "  step 80: local=0.0577 global=0.5208\n",
            "  step 90: local=0.0574 global=0.5674\n",
            "  Layer 17 not converged (global=0.5345 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0609 global=0.5259\n",
            "  step 10: local=0.0562 global=0.5201\n",
            "  step 20: local=0.0596 global=0.5402\n",
            "  step 30: local=0.0580 global=0.5365\n",
            "  step 40: local=0.0583 global=0.5112\n",
            "  step 50: local=0.0582 global=0.5065\n",
            "  step 60: local=0.0588 global=0.5379\n",
            "  step 70: local=0.0586 global=0.5416\n",
            "  step 80: local=0.0562 global=0.5173\n",
            "  step 90: local=0.0609 global=0.5813\n",
            "  Layer 17 not converged (global=0.5268 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0578 global=0.5319\n",
            "  step 10: local=0.0576 global=0.5186\n",
            "  step 20: local=0.0595 global=0.5419\n",
            "  step 30: local=0.0578 global=0.5416\n",
            "  step 40: local=0.0590 global=0.5635\n",
            "  step 50: local=0.0623 global=0.5407\n",
            "  step 60: local=0.0599 global=0.5256\n",
            "  step 70: local=0.0575 global=0.5346\n",
            "  step 80: local=0.0594 global=0.5301\n",
            "  step 90: local=0.0578 global=0.5684\n",
            "  Layer 17 not converged (global=0.5536 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0581 global=0.5163\n",
            "  step 10: local=0.0590 global=0.5510\n",
            "  step 20: local=0.0599 global=0.4960\n",
            "  step 30: local=0.0588 global=0.5639\n",
            "  step 40: local=0.0569 global=0.5226\n",
            "  step 50: local=0.0578 global=0.5689\n",
            "  step 60: local=0.0587 global=0.5277\n",
            "  step 70: local=0.0607 global=0.5545\n",
            "  step 80: local=0.0585 global=0.5974\n",
            "  step 90: local=0.0589 global=0.5559\n",
            "  Layer 17 not converged (global=0.5342 > 0.4), repeating...\n",
            "\n",
            "--- Layer 17/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0606 global=0.5218\n",
            "  step 10: local=0.0604 global=0.5463\n",
            "  step 20: local=0.0608 global=0.5755\n",
            "  step 30: local=0.0586 global=0.5314\n",
            "  step 40: local=0.0564 global=0.5396\n",
            "  step 50: local=0.0623 global=0.5596\n",
            "  step 60: local=0.0597 global=0.5091\n",
            "  step 70: local=0.0582 global=0.5669\n",
            "  step 80: local=0.0596 global=0.5777\n",
            "  step 90: local=0.0571 global=0.5730\n",
            "  [WARN] Layer 17 did not converge after 20 repeats (global=0.5149)\n",
            "\n",
            "--- Layer 18/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0909 global=0.5829\n",
            "  step 10: local=0.0911 global=0.5453\n",
            "  step 20: local=0.0854 global=0.5471\n",
            "  step 30: local=0.0880 global=0.5648\n",
            "  step 40: local=0.0871 global=0.5367\n",
            "  step 50: local=0.0868 global=0.5770\n",
            "  step 60: local=0.0818 global=0.5676\n",
            "  step 70: local=0.0767 global=0.5494\n",
            "  step 80: local=0.0840 global=0.5428\n",
            "  step 90: local=0.0818 global=0.5171\n",
            "  Layer 18 not converged (global=0.5957 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0811 global=0.5237\n",
            "  step 10: local=0.0788 global=0.5446\n",
            "  step 20: local=0.0799 global=0.5437\n",
            "  step 30: local=0.0789 global=0.5530\n",
            "  step 40: local=0.0779 global=0.5623\n",
            "  step 50: local=0.0764 global=0.5468\n",
            "  step 60: local=0.0763 global=0.5296\n",
            "  step 70: local=0.0795 global=0.5533\n",
            "  step 80: local=0.0811 global=0.5079\n",
            "  step 90: local=0.0771 global=0.5625\n",
            "  Layer 18 not converged (global=0.5747 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0769 global=0.5490\n",
            "  step 10: local=0.0757 global=0.5072\n",
            "  step 20: local=0.0755 global=0.5447\n",
            "  step 30: local=0.0826 global=0.5564\n",
            "  step 40: local=0.0768 global=0.5626\n",
            "  step 50: local=0.0780 global=0.5155\n",
            "  step 60: local=0.0742 global=0.5396\n",
            "  step 70: local=0.0751 global=0.5498\n",
            "  step 80: local=0.0717 global=0.5533\n",
            "  step 90: local=0.0728 global=0.5134\n",
            "  Layer 18 not converged (global=0.5469 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0753 global=0.5062\n",
            "  step 10: local=0.0760 global=0.5298\n",
            "  step 20: local=0.0704 global=0.5167\n",
            "  step 30: local=0.0722 global=0.5518\n",
            "  step 40: local=0.0767 global=0.5593\n",
            "  step 50: local=0.0711 global=0.5138\n",
            "  step 60: local=0.0743 global=0.5018\n",
            "  step 70: local=0.0752 global=0.5273\n",
            "  step 80: local=0.0737 global=0.5193\n",
            "  step 90: local=0.0687 global=0.5503\n",
            "  Layer 18 not converged (global=0.5507 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0718 global=0.5307\n",
            "  step 10: local=0.0705 global=0.5512\n",
            "  step 20: local=0.0706 global=0.5188\n",
            "  step 30: local=0.0732 global=0.5194\n",
            "  step 40: local=0.0685 global=0.5323\n",
            "  step 50: local=0.0704 global=0.5159\n",
            "  step 60: local=0.0725 global=0.5192\n",
            "  step 70: local=0.0737 global=0.5056\n",
            "  step 80: local=0.0699 global=0.5325\n",
            "  step 90: local=0.0664 global=0.5484\n",
            "  Layer 18 not converged (global=0.5060 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0689 global=0.5502\n",
            "  step 10: local=0.0721 global=0.5163\n",
            "  step 20: local=0.0709 global=0.5474\n",
            "  step 30: local=0.0698 global=0.5437\n",
            "  step 40: local=0.0759 global=0.5195\n",
            "  step 50: local=0.0731 global=0.5250\n",
            "  step 60: local=0.0744 global=0.5209\n",
            "  step 70: local=0.0699 global=0.5093\n",
            "  step 80: local=0.0748 global=0.5380\n",
            "  step 90: local=0.0694 global=0.5332\n",
            "  Layer 18 not converged (global=0.5395 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0715 global=0.5225\n",
            "  step 10: local=0.0718 global=0.4883\n",
            "  step 20: local=0.0737 global=0.5061\n",
            "  step 30: local=0.0699 global=0.5561\n",
            "  step 40: local=0.0727 global=0.5125\n",
            "  step 50: local=0.0670 global=0.5093\n",
            "  step 60: local=0.0767 global=0.5285\n",
            "  step 70: local=0.0763 global=0.5236\n",
            "  step 80: local=0.0698 global=0.4999\n",
            "  step 90: local=0.0745 global=0.4931\n",
            "  Layer 18 not converged (global=0.5144 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0735 global=0.5244\n",
            "  step 10: local=0.0686 global=0.5286\n",
            "  step 20: local=0.0680 global=0.5036\n",
            "  step 30: local=0.0740 global=0.5636\n",
            "  step 40: local=0.0723 global=0.5175\n",
            "  step 50: local=0.0681 global=0.5028\n",
            "  step 60: local=0.0713 global=0.5329\n",
            "  step 70: local=0.0721 global=0.5293\n",
            "  step 80: local=0.0750 global=0.5494\n",
            "  step 90: local=0.0688 global=0.5262\n",
            "  Layer 18 not converged (global=0.4969 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0684 global=0.5120\n",
            "  step 10: local=0.0740 global=0.5217\n",
            "  step 20: local=0.0736 global=0.5171\n",
            "  step 30: local=0.0705 global=0.5510\n",
            "  step 40: local=0.0686 global=0.5026\n",
            "  step 50: local=0.0722 global=0.5360\n",
            "  step 60: local=0.0655 global=0.4812\n",
            "  step 70: local=0.0729 global=0.5496\n",
            "  step 80: local=0.0683 global=0.5082\n",
            "  step 90: local=0.0684 global=0.5556\n",
            "  Layer 18 not converged (global=0.5479 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0727 global=0.5133\n",
            "  step 10: local=0.0682 global=0.5413\n",
            "  step 20: local=0.0745 global=0.5851\n",
            "  step 30: local=0.0675 global=0.5416\n",
            "  step 40: local=0.0684 global=0.5064\n",
            "  step 50: local=0.0701 global=0.5310\n",
            "  step 60: local=0.0736 global=0.5659\n",
            "  step 70: local=0.0738 global=0.5147\n",
            "  step 80: local=0.0716 global=0.5229\n",
            "  step 90: local=0.0763 global=0.5416\n",
            "  Layer 18 not converged (global=0.4969 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0668 global=0.4970\n",
            "  step 10: local=0.0751 global=0.5483\n",
            "  step 20: local=0.0685 global=0.5613\n",
            "  step 30: local=0.0723 global=0.5574\n",
            "  step 40: local=0.0698 global=0.5437\n",
            "  step 50: local=0.0684 global=0.5088\n",
            "  step 60: local=0.0707 global=0.5103\n",
            "  step 70: local=0.0689 global=0.5314\n",
            "  step 80: local=0.0738 global=0.5039\n",
            "  step 90: local=0.0713 global=0.5420\n",
            "  Layer 18 not converged (global=0.5171 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0695 global=0.5364\n",
            "  step 10: local=0.0757 global=0.5187\n",
            "  step 20: local=0.0692 global=0.5110\n",
            "  step 30: local=0.0735 global=0.4880\n",
            "  step 40: local=0.0729 global=0.4950\n",
            "  step 50: local=0.0706 global=0.5149\n",
            "  step 60: local=0.0688 global=0.5161\n",
            "  step 70: local=0.0669 global=0.5271\n",
            "  step 80: local=0.0662 global=0.5349\n",
            "  step 90: local=0.0679 global=0.5203\n",
            "  Layer 18 not converged (global=0.5193 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0698 global=0.5063\n",
            "  step 10: local=0.0699 global=0.5290\n",
            "  step 20: local=0.0758 global=0.4847\n",
            "  step 30: local=0.0779 global=0.5382\n",
            "  step 40: local=0.0686 global=0.5256\n",
            "  step 50: local=0.0707 global=0.4867\n",
            "  step 60: local=0.0697 global=0.5224\n",
            "  step 70: local=0.0702 global=0.5334\n",
            "  step 80: local=0.0696 global=0.5406\n",
            "  step 90: local=0.0676 global=0.4965\n",
            "  Layer 18 not converged (global=0.5023 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0729 global=0.5208\n",
            "  step 10: local=0.0686 global=0.5313\n",
            "  step 20: local=0.0684 global=0.5359\n",
            "  step 30: local=0.0693 global=0.4957\n",
            "  step 40: local=0.0678 global=0.4895\n",
            "  step 50: local=0.0694 global=0.5127\n",
            "  step 60: local=0.0738 global=0.5013\n",
            "  step 70: local=0.0737 global=0.5359\n",
            "  step 80: local=0.0702 global=0.5444\n",
            "  step 90: local=0.0705 global=0.5002\n",
            "  Layer 18 not converged (global=0.5170 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0707 global=0.4897\n",
            "  step 10: local=0.0690 global=0.5130\n",
            "  step 20: local=0.0708 global=0.5066\n",
            "  step 30: local=0.0765 global=0.5381\n",
            "  step 40: local=0.0718 global=0.5178\n",
            "  step 50: local=0.0694 global=0.5385\n",
            "  step 60: local=0.0684 global=0.5065\n",
            "  step 70: local=0.0701 global=0.5086\n",
            "  step 80: local=0.0674 global=0.5225\n",
            "  step 90: local=0.0676 global=0.5061\n",
            "  Layer 18 not converged (global=0.5467 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0702 global=0.5094\n",
            "  step 10: local=0.0774 global=0.4964\n",
            "  step 20: local=0.0670 global=0.5227\n",
            "  step 30: local=0.0659 global=0.5383\n",
            "  step 40: local=0.0705 global=0.5399\n",
            "  step 50: local=0.0690 global=0.5079\n",
            "  step 60: local=0.0726 global=0.5383\n",
            "  step 70: local=0.0698 global=0.5350\n",
            "  step 80: local=0.0690 global=0.5108\n",
            "  step 90: local=0.0717 global=0.5169\n",
            "  Layer 18 not converged (global=0.4689 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0708 global=0.5128\n",
            "  step 10: local=0.0659 global=0.5013\n",
            "  step 20: local=0.0691 global=0.5291\n",
            "  step 30: local=0.0686 global=0.5251\n",
            "  step 40: local=0.0693 global=0.5141\n",
            "  step 50: local=0.0723 global=0.4806\n",
            "  step 60: local=0.0679 global=0.4988\n",
            "  step 70: local=0.0686 global=0.5490\n",
            "  step 80: local=0.0710 global=0.5061\n",
            "  step 90: local=0.0674 global=0.5020\n",
            "  Layer 18 not converged (global=0.5869 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0692 global=0.5207\n",
            "  step 10: local=0.0724 global=0.5167\n",
            "  step 20: local=0.0685 global=0.4930\n",
            "  step 30: local=0.0669 global=0.4874\n",
            "  step 40: local=0.0724 global=0.5167\n",
            "  step 50: local=0.0659 global=0.5221\n",
            "  step 60: local=0.0692 global=0.4977\n",
            "  step 70: local=0.0718 global=0.5573\n",
            "  step 80: local=0.0701 global=0.5122\n",
            "  step 90: local=0.0766 global=0.4968\n",
            "  Layer 18 not converged (global=0.5216 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0718 global=0.5264\n",
            "  step 10: local=0.0685 global=0.5227\n",
            "  step 20: local=0.0720 global=0.5430\n",
            "  step 30: local=0.0709 global=0.5208\n",
            "  step 40: local=0.0692 global=0.5054\n",
            "  step 50: local=0.0647 global=0.5165\n",
            "  step 60: local=0.0674 global=0.5117\n",
            "  step 70: local=0.0696 global=0.5452\n",
            "  step 80: local=0.0774 global=0.4977\n",
            "  step 90: local=0.0695 global=0.5303\n",
            "  Layer 18 not converged (global=0.4966 > 0.4), repeating...\n",
            "\n",
            "--- Layer 18/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0659 global=0.4772\n",
            "  step 10: local=0.0716 global=0.5447\n",
            "  step 20: local=0.0684 global=0.5037\n",
            "  step 30: local=0.0697 global=0.5507\n",
            "  step 40: local=0.0698 global=0.5076\n",
            "  step 50: local=0.0714 global=0.5357\n",
            "  step 60: local=0.0714 global=0.5801\n",
            "  step 70: local=0.0681 global=0.5365\n",
            "  step 80: local=0.0688 global=0.5019\n",
            "  step 90: local=0.0752 global=0.5263\n",
            "  [WARN] Layer 18 did not converge after 20 repeats (global=0.5205)\n",
            "\n",
            "--- Layer 19/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0962 global=0.6667\n",
            "  step 10: local=0.1008 global=0.6022\n",
            "  step 20: local=0.1043 global=0.6053\n",
            "  step 30: local=0.0981 global=0.6229\n",
            "  step 40: local=0.1119 global=0.5664\n",
            "  step 50: local=0.0918 global=0.6251\n",
            "  step 60: local=0.1023 global=0.6378\n",
            "  step 70: local=0.1015 global=0.6237\n",
            "  step 80: local=0.0945 global=0.6002\n",
            "  step 90: local=0.1090 global=0.5662\n",
            "  Layer 19 not converged (global=0.5675 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1099 global=0.5705\n",
            "  step 10: local=0.0966 global=0.5851\n",
            "  step 20: local=0.1028 global=0.5577\n",
            "  step 30: local=0.0970 global=0.5952\n",
            "  step 40: local=0.0856 global=0.5831\n",
            "  step 50: local=0.0912 global=0.5694\n",
            "  step 60: local=0.0911 global=0.5599\n",
            "  step 70: local=0.0981 global=0.5335\n",
            "  step 80: local=0.0981 global=0.5382\n",
            "  step 90: local=0.0986 global=0.5577\n",
            "  Layer 19 not converged (global=0.5587 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0890 global=0.5586\n",
            "  step 10: local=0.0934 global=0.5688\n",
            "  step 20: local=0.0888 global=0.5765\n",
            "  step 30: local=0.0912 global=0.5595\n",
            "  step 40: local=0.0955 global=0.5410\n",
            "  step 50: local=0.0853 global=0.5690\n",
            "  step 60: local=0.0875 global=0.5161\n",
            "  step 70: local=0.0919 global=0.5753\n",
            "  step 80: local=0.0808 global=0.5557\n",
            "  step 90: local=0.0871 global=0.5133\n",
            "  Layer 19 not converged (global=0.5490 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0900 global=0.5535\n",
            "  step 10: local=0.0848 global=0.5658\n",
            "  step 20: local=0.0832 global=0.5722\n",
            "  step 30: local=0.0873 global=0.5238\n",
            "  step 40: local=0.0830 global=0.5515\n",
            "  step 50: local=0.0929 global=0.5625\n",
            "  step 60: local=0.0831 global=0.5617\n",
            "  step 70: local=0.0778 global=0.5170\n",
            "  step 80: local=0.0864 global=0.5170\n",
            "  step 90: local=0.0803 global=0.5364\n",
            "  Layer 19 not converged (global=0.5259 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0791 global=0.5203\n",
            "  step 10: local=0.0770 global=0.5626\n",
            "  step 20: local=0.0866 global=0.5649\n",
            "  step 30: local=0.0921 global=0.5224\n",
            "  step 40: local=0.0980 global=0.5100\n",
            "  step 50: local=0.0897 global=0.5325\n",
            "  step 60: local=0.0781 global=0.5258\n",
            "  step 70: local=0.0878 global=0.5552\n",
            "  step 80: local=0.0930 global=0.5354\n",
            "  step 90: local=0.0893 global=0.5603\n",
            "  Layer 19 not converged (global=0.5425 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0817 global=0.5222\n",
            "  step 10: local=0.0786 global=0.5308\n",
            "  step 20: local=0.1048 global=0.5379\n",
            "  step 30: local=0.0844 global=0.5206\n",
            "  step 40: local=0.0848 global=0.5285\n",
            "  step 50: local=0.0763 global=0.5162\n",
            "  step 60: local=0.0822 global=0.5407\n",
            "  step 70: local=0.0884 global=0.5568\n",
            "  step 80: local=0.0960 global=0.5629\n",
            "  step 90: local=0.0823 global=0.5241\n",
            "  Layer 19 not converged (global=0.5200 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0892 global=0.5589\n",
            "  step 10: local=0.0867 global=0.5497\n",
            "  step 20: local=0.0899 global=0.5236\n",
            "  step 30: local=0.0922 global=0.5291\n",
            "  step 40: local=0.0850 global=0.5254\n",
            "  step 50: local=0.0954 global=0.5151\n",
            "  step 60: local=0.0837 global=0.5466\n",
            "  step 70: local=0.0797 global=0.5429\n",
            "  step 80: local=0.0869 global=0.5298\n",
            "  step 90: local=0.0776 global=0.4958\n",
            "  Layer 19 not converged (global=0.5355 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0847 global=0.5131\n",
            "  step 10: local=0.0899 global=0.5673\n",
            "  step 20: local=0.0959 global=0.5187\n",
            "  step 30: local=0.0893 global=0.5159\n",
            "  step 40: local=0.0758 global=0.5367\n",
            "  step 50: local=0.0908 global=0.5303\n",
            "  step 60: local=0.0909 global=0.5062\n",
            "  step 70: local=0.0906 global=0.5030\n",
            "  step 80: local=0.0891 global=0.5280\n",
            "  step 90: local=0.0812 global=0.5370\n",
            "  Layer 19 not converged (global=0.5086 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0833 global=0.5100\n",
            "  step 10: local=0.0848 global=0.5772\n",
            "  step 20: local=0.0836 global=0.5220\n",
            "  step 30: local=0.0748 global=0.5073\n",
            "  step 40: local=0.0865 global=0.5355\n",
            "  step 50: local=0.0844 global=0.5427\n",
            "  step 60: local=0.0864 global=0.5622\n",
            "  step 70: local=0.0825 global=0.5321\n",
            "  step 80: local=0.0850 global=0.5168\n",
            "  step 90: local=0.0847 global=0.5351\n",
            "  Layer 19 not converged (global=0.5127 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0957 global=0.5302\n",
            "  step 10: local=0.0949 global=0.5613\n",
            "  step 20: local=0.0814 global=0.5139\n",
            "  step 30: local=0.0789 global=0.5424\n",
            "  step 40: local=0.0817 global=0.4899\n",
            "  step 50: local=0.0828 global=0.5586\n",
            "  step 60: local=0.0833 global=0.5136\n",
            "  step 70: local=0.0787 global=0.5632\n",
            "  step 80: local=0.0774 global=0.5205\n",
            "  step 90: local=0.0900 global=0.5472\n",
            "  Layer 19 not converged (global=0.5214 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0911 global=0.5994\n",
            "  step 10: local=0.0783 global=0.5498\n",
            "  step 20: local=0.0818 global=0.5135\n",
            "  step 30: local=0.0851 global=0.5373\n",
            "  step 40: local=0.0840 global=0.5747\n",
            "  step 50: local=0.0925 global=0.5231\n",
            "  step 60: local=0.0813 global=0.5288\n",
            "  step 70: local=0.0843 global=0.5504\n",
            "  step 80: local=0.0845 global=0.5017\n",
            "  step 90: local=0.0784 global=0.5592\n",
            "  Layer 19 not converged (global=0.5180 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0863 global=0.5719\n",
            "  step 10: local=0.0895 global=0.5666\n",
            "  step 20: local=0.0922 global=0.5496\n",
            "  step 30: local=0.0820 global=0.5157\n",
            "  step 40: local=0.0888 global=0.5194\n",
            "  step 50: local=0.0783 global=0.5380\n",
            "  step 60: local=0.0823 global=0.5102\n",
            "  step 70: local=0.0919 global=0.5487\n",
            "  step 80: local=0.0863 global=0.5404\n",
            "  step 90: local=0.0835 global=0.5265\n",
            "  Layer 19 not converged (global=0.5306 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0928 global=0.5190\n",
            "  step 10: local=0.0879 global=0.4952\n",
            "  step 20: local=0.0845 global=0.5016\n",
            "  step 30: local=0.0922 global=0.5211\n",
            "  step 40: local=0.0860 global=0.5236\n",
            "  step 50: local=0.0819 global=0.5344\n",
            "  step 60: local=0.0773 global=0.5445\n",
            "  step 70: local=0.0895 global=0.5290\n",
            "  step 80: local=0.0876 global=0.5132\n",
            "  step 90: local=0.0898 global=0.5381\n",
            "  Layer 19 not converged (global=0.5037 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0780 global=0.4899\n",
            "  step 10: local=0.0862 global=0.5483\n",
            "  step 20: local=0.0811 global=0.5315\n",
            "  step 30: local=0.0770 global=0.4907\n",
            "  step 40: local=0.0811 global=0.5280\n",
            "  step 50: local=0.0886 global=0.5407\n",
            "  step 60: local=0.0756 global=0.5487\n",
            "  step 70: local=0.0857 global=0.5043\n",
            "  step 80: local=0.0842 global=0.5300\n",
            "  step 90: local=0.0840 global=0.5424\n",
            "  Layer 19 not converged (global=0.5221 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0756 global=0.5428\n",
            "  step 10: local=0.0838 global=0.5001\n",
            "  step 20: local=0.0746 global=0.4995\n",
            "  step 30: local=0.0808 global=0.5201\n",
            "  step 40: local=0.0822 global=0.5052\n",
            "  step 50: local=0.0798 global=0.5478\n",
            "  step 60: local=0.0858 global=0.5499\n",
            "  step 70: local=0.0817 global=0.5094\n",
            "  step 80: local=0.0918 global=0.4978\n",
            "  step 90: local=0.0861 global=0.5196\n",
            "  Layer 19 not converged (global=0.5525 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0863 global=0.5138\n",
            "  step 10: local=0.0846 global=0.5426\n",
            "  step 20: local=0.0860 global=0.5233\n",
            "  step 30: local=0.0871 global=0.5475\n",
            "  step 40: local=0.0777 global=0.5109\n",
            "  step 50: local=0.0997 global=0.5195\n",
            "  step 60: local=0.0843 global=0.5266\n",
            "  step 70: local=0.0799 global=0.5109\n",
            "  step 80: local=0.0853 global=0.5175\n",
            "  step 90: local=0.0768 global=0.5067\n",
            "  Layer 19 not converged (global=0.5674 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0759 global=0.5311\n",
            "  step 10: local=0.0887 global=0.5471\n",
            "  step 20: local=0.0879 global=0.5527\n",
            "  step 30: local=0.0732 global=0.5157\n",
            "  step 40: local=0.0879 global=0.5485\n",
            "  step 50: local=0.0866 global=0.5401\n",
            "  step 60: local=0.0882 global=0.5148\n",
            "  step 70: local=0.0915 global=0.5220\n",
            "  step 80: local=0.0820 global=0.5170\n",
            "  step 90: local=0.0872 global=0.5079\n",
            "  Layer 19 not converged (global=0.5013 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0757 global=0.5393\n",
            "  step 10: local=0.0834 global=0.5344\n",
            "  step 20: local=0.0834 global=0.5215\n",
            "  step 30: local=0.0783 global=0.4880\n",
            "  step 40: local=0.0831 global=0.5042\n",
            "  step 50: local=0.0903 global=0.5597\n",
            "  step 60: local=0.0846 global=0.5125\n",
            "  step 70: local=0.0726 global=0.5092\n",
            "  step 80: local=0.0858 global=0.5298\n",
            "  step 90: local=0.0800 global=0.5238\n",
            "  Layer 19 not converged (global=0.5206 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0828 global=0.5001\n",
            "  step 10: local=0.0917 global=0.4975\n",
            "  step 20: local=0.0763 global=0.5220\n",
            "  step 30: local=0.0831 global=0.5299\n",
            "  step 40: local=0.0946 global=0.5040\n",
            "  step 50: local=0.0722 global=0.5698\n",
            "  step 60: local=0.0843 global=0.5164\n",
            "  step 70: local=0.0779 global=0.5024\n",
            "  step 80: local=0.0841 global=0.5290\n",
            "  step 90: local=0.0877 global=0.5368\n",
            "  Layer 19 not converged (global=0.5175 > 0.4), repeating...\n",
            "\n",
            "--- Layer 19/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0761 global=0.5562\n",
            "  step 10: local=0.0877 global=0.5275\n",
            "  step 20: local=0.0933 global=0.5108\n",
            "  step 30: local=0.0767 global=0.5300\n",
            "  step 40: local=0.0889 global=0.5236\n",
            "  step 50: local=0.0776 global=0.5554\n",
            "  step 60: local=0.0910 global=0.5087\n",
            "  step 70: local=0.0919 global=0.5375\n",
            "  step 80: local=0.0795 global=0.4844\n",
            "  step 90: local=0.0757 global=0.5537\n",
            "  [WARN] Layer 19 did not converge after 20 repeats (global=0.5443)\n",
            "\n",
            "--- Layer 20/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0882 global=0.5368\n",
            "  step 10: local=0.0873 global=0.5841\n",
            "  step 20: local=0.0827 global=0.5406\n",
            "  step 30: local=0.0832 global=0.5663\n",
            "  step 40: local=0.0814 global=0.6164\n",
            "  step 50: local=0.0817 global=0.5670\n",
            "  step 60: local=0.0808 global=0.5281\n",
            "  step 70: local=0.0756 global=0.5527\n",
            "  step 80: local=0.0803 global=0.5937\n",
            "  step 90: local=0.0813 global=0.5406\n",
            "  Layer 20 not converged (global=0.5464 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0762 global=0.5419\n",
            "  step 10: local=0.0772 global=0.5661\n",
            "  step 20: local=0.0809 global=0.5152\n",
            "  step 30: local=0.0769 global=0.5719\n",
            "  step 40: local=0.0756 global=0.5821\n",
            "  step 50: local=0.0783 global=0.5795\n",
            "  step 60: local=0.0751 global=0.5590\n",
            "  step 70: local=0.0703 global=0.5232\n",
            "  step 80: local=0.0775 global=0.5278\n",
            "  step 90: local=0.0744 global=0.5460\n",
            "  Layer 20 not converged (global=0.5507 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0728 global=0.5175\n",
            "  step 10: local=0.0742 global=0.5585\n",
            "  step 20: local=0.0704 global=0.5501\n",
            "  step 30: local=0.0756 global=0.5353\n",
            "  step 40: local=0.0719 global=0.5250\n",
            "  step 50: local=0.0737 global=0.4997\n",
            "  step 60: local=0.0716 global=0.5076\n",
            "  step 70: local=0.0716 global=0.5264\n",
            "  step 80: local=0.0711 global=0.5304\n",
            "  step 90: local=0.0696 global=0.5369\n",
            "  Layer 20 not converged (global=0.5196 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0719 global=0.5486\n",
            "  step 10: local=0.0722 global=0.5304\n",
            "  step 20: local=0.0676 global=0.5144\n",
            "  step 30: local=0.0721 global=0.5428\n",
            "  step 40: local=0.0702 global=0.4930\n",
            "  step 50: local=0.0712 global=0.5494\n",
            "  step 60: local=0.0677 global=0.5319\n",
            "  step 70: local=0.0672 global=0.4928\n",
            "  step 80: local=0.0691 global=0.5283\n",
            "  step 90: local=0.0703 global=0.5423\n",
            "  Layer 20 not converged (global=0.5475 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0693 global=0.5520\n",
            "  step 10: local=0.0737 global=0.5048\n",
            "  step 20: local=0.0696 global=0.5307\n",
            "  step 30: local=0.0717 global=0.5431\n",
            "  step 40: local=0.0686 global=0.5430\n",
            "  step 50: local=0.0711 global=0.4989\n",
            "  step 60: local=0.0690 global=0.4988\n",
            "  step 70: local=0.0665 global=0.5165\n",
            "  step 80: local=0.0668 global=0.5032\n",
            "  step 90: local=0.0702 global=0.5429\n",
            "  Layer 20 not converged (global=0.5854 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0657 global=0.5464\n",
            "  step 10: local=0.0696 global=0.5101\n",
            "  step 20: local=0.0648 global=0.4958\n",
            "  step 30: local=0.0710 global=0.5175\n",
            "  step 40: local=0.0658 global=0.5119\n",
            "  step 50: local=0.0750 global=0.5391\n",
            "  step 60: local=0.0703 global=0.5216\n",
            "  step 70: local=0.0651 global=0.5438\n",
            "  step 80: local=0.0690 global=0.5069\n",
            "  step 90: local=0.0695 global=0.5179\n",
            "  Layer 20 not converged (global=0.5145 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0655 global=0.5227\n",
            "  step 10: local=0.0644 global=0.5116\n",
            "  step 20: local=0.0675 global=0.5123\n",
            "  step 30: local=0.0661 global=0.5037\n",
            "  step 40: local=0.0659 global=0.5283\n",
            "  step 50: local=0.0632 global=0.5438\n",
            "  step 60: local=0.0662 global=0.5476\n",
            "  step 70: local=0.0695 global=0.5100\n",
            "  step 80: local=0.0665 global=0.5456\n",
            "  step 90: local=0.0656 global=0.5384\n",
            "  Layer 20 not converged (global=0.5489 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0695 global=0.5106\n",
            "  step 10: local=0.0668 global=0.5178\n",
            "  step 20: local=0.0659 global=0.5133\n",
            "  step 30: local=0.0641 global=0.5013\n",
            "  step 40: local=0.0665 global=0.5322\n",
            "  step 50: local=0.0692 global=0.5333\n",
            "  step 60: local=0.0630 global=0.5169\n",
            "  step 70: local=0.0688 global=0.4850\n",
            "  step 80: local=0.0657 global=0.4982\n",
            "  step 90: local=0.0684 global=0.5566\n",
            "  Layer 20 not converged (global=0.5269 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0647 global=0.5083\n",
            "  step 10: local=0.0646 global=0.5036\n",
            "  step 20: local=0.0646 global=0.5237\n",
            "  step 30: local=0.0672 global=0.5182\n",
            "  step 40: local=0.0653 global=0.4952\n",
            "  step 50: local=0.0642 global=0.4932\n",
            "  step 60: local=0.0677 global=0.5159\n",
            "  step 70: local=0.0666 global=0.5271\n",
            "  step 80: local=0.0648 global=0.4990\n",
            "  step 90: local=0.0680 global=0.5624\n",
            "  Layer 20 not converged (global=0.5055 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0664 global=0.5135\n",
            "  step 10: local=0.0668 global=0.4964\n",
            "  step 20: local=0.0656 global=0.5248\n",
            "  step 30: local=0.0663 global=0.5322\n",
            "  step 40: local=0.0644 global=0.5515\n",
            "  step 50: local=0.0679 global=0.5217\n",
            "  step 60: local=0.0666 global=0.5029\n",
            "  step 70: local=0.0659 global=0.5214\n",
            "  step 80: local=0.0664 global=0.5177\n",
            "  step 90: local=0.0661 global=0.5522\n",
            "  Layer 20 not converged (global=0.5391 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0639 global=0.5024\n",
            "  step 10: local=0.0683 global=0.5313\n",
            "  step 20: local=0.0724 global=0.4791\n",
            "  step 30: local=0.0668 global=0.5495\n",
            "  step 40: local=0.0677 global=0.5060\n",
            "  step 50: local=0.0665 global=0.5528\n",
            "  step 60: local=0.0643 global=0.5114\n",
            "  step 70: local=0.0668 global=0.5380\n",
            "  step 80: local=0.0668 global=0.5871\n",
            "  step 90: local=0.0693 global=0.5399\n",
            "  Layer 20 not converged (global=0.5137 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0674 global=0.5022\n",
            "  step 10: local=0.0634 global=0.5278\n",
            "  step 20: local=0.0648 global=0.5657\n",
            "  step 30: local=0.0686 global=0.5149\n",
            "  step 40: local=0.0628 global=0.5177\n",
            "  step 50: local=0.0679 global=0.5421\n",
            "  step 60: local=0.0708 global=0.4935\n",
            "  step 70: local=0.0656 global=0.5496\n",
            "  step 80: local=0.0668 global=0.5575\n",
            "  step 90: local=0.0692 global=0.5573\n",
            "  Layer 20 not converged (global=0.4976 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0664 global=0.5377\n",
            "  step 10: local=0.0643 global=0.5034\n",
            "  step 20: local=0.0680 global=0.5077\n",
            "  step 30: local=0.0666 global=0.5274\n",
            "  step 40: local=0.0645 global=0.4984\n",
            "  step 50: local=0.0648 global=0.5380\n",
            "  step 60: local=0.0705 global=0.5318\n",
            "  step 70: local=0.0651 global=0.5184\n",
            "  step 80: local=0.0668 global=0.5081\n",
            "  step 90: local=0.0679 global=0.4829\n",
            "  Layer 20 not converged (global=0.5629 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0646 global=0.4919\n",
            "  step 10: local=0.0632 global=0.5106\n",
            "  step 20: local=0.0654 global=0.5164\n",
            "  step 30: local=0.0663 global=0.5224\n",
            "  step 40: local=0.0669 global=0.5342\n",
            "  step 50: local=0.0683 global=0.5175\n",
            "  step 60: local=0.0638 global=0.5028\n",
            "  step 70: local=0.0659 global=0.5302\n",
            "  step 80: local=0.0647 global=0.4811\n",
            "  step 90: local=0.0658 global=0.5371\n",
            "  Layer 20 not converged (global=0.5458 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0648 global=0.5223\n",
            "  step 10: local=0.0716 global=0.4827\n",
            "  step 20: local=0.0671 global=0.5182\n",
            "  step 30: local=0.0640 global=0.5314\n",
            "  step 40: local=0.0663 global=0.5397\n",
            "  step 50: local=0.0666 global=0.4961\n",
            "  step 60: local=0.0683 global=0.5201\n",
            "  step 70: local=0.0713 global=0.5337\n",
            "  step 80: local=0.0657 global=0.5334\n",
            "  step 90: local=0.0622 global=0.4905\n",
            "  Layer 20 not converged (global=0.5263 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0695 global=0.4902\n",
            "  step 10: local=0.0674 global=0.5079\n",
            "  step 20: local=0.0665 global=0.4948\n",
            "  step 30: local=0.0704 global=0.5354\n",
            "  step 40: local=0.0661 global=0.5379\n",
            "  step 50: local=0.0648 global=0.5018\n",
            "  step 60: local=0.0671 global=0.4887\n",
            "  step 70: local=0.0661 global=0.5098\n",
            "  step 80: local=0.0647 global=0.5053\n",
            "  step 90: local=0.0724 global=0.5316\n",
            "  Layer 20 not converged (global=0.5344 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0663 global=0.5142\n",
            "  step 10: local=0.0663 global=0.5376\n",
            "  step 20: local=0.0689 global=0.4999\n",
            "  step 30: local=0.0695 global=0.5108\n",
            "  step 40: local=0.0656 global=0.5154\n",
            "  step 50: local=0.0673 global=0.5044\n",
            "  step 60: local=0.0701 global=0.5060\n",
            "  step 70: local=0.0651 global=0.4980\n",
            "  step 80: local=0.0668 global=0.5221\n",
            "  step 90: local=0.0634 global=0.5379\n",
            "  Layer 20 not converged (global=0.4949 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0651 global=0.5416\n",
            "  step 10: local=0.0628 global=0.5050\n",
            "  step 20: local=0.0663 global=0.5397\n",
            "  step 30: local=0.0622 global=0.5326\n",
            "  step 40: local=0.0711 global=0.5035\n",
            "  step 50: local=0.0671 global=0.5125\n",
            "  step 60: local=0.0634 global=0.5079\n",
            "  step 70: local=0.0669 global=0.4965\n",
            "  step 80: local=0.0708 global=0.5271\n",
            "  step 90: local=0.0678 global=0.5275\n",
            "  Layer 20 not converged (global=0.5348 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0694 global=0.5121\n",
            "  step 10: local=0.0678 global=0.4804\n",
            "  step 20: local=0.0668 global=0.4928\n",
            "  step 30: local=0.0630 global=0.5517\n",
            "  step 40: local=0.0671 global=0.5030\n",
            "  step 50: local=0.0625 global=0.4976\n",
            "  step 60: local=0.0674 global=0.5188\n",
            "  step 70: local=0.0612 global=0.5134\n",
            "  step 80: local=0.0702 global=0.4907\n",
            "  step 90: local=0.0686 global=0.4894\n",
            "  Layer 20 not converged (global=0.5066 > 0.4), repeating...\n",
            "\n",
            "--- Layer 20/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0646 global=0.5111\n",
            "  step 10: local=0.0651 global=0.5220\n",
            "  step 20: local=0.0641 global=0.4943\n",
            "  step 30: local=0.0605 global=0.5577\n",
            "  step 40: local=0.0649 global=0.5088\n",
            "  step 50: local=0.0662 global=0.4926\n",
            "  step 60: local=0.0655 global=0.5202\n",
            "  step 70: local=0.0689 global=0.5276\n",
            "  step 80: local=0.0659 global=0.5474\n",
            "  step 90: local=0.0665 global=0.5186\n",
            "  [WARN] Layer 20 did not converge after 20 repeats (global=0.4882)\n",
            "\n",
            "--- Layer 21/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0791 global=0.5171\n",
            "  step 10: local=0.0756 global=0.5357\n",
            "  step 20: local=0.0759 global=0.5294\n",
            "  step 30: local=0.0761 global=0.5650\n",
            "  step 40: local=0.0735 global=0.5144\n",
            "  step 50: local=0.0734 global=0.5442\n",
            "  step 60: local=0.0719 global=0.4882\n",
            "  step 70: local=0.0704 global=0.5606\n",
            "  step 80: local=0.0719 global=0.5158\n",
            "  step 90: local=0.0694 global=0.5621\n",
            "  Layer 21 not converged (global=0.5540 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0684 global=0.5211\n",
            "  step 10: local=0.0754 global=0.5457\n",
            "  step 20: local=0.0709 global=0.5954\n",
            "  step 30: local=0.0689 global=0.5502\n",
            "  step 40: local=0.0696 global=0.5084\n",
            "  step 50: local=0.0660 global=0.5349\n",
            "  step 60: local=0.0653 global=0.5735\n",
            "  step 70: local=0.0635 global=0.5228\n",
            "  step 80: local=0.0635 global=0.5227\n",
            "  step 90: local=0.0652 global=0.5495\n",
            "  Layer 21 not converged (global=0.5055 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0647 global=0.4980\n",
            "  step 10: local=0.0677 global=0.5537\n",
            "  step 20: local=0.0651 global=0.5636\n",
            "  step 30: local=0.0649 global=0.5652\n",
            "  step 40: local=0.0621 global=0.5418\n",
            "  step 50: local=0.0625 global=0.5052\n",
            "  step 60: local=0.0602 global=0.5087\n",
            "  step 70: local=0.0636 global=0.5284\n",
            "  step 80: local=0.0610 global=0.5007\n",
            "  step 90: local=0.0621 global=0.5398\n",
            "  Layer 21 not converged (global=0.5165 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0617 global=0.5322\n",
            "  step 10: local=0.0579 global=0.5196\n",
            "  step 20: local=0.0595 global=0.5087\n",
            "  step 30: local=0.0612 global=0.4828\n",
            "  step 40: local=0.0602 global=0.4924\n",
            "  step 50: local=0.0623 global=0.5102\n",
            "  step 60: local=0.0578 global=0.5168\n",
            "  step 70: local=0.0606 global=0.5215\n",
            "  step 80: local=0.0595 global=0.5318\n",
            "  step 90: local=0.0592 global=0.5168\n",
            "  Layer 21 not converged (global=0.5128 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0564 global=0.5000\n",
            "  step 10: local=0.0583 global=0.5285\n",
            "  step 20: local=0.0556 global=0.4806\n",
            "  step 30: local=0.0569 global=0.5356\n",
            "  step 40: local=0.0558 global=0.5192\n",
            "  step 50: local=0.0582 global=0.4792\n",
            "  step 60: local=0.0573 global=0.5170\n",
            "  step 70: local=0.0544 global=0.5307\n",
            "  step 80: local=0.0592 global=0.5358\n",
            "  step 90: local=0.0561 global=0.4926\n",
            "  Layer 21 not converged (global=0.4990 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0568 global=0.5196\n",
            "  step 10: local=0.0539 global=0.5300\n",
            "  step 20: local=0.0570 global=0.5308\n",
            "  step 30: local=0.0577 global=0.4866\n",
            "  step 40: local=0.0558 global=0.4868\n",
            "  step 50: local=0.0547 global=0.5073\n",
            "  step 60: local=0.0573 global=0.4918\n",
            "  step 70: local=0.0566 global=0.5312\n",
            "  step 80: local=0.0575 global=0.5357\n",
            "  step 90: local=0.0557 global=0.4982\n",
            "  Layer 21 not converged (global=0.5129 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0590 global=0.4850\n",
            "  step 10: local=0.0567 global=0.5073\n",
            "  step 20: local=0.0563 global=0.5001\n",
            "  step 30: local=0.0577 global=0.5284\n",
            "  step 40: local=0.0558 global=0.5134\n",
            "  step 50: local=0.0549 global=0.5328\n",
            "  step 60: local=0.0555 global=0.4967\n",
            "  step 70: local=0.0551 global=0.5062\n",
            "  step 80: local=0.0551 global=0.5102\n",
            "  step 90: local=0.0538 global=0.4987\n",
            "  Layer 21 not converged (global=0.5360 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0560 global=0.5011\n",
            "  step 10: local=0.0576 global=0.4941\n",
            "  step 20: local=0.0545 global=0.5173\n",
            "  step 30: local=0.0578 global=0.5333\n",
            "  step 40: local=0.0579 global=0.5360\n",
            "  step 50: local=0.0566 global=0.4987\n",
            "  step 60: local=0.0559 global=0.5338\n",
            "  step 70: local=0.0576 global=0.5277\n",
            "  step 80: local=0.0551 global=0.4989\n",
            "  step 90: local=0.0584 global=0.5074\n",
            "  Layer 21 not converged (global=0.4579 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0585 global=0.5039\n",
            "  step 10: local=0.0557 global=0.4920\n",
            "  step 20: local=0.0541 global=0.5236\n",
            "  step 30: local=0.0548 global=0.5200\n",
            "  step 40: local=0.0563 global=0.5049\n",
            "  step 50: local=0.0540 global=0.4759\n",
            "  step 60: local=0.0553 global=0.4878\n",
            "  step 70: local=0.0561 global=0.5484\n",
            "  step 80: local=0.0558 global=0.4991\n",
            "  step 90: local=0.0581 global=0.4906\n",
            "  Layer 21 not converged (global=0.5772 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0568 global=0.5138\n",
            "  step 10: local=0.0516 global=0.5058\n",
            "  step 20: local=0.0537 global=0.4858\n",
            "  step 30: local=0.0547 global=0.4826\n",
            "  step 40: local=0.0579 global=0.5053\n",
            "  step 50: local=0.0552 global=0.5166\n",
            "  step 60: local=0.0552 global=0.4905\n",
            "  step 70: local=0.0555 global=0.5529\n",
            "  step 80: local=0.0531 global=0.5037\n",
            "  step 90: local=0.0545 global=0.4876\n",
            "  Layer 21 not converged (global=0.5115 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0564 global=0.5129\n",
            "  step 10: local=0.0529 global=0.5208\n",
            "  step 20: local=0.0564 global=0.5407\n",
            "  step 30: local=0.0541 global=0.5147\n",
            "  step 40: local=0.0552 global=0.4926\n",
            "  step 50: local=0.0537 global=0.5112\n",
            "  step 60: local=0.0576 global=0.5076\n",
            "  step 70: local=0.0563 global=0.5419\n",
            "  step 80: local=0.0567 global=0.4929\n",
            "  step 90: local=0.0565 global=0.5224\n",
            "  Layer 21 not converged (global=0.4898 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0527 global=0.4705\n",
            "  step 10: local=0.0552 global=0.5398\n",
            "  step 20: local=0.0577 global=0.4976\n",
            "  step 30: local=0.0568 global=0.5431\n",
            "  step 40: local=0.0555 global=0.5024\n",
            "  step 50: local=0.0565 global=0.5271\n",
            "  step 60: local=0.0559 global=0.5772\n",
            "  step 70: local=0.0525 global=0.5319\n",
            "  step 80: local=0.0552 global=0.4923\n",
            "  step 90: local=0.0550 global=0.5178\n",
            "  Layer 21 not converged (global=0.5099 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0566 global=0.5546\n",
            "  step 10: local=0.0543 global=0.5065\n",
            "  step 20: local=0.0539 global=0.5075\n",
            "  step 30: local=0.0568 global=0.5333\n",
            "  step 40: local=0.0554 global=0.4836\n",
            "  step 50: local=0.0573 global=0.5385\n",
            "  step 60: local=0.0570 global=0.5473\n",
            "  step 70: local=0.0569 global=0.5502\n",
            "  step 80: local=0.0543 global=0.5275\n",
            "  step 90: local=0.0554 global=0.4927\n",
            "  Layer 21 not converged (global=0.4937 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0552 global=0.4965\n",
            "  step 10: local=0.0563 global=0.5165\n",
            "  step 20: local=0.0548 global=0.4889\n",
            "  step 30: local=0.0546 global=0.5270\n",
            "  step 40: local=0.0522 global=0.5208\n",
            "  step 50: local=0.0545 global=0.5087\n",
            "  step 60: local=0.0545 global=0.4981\n",
            "  step 70: local=0.0557 global=0.4724\n",
            "  step 80: local=0.0554 global=0.4829\n",
            "  step 90: local=0.0545 global=0.5008\n",
            "  Layer 21 not converged (global=0.5044 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0534 global=0.5070\n",
            "  step 10: local=0.0562 global=0.5121\n",
            "  step 20: local=0.0534 global=0.5227\n",
            "  step 30: local=0.0562 global=0.5073\n",
            "  step 40: local=0.0512 global=0.4921\n",
            "  step 50: local=0.0570 global=0.5198\n",
            "  step 60: local=0.0534 global=0.4726\n",
            "  step 70: local=0.0575 global=0.5286\n",
            "  step 80: local=0.0530 global=0.5120\n",
            "  step 90: local=0.0502 global=0.4725\n",
            "  Layer 21 not converged (global=0.5095 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0565 global=0.5099\n",
            "  step 10: local=0.0521 global=0.5236\n",
            "  step 20: local=0.0563 global=0.5281\n",
            "  step 30: local=0.0560 global=0.4860\n",
            "  step 40: local=0.0570 global=0.5124\n",
            "  step 50: local=0.0544 global=0.5226\n",
            "  step 60: local=0.0549 global=0.5245\n",
            "  step 70: local=0.0556 global=0.4809\n",
            "  step 80: local=0.0536 global=0.4810\n",
            "  step 90: local=0.0531 global=0.5016\n",
            "  Layer 21 not converged (global=0.4936 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0540 global=0.4860\n",
            "  step 10: local=0.0535 global=0.5251\n",
            "  step 20: local=0.0555 global=0.5299\n",
            "  step 30: local=0.0538 global=0.4923\n",
            "  step 40: local=0.0554 global=0.4796\n",
            "  step 50: local=0.0525 global=0.5012\n",
            "  step 60: local=0.0550 global=0.4942\n",
            "  step 70: local=0.0557 global=0.5227\n",
            "  step 80: local=0.0579 global=0.5076\n",
            "  step 90: local=0.0527 global=0.5276\n",
            "  Layer 21 not converged (global=0.5129 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0530 global=0.4917\n",
            "  step 10: local=0.0552 global=0.5009\n",
            "  step 20: local=0.0525 global=0.5058\n",
            "  step 30: local=0.0581 global=0.4941\n",
            "  step 40: local=0.0546 global=0.4959\n",
            "  step 50: local=0.0556 global=0.4890\n",
            "  step 60: local=0.0537 global=0.5126\n",
            "  step 70: local=0.0528 global=0.5291\n",
            "  step 80: local=0.0551 global=0.5311\n",
            "  step 90: local=0.0528 global=0.4946\n",
            "  Layer 21 not converged (global=0.4956 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0560 global=0.5292\n",
            "  step 10: local=0.0546 global=0.5228\n",
            "  step 20: local=0.0540 global=0.4942\n",
            "  step 30: local=0.0582 global=0.5036\n",
            "  step 40: local=0.0528 global=0.4985\n",
            "  step 50: local=0.0552 global=0.4876\n",
            "  step 60: local=0.0538 global=0.5190\n",
            "  step 70: local=0.0541 global=0.5160\n",
            "  step 80: local=0.0546 global=0.5006\n",
            "  step 90: local=0.0519 global=0.4719\n",
            "  Layer 21 not converged (global=0.5060 > 0.4), repeating...\n",
            "\n",
            "--- Layer 21/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0535 global=0.4839\n",
            "  step 10: local=0.0554 global=0.5440\n",
            "  step 20: local=0.0524 global=0.4954\n",
            "  step 30: local=0.0542 global=0.4871\n",
            "  step 40: local=0.0555 global=0.5095\n",
            "  step 50: local=0.0552 global=0.5023\n",
            "  step 60: local=0.0582 global=0.4828\n",
            "  step 70: local=0.0561 global=0.4790\n",
            "  step 80: local=0.0545 global=0.5018\n",
            "  step 90: local=0.0552 global=0.5126\n",
            "  [WARN] Layer 21 did not converge after 20 repeats (global=0.4866)\n",
            "\n",
            "--- Layer 22/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0787 global=0.5003\n",
            "  step 10: local=0.0813 global=0.5648\n",
            "  step 20: local=0.0799 global=0.5128\n",
            "  step 30: local=0.0803 global=0.4965\n",
            "  step 40: local=0.0787 global=0.5200\n",
            "  step 50: local=0.0770 global=0.5321\n",
            "  step 60: local=0.0731 global=0.5514\n",
            "  step 70: local=0.0708 global=0.5227\n",
            "  step 80: local=0.0716 global=0.4990\n",
            "  step 90: local=0.0689 global=0.5161\n",
            "  Layer 22 not converged (global=0.4990 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0720 global=0.5166\n",
            "  step 10: local=0.0761 global=0.5484\n",
            "  step 20: local=0.0699 global=0.5002\n",
            "  step 30: local=0.0665 global=0.5303\n",
            "  step 40: local=0.0697 global=0.4736\n",
            "  step 50: local=0.0665 global=0.5461\n",
            "  step 60: local=0.0690 global=0.5031\n",
            "  step 70: local=0.0628 global=0.5487\n",
            "  step 80: local=0.0707 global=0.5081\n",
            "  step 90: local=0.0647 global=0.5301\n",
            "  Layer 22 not converged (global=0.5069 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0662 global=0.5811\n",
            "  step 10: local=0.0652 global=0.5372\n",
            "  step 20: local=0.0661 global=0.4954\n",
            "  step 30: local=0.0609 global=0.5211\n",
            "  step 40: local=0.0642 global=0.5566\n",
            "  step 50: local=0.0658 global=0.5094\n",
            "  step 60: local=0.0605 global=0.5108\n",
            "  step 70: local=0.0580 global=0.5372\n",
            "  step 80: local=0.0629 global=0.4859\n",
            "  step 90: local=0.0638 global=0.5414\n",
            "  Layer 22 not converged (global=0.5029 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0615 global=0.5483\n",
            "  step 10: local=0.0598 global=0.5533\n",
            "  step 20: local=0.0572 global=0.5320\n",
            "  step 30: local=0.0592 global=0.4947\n",
            "  step 40: local=0.0571 global=0.4977\n",
            "  step 50: local=0.0554 global=0.5163\n",
            "  step 60: local=0.0578 global=0.4900\n",
            "  step 70: local=0.0542 global=0.5264\n",
            "  step 80: local=0.0545 global=0.5223\n",
            "  step 90: local=0.0538 global=0.5104\n",
            "  Layer 22 not converged (global=0.5112 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0565 global=0.4986\n",
            "  step 10: local=0.0556 global=0.4720\n",
            "  step 20: local=0.0548 global=0.4813\n",
            "  step 30: local=0.0592 global=0.5011\n",
            "  step 40: local=0.0533 global=0.5067\n",
            "  step 50: local=0.0538 global=0.5121\n",
            "  step 60: local=0.0541 global=0.5226\n",
            "  step 70: local=0.0506 global=0.5071\n",
            "  step 80: local=0.0528 global=0.4915\n",
            "  step 90: local=0.0546 global=0.5188\n",
            "  Layer 22 not converged (global=0.4799 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0552 global=0.4726\n",
            "  step 10: local=0.0572 global=0.5306\n",
            "  step 20: local=0.0521 global=0.5117\n",
            "  step 30: local=0.0514 global=0.4712\n",
            "  step 40: local=0.0527 global=0.5085\n",
            "  step 50: local=0.0517 global=0.5217\n",
            "  step 60: local=0.0529 global=0.5267\n",
            "  step 70: local=0.0535 global=0.4830\n",
            "  step 80: local=0.0534 global=0.5097\n",
            "  step 90: local=0.0533 global=0.5204\n",
            "  Layer 22 not converged (global=0.4996 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0549 global=0.5235\n",
            "  step 10: local=0.0534 global=0.4790\n",
            "  step 20: local=0.0531 global=0.4789\n",
            "  step 30: local=0.0507 global=0.5001\n",
            "  step 40: local=0.0564 global=0.4831\n",
            "  step 50: local=0.0519 global=0.5239\n",
            "  step 60: local=0.0537 global=0.5277\n",
            "  step 70: local=0.0537 global=0.4922\n",
            "  step 80: local=0.0542 global=0.4770\n",
            "  step 90: local=0.0522 global=0.4976\n",
            "  Layer 22 not converged (global=0.5260 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0525 global=0.4920\n",
            "  step 10: local=0.0552 global=0.5196\n",
            "  step 20: local=0.0506 global=0.5058\n",
            "  step 30: local=0.0516 global=0.5247\n",
            "  step 40: local=0.0492 global=0.4893\n",
            "  step 50: local=0.0532 global=0.4984\n",
            "  step 60: local=0.0520 global=0.5033\n",
            "  step 70: local=0.0547 global=0.4914\n",
            "  step 80: local=0.0499 global=0.4915\n",
            "  step 90: local=0.0527 global=0.4870\n",
            "  Layer 22 not converged (global=0.5396 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0512 global=0.5100\n",
            "  step 10: local=0.0532 global=0.5269\n",
            "  step 20: local=0.0523 global=0.5278\n",
            "  step 30: local=0.0505 global=0.4915\n",
            "  step 40: local=0.0540 global=0.5266\n",
            "  step 50: local=0.0535 global=0.5198\n",
            "  step 60: local=0.0540 global=0.4911\n",
            "  step 70: local=0.0533 global=0.5006\n",
            "  step 80: local=0.0529 global=0.4967\n",
            "  step 90: local=0.0541 global=0.4848\n",
            "  Layer 22 not converged (global=0.4786 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0555 global=0.5149\n",
            "  step 10: local=0.0524 global=0.5131\n",
            "  step 20: local=0.0522 global=0.4995\n",
            "  step 30: local=0.0527 global=0.4690\n",
            "  step 40: local=0.0491 global=0.4802\n",
            "  step 50: local=0.0581 global=0.5396\n",
            "  step 60: local=0.0527 global=0.4915\n",
            "  step 70: local=0.0487 global=0.4851\n",
            "  step 80: local=0.0523 global=0.5040\n",
            "  step 90: local=0.0511 global=0.4972\n",
            "  Layer 22 not converged (global=0.4980 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0549 global=0.4809\n",
            "  step 10: local=0.0519 global=0.4745\n",
            "  step 20: local=0.0548 global=0.4976\n",
            "  step 30: local=0.0526 global=0.5103\n",
            "  step 40: local=0.0509 global=0.4837\n",
            "  step 50: local=0.0514 global=0.5470\n",
            "  step 60: local=0.0536 global=0.4976\n",
            "  step 70: local=0.0556 global=0.4822\n",
            "  step 80: local=0.0575 global=0.5046\n",
            "  step 90: local=0.0565 global=0.5156\n",
            "  Layer 22 not converged (global=0.4985 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0495 global=0.5353\n",
            "  step 10: local=0.0547 global=0.5089\n",
            "  step 20: local=0.0545 global=0.4854\n",
            "  step 30: local=0.0522 global=0.5024\n",
            "  step 40: local=0.0487 global=0.5027\n",
            "  step 50: local=0.0522 global=0.5346\n",
            "  step 60: local=0.0535 global=0.4869\n",
            "  step 70: local=0.0478 global=0.5165\n",
            "  step 80: local=0.0538 global=0.4630\n",
            "  step 90: local=0.0536 global=0.5337\n",
            "  Layer 22 not converged (global=0.5230 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0503 global=0.4912\n",
            "  step 10: local=0.0531 global=0.5359\n",
            "  step 20: local=0.0534 global=0.4965\n",
            "  step 30: local=0.0510 global=0.5191\n",
            "  step 40: local=0.0531 global=0.5692\n",
            "  step 50: local=0.0555 global=0.5267\n",
            "  step 60: local=0.0526 global=0.4853\n",
            "  step 70: local=0.0536 global=0.5110\n",
            "  step 80: local=0.0600 global=0.5458\n",
            "  step 90: local=0.0485 global=0.4995\n",
            "  Layer 22 not converged (global=0.5069 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0520 global=0.5022\n",
            "  step 10: local=0.0574 global=0.5264\n",
            "  step 20: local=0.0526 global=0.4784\n",
            "  step 30: local=0.0544 global=0.5330\n",
            "  step 40: local=0.0514 global=0.5380\n",
            "  step 50: local=0.0489 global=0.5443\n",
            "  step 60: local=0.0502 global=0.5239\n",
            "  step 70: local=0.0526 global=0.4875\n",
            "  step 80: local=0.0521 global=0.4906\n",
            "  step 90: local=0.0497 global=0.5089\n",
            "  Layer 22 not converged (global=0.5145 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0525 global=0.4833\n",
            "  step 10: local=0.0517 global=0.5183\n",
            "  step 20: local=0.0524 global=0.5154\n",
            "  step 30: local=0.0510 global=0.5037\n",
            "  step 40: local=0.0511 global=0.4922\n",
            "  step 50: local=0.0523 global=0.4660\n",
            "  step 60: local=0.0490 global=0.4760\n",
            "  step 70: local=0.0544 global=0.4947\n",
            "  step 80: local=0.0530 global=0.5005\n",
            "  step 90: local=0.0507 global=0.5062\n",
            "  Layer 22 not converged (global=0.4904 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0527 global=0.5173\n",
            "  step 10: local=0.0515 global=0.5011\n",
            "  step 20: local=0.0513 global=0.4865\n",
            "  step 30: local=0.0537 global=0.5129\n",
            "  step 40: local=0.0533 global=0.4672\n",
            "  step 50: local=0.0548 global=0.5256\n",
            "  step 60: local=0.0507 global=0.5071\n",
            "  step 70: local=0.0500 global=0.4670\n",
            "  step 80: local=0.0513 global=0.5034\n",
            "  step 90: local=0.0486 global=0.5161\n",
            "  Layer 22 not converged (global=0.5230 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0520 global=0.5215\n",
            "  step 10: local=0.0507 global=0.4786\n",
            "  step 20: local=0.0519 global=0.5053\n",
            "  step 30: local=0.0557 global=0.5159\n",
            "  step 40: local=0.0516 global=0.5186\n",
            "  step 50: local=0.0501 global=0.4749\n",
            "  step 60: local=0.0548 global=0.4749\n",
            "  step 70: local=0.0514 global=0.4962\n",
            "  step 80: local=0.0512 global=0.4789\n",
            "  step 90: local=0.0554 global=0.5193\n",
            "  Layer 22 not converged (global=0.5605 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0522 global=0.5237\n",
            "  step 10: local=0.0503 global=0.4880\n",
            "  step 20: local=0.0522 global=0.4738\n",
            "  step 30: local=0.0537 global=0.4930\n",
            "  step 40: local=0.0538 global=0.4876\n",
            "  step 50: local=0.0491 global=0.5156\n",
            "  step 60: local=0.0482 global=0.5013\n",
            "  step 70: local=0.0485 global=0.5210\n",
            "  step 80: local=0.0509 global=0.4860\n",
            "  step 90: local=0.0525 global=0.4951\n",
            "  Layer 22 not converged (global=0.4917 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0537 global=0.5000\n",
            "  step 10: local=0.0532 global=0.4882\n",
            "  step 20: local=0.0502 global=0.4883\n",
            "  step 30: local=0.0526 global=0.4837\n",
            "  step 40: local=0.0506 global=0.5066\n",
            "  step 50: local=0.0544 global=0.5238\n",
            "  step 60: local=0.0519 global=0.5242\n",
            "  step 70: local=0.0536 global=0.4887\n",
            "  step 80: local=0.0526 global=0.5228\n",
            "  step 90: local=0.0482 global=0.5165\n",
            "  Layer 22 not converged (global=0.5284 > 0.4), repeating...\n",
            "\n",
            "--- Layer 22/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0545 global=0.4879\n",
            "  step 10: local=0.0533 global=0.4975\n",
            "  step 20: local=0.0523 global=0.4943\n",
            "  step 30: local=0.0545 global=0.4824\n",
            "  step 40: local=0.0523 global=0.5113\n",
            "  step 50: local=0.0499 global=0.5101\n",
            "  step 60: local=0.0538 global=0.4966\n",
            "  step 70: local=0.0497 global=0.4659\n",
            "  step 80: local=0.0523 global=0.4773\n",
            "  step 90: local=0.0500 global=0.5366\n",
            "  [WARN] Layer 22 did not converge after 20 repeats (global=0.5074)\n",
            "\n",
            "--- Layer 23/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0662 global=0.4989\n",
            "  step 10: local=0.0718 global=0.4922\n",
            "  step 20: local=0.0679 global=0.5094\n",
            "  step 30: local=0.0691 global=0.5028\n",
            "  step 40: local=0.0675 global=0.4878\n",
            "  step 50: local=0.0650 global=0.4797\n",
            "  step 60: local=0.0654 global=0.5028\n",
            "  step 70: local=0.0638 global=0.5170\n",
            "  step 80: local=0.0595 global=0.4880\n",
            "  step 90: local=0.0627 global=0.5530\n",
            "  Layer 23 not converged (global=0.4927 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0663 global=0.5018\n",
            "  step 10: local=0.0602 global=0.4865\n",
            "  step 20: local=0.0630 global=0.5088\n",
            "  step 30: local=0.0604 global=0.5208\n",
            "  step 40: local=0.0621 global=0.5414\n",
            "  step 50: local=0.0633 global=0.5125\n",
            "  step 60: local=0.0659 global=0.4897\n",
            "  step 70: local=0.0572 global=0.5047\n",
            "  step 80: local=0.0561 global=0.5069\n",
            "  step 90: local=0.0591 global=0.5366\n",
            "  Layer 23 not converged (global=0.5276 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0556 global=0.4881\n",
            "  step 10: local=0.0567 global=0.5192\n",
            "  step 20: local=0.0589 global=0.4646\n",
            "  step 30: local=0.0605 global=0.5360\n",
            "  step 40: local=0.0553 global=0.4903\n",
            "  step 50: local=0.0559 global=0.5382\n",
            "  step 60: local=0.0555 global=0.4964\n",
            "  step 70: local=0.0520 global=0.5198\n",
            "  step 80: local=0.0549 global=0.5697\n",
            "  step 90: local=0.0518 global=0.5280\n",
            "  Layer 23 not converged (global=0.4983 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0505 global=0.4865\n",
            "  step 10: local=0.0526 global=0.5114\n",
            "  step 20: local=0.0503 global=0.5447\n",
            "  step 30: local=0.0496 global=0.5021\n",
            "  step 40: local=0.0500 global=0.5019\n",
            "  step 50: local=0.0514 global=0.5282\n",
            "  step 60: local=0.0493 global=0.4791\n",
            "  step 70: local=0.0480 global=0.5330\n",
            "  step 80: local=0.0546 global=0.5378\n",
            "  step 90: local=0.0496 global=0.5422\n",
            "  Layer 23 not converged (global=0.4837 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0499 global=0.5222\n",
            "  step 10: local=0.0473 global=0.4862\n",
            "  step 20: local=0.0510 global=0.4898\n",
            "  step 30: local=0.0488 global=0.5079\n",
            "  step 40: local=0.0498 global=0.4822\n",
            "  step 50: local=0.0482 global=0.5180\n",
            "  step 60: local=0.0468 global=0.5131\n",
            "  step 70: local=0.0491 global=0.5029\n",
            "  step 80: local=0.0495 global=0.4899\n",
            "  step 90: local=0.0498 global=0.4635\n",
            "  Layer 23 not converged (global=0.5440 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0494 global=0.4766\n",
            "  step 10: local=0.0493 global=0.4935\n",
            "  step 20: local=0.0468 global=0.5005\n",
            "  step 30: local=0.0485 global=0.5040\n",
            "  step 40: local=0.0484 global=0.5144\n",
            "  step 50: local=0.0500 global=0.4983\n",
            "  step 60: local=0.0501 global=0.4842\n",
            "  step 70: local=0.0459 global=0.5097\n",
            "  step 80: local=0.0483 global=0.4647\n",
            "  step 90: local=0.0499 global=0.5245\n",
            "  Layer 23 not converged (global=0.5275 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0457 global=0.5049\n",
            "  step 10: local=0.0490 global=0.4640\n",
            "  step 20: local=0.0471 global=0.5011\n",
            "  step 30: local=0.0500 global=0.5129\n",
            "  step 40: local=0.0469 global=0.5184\n",
            "  step 50: local=0.0475 global=0.4767\n",
            "  step 60: local=0.0469 global=0.5040\n",
            "  step 70: local=0.0463 global=0.5140\n",
            "  step 80: local=0.0455 global=0.5162\n",
            "  step 90: local=0.0501 global=0.4717\n",
            "  Layer 23 not converged (global=0.5035 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0538 global=0.4724\n",
            "  step 10: local=0.0478 global=0.4925\n",
            "  step 20: local=0.0469 global=0.4764\n",
            "  step 30: local=0.0501 global=0.5164\n",
            "  step 40: local=0.0462 global=0.5195\n",
            "  step 50: local=0.0528 global=0.4872\n",
            "  step 60: local=0.0453 global=0.4714\n",
            "  step 70: local=0.0463 global=0.4892\n",
            "  step 80: local=0.0467 global=0.4865\n",
            "  step 90: local=0.0498 global=0.5118\n",
            "  Layer 23 not converged (global=0.5171 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0485 global=0.4973\n",
            "  step 10: local=0.0464 global=0.5167\n",
            "  step 20: local=0.0454 global=0.4832\n",
            "  step 30: local=0.0467 global=0.4909\n",
            "  step 40: local=0.0476 global=0.4962\n",
            "  step 50: local=0.0463 global=0.4865\n",
            "  step 60: local=0.0467 global=0.4851\n",
            "  step 70: local=0.0489 global=0.4803\n",
            "  step 80: local=0.0481 global=0.5025\n",
            "  step 90: local=0.0482 global=0.5208\n",
            "  Layer 23 not converged (global=0.4791 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0470 global=0.5211\n",
            "  step 10: local=0.0458 global=0.4852\n",
            "  step 20: local=0.0477 global=0.5182\n",
            "  step 30: local=0.0467 global=0.5133\n",
            "  step 40: local=0.0465 global=0.4834\n",
            "  step 50: local=0.0480 global=0.4951\n",
            "  step 60: local=0.0447 global=0.4905\n",
            "  step 70: local=0.0507 global=0.4792\n",
            "  step 80: local=0.0469 global=0.5090\n",
            "  step 90: local=0.0472 global=0.5063\n",
            "  Layer 23 not converged (global=0.5143 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0474 global=0.4944\n",
            "  step 10: local=0.0481 global=0.4617\n",
            "  step 20: local=0.0461 global=0.4743\n",
            "  step 30: local=0.0482 global=0.5332\n",
            "  step 40: local=0.0456 global=0.4863\n",
            "  step 50: local=0.0458 global=0.4785\n",
            "  step 60: local=0.0454 global=0.4971\n",
            "  step 70: local=0.0468 global=0.4907\n",
            "  step 80: local=0.0500 global=0.4770\n",
            "  step 90: local=0.0462 global=0.4692\n",
            "  Layer 23 not converged (global=0.4879 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0453 global=0.4906\n",
            "  step 10: local=0.0445 global=0.5045\n",
            "  step 20: local=0.0452 global=0.4779\n",
            "  step 30: local=0.0491 global=0.5414\n",
            "  step 40: local=0.0505 global=0.4911\n",
            "  step 50: local=0.0450 global=0.4761\n",
            "  step 60: local=0.0496 global=0.4987\n",
            "  step 70: local=0.0485 global=0.5097\n",
            "  step 80: local=0.0468 global=0.5305\n",
            "  step 90: local=0.0481 global=0.5022\n",
            "  Layer 23 not converged (global=0.4687 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0471 global=0.4797\n",
            "  step 10: local=0.0462 global=0.4953\n",
            "  step 20: local=0.0467 global=0.4983\n",
            "  step 30: local=0.0462 global=0.5264\n",
            "  step 40: local=0.0441 global=0.4788\n",
            "  step 50: local=0.0450 global=0.5097\n",
            "  step 60: local=0.0461 global=0.4578\n",
            "  step 70: local=0.0462 global=0.5271\n",
            "  step 80: local=0.0443 global=0.4831\n",
            "  step 90: local=0.0489 global=0.5302\n",
            "  Layer 23 not converged (global=0.5264 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0452 global=0.4891\n",
            "  step 10: local=0.0502 global=0.5124\n",
            "  step 20: local=0.0484 global=0.5621\n",
            "  step 30: local=0.0469 global=0.5202\n",
            "  step 40: local=0.0451 global=0.4789\n",
            "  step 50: local=0.0458 global=0.5046\n",
            "  step 60: local=0.0509 global=0.5371\n",
            "  step 70: local=0.0456 global=0.4955\n",
            "  step 80: local=0.0441 global=0.4954\n",
            "  step 90: local=0.0454 global=0.5207\n",
            "  Layer 23 not converged (global=0.4816 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0485 global=0.4736\n",
            "  step 10: local=0.0488 global=0.5266\n",
            "  step 20: local=0.0478 global=0.5319\n",
            "  step 30: local=0.0479 global=0.5361\n",
            "  step 40: local=0.0498 global=0.5153\n",
            "  step 50: local=0.0463 global=0.4809\n",
            "  step 60: local=0.0481 global=0.4843\n",
            "  step 70: local=0.0478 global=0.5032\n",
            "  step 80: local=0.0460 global=0.4769\n",
            "  step 90: local=0.0452 global=0.5122\n",
            "  Layer 23 not converged (global=0.4946 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0449 global=0.5085\n",
            "  step 10: local=0.0517 global=0.4980\n",
            "  step 20: local=0.0458 global=0.4857\n",
            "  step 30: local=0.0454 global=0.4591\n",
            "  step 40: local=0.0488 global=0.4715\n",
            "  step 50: local=0.0469 global=0.4892\n",
            "  step 60: local=0.0455 global=0.4961\n",
            "  step 70: local=0.0477 global=0.4996\n",
            "  step 80: local=0.0459 global=0.5105\n",
            "  step 90: local=0.0460 global=0.4940\n",
            "  Layer 23 not converged (global=0.4927 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0458 global=0.4806\n",
            "  step 10: local=0.0475 global=0.5052\n",
            "  step 20: local=0.0499 global=0.4611\n",
            "  step 30: local=0.0478 global=0.5204\n",
            "  step 40: local=0.0452 global=0.5005\n",
            "  step 50: local=0.0460 global=0.4604\n",
            "  step 60: local=0.0468 global=0.4972\n",
            "  step 70: local=0.0466 global=0.5084\n",
            "  step 80: local=0.0457 global=0.5144\n",
            "  step 90: local=0.0481 global=0.4730\n",
            "  Layer 23 not converged (global=0.4819 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0457 global=0.5011\n",
            "  step 10: local=0.0434 global=0.5106\n",
            "  step 20: local=0.0450 global=0.5135\n",
            "  step 30: local=0.0440 global=0.4686\n",
            "  step 40: local=0.0462 global=0.4689\n",
            "  step 50: local=0.0444 global=0.4888\n",
            "  step 60: local=0.0464 global=0.4733\n",
            "  step 70: local=0.0471 global=0.5133\n",
            "  step 80: local=0.0454 global=0.5162\n",
            "  step 90: local=0.0473 global=0.4840\n",
            "  Layer 23 not converged (global=0.4999 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0458 global=0.4690\n",
            "  step 10: local=0.0459 global=0.4856\n",
            "  step 20: local=0.0473 global=0.4837\n",
            "  step 30: local=0.0469 global=0.5092\n",
            "  step 40: local=0.0464 global=0.4935\n",
            "  step 50: local=0.0454 global=0.5134\n",
            "  step 60: local=0.0510 global=0.4800\n",
            "  step 70: local=0.0459 global=0.4881\n",
            "  step 80: local=0.0448 global=0.4937\n",
            "  step 90: local=0.0462 global=0.4843\n",
            "  Layer 23 not converged (global=0.5155 > 0.4), repeating...\n",
            "\n",
            "--- Layer 23/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0539 global=0.4823\n",
            "  step 10: local=0.0479 global=0.4780\n",
            "  step 20: local=0.0448 global=0.4999\n",
            "  step 30: local=0.0469 global=0.5178\n",
            "  step 40: local=0.0458 global=0.5180\n",
            "  step 50: local=0.0469 global=0.4833\n",
            "  step 60: local=0.0525 global=0.5144\n",
            "  step 70: local=0.0452 global=0.5107\n",
            "  step 80: local=0.0446 global=0.4809\n",
            "  step 90: local=0.0473 global=0.4927\n",
            "  [WARN] Layer 23 did not converge after 20 repeats (global=0.4418)\n",
            "\n",
            "--- Layer 24/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0687 global=0.4973\n",
            "  step 10: local=0.0668 global=0.4842\n",
            "  step 20: local=0.0663 global=0.5134\n",
            "  step 30: local=0.0647 global=0.5123\n",
            "  step 40: local=0.0642 global=0.5003\n",
            "  step 50: local=0.0646 global=0.4673\n",
            "  step 60: local=0.0621 global=0.4786\n",
            "  step 70: local=0.0644 global=0.5392\n",
            "  step 80: local=0.0606 global=0.4915\n",
            "  step 90: local=0.0609 global=0.4824\n",
            "  Layer 24 not converged (global=0.5680 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0616 global=0.5022\n",
            "  step 10: local=0.0591 global=0.4933\n",
            "  step 20: local=0.0616 global=0.4806\n",
            "  step 30: local=0.0635 global=0.4722\n",
            "  step 40: local=0.0603 global=0.4934\n",
            "  step 50: local=0.0588 global=0.5070\n",
            "  step 60: local=0.0545 global=0.4812\n",
            "  step 70: local=0.0572 global=0.5427\n",
            "  step 80: local=0.0543 global=0.4946\n",
            "  step 90: local=0.0552 global=0.4789\n",
            "  Layer 24 not converged (global=0.4998 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0592 global=0.5007\n",
            "  step 10: local=0.0553 global=0.5097\n",
            "  step 20: local=0.0531 global=0.5323\n",
            "  step 30: local=0.0545 global=0.5037\n",
            "  step 40: local=0.0524 global=0.4816\n",
            "  step 50: local=0.0529 global=0.4966\n",
            "  step 60: local=0.0543 global=0.5006\n",
            "  step 70: local=0.0528 global=0.5271\n",
            "  step 80: local=0.0519 global=0.4787\n",
            "  step 90: local=0.0484 global=0.5124\n",
            "  Layer 24 not converged (global=0.4778 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0508 global=0.4587\n",
            "  step 10: local=0.0516 global=0.5265\n",
            "  step 20: local=0.0508 global=0.4850\n",
            "  step 30: local=0.0503 global=0.5297\n",
            "  step 40: local=0.0466 global=0.4899\n",
            "  step 50: local=0.0484 global=0.5138\n",
            "  step 60: local=0.0511 global=0.5620\n",
            "  step 70: local=0.0497 global=0.5201\n",
            "  step 80: local=0.0486 global=0.4785\n",
            "  step 90: local=0.0481 global=0.5058\n",
            "  Layer 24 not converged (global=0.4948 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0499 global=0.5350\n",
            "  step 10: local=0.0465 global=0.4952\n",
            "  step 20: local=0.0484 global=0.4960\n",
            "  step 30: local=0.0463 global=0.5204\n",
            "  step 40: local=0.0473 global=0.4743\n",
            "  step 50: local=0.0476 global=0.5268\n",
            "  step 60: local=0.0476 global=0.5307\n",
            "  step 70: local=0.0479 global=0.5361\n",
            "  step 80: local=0.0454 global=0.5152\n",
            "  step 90: local=0.0459 global=0.4791\n",
            "  Layer 24 not converged (global=0.4812 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0468 global=0.4828\n",
            "  step 10: local=0.0447 global=0.5030\n",
            "  step 20: local=0.0468 global=0.4758\n",
            "  step 30: local=0.0461 global=0.5100\n",
            "  step 40: local=0.0453 global=0.5055\n",
            "  step 50: local=0.0447 global=0.4951\n",
            "  step 60: local=0.0463 global=0.4837\n",
            "  step 70: local=0.0462 global=0.4576\n",
            "  step 80: local=0.0437 global=0.4714\n",
            "  step 90: local=0.0459 global=0.4880\n",
            "  Layer 24 not converged (global=0.4914 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0468 global=0.4947\n",
            "  step 10: local=0.0457 global=0.4964\n",
            "  step 20: local=0.0459 global=0.5092\n",
            "  step 30: local=0.0447 global=0.4930\n",
            "  step 40: local=0.0467 global=0.4788\n",
            "  step 50: local=0.0454 global=0.5030\n",
            "  step 60: local=0.0457 global=0.4598\n",
            "  step 70: local=0.0438 global=0.5185\n",
            "  step 80: local=0.0468 global=0.4995\n",
            "  step 90: local=0.0441 global=0.4589\n",
            "  Layer 24 not converged (global=0.4972 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0456 global=0.4951\n",
            "  step 10: local=0.0452 global=0.5060\n",
            "  step 20: local=0.0458 global=0.5113\n",
            "  step 30: local=0.0450 global=0.4704\n",
            "  step 40: local=0.0454 global=0.4984\n",
            "  step 50: local=0.0462 global=0.5080\n",
            "  step 60: local=0.0446 global=0.5112\n",
            "  step 70: local=0.0453 global=0.4652\n",
            "  step 80: local=0.0475 global=0.4686\n",
            "  step 90: local=0.0443 global=0.4876\n",
            "  Layer 24 not converged (global=0.4825 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0450 global=0.4724\n",
            "  step 10: local=0.0463 global=0.5106\n",
            "  step 20: local=0.0457 global=0.5158\n",
            "  step 30: local=0.0455 global=0.4821\n",
            "  step 40: local=0.0470 global=0.4666\n",
            "  step 50: local=0.0454 global=0.4844\n",
            "  step 60: local=0.0448 global=0.4807\n",
            "  step 70: local=0.0465 global=0.5080\n",
            "  step 80: local=0.0429 global=0.4905\n",
            "  step 90: local=0.0446 global=0.5104\n",
            "  Layer 24 not converged (global=0.4989 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0449 global=0.4782\n",
            "  step 10: local=0.0449 global=0.4855\n",
            "  step 20: local=0.0437 global=0.4911\n",
            "  step 30: local=0.0475 global=0.4833\n",
            "  step 40: local=0.0441 global=0.4800\n",
            "  step 50: local=0.0460 global=0.4756\n",
            "  step 60: local=0.0473 global=0.4969\n",
            "  step 70: local=0.0451 global=0.5155\n",
            "  step 80: local=0.0454 global=0.5148\n",
            "  step 90: local=0.0447 global=0.4809\n",
            "  Layer 24 not converged (global=0.4856 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0432 global=0.5100\n",
            "  step 10: local=0.0445 global=0.5066\n",
            "  step 20: local=0.0459 global=0.4768\n",
            "  step 30: local=0.0439 global=0.4885\n",
            "  step 40: local=0.0464 global=0.4863\n",
            "  step 50: local=0.0453 global=0.4741\n",
            "  step 60: local=0.0472 global=0.5027\n",
            "  step 70: local=0.0475 global=0.5019\n",
            "  step 80: local=0.0453 global=0.4890\n",
            "  step 90: local=0.0435 global=0.4575\n",
            "  Layer 24 not converged (global=0.4874 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0444 global=0.4694\n",
            "  step 10: local=0.0457 global=0.5275\n",
            "  step 20: local=0.0452 global=0.4826\n",
            "  step 30: local=0.0442 global=0.4732\n",
            "  step 40: local=0.0458 global=0.4925\n",
            "  step 50: local=0.0458 global=0.4843\n",
            "  step 60: local=0.0437 global=0.4724\n",
            "  step 70: local=0.0451 global=0.4652\n",
            "  step 80: local=0.0463 global=0.4848\n",
            "  step 90: local=0.0439 global=0.4982\n",
            "  Layer 24 not converged (global=0.4736 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0446 global=0.4736\n",
            "  step 10: local=0.0461 global=0.5339\n",
            "  step 20: local=0.0446 global=0.4871\n",
            "  step 30: local=0.0433 global=0.4716\n",
            "  step 40: local=0.0474 global=0.4929\n",
            "  step 50: local=0.0458 global=0.5013\n",
            "  step 60: local=0.0431 global=0.5242\n",
            "  step 70: local=0.0447 global=0.4966\n",
            "  step 80: local=0.0429 global=0.4752\n",
            "  step 90: local=0.0424 global=0.4900\n",
            "  Layer 24 not converged (global=0.4795 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0476 global=0.4946\n",
            "  step 10: local=0.0463 global=0.5200\n",
            "  step 20: local=0.0452 global=0.4725\n",
            "  step 30: local=0.0430 global=0.5058\n",
            "  step 40: local=0.0438 global=0.4535\n",
            "  step 50: local=0.0449 global=0.5202\n",
            "  step 60: local=0.0430 global=0.4795\n",
            "  step 70: local=0.0461 global=0.5241\n",
            "  step 80: local=0.0439 global=0.4846\n",
            "  step 90: local=0.0445 global=0.5077\n",
            "  Layer 24 not converged (global=0.4842 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0458 global=0.5565\n",
            "  step 10: local=0.0471 global=0.5142\n",
            "  step 20: local=0.0469 global=0.4742\n",
            "  step 30: local=0.0426 global=0.5011\n",
            "  step 40: local=0.0467 global=0.5284\n",
            "  step 50: local=0.0441 global=0.4904\n",
            "  step 60: local=0.0452 global=0.4918\n",
            "  step 70: local=0.0436 global=0.5157\n",
            "  step 80: local=0.0450 global=0.4705\n",
            "  step 90: local=0.0454 global=0.5219\n",
            "  Layer 24 not converged (global=0.4811 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0448 global=0.5259\n",
            "  step 10: local=0.0458 global=0.5316\n",
            "  step 20: local=0.0434 global=0.5107\n",
            "  step 30: local=0.0435 global=0.4760\n",
            "  step 40: local=0.0467 global=0.4785\n",
            "  step 50: local=0.0441 global=0.4992\n",
            "  step 60: local=0.0443 global=0.4722\n",
            "  step 70: local=0.0460 global=0.5060\n",
            "  step 80: local=0.0435 global=0.5022\n",
            "  step 90: local=0.0440 global=0.4917\n",
            "  Layer 24 not converged (global=0.4957 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0450 global=0.4808\n",
            "  step 10: local=0.0448 global=0.4545\n",
            "  step 20: local=0.0429 global=0.4681\n",
            "  step 30: local=0.0461 global=0.4848\n",
            "  step 40: local=0.0444 global=0.4910\n",
            "  step 50: local=0.0424 global=0.4932\n",
            "  step 60: local=0.0416 global=0.5057\n",
            "  step 70: local=0.0441 global=0.4896\n",
            "  step 80: local=0.0454 global=0.4760\n",
            "  step 90: local=0.0442 global=0.4994\n",
            "  Layer 24 not converged (global=0.4620 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0421 global=0.4573\n",
            "  step 10: local=0.0446 global=0.5150\n",
            "  step 20: local=0.0431 global=0.4963\n",
            "  step 30: local=0.0433 global=0.4567\n",
            "  step 40: local=0.0451 global=0.4918\n",
            "  step 50: local=0.0443 global=0.5027\n",
            "  step 60: local=0.0443 global=0.5079\n",
            "  step 70: local=0.0438 global=0.4679\n",
            "  step 80: local=0.0464 global=0.4960\n",
            "  step 90: local=0.0459 global=0.5058\n",
            "  Layer 24 not converged (global=0.4840 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0465 global=0.5087\n",
            "  step 10: local=0.0443 global=0.4629\n",
            "  step 20: local=0.0459 global=0.4663\n",
            "  step 30: local=0.0442 global=0.4847\n",
            "  step 40: local=0.0436 global=0.4696\n",
            "  step 50: local=0.0443 global=0.5082\n",
            "  step 60: local=0.0446 global=0.5134\n",
            "  step 70: local=0.0428 global=0.4796\n",
            "  step 80: local=0.0423 global=0.4644\n",
            "  step 90: local=0.0446 global=0.4817\n",
            "  Layer 24 not converged (global=0.5083 > 0.4), repeating...\n",
            "\n",
            "--- Layer 24/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0426 global=0.4787\n",
            "  step 10: local=0.0439 global=0.5057\n",
            "  step 20: local=0.0447 global=0.4883\n",
            "  step 30: local=0.0456 global=0.5081\n",
            "  step 40: local=0.0450 global=0.4757\n",
            "  step 50: local=0.0450 global=0.4827\n",
            "  step 60: local=0.0446 global=0.4893\n",
            "  step 70: local=0.0449 global=0.4814\n",
            "  step 80: local=0.0438 global=0.4780\n",
            "  step 90: local=0.0434 global=0.4739\n",
            "  [WARN] Layer 24 did not converge after 20 repeats (global=0.5267)\n",
            "\n",
            "--- Layer 25/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0902 global=0.5059\n",
            "  step 10: local=0.0902 global=0.5217\n",
            "  step 20: local=0.0851 global=0.5221\n",
            "  step 30: local=0.0842 global=0.4874\n",
            "  step 40: local=0.0950 global=0.5156\n",
            "  step 50: local=0.0972 global=0.5133\n",
            "  step 60: local=0.0828 global=0.4819\n",
            "  step 70: local=0.0853 global=0.4954\n",
            "  step 80: local=0.0898 global=0.4910\n",
            "  step 90: local=0.0871 global=0.4811\n",
            "  Layer 25 not converged (global=0.4733 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0923 global=0.5101\n",
            "  step 10: local=0.0928 global=0.5069\n",
            "  step 20: local=0.0851 global=0.4935\n",
            "  step 30: local=0.0876 global=0.4618\n",
            "  step 40: local=0.0832 global=0.4733\n",
            "  step 50: local=0.0863 global=0.5336\n",
            "  step 60: local=0.0903 global=0.4881\n",
            "  step 70: local=0.0890 global=0.4775\n",
            "  step 80: local=0.0920 global=0.4956\n",
            "  step 90: local=0.0854 global=0.4880\n",
            "  Layer 25 not converged (global=0.4883 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0879 global=0.4758\n",
            "  step 10: local=0.0882 global=0.4701\n",
            "  step 20: local=0.0794 global=0.4871\n",
            "  step 30: local=0.0873 global=0.5021\n",
            "  step 40: local=0.0832 global=0.4771\n",
            "  step 50: local=0.0811 global=0.5384\n",
            "  step 60: local=0.0768 global=0.4915\n",
            "  step 70: local=0.0823 global=0.4751\n",
            "  step 80: local=0.0756 global=0.4957\n",
            "  step 90: local=0.0823 global=0.5027\n",
            "  Layer 25 not converged (global=0.4915 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0749 global=0.5270\n",
            "  step 10: local=0.0726 global=0.4988\n",
            "  step 20: local=0.0826 global=0.4751\n",
            "  step 30: local=0.0784 global=0.4917\n",
            "  step 40: local=0.0742 global=0.4976\n",
            "  step 50: local=0.0805 global=0.5224\n",
            "  step 60: local=0.0753 global=0.4759\n",
            "  step 70: local=0.0809 global=0.5076\n",
            "  step 80: local=0.0768 global=0.4545\n",
            "  step 90: local=0.0816 global=0.5210\n",
            "  Layer 25 not converged (global=0.5148 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0796 global=0.4796\n",
            "  step 10: local=0.0732 global=0.5245\n",
            "  step 20: local=0.0864 global=0.4864\n",
            "  step 30: local=0.0795 global=0.5094\n",
            "  step 40: local=0.0776 global=0.5566\n",
            "  step 50: local=0.0844 global=0.5139\n",
            "  step 60: local=0.0777 global=0.4755\n",
            "  step 70: local=0.0749 global=0.5016\n",
            "  step 80: local=0.0703 global=0.5287\n",
            "  step 90: local=0.0769 global=0.4918\n",
            "  Layer 25 not converged (global=0.4968 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0775 global=0.4930\n",
            "  step 10: local=0.0848 global=0.5156\n",
            "  step 20: local=0.0725 global=0.4720\n",
            "  step 30: local=0.0849 global=0.5222\n",
            "  step 40: local=0.0751 global=0.5263\n",
            "  step 50: local=0.0811 global=0.5327\n",
            "  step 60: local=0.0778 global=0.5101\n",
            "  step 70: local=0.0711 global=0.4757\n",
            "  step 80: local=0.0842 global=0.4785\n",
            "  step 90: local=0.0786 global=0.4991\n",
            "  Layer 25 not converged (global=0.5056 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0844 global=0.4725\n",
            "  step 10: local=0.0783 global=0.5063\n",
            "  step 20: local=0.0731 global=0.5020\n",
            "  step 30: local=0.0743 global=0.4915\n",
            "  step 40: local=0.0787 global=0.4811\n",
            "  step 50: local=0.0699 global=0.4542\n",
            "  step 60: local=0.0814 global=0.4681\n",
            "  step 70: local=0.0831 global=0.4856\n",
            "  step 80: local=0.0702 global=0.4906\n",
            "  step 90: local=0.0637 global=0.4933\n",
            "  Layer 25 not converged (global=0.4833 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0699 global=0.5050\n",
            "  step 10: local=0.0760 global=0.4882\n",
            "  step 20: local=0.0717 global=0.4769\n",
            "  step 30: local=0.0751 global=0.4987\n",
            "  step 40: local=0.0746 global=0.4577\n",
            "  step 50: local=0.0782 global=0.5144\n",
            "  step 60: local=0.0786 global=0.4963\n",
            "  step 70: local=0.0807 global=0.4560\n",
            "  step 80: local=0.0763 global=0.4911\n",
            "  step 90: local=0.0760 global=0.5023\n",
            "  Layer 25 not converged (global=0.5122 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0715 global=0.5083\n",
            "  step 10: local=0.0791 global=0.4682\n",
            "  step 20: local=0.0798 global=0.4975\n",
            "  step 30: local=0.0828 global=0.5045\n",
            "  step 40: local=0.0770 global=0.5074\n",
            "  step 50: local=0.0740 global=0.4626\n",
            "  step 60: local=0.0779 global=0.4656\n",
            "  step 70: local=0.0702 global=0.4842\n",
            "  step 80: local=0.0735 global=0.4696\n",
            "  step 90: local=0.0736 global=0.5067\n",
            "  Layer 25 not converged (global=0.5464 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0788 global=0.5121\n",
            "  step 10: local=0.0705 global=0.4784\n",
            "  step 20: local=0.0732 global=0.4640\n",
            "  step 30: local=0.0775 global=0.4800\n",
            "  step 40: local=0.0762 global=0.4782\n",
            "  step 50: local=0.0736 global=0.5044\n",
            "  step 60: local=0.0732 global=0.4872\n",
            "  step 70: local=0.0777 global=0.5070\n",
            "  step 80: local=0.0685 global=0.4759\n",
            "  step 90: local=0.0752 global=0.4823\n",
            "  Layer 25 not converged (global=0.4796 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0703 global=0.4880\n",
            "  step 10: local=0.0723 global=0.4812\n",
            "  step 20: local=0.0762 global=0.4782\n",
            "  step 30: local=0.0725 global=0.4733\n",
            "  step 40: local=0.0691 global=0.4944\n",
            "  step 50: local=0.0732 global=0.5112\n",
            "  step 60: local=0.0712 global=0.5117\n",
            "  step 70: local=0.0755 global=0.4788\n",
            "  step 80: local=0.0665 global=0.5049\n",
            "  step 90: local=0.0675 global=0.5030\n",
            "  Layer 25 not converged (global=0.5150 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0717 global=0.4739\n",
            "  step 10: local=0.0696 global=0.4866\n",
            "  step 20: local=0.0722 global=0.4839\n",
            "  step 30: local=0.0716 global=0.4734\n",
            "  step 40: local=0.0792 global=0.5016\n",
            "  step 50: local=0.0789 global=0.4993\n",
            "  step 60: local=0.0757 global=0.4854\n",
            "  step 70: local=0.0821 global=0.4549\n",
            "  step 80: local=0.0783 global=0.4657\n",
            "  step 90: local=0.0796 global=0.5249\n",
            "  Layer 25 not converged (global=0.4958 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0756 global=0.4813\n",
            "  step 10: local=0.0794 global=0.4709\n",
            "  step 20: local=0.0702 global=0.4893\n",
            "  step 30: local=0.0640 global=0.4812\n",
            "  step 40: local=0.0747 global=0.4696\n",
            "  step 50: local=0.0815 global=0.4645\n",
            "  step 60: local=0.0724 global=0.4809\n",
            "  step 70: local=0.0801 global=0.4948\n",
            "  step 80: local=0.0777 global=0.4716\n",
            "  step 90: local=0.0752 global=0.5312\n",
            "  Layer 25 not converged (global=0.4752 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0742 global=0.4861\n",
            "  step 10: local=0.0719 global=0.4698\n",
            "  step 20: local=0.0766 global=0.4897\n",
            "  step 30: local=0.0761 global=0.4968\n",
            "  step 40: local=0.0717 global=0.5205\n",
            "  step 50: local=0.0804 global=0.4937\n",
            "  step 60: local=0.0698 global=0.4715\n",
            "  step 70: local=0.0662 global=0.4875\n",
            "  step 80: local=0.0769 global=0.4931\n",
            "  step 90: local=0.0813 global=0.5180\n",
            "  Layer 25 not converged (global=0.5108 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0745 global=0.4717\n",
            "  step 10: local=0.0761 global=0.5029\n",
            "  step 20: local=0.0757 global=0.4513\n",
            "  step 30: local=0.0738 global=0.5168\n",
            "  step 40: local=0.0753 global=0.4759\n",
            "  step 50: local=0.0756 global=0.5203\n",
            "  step 60: local=0.0720 global=0.4824\n",
            "  step 70: local=0.0751 global=0.5056\n",
            "  step 80: local=0.0731 global=0.5526\n",
            "  step 90: local=0.0806 global=0.5099\n",
            "  Layer 25 not converged (global=0.4817 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0721 global=0.4726\n",
            "  step 10: local=0.0742 global=0.4985\n",
            "  step 20: local=0.0717 global=0.5249\n",
            "  step 30: local=0.0763 global=0.4888\n",
            "  step 40: local=0.0699 global=0.4898\n",
            "  step 50: local=0.0740 global=0.5116\n",
            "  step 60: local=0.0761 global=0.4696\n",
            "  step 70: local=0.0742 global=0.5182\n",
            "  step 80: local=0.0737 global=0.5230\n",
            "  step 90: local=0.0810 global=0.5293\n",
            "  Layer 25 not converged (global=0.4715 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0715 global=0.5072\n",
            "  step 10: local=0.0749 global=0.4734\n",
            "  step 20: local=0.0789 global=0.4764\n",
            "  step 30: local=0.0711 global=0.4965\n",
            "  step 40: local=0.0735 global=0.4696\n",
            "  step 50: local=0.0762 global=0.5025\n",
            "  step 60: local=0.0722 global=0.4995\n",
            "  step 70: local=0.0778 global=0.4885\n",
            "  step 80: local=0.0720 global=0.4785\n",
            "  step 90: local=0.0725 global=0.4520\n",
            "  Layer 25 not converged (global=0.5309 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0715 global=0.4660\n",
            "  step 10: local=0.0807 global=0.4833\n",
            "  step 20: local=0.0689 global=0.4883\n",
            "  step 30: local=0.0743 global=0.4907\n",
            "  step 40: local=0.0753 global=0.5017\n",
            "  step 50: local=0.0763 global=0.4857\n",
            "  step 60: local=0.0721 global=0.4752\n",
            "  step 70: local=0.0786 global=0.4963\n",
            "  step 80: local=0.0781 global=0.4559\n",
            "  step 90: local=0.0771 global=0.5123\n",
            "  Layer 25 not converged (global=0.5143 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0719 global=0.4944\n",
            "  step 10: local=0.0722 global=0.4543\n",
            "  step 20: local=0.0734 global=0.4888\n",
            "  step 30: local=0.0703 global=0.5003\n",
            "  step 40: local=0.0738 global=0.5055\n",
            "  step 50: local=0.0701 global=0.4662\n",
            "  step 60: local=0.0727 global=0.4953\n",
            "  step 70: local=0.0820 global=0.5022\n",
            "  step 80: local=0.0698 global=0.5059\n",
            "  step 90: local=0.0755 global=0.4612\n",
            "  Layer 25 not converged (global=0.4918 > 0.4), repeating...\n",
            "\n",
            "--- Layer 25/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0739 global=0.4639\n",
            "  step 10: local=0.0780 global=0.4822\n",
            "  step 20: local=0.0775 global=0.4679\n",
            "  step 30: local=0.0703 global=0.5051\n",
            "  step 40: local=0.0697 global=0.5098\n",
            "  step 50: local=0.0713 global=0.4767\n",
            "  step 60: local=0.0756 global=0.4625\n",
            "  step 70: local=0.0696 global=0.4777\n",
            "  step 80: local=0.0681 global=0.4765\n",
            "  step 90: local=0.0761 global=0.5027\n",
            "  [WARN] Layer 25 did not converge after 20 repeats (global=0.5071)\n",
            "\n",
            "--- Layer 26/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0937 global=0.5101\n",
            "  step 10: local=0.0962 global=0.5319\n",
            "  step 20: local=0.0969 global=0.4963\n",
            "  step 30: local=0.1019 global=0.5004\n",
            "  step 40: local=0.0905 global=0.5066\n",
            "  step 50: local=0.0960 global=0.4998\n",
            "  step 60: local=0.0918 global=0.4951\n",
            "  step 70: local=0.0848 global=0.4881\n",
            "  step 80: local=0.0854 global=0.5106\n",
            "  step 90: local=0.0951 global=0.5272\n",
            "  Layer 26 not converged (global=0.4870 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0830 global=0.5277\n",
            "  step 10: local=0.0887 global=0.4931\n",
            "  step 20: local=0.0858 global=0.5198\n",
            "  step 30: local=0.0963 global=0.5179\n",
            "  step 40: local=0.0821 global=0.4883\n",
            "  step 50: local=0.0839 global=0.4982\n",
            "  step 60: local=0.0870 global=0.4983\n",
            "  step 70: local=0.0777 global=0.4819\n",
            "  step 80: local=0.0802 global=0.5131\n",
            "  step 90: local=0.1052 global=0.5073\n",
            "  Layer 26 not converged (global=0.5189 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0902 global=0.4973\n",
            "  step 10: local=0.0804 global=0.4670\n",
            "  step 20: local=0.0778 global=0.4764\n",
            "  step 30: local=0.0813 global=0.5329\n",
            "  step 40: local=0.0802 global=0.4914\n",
            "  step 50: local=0.0825 global=0.4789\n",
            "  step 60: local=0.0707 global=0.4966\n",
            "  step 70: local=0.0713 global=0.4911\n",
            "  step 80: local=0.0718 global=0.4774\n",
            "  step 90: local=0.0706 global=0.4713\n",
            "  Layer 26 not converged (global=0.4902 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0732 global=0.4909\n",
            "  step 10: local=0.0741 global=0.5014\n",
            "  step 20: local=0.0678 global=0.4789\n",
            "  step 30: local=0.0731 global=0.5424\n",
            "  step 40: local=0.0673 global=0.4911\n",
            "  step 50: local=0.0736 global=0.4758\n",
            "  step 60: local=0.0710 global=0.4947\n",
            "  step 70: local=0.0671 global=0.5057\n",
            "  step 80: local=0.0789 global=0.5269\n",
            "  step 90: local=0.0752 global=0.4994\n",
            "  Layer 26 not converged (global=0.4726 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0691 global=0.4772\n",
            "  step 10: local=0.0642 global=0.4928\n",
            "  step 20: local=0.0650 global=0.4995\n",
            "  step 30: local=0.0645 global=0.5230\n",
            "  step 40: local=0.0652 global=0.4781\n",
            "  step 50: local=0.0748 global=0.5078\n",
            "  step 60: local=0.0622 global=0.4549\n",
            "  step 70: local=0.0649 global=0.5215\n",
            "  step 80: local=0.0700 global=0.4808\n",
            "  step 90: local=0.0641 global=0.5241\n",
            "  Layer 26 not converged (global=0.5253 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0695 global=0.4874\n",
            "  step 10: local=0.0775 global=0.5123\n",
            "  step 20: local=0.0672 global=0.5569\n",
            "  step 30: local=0.0627 global=0.5161\n",
            "  step 40: local=0.0692 global=0.4773\n",
            "  step 50: local=0.0630 global=0.5006\n",
            "  step 60: local=0.0675 global=0.5288\n",
            "  step 70: local=0.0650 global=0.4922\n",
            "  step 80: local=0.0693 global=0.4926\n",
            "  step 90: local=0.0628 global=0.5155\n",
            "  Layer 26 not converged (global=0.4780 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0603 global=0.4742\n",
            "  step 10: local=0.0634 global=0.5205\n",
            "  step 20: local=0.0637 global=0.5283\n",
            "  step 30: local=0.0669 global=0.5350\n",
            "  step 40: local=0.0629 global=0.5100\n",
            "  step 50: local=0.0692 global=0.4765\n",
            "  step 60: local=0.0664 global=0.4803\n",
            "  step 70: local=0.0692 global=0.4984\n",
            "  step 80: local=0.0633 global=0.4730\n",
            "  step 90: local=0.0639 global=0.5069\n",
            "  Layer 26 not converged (global=0.4933 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0706 global=0.5022\n",
            "  step 10: local=0.0599 global=0.4910\n",
            "  step 20: local=0.0615 global=0.4840\n",
            "  step 30: local=0.0643 global=0.4573\n",
            "  step 40: local=0.0637 global=0.4684\n",
            "  step 50: local=0.0655 global=0.4859\n",
            "  step 60: local=0.0646 global=0.4912\n",
            "  step 70: local=0.0607 global=0.4942\n",
            "  step 80: local=0.0681 global=0.5055\n",
            "  step 90: local=0.0648 global=0.4888\n",
            "  Layer 26 not converged (global=0.4912 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0690 global=0.4769\n",
            "  step 10: local=0.0594 global=0.4983\n",
            "  step 20: local=0.0632 global=0.4570\n",
            "  step 30: local=0.0614 global=0.5152\n",
            "  step 40: local=0.0737 global=0.4971\n",
            "  step 50: local=0.0754 global=0.4578\n",
            "  step 60: local=0.0665 global=0.4903\n",
            "  step 70: local=0.0586 global=0.5015\n",
            "  step 80: local=0.0628 global=0.5090\n",
            "  step 90: local=0.0703 global=0.4682\n",
            "  Layer 26 not converged (global=0.4788 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0639 global=0.4978\n",
            "  step 10: local=0.0635 global=0.5040\n",
            "  step 20: local=0.0644 global=0.5086\n",
            "  step 30: local=0.0656 global=0.4642\n",
            "  step 40: local=0.0644 global=0.4642\n",
            "  step 50: local=0.0649 global=0.4817\n",
            "  step 60: local=0.0603 global=0.4701\n",
            "  step 70: local=0.0644 global=0.5062\n",
            "  step 80: local=0.0751 global=0.5121\n",
            "  step 90: local=0.0581 global=0.4791\n",
            "  Layer 26 not converged (global=0.4973 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0730 global=0.4633\n",
            "  step 10: local=0.0664 global=0.4774\n",
            "  step 20: local=0.0635 global=0.4784\n",
            "  step 30: local=0.0644 global=0.5050\n",
            "  step 40: local=0.0585 global=0.4862\n",
            "  step 50: local=0.0723 global=0.5087\n",
            "  step 60: local=0.0627 global=0.4766\n",
            "  step 70: local=0.0723 global=0.4827\n",
            "  step 80: local=0.0644 global=0.4878\n",
            "  step 90: local=0.0598 global=0.4821\n",
            "  Layer 26 not converged (global=0.5129 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0615 global=0.4786\n",
            "  step 10: local=0.0707 global=0.4719\n",
            "  step 20: local=0.0667 global=0.4948\n",
            "  step 30: local=0.0748 global=0.5114\n",
            "  step 40: local=0.0609 global=0.5118\n",
            "  step 50: local=0.0625 global=0.4802\n",
            "  step 60: local=0.0591 global=0.5048\n",
            "  step 70: local=0.0605 global=0.5046\n",
            "  step 80: local=0.0625 global=0.4748\n",
            "  step 90: local=0.0654 global=0.4868\n",
            "  Layer 26 not converged (global=0.4400 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0854 global=0.4865\n",
            "  step 10: local=0.0659 global=0.4711\n",
            "  step 20: local=0.0647 global=0.5018\n",
            "  step 30: local=0.0643 global=0.4979\n",
            "  step 40: local=0.0613 global=0.4854\n",
            "  step 50: local=0.0749 global=0.4575\n",
            "  step 60: local=0.0733 global=0.4668\n",
            "  step 70: local=0.0622 global=0.5224\n",
            "  step 80: local=0.0623 global=0.4826\n",
            "  step 90: local=0.0661 global=0.4701\n",
            "  Layer 26 not converged (global=0.5560 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0626 global=0.4889\n",
            "  step 10: local=0.0627 global=0.4823\n",
            "  step 20: local=0.0618 global=0.4698\n",
            "  step 30: local=0.0724 global=0.4648\n",
            "  step 40: local=0.0603 global=0.4827\n",
            "  step 50: local=0.0712 global=0.4937\n",
            "  step 60: local=0.0661 global=0.4726\n",
            "  step 70: local=0.0583 global=0.5334\n",
            "  step 80: local=0.0589 global=0.4849\n",
            "  step 90: local=0.0662 global=0.4697\n",
            "  Layer 26 not converged (global=0.4902 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0634 global=0.4887\n",
            "  step 10: local=0.0601 global=0.4981\n",
            "  step 20: local=0.0598 global=0.5201\n",
            "  step 30: local=0.0611 global=0.4929\n",
            "  step 40: local=0.0720 global=0.4713\n",
            "  step 50: local=0.0765 global=0.4871\n",
            "  step 60: local=0.0639 global=0.4944\n",
            "  step 70: local=0.0631 global=0.5172\n",
            "  step 80: local=0.0621 global=0.4722\n",
            "  step 90: local=0.0645 global=0.5028\n",
            "  Layer 26 not converged (global=0.4710 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0718 global=0.4511\n",
            "  step 10: local=0.0680 global=0.5169\n",
            "  step 20: local=0.0605 global=0.4763\n",
            "  step 30: local=0.0675 global=0.5191\n",
            "  step 40: local=0.0597 global=0.4820\n",
            "  step 50: local=0.0636 global=0.5082\n",
            "  step 60: local=0.0680 global=0.5517\n",
            "  step 70: local=0.0643 global=0.5117\n",
            "  step 80: local=0.0651 global=0.4735\n",
            "  step 90: local=0.0601 global=0.4969\n",
            "  Layer 26 not converged (global=0.4858 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0626 global=0.5250\n",
            "  step 10: local=0.0604 global=0.4886\n",
            "  step 20: local=0.0595 global=0.4893\n",
            "  step 30: local=0.0629 global=0.5110\n",
            "  step 40: local=0.0652 global=0.4702\n",
            "  step 50: local=0.0698 global=0.5165\n",
            "  step 60: local=0.0663 global=0.5236\n",
            "  step 70: local=0.0660 global=0.5302\n",
            "  step 80: local=0.0640 global=0.5065\n",
            "  step 90: local=0.0642 global=0.4739\n",
            "  Layer 26 not converged (global=0.4744 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0655 global=0.4774\n",
            "  step 10: local=0.0638 global=0.4953\n",
            "  step 20: local=0.0683 global=0.4697\n",
            "  step 30: local=0.0618 global=0.5033\n",
            "  step 40: local=0.0746 global=0.4989\n",
            "  step 50: local=0.0629 global=0.4875\n",
            "  step 60: local=0.0638 global=0.4811\n",
            "  step 70: local=0.0663 global=0.4542\n",
            "  step 80: local=0.0591 global=0.4659\n",
            "  step 90: local=0.0731 global=0.4832\n",
            "  Layer 26 not converged (global=0.4868 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0588 global=0.4892\n",
            "  step 10: local=0.0614 global=0.4916\n",
            "  step 20: local=0.0626 global=0.5020\n",
            "  step 30: local=0.0679 global=0.4859\n",
            "  step 40: local=0.0659 global=0.4746\n",
            "  step 50: local=0.0608 global=0.4954\n",
            "  step 60: local=0.0675 global=0.4547\n",
            "  step 70: local=0.0740 global=0.5124\n",
            "  step 80: local=0.0757 global=0.4949\n",
            "  step 90: local=0.0608 global=0.4561\n",
            "  Layer 26 not converged (global=0.4932 > 0.4), repeating...\n",
            "\n",
            "--- Layer 26/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0592 global=0.4880\n",
            "  step 10: local=0.0586 global=0.4993\n",
            "  step 20: local=0.0588 global=0.5070\n",
            "  step 30: local=0.0606 global=0.4659\n",
            "  step 40: local=0.0627 global=0.4950\n",
            "  step 50: local=0.0617 global=0.5023\n",
            "  step 60: local=0.0643 global=0.5062\n",
            "  step 70: local=0.0669 global=0.4619\n",
            "  step 80: local=0.0601 global=0.4624\n",
            "  step 90: local=0.0592 global=0.4802\n",
            "  [WARN] Layer 26 did not converge after 20 repeats (global=0.4805)\n",
            "\n",
            "--- Layer 27/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.2721 global=0.6127\n",
            "  step 10: local=0.2655 global=0.6345\n",
            "  step 20: local=0.1447 global=0.6202\n",
            "  step 30: local=0.1567 global=0.5815\n",
            "  step 40: local=0.2696 global=0.5529\n",
            "  step 50: local=0.2671 global=0.5603\n",
            "  step 60: local=0.2450 global=0.5497\n",
            "  step 70: local=0.1769 global=0.5739\n",
            "  step 80: local=0.1391 global=0.5480\n",
            "  step 90: local=0.2725 global=0.5736\n",
            "  Layer 27 not converged (global=0.5556 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1269 global=0.5355\n",
            "  step 10: local=0.2578 global=0.5354\n",
            "  step 20: local=0.2478 global=0.5388\n",
            "  step 30: local=0.1296 global=0.5360\n",
            "  step 40: local=0.1208 global=0.5265\n",
            "  step 50: local=0.2419 global=0.5167\n",
            "  step 60: local=0.2325 global=0.5381\n",
            "  step 70: local=0.1514 global=0.5579\n",
            "  step 80: local=0.2403 global=0.5552\n",
            "  step 90: local=0.2501 global=0.5168\n",
            "  Layer 27 not converged (global=0.5224 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.2415 global=0.5460\n",
            "  step 10: local=0.1252 global=0.5422\n",
            "  step 20: local=0.2342 global=0.5123\n",
            "  step 30: local=0.2374 global=0.5250\n",
            "  step 40: local=0.2520 global=0.5178\n",
            "  step 50: local=0.1392 global=0.5069\n",
            "  step 60: local=0.2422 global=0.5365\n",
            "  step 70: local=0.1330 global=0.5332\n",
            "  step 80: local=0.1243 global=0.5233\n",
            "  step 90: local=0.2389 global=0.4882\n",
            "  Layer 27 not converged (global=0.5130 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.2266 global=0.4986\n",
            "  step 10: local=0.2364 global=0.5538\n",
            "  step 20: local=0.2170 global=0.5090\n",
            "  step 30: local=0.1261 global=0.5004\n",
            "  step 40: local=0.2126 global=0.5169\n",
            "  step 50: local=0.2280 global=0.5098\n",
            "  step 60: local=0.2326 global=0.4966\n",
            "  step 70: local=0.2101 global=0.4914\n",
            "  step 80: local=0.2109 global=0.5124\n",
            "  step 90: local=0.0987 global=0.5198\n",
            "  Layer 27 not converged (global=0.4929 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1112 global=0.4975\n",
            "  step 10: local=0.2251 global=0.5635\n",
            "  step 20: local=0.1218 global=0.5096\n",
            "  step 30: local=0.2207 global=0.4920\n",
            "  step 40: local=0.1233 global=0.5134\n",
            "  step 50: local=0.1137 global=0.5224\n",
            "  step 60: local=0.1187 global=0.5429\n",
            "  step 70: local=0.2163 global=0.5149\n",
            "  step 80: local=0.2123 global=0.4937\n",
            "  step 90: local=0.2195 global=0.5075\n",
            "  Layer 27 not converged (global=0.4976 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1197 global=0.5218\n",
            "  step 10: local=0.1194 global=0.5425\n",
            "  step 20: local=0.2142 global=0.4949\n",
            "  step 30: local=0.2141 global=0.5276\n",
            "  step 40: local=0.1195 global=0.4737\n",
            "  step 50: local=0.2106 global=0.5412\n",
            "  step 60: local=0.2110 global=0.4986\n",
            "  step 70: local=0.2406 global=0.5417\n",
            "  step 80: local=0.2184 global=0.5028\n",
            "  step 90: local=0.2044 global=0.5314\n",
            "  Layer 27 not converged (global=0.5040 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.2047 global=0.5791\n",
            "  step 10: local=0.2265 global=0.5364\n",
            "  step 20: local=0.1960 global=0.4928\n",
            "  step 30: local=0.2149 global=0.5191\n",
            "  step 40: local=0.2037 global=0.5458\n",
            "  step 50: local=0.2150 global=0.5114\n",
            "  step 60: local=0.2191 global=0.5060\n",
            "  step 70: local=0.2123 global=0.5305\n",
            "  step 80: local=0.1200 global=0.4900\n",
            "  step 90: local=0.1018 global=0.5358\n",
            "  Layer 27 not converged (global=0.4979 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1135 global=0.5466\n",
            "  step 10: local=0.2021 global=0.5500\n",
            "  step 20: local=0.0905 global=0.5262\n",
            "  step 30: local=0.2137 global=0.4976\n",
            "  step 40: local=0.1106 global=0.4934\n",
            "  step 50: local=0.2045 global=0.5175\n",
            "  step 60: local=0.1125 global=0.4887\n",
            "  step 70: local=0.1152 global=0.5222\n",
            "  step 80: local=0.2128 global=0.5184\n",
            "  step 90: local=0.1072 global=0.5066\n",
            "  Layer 27 not converged (global=0.5155 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.2150 global=0.5001\n",
            "  step 10: local=0.2119 global=0.4742\n",
            "  step 20: local=0.2123 global=0.4883\n",
            "  step 30: local=0.1012 global=0.5010\n",
            "  step 40: local=0.1066 global=0.5105\n",
            "  step 50: local=0.1057 global=0.5071\n",
            "  step 60: local=0.1130 global=0.5216\n",
            "  step 70: local=0.2106 global=0.5053\n",
            "  step 80: local=0.2105 global=0.4895\n",
            "  step 90: local=0.1140 global=0.5161\n",
            "  Layer 27 not converged (global=0.4772 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.2172 global=0.4733\n",
            "  step 10: local=0.1095 global=0.5330\n",
            "  step 20: local=0.2099 global=0.5137\n",
            "  step 30: local=0.2175 global=0.4786\n",
            "  step 40: local=0.2084 global=0.5025\n",
            "  step 50: local=0.2099 global=0.5214\n",
            "  step 60: local=0.1961 global=0.5263\n",
            "  step 70: local=0.2030 global=0.4846\n",
            "  step 80: local=0.2180 global=0.5131\n",
            "  step 90: local=0.1166 global=0.5205\n",
            "  Layer 27 not converged (global=0.5002 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.2087 global=0.5268\n",
            "  step 10: local=0.2095 global=0.4801\n",
            "  step 20: local=0.2054 global=0.4834\n",
            "  step 30: local=0.2038 global=0.4952\n",
            "  step 40: local=0.1142 global=0.4886\n",
            "  step 50: local=0.1013 global=0.5260\n",
            "  step 60: local=0.0953 global=0.5266\n",
            "  step 70: local=0.1057 global=0.4946\n",
            "  step 80: local=0.1100 global=0.4805\n",
            "  step 90: local=0.1042 global=0.4930\n",
            "  Layer 27 not converged (global=0.5239 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.2033 global=0.4920\n",
            "  step 10: local=0.1085 global=0.5254\n",
            "  step 20: local=0.2078 global=0.5026\n",
            "  step 30: local=0.2222 global=0.5299\n",
            "  step 40: local=0.1134 global=0.4908\n",
            "  step 50: local=0.0921 global=0.4980\n",
            "  step 60: local=0.1088 global=0.5038\n",
            "  step 70: local=0.1976 global=0.5018\n",
            "  step 80: local=0.1157 global=0.4924\n",
            "  step 90: local=0.0969 global=0.4899\n",
            "  Layer 27 not converged (global=0.5502 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1106 global=0.5091\n",
            "  step 10: local=0.1228 global=0.5277\n",
            "  step 20: local=0.1198 global=0.5279\n",
            "  step 30: local=0.2070 global=0.4956\n",
            "  step 40: local=0.1935 global=0.5194\n",
            "  step 50: local=0.2175 global=0.5193\n",
            "  step 60: local=0.1980 global=0.4912\n",
            "  step 70: local=0.2036 global=0.5043\n",
            "  step 80: local=0.2025 global=0.5008\n",
            "  step 90: local=0.1931 global=0.4895\n",
            "  Layer 27 not converged (global=0.4841 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1115 global=0.5214\n",
            "  step 10: local=0.2191 global=0.5177\n",
            "  step 20: local=0.1215 global=0.5048\n",
            "  step 30: local=0.1019 global=0.4742\n",
            "  step 40: local=0.1039 global=0.4821\n",
            "  step 50: local=0.2200 global=0.5377\n",
            "  step 60: local=0.1054 global=0.4964\n",
            "  step 70: local=0.2195 global=0.4888\n",
            "  step 80: local=0.2146 global=0.5032\n",
            "  step 90: local=0.2106 global=0.4977\n",
            "  Layer 27 not converged (global=0.4986 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1986 global=0.4881\n",
            "  step 10: local=0.1076 global=0.4847\n",
            "  step 20: local=0.2103 global=0.5026\n",
            "  step 30: local=0.2053 global=0.5094\n",
            "  step 40: local=0.1213 global=0.4888\n",
            "  step 50: local=0.2142 global=0.5500\n",
            "  step 60: local=0.2073 global=0.5010\n",
            "  step 70: local=0.2105 global=0.4850\n",
            "  step 80: local=0.2111 global=0.5064\n",
            "  step 90: local=0.1135 global=0.5145\n",
            "  Layer 27 not converged (global=0.5005 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.2093 global=0.5357\n",
            "  step 10: local=0.1091 global=0.5094\n",
            "  step 20: local=0.2129 global=0.4869\n",
            "  step 30: local=0.1981 global=0.5024\n",
            "  step 40: local=0.1988 global=0.5110\n",
            "  step 50: local=0.1095 global=0.5312\n",
            "  step 60: local=0.1050 global=0.4863\n",
            "  step 70: local=0.2091 global=0.5205\n",
            "  step 80: local=0.1127 global=0.4678\n",
            "  step 90: local=0.1023 global=0.5334\n",
            "  Layer 27 not converged (global=0.5287 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.2012 global=0.4917\n",
            "  step 10: local=0.2043 global=0.5372\n",
            "  step 20: local=0.1978 global=0.4969\n",
            "  step 30: local=0.2076 global=0.5249\n",
            "  step 40: local=0.1066 global=0.5735\n",
            "  step 50: local=0.1001 global=0.5288\n",
            "  step 60: local=0.2034 global=0.4887\n",
            "  step 70: local=0.1027 global=0.5139\n",
            "  step 80: local=0.1072 global=0.5398\n",
            "  step 90: local=0.1241 global=0.5049\n",
            "  Layer 27 not converged (global=0.5077 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1073 global=0.4996\n",
            "  step 10: local=0.2059 global=0.5279\n",
            "  step 20: local=0.2022 global=0.4859\n",
            "  step 30: local=0.1969 global=0.5326\n",
            "  step 40: local=0.1094 global=0.5389\n",
            "  step 50: local=0.1049 global=0.5453\n",
            "  step 60: local=0.1195 global=0.5241\n",
            "  step 70: local=0.2065 global=0.4931\n",
            "  step 80: local=0.2119 global=0.4904\n",
            "  step 90: local=0.2168 global=0.5122\n",
            "  Layer 27 not converged (global=0.5201 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1997 global=0.4847\n",
            "  step 10: local=0.2001 global=0.5186\n",
            "  step 20: local=0.1993 global=0.5152\n",
            "  step 30: local=0.1981 global=0.5030\n",
            "  step 40: local=0.1142 global=0.4972\n",
            "  step 50: local=0.2005 global=0.4698\n",
            "  step 60: local=0.1024 global=0.4821\n",
            "  step 70: local=0.1077 global=0.4984\n",
            "  step 80: local=0.2126 global=0.5058\n",
            "  step 90: local=0.1984 global=0.5056\n",
            "  Layer 27 not converged (global=0.4997 > 0.4), repeating...\n",
            "\n",
            "--- Layer 27/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.2154 global=0.5185\n",
            "  step 10: local=0.2102 global=0.5026\n",
            "  step 20: local=0.0933 global=0.4856\n",
            "  step 30: local=0.2162 global=0.5108\n",
            "  step 40: local=0.2007 global=0.4707\n",
            "  step 50: local=0.2146 global=0.5270\n",
            "  step 60: local=0.2163 global=0.5112\n",
            "  step 70: local=0.1008 global=0.4736\n",
            "  step 80: local=0.2052 global=0.4995\n",
            "  step 90: local=0.2087 global=0.5173\n",
            "  [WARN] Layer 27 did not converge after 20 repeats (global=0.5292)\n",
            "\n",
            "============================================================\n",
            "PASS 4: E2E Quantizer Tuning (f-only)\n",
            "============================================================\n",
            "  Trainable f parameters: 196\n",
            "  step 0: loss=1.6576\n",
            "  step 10: loss=1.6447\n",
            "  step 20: loss=1.6822\n",
            "  step 30: loss=1.6697\n",
            "  step 40: loss=1.6467\n",
            "  step 50: loss=1.5488\n",
            "  step 60: loss=1.5807\n",
            "  step 70: loss=1.5974\n",
            "  step 80: loss=1.6489\n",
            "  step 90: loss=1.6708\n",
            "  step 100: loss=1.5812\n",
            "  step 110: loss=1.5827\n",
            "  step 120: loss=1.5830\n",
            "  step 130: loss=1.6217\n",
            "  step 140: loss=1.5734\n",
            "  step 150: loss=1.6424\n",
            "  step 160: loss=1.5592\n",
            "  step 170: loss=1.6604\n",
            "  step 180: loss=1.5620\n",
            "  step 190: loss=1.5479\n",
            "  step 200: loss=1.5731\n",
            "  step 210: loss=1.5688\n",
            "  step 220: loss=1.5518\n",
            "  step 230: loss=1.5483\n",
            "  step 240: loss=1.6283\n",
            "  step 250: loss=1.6468\n",
            "  step 260: loss=1.6207\n",
            "  step 270: loss=1.6319\n",
            "  step 280: loss=1.6546\n",
            "  step 290: loss=1.6259\n",
            "  step 300: loss=1.5761\n",
            "  step 310: loss=1.6031\n",
            "  step 320: loss=1.5874\n",
            "  step 330: loss=1.6059\n",
            "  step 340: loss=1.6485\n",
            "  step 350: loss=1.6596\n",
            "  step 360: loss=1.6550\n",
            "  step 370: loss=1.5829\n",
            "  step 380: loss=1.6368\n",
            "  step 390: loss=1.6529\n",
            "  step 400: loss=1.5896\n",
            "  step 410: loss=1.6242\n",
            "  step 420: loss=1.6334\n",
            "  step 430: loss=1.5858\n",
            "  step 440: loss=1.5504\n",
            "  step 450: loss=1.5853\n",
            "  step 460: loss=1.6134\n",
            "  step 470: loss=1.6332\n",
            "  step 480: loss=1.5385\n",
            "  step 490: loss=1.7062\n",
            "\n",
            "============================================================\n",
            "Saving outputs\n",
            "============================================================\n",
            "  Model saved to: runs/progressive_qat_v1/qat_state_dict.pt\n",
            "  Loss log saved to: runs/progressive_qat_v1/loss_per_layer.csv\n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# Full Progressive: MLP pass + E2E f-only (skip attention/refinement for v1)\n",
        "# For full 3-pass training, remove --skip_attention_pass and --skip_mlp_refinement\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/train_qat_progressive.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --output_dir {RUN_DIR_PROGRESSIVE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --steps_per_layer_mlp {STEPS_PER_LAYER_MLP} \\\n",
        "  --e2e_steps {E2E_STEPS} \\\n",
        "  --local_weight {LOCAL_WEIGHT} \\\n",
        "  --global_weight {GLOBAL_WEIGHT} \\\n",
        "  --local_token_samples {LOCAL_TOKEN_SAMPLES} \\\n",
        "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
        "  --learning_rate {LR_PROGRESSIVE} \\\n",
        "  --e2e_learning_rate {LR_E2E} \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --skip_attention_pass \\\n",
        "  --max_layer_repeats 20 \\\n",
        "  --max_backtrack 5 \\\n",
        "  --layer_converge_threshold 0.4 \\\n",
        "  --skip_mlp_refinement"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# SAVE PROGRESSIVE QAT CHECKPOINT TO GOOGLE DRIVE\n",
        "# ============================================================\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "\n",
        "# Mount Google Drive if not already mounted\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Source directory\n",
        "RUN_NAME = \"progressive_qat_v1\"\n",
        "RUN_NAME = \"qwen3_kdqat_cache_q2_3\"\n",
        "\n",
        "RUN_DIR = f\"runs/{RUN_NAME}\"\n",
        "\n",
        "# Destination on Google Drive\n",
        "DEST_DIR_GD = \"/content/drive/MyDrive/qwen3_runs/\"\n",
        "!mkdir -p {DEST_DIR_GD}\n",
        "\n",
        "# Check if run directory exists and has content\n",
        "if os.path.isdir(RUN_DIR) and os.listdir(RUN_DIR):\n",
        "    # Compress the run directory\n",
        "    print(f\"[archive] Compressing {RUN_DIR}...\")\n",
        "    !tar -zcvf {RUN_NAME}.tgz -C runs {RUN_NAME}\n",
        "\n",
        "    # Copy to Google Drive\n",
        "    print(f\"[save] Copying {RUN_NAME}.tgz to Google Drive...\")\n",
        "    !rsync -ah --info=progress2 {RUN_NAME}.tgz {DEST_DIR_GD}\n",
        "\n",
        "    # Verify\n",
        "    gd_size = os.path.getsize(f\"{DEST_DIR_GD}/{RUN_NAME}.tgz\")\n",
        "    print(f\"[save] Saved to Google Drive: {gd_size / (1024**3):.2f} GB\")\n",
        "\n",
        "    # Cleanup local archive (optional)\n",
        "    # !rm {RUN_NAME}.tgz\n",
        "else:\n",
        "    print(f\"[save] ERROR: {RUN_DIR} is empty or doesn't exist\")\n",
        "    print(\"[save] Run progressive training first\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBFCVCZjwBo_",
        "outputId": "8eaba02b-c9ea-4930-df9b-69a5f75e180d"
      },
      "id": "dBFCVCZjwBo_",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[archive] Compressing runs/qwen3_kdqat_cache_q2_3...\n",
            "qwen3_kdqat_cache_q2_3/\n",
            "qwen3_kdqat_cache_q2_3/special_tokens_map.json\n",
            "qwen3_kdqat_cache_q2_3/loss.csv\n",
            "qwen3_kdqat_cache_q2_3/added_tokens.json\n",
            "qwen3_kdqat_cache_q2_3/tokenizer_config.json\n",
            "qwen3_kdqat_cache_q2_3/run_state.json\n",
            "qwen3_kdqat_cache_q2_3/merges.txt\n",
            "qwen3_kdqat_cache_q2_3/training_args.json\n",
            "qwen3_kdqat_cache_q2_3/chat_template.jinja\n",
            "qwen3_kdqat_cache_q2_3/vocab.json\n",
            "qwen3_kdqat_cache_q2_3/final_state_dict.pt\n",
            "qwen3_kdqat_cache_q2_3/qat_state_dict.pt\n",
            "qwen3_kdqat_cache_q2_3/tokenizer.json\n",
            "[save] Copying qwen3_kdqat_cache_q2_3.tgz to Google Drive...\n",
            "          1.90G 100%  430.50MB/s    0:00:04 (xfr#1, to-chk=0/1)\n",
            "[save] Saved to Google Drive: 1.77 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qiaq60t466e",
      "metadata": {
        "id": "qiaq60t466e"
      },
      "outputs": [],
      "source": [
        "# v3: Full 3-pass progressive training\n",
        "# MLP -> Attention -> MLP refinement -> E2E f-only\n",
        "# WARNING: May show instability at later layers for 2-bit\n",
        "\n",
        "RUN_DIR_PROGRESSIVE_V3 = \"runs/progressive_qat_v3\"\n",
        "\n",
        "!python scripts/train_qat_progressive.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --output_dir {RUN_DIR_PROGRESSIVE_V3} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --steps_per_layer_mlp {STEPS_PER_LAYER_MLP} \\\n",
        "  --steps_per_layer_attn {STEPS_PER_LAYER_ATTN} \\\n",
        "  --e2e_steps {E2E_STEPS} \\\n",
        "  --local_weight {LOCAL_WEIGHT} \\\n",
        "  --global_weight {GLOBAL_WEIGHT} \\\n",
        "  --local_token_samples {LOCAL_TOKEN_SAMPLES} \\\n",
        "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
        "  --learning_rate {LR_PROGRESSIVE} \\\n",
        "  --e2e_learning_rate {LR_E2E} \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ix3xjm2h0xq",
      "metadata": {
        "id": "ix3xjm2h0xq"
      },
      "outputs": [],
      "source": [
        "# Plot per-layer training progress\n",
        "# Change PLOT_RUN to visualize different runs\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Choose which run to visualize\n",
        "PLOT_RUN = RUN_DIR_E2E_FONLY  # or RUN_DIR_PROGRESSIVE, RUN_DIR_PROGRESSIVE_V3\n",
        "\n",
        "csv_path = f\"{PLOT_RUN}/loss_per_layer.csv\"\n",
        "if not os.path.exists(csv_path):\n",
        "    print(f\"Loss CSV not found at {csv_path}\")\n",
        "    print(\"Run training first or check the path.\")\n",
        "else:\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "    # Pass 1: MLP training (local loss)\n",
        "    mlp_df = df[(df['pass'] == 1) & (df['component'] == 'mlp')]\n",
        "    if not mlp_df.empty and 'local' in mlp_df.columns:\n",
        "        for layer in mlp_df['layer'].unique():\n",
        "            layer_df = mlp_df[mlp_df['layer'] == layer]\n",
        "            axes[0, 0].plot(layer_df['step'], layer_df['local'], label=f'L{layer}', alpha=0.7)\n",
        "        axes[0, 0].set_title('Pass 1: MLP Local Loss per Layer')\n",
        "        axes[0, 0].set_xlabel('Step')\n",
        "        axes[0, 0].set_ylabel('Loss')\n",
        "        axes[0, 0].legend(ncol=4, fontsize=6)\n",
        "    else:\n",
        "        axes[0, 0].set_title('Pass 1: MLP Local Loss (skipped or no local loss)')\n",
        "\n",
        "    # Pass 1: MLP global loss\n",
        "    if not mlp_df.empty and 'global' in mlp_df.columns:\n",
        "        for layer in mlp_df['layer'].unique():\n",
        "            layer_df = mlp_df[mlp_df['layer'] == layer]\n",
        "            axes[0, 1].plot(layer_df['step'], layer_df['global'], label=f'L{layer}', alpha=0.7)\n",
        "        axes[0, 1].set_title('Pass 1: MLP Global KD Loss per Layer')\n",
        "        axes[0, 1].set_xlabel('Step')\n",
        "        axes[0, 1].set_ylabel('Loss')\n",
        "        axes[0, 1].legend(ncol=4, fontsize=6)\n",
        "    else:\n",
        "        axes[0, 1].set_title('Pass 1: MLP Global Loss (skipped)')\n",
        "\n",
        "    # Pass 2: Attention training\n",
        "    attn_df = df[(df['pass'] == 2) & (df['component'] == 'attn')]\n",
        "    if not attn_df.empty and 'global' in attn_df.columns:\n",
        "        for layer in attn_df['layer'].unique():\n",
        "            layer_df = attn_df[attn_df['layer'] == layer]\n",
        "            axes[1, 0].plot(layer_df['step'], layer_df['global'], label=f'L{layer}', alpha=0.7)\n",
        "        axes[1, 0].set_title('Pass 2: Attention Global KD Loss per Layer')\n",
        "        axes[1, 0].set_xlabel('Step')\n",
        "        axes[1, 0].set_ylabel('Loss')\n",
        "        axes[1, 0].legend(ncol=4, fontsize=6)\n",
        "    else:\n",
        "        axes[1, 0].set_title('Pass 2: Attention (skipped)')\n",
        "        axes[1, 0].text(0.5, 0.5, 'Not run', ha='center', va='center', transform=axes[1, 0].transAxes)\n",
        "\n",
        "    # Pass 4: E2E f-only tuning\n",
        "    e2e_df = df[(df['pass'] == 4)]\n",
        "    if not e2e_df.empty and 'global' in e2e_df.columns:\n",
        "        axes[1, 1].plot(e2e_df['step'], e2e_df['global'], 'b-', linewidth=2)\n",
        "        axes[1, 1].set_title('Pass 4: E2E f-only Tuning')\n",
        "        axes[1, 1].set_xlabel('Step')\n",
        "        axes[1, 1].set_ylabel('Global KD Loss')\n",
        "    else:\n",
        "        axes[1, 1].set_title('Pass 4: E2E (not yet run)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{PLOT_RUN}/loss_per_layer.png\", dpi=150)\n",
        "    plt.show()\n",
        "    print(f\"Saved to {PLOT_RUN}/loss_per_layer.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qgtw9rd3cr",
      "metadata": {
        "id": "qgtw9rd3cr"
      },
      "source": [
        "### Inference Check: Progressive QAT Results\n",
        "\n",
        "Test the progressive QAT checkpoint with a quick inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "4coakmebsik",
      "metadata": {
        "id": "4coakmebsik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49e3bafc-7fe3-4ed6-cde9-de22e77cb02d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-24 10:27:10.822627: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-24 10:27:10.842835: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766572030.868200  151847 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766572030.873694  151847 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766572030.887622  151847 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572030.887657  151847 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572030.887660  151847 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572030.887663  151847 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 10:27:10.891867: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "user\n",
            "What is the capital of France?\n",
            "assistant\n",
            "<think>\n",
            "\n",
            "</think>\n",
            "\n",
            "The capital of France is **Paris**.\n"
          ]
        }
      ],
      "source": [
        "# Test inference with progressive QAT checkpoint\n",
        "# Change RUN_DIR to test different runs:\n",
        "#   RUN_DIR_E2E_FONLY, RUN_DIR_PROGRESSIVE_FONLY, RUN_DIR_PROGRESSIVE\n",
        "# progressive_qat_v1/qat_state_dict.pt\n",
        "\n",
        "#TEST_RUN = RUN_DIR_E2E_FONLY  # Change this to test other runs\n",
        "\n",
        "TEST_RUN =  \"runs/progressive_qat_v1\"\n",
        "TEST_RUN =  \"runs/qwen3_kdqat_cache_q2_4\"\n",
        "\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {TEST_RUN}/qat_state_dict.pt \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --prompt \"What is the capital of France?\" \\\n",
        "  --do_sample true \\\n",
        "  --max_new_tokens 64"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yq8GHX0o3c2y",
      "metadata": {
        "id": "Yq8GHX0o3c2y"
      },
      "source": [
        "## 6) Stage B: LoRA recovery\n",
        "\n",
        "Two options:\n",
        "- **SFT LoRA** (Alpaca-style instruction tuning)\n",
        "- **Cached KD-LoRA** (preserve teacher distribution; no new “skills”)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B1yimj8aS11y",
      "metadata": {
        "id": "B1yimj8aS11y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "LcGFKyxO3c2y",
      "metadata": {
        "id": "LcGFKyxO3c2y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b358c6fd-868d-4290-d664-c05e50f0831f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "[device] cuda | amp_dtype=torch.bfloat16 | param_dtype=torch.bfloat16\n",
            "2025-12-24 10:27:43.643798: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-24 10:27:43.663498: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766572063.688940  152049 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766572063.694401  152049 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766572063.707764  152049 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572063.707788  152049 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572063.707791  152049 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572063.707794  152049 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 10:27:43.711890: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[qat] weight_bits=4\n",
            "Loaded QAT checkpoint. missing=0 unexpected=0\n",
            "Enabled LoRA on (196, 20185088) layers. Trainable params: 20,185,088\n",
            "[kd_cache] Note: cache max_length=128 (you passed --max_length=1024). --max_length is ignored in cache mode.\n",
            "[kd_cache] cache topk=32\n",
            "[kd_cache] Enabled cached KD-LoRA. T=2.0 weight=1.0 hard_top1=0.02 hard_full_top1=0.01\n",
            "[loss.csv] Rotated existing loss.csv (last_step=1000) -> loss_prev_20251224_102754.csv\n",
            "opt_step: 100% 1000/1000 [09:39<00:00,  1.72step/s, loss=0.4478, lr=0.00e+00]\n",
            "Done. Saved LoRA adapter to: runs/qwen3_lora_recovery_cached_r32/lora_only_state_dict.pt\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# STAGE B: LoRA Recovery (Cached KD-LoRA)\n",
        "# ============================================================\n",
        "# Train LoRA adapters on top of QAT checkpoint\n",
        "\n",
        "CACHE_DIR = CACHE_DIR_CHAT\n",
        "#RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2\"\n",
        "RUN_DIR_CACHE  = \"runs/progressive_qat_v1\"\n",
        "RUN_DIR_CACHE  = \"runs/qwen3_kdqat_cache_q2_4\"\n",
        "\n",
        "LORA_DIM = 32\n",
        "LORA_RUN_KD = f\"runs/qwen3_lora_recovery_cached_r{LORA_DIM}\"\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/train_lora_recovery.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR_CACHE}/qat_state_dict.pt \\\n",
        "  --output_dir {LORA_RUN_KD} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --gradient_accumulation_steps 2 \\\n",
        "  --learning_rate 1e-5 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 1000 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 2 \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} \\\n",
        "  --lora_alpha {LORA_DIM} \\\n",
        "  --lora_dropout 0.0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.02 \\\n",
        "  --hard-full-top1-weight 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Defule6L3c2y",
      "metadata": {
        "id": "Defule6L3c2y"
      },
      "source": [
        "## 7) Plot loss\n",
        "\n",
        "In Colab, use `--no_show` + `--save` then display the PNG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QM1RNfDh3c2y",
      "metadata": {
        "id": "QM1RNfDh3c2y"
      },
      "outputs": [],
      "source": [
        "!python scripts/plot_loss.py --run_dir {RUN_DIR} --source csv --no_show --save {RUN_DIR}/loss.png\n",
        "from PIL import Image\n",
        "display(Image.open(f\"{RUN_DIR}/loss.png\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p3bOUiMt3c2y",
      "metadata": {
        "id": "p3bOUiMt3c2y"
      },
      "source": [
        "## 8) Inference sanity checks\n",
        "\n",
        "Greedy decode (`--do_sample false`) and keep outputs short (`--max_new_tokens 16`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "aA_cJHvT3c2y",
      "metadata": {
        "id": "aA_cJHvT3c2y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be3b0f24-91a2-4d52-e9ec-314d00de7ea7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "2025-12-24 10:39:05.017278: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-24 10:39:05.036974: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766572745.061841  155066 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766572745.067157  155066 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766572745.080472  155066 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572745.080496  155066 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572745.080499  155066 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572745.080502  155066 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 10:39:05.084617: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[lora] loaded adapters for 196 layers from runs/qwen3_lora_recovery_cached_r32/lora_only_state_dict.pt\n",
            "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "user\n",
            "What is capital of France?\n",
            "assistant\n",
            "<think>\n",
            "<think>\n",
            "Okay, I need to figure out the capital of France. Let me start by recalling some basic geography facts. France is a European country, and I know that the capital is Paris\n"
          ]
        }
      ],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_4\"\n",
        "#RUN_DIR = \"runs/progressive_qat_v1\"\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --lora_checkpoint \"runs/qwen3_lora_recovery_cached_r32/lora_only_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
        "  --prompt \"What is capital of France?\" \\\n",
        "  --do_sample false \\\n",
        "  --enable_thinking true \\\n",
        "  --max_new_tokens 40\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EJlu4sOZCNpY",
      "metadata": {
        "id": "EJlu4sOZCNpY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "1cD985HdXlm0",
      "metadata": {
        "id": "1cD985HdXlm0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65053f76-705a-419c-fddb-8ae1cff1fda1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-24 10:41:54.709024: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-24 10:41:54.729231: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766572914.754556  155967 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766572914.759941  155967 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766572914.773878  155967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572914.773902  155967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572914.773905  155967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766572914.773908  155967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 10:41:54.777961: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[lora] loaded adapters for 196 layers from runs/qwen3_lora_recovery_cached_r32/lora_only_state_dict.pt\n",
            "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "user\n",
            "Explain how neural networks learn in simple terms\n",
            "assistant\n",
            "<think>\n",
            "<think>\n",
            "Okay, let's start with the basics. Neural networks are like a brain in a computer, right? They process information in layers, just like a human brain does. Each layer has some connections between neurons, and each neuron can process a single input value. \n",
            "\n",
            "So, if we have a simple example, like a simple neural network that takes a single input and outputs a single output, how does it work? Let's imagine a simple example. Let's say the input is a number, say 10. The first layer is the input layer, and the second layer is the output layer. The input layer has a single neuron, and the output layer has a single neuron. \n",
            "\n",
            "The input layer takes the input number and processes it. Then, the output layer takes the result and passes it back. So, the process is: input → output. \n",
            "\n",
            "But how does this work in practice? It's like a simple machine learning model. The input is the data, the output is the result, and the model learns by adjusting the weights and biases to minimize the error. \n",
            "\n",
            "So, in simple terms, neural networks work by taking a set of inputs, applying some operations to them, and then producing a set of outputs. The process is iterative, and the model learns over time to make better predictions or decisions. \n",
            "\n",
            "I think that's a good explanation. Let me make sure it's simple and not too technical. The key is that the neural network is like a computer, and the process of learning is similar to how a human brain learns. \n",
            "\n",
            "I think that's a good explanation. Let me check if I'm missing anything. No, I think I've covered the basics. If there's anything else, I can add more details, but I'm already confident in this explanation.\n",
            "</think>\n",
            "\n",
            "**Explanation of How Neural Networks Learn in Simple Terms:**\n",
            "\n",
            "**1. Introduction:**\n",
            "- **Neural Networks (NNs):** They are like computers in the brain, where each neuron processes information.\n",
            "- **Input Layer:** Takes the input data and processes it.\n",
            "- **Output Layer:** Produces the output, which is the result of the processing.\n",
            "\n",
            "**2. Simple Example:**\n",
            "- **Input:** A single input value, say 10.\n",
            "- **First Layer (Input Layer):** Processes the input, maybe by adding some weights and biases.\n",
            "- **Output Layer (Output Layer):** Passes the result back, with the final output being the result of the processing.\n",
            "\n",
            "**3. How It Works:**\n",
            "- **Processing:** Each neuron in a layer takes the input and processes it.\n",
            "- **Learning:** The network learns to adjust its weights and biases over time to minimize the error.\n",
            "- **Iteration:** The process is repeated over time, and the model improves its performance.\n",
            "\n",
            "**4. Simple Explanation in Words:**\n",
            "- Neural networks are like a computer that learns by doing tasks. The input is the data, the output is the result, and the model learns to make better predictions.\n",
            "\n",
            "**5. Conclusion:**\n",
            "- Neural networks are a type of machine learning model that processes information in layers, just like a human brain does. They are simple, can be used for various tasks, and are very effective in learning.\n",
            "\n",
            "This explanation is simple, visual, and easy to understand.\n"
          ]
        }
      ],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_4\"\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --lora_checkpoint \"runs/qwen3_lora_recovery_cached_r32/lora_only_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
        "  --prompt \"Explain how neural networks learn in simple terms\" \\\n",
        "  --do_sample false \\\n",
        "  --enable_thinking true \\\n",
        "  --max_new_tokens 1024\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xZ6aSDAa3c2y",
      "metadata": {
        "id": "xZ6aSDAa3c2y"
      },
      "outputs": [],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --lora_checkpoint \"runs/qwen3_lora_recovery_cached_r64/lora_only_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
        "  --prompt \"2+2=\" \\\n",
        "  --do_sample false \\\n",
        "  --enable_thinking true \\\n",
        "  --max_new_tokens 90\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZOIo97VlyAbh",
      "metadata": {
        "id": "ZOIo97VlyAbh"
      },
      "outputs": [],
      "source": [
        "LORA_DIM = 64\n",
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2\"\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
        "  --prompt \"What is capital of France?\" \\\n",
        "  --do_sample true \\\n",
        "  --max_new_tokens 64\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ef_Q8P0i3c2y",
      "metadata": {
        "id": "Ef_Q8P0i3c2y"
      },
      "source": [
        "## 9) Optional: snap weights to the exact grid\n",
        "\n",
        "This produces a float checkpoint with weights snapped to the N-bit codebook (not bitpacked)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jk_qSZIs3c2y",
      "metadata": {
        "id": "jk_qSZIs3c2y"
      },
      "outputs": [],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "!python scripts/hard_quantize_checkpoint.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}/checkpoint_last.pt \\\n",
        "  --output_path {RUN_DIR}/hard_quant_full_state_dict.pt \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7HnE7IFS3dK",
      "metadata": {
        "id": "f7HnE7IFS3dK"
      },
      "outputs": [],
      "source": [
        "%cd qwen3_apple_style_2bit_qat_lora\n",
        "%ls -l runs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beQD2eyDWWnb",
      "metadata": {
        "id": "beQD2eyDWWnb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}