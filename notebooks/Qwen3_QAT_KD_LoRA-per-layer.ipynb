{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Anemll/qwen3_apple_style_2bit_qat_lora/blob/main/notebooks%5CQwen3_QAT_KD_LoRA-per-layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rRgC0uK43c2v",
   "metadata": {
    "id": "rRgC0uK43c2v"
   },
   "source": [
    "# Qwen3-0.6B — Apple-style QAT (2-bit / 4-bit) + KD + LoRA recovery\n",
    "\n",
    "This notebook mirrors the structure of common “phone deployment” notebooks, but uses **this repo’s** pipeline:\n",
    "\n",
    "- **Stage A (recommended default):** KD-QAT on plain text (C4 streaming) or KD-cache QAT\n",
    "- **Stage B:** LoRA recovery (either SFT or cached KD-LoRA)\n",
    "- Plot `loss.csv`\n",
    "- Run inference sanity checks\n",
    "\n",
    "Notes:\n",
    "- Qwen3 requires `transformers>=4.51.0`.\n",
    "- For disk usage: C4 is huge; prefer `--streaming` unless you explicitly want to download.\n",
    "- Bitwidth: use `-q 2` (default) or `-q 4` (less aggressive). Checkpoints persist the bitwidth per layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i6Pb9Kki3c2w",
   "metadata": {
    "id": "i6Pb9Kki3c2w"
   },
   "source": [
    "## 0) Setup (Colab / local)\n",
    "\n",
    "If you’re in Colab, clone the repo. If you’re already in the repo directory locally, you can skip this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UXlLPdtkGM35",
   "metadata": {
    "id": "UXlLPdtkGM35"
   },
   "outputs": [],
   "source": [
    "# ---- Config (edit these) ----\n",
    "#MODEL_NAME = 'Qwen/Qwen3-4B-Thinking-2507'\n",
    "MODEL_NAME = 'Qwen/Qwen3-0.6B'\n",
    "TEACHER_NAME = MODEL_NAME\n",
    "QUANT_BITS = 2  # 2 or 4\n",
    "DEVICE = 'auto'\n",
    "AMP_DTYPE = 'auto'\n",
    "PARAM_DTYPE = 'auto'\n",
    "DTYPE = 'auto'\n",
    "\n",
    "# Cache dirs\n",
    "CACHE_DIR_CHAT = 'caches/alpaca_chat_think_L128_K32_R256'\n",
    "CACHE_DIR_TEXT = 'caches/c4_qwen3_L64_K32_R256'\n",
    "#CACHE_DIR_CHAT = 'caches/Q4B_alpaca_chat_think_L128_K32_R256'\n",
    "#CACHE_DIR_TEXT = 'caches/Q4B_c4_qwen3_L64_K32_R256'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u2vonfu23c2w",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "u2vonfu23c2w",
    "outputId": "0ba0ffbe-2f90-42b2-9bf1-e4c8962b2c2f"
   },
   "outputs": [],
   "source": [
    "# Colab-only:\n",
    "!git clone https://github.com/Anemll/qwen3_apple_style_2bit_qat_lora\n",
    "%cd qwen3_apple_style_2bit_qat_lora\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZUmiISSL3c2w",
   "metadata": {
    "id": "ZUmiISSL3c2w"
   },
   "source": [
    "## 1) Install dependencies (uv)\n",
    "\n",
    "This repo is set up to work with `uv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ACRjVuVa3c2x",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ACRjVuVa3c2x",
    "outputId": "8ca711c9-6384-4da3-a338-bdcc0dc75bcf"
   },
   "outputs": [],
   "source": [
    "!pip -q install uv\n",
    "!uv pip install -r requirements.txt\n",
    "!uv pip install -e .\n",
    "# plotting\n",
    "!uv pip install -q matplotlib\n",
    "!uv pip install -q plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yfoLPbBk3c2x",
   "metadata": {
    "id": "yfoLPbBk3c2x"
   },
   "source": [
    "## 2) Optional: Hugging Face login\n",
    "\n",
    "If you hit gated model/dataset errors, log in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wLMWX7E23c2x",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387,
     "referenced_widgets": [
      "afc730ace4234ac5a577a5a5220c6a4d",
      "c5b39aec0c1b4bfcb670602c50af7ec2",
      "a53a1357d67848c49b047de5845ba284",
      "e38abc2fa8264921844ca4a71863ca4b",
      "5060a342ef4d42898688600637345d44",
      "33619148b8e846778394442403694063",
      "2c06627c65d445a69f0683f64826f597",
      "bcd8b12c61574d908255a9fad3a4bffc",
      "21cbc3b64c344853af951223c7c4e955",
      "063d57549bdc4b90946e8173797a7900",
      "93680907491a4699b13afdfa674f12bf",
      "d76812431e314faeafb996c7a8a84507",
      "96c08bce8465490e84ed970275181395",
      "c7636c01eecf453bac5d94ce00dd7d4e",
      "6e2e98db599b4999bd7aacab2eaad671",
      "00ef006975b84f8c87312dbe1425f921",
      "10a9646f4bbd484ab15d21032401363a"
     ]
    },
    "id": "wLMWX7E23c2x",
    "outputId": "c75fd263-3aee-4f50-958c-0122d1ec3130"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()  # paste token when prompted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fA37ilf33c2x",
   "metadata": {
    "id": "fA37ilf33c2x"
   },
   "source": [
    "## 3) Quick environment check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VUCXehkU3c2x",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VUCXehkU3c2x",
    "outputId": "0ae6bcd7-8940-42fc-e784-a1a3f52bb183"
   },
   "outputs": [],
   "source": [
    "import torch, transformers\n",
    "print('torch', torch.__version__)\n",
    "print('transformers', transformers.__version__)\n",
    "print('cuda', torch.cuda.is_available())\n",
    "print('mps', torch.backends.mps.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GCXUYtVC3c2x",
   "metadata": {
    "id": "GCXUYtVC3c2x"
   },
   "source": [
    "## 4) Stage A ((Optional)): KD-QAT on streaming C4\n",
    "\n",
    "This preserves the base model’s behavior under low-bit fake-quant weights.\n",
    "\n",
    "Tips:\n",
    "- Start with a small run (`--max_steps 50`) to validate the pipeline.\n",
    "- Use `-q 4` if 2-bit is too unstable; 4-bit is less aggressive.\n",
    "- On MPS, prefer `--ema_decay 0` for KD-QAT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jERGktjwjz29",
   "metadata": {
    "id": "jERGktjwjz29"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TpT02cskrs6D",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "TpT02cskrs6D",
    "outputId": "0fff54f4-d143-4db8-ad8f-6a702fab52fe"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GENERATE THINKING DATASET (Alpaca chat format)\n",
    "# ============================================================\n",
    "# SKIP THIS CELL if you already have the cache on Google Drive!\n",
    "# Use the \"LOAD FROM GOOGLE DRIVE\" cell instead.\n",
    "\n",
    "!python scripts/precompute_teacher_topk.py \\\n",
    "  --teacher_model_name_or_path {MODEL_NAME} \\\n",
    "  --dataset_name tatsu-lab/alpaca \\\n",
    "  --dataset_split train \\\n",
    "  --dataset_format alpaca_chat \\\n",
    "  --enable_thinking true \\\n",
    "  --max_length 128 \\\n",
    "  --topk 32 \\\n",
    "  --rand_neg 256 \\\n",
    "  --num_sequences 20000 \\\n",
    "  --batch_size 1 \\\n",
    "  --shard_size 512 \\\n",
    "  --device {DEVICE} \\\n",
    "  --dtype {DTYPE} \\\n",
    "  --output_dir {CACHE_DIR_CHAT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rlxJ3X8a3c2x",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rlxJ3X8a3c2x",
    "outputId": "d4d74e2f-4088-423f-dc65-0eae9e6e7c99"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GENERATE TEXT DATASET (C4 streaming)\n",
    "# ============================================================\n",
    "# SKIP THIS CELL if you already have the cache on Google Drive!\n",
    "# Use the \"LOAD FROM GOOGLE DRIVE\" cell instead.\n",
    "\n",
    "import os\n",
    "\n",
    "CACHE_DIR = CACHE_DIR_TEXT\n",
    "\n",
    "if not os.path.isdir(CACHE_DIR):\n",
    "    print(f\"[cache] {CACHE_DIR} not found -> generating cache...\")\n",
    "\n",
    "    !python scripts/precompute_teacher_topk.py \\\n",
    "      --teacher_model_name_or_path {MODEL_NAME} \\\n",
    "      --dataset_name allenai/c4 \\\n",
    "      --dataset_config_name en \\\n",
    "      --dataset_split train \\\n",
    "      --dataset_text_field text \\\n",
    "      --streaming \\\n",
    "      --shuffle_buffer 10000 \\\n",
    "      --max_length 64 \\\n",
    "      --topk 32 \\\n",
    "      --rand_neg 256 \\\n",
    "      --num_sequences 2000 \\\n",
    "      --batch_size 1 \\\n",
    "      --shard_size 512 \\\n",
    "      --device {DEVICE} \\\n",
    "      --dtype {DTYPE} \\\n",
    "      --output_dir {CACHE_DIR}\n",
    "\n",
    "else:\n",
    "    print(f\"[cache] {CACHE_DIR} already exists -> skipping generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Nt45qTnlKO7a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nt45qTnlKO7a",
    "outputId": "e5e8d6cc-3874-465f-a025-6e22729e6bb2"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPRESS CHAT CACHE (for Google Drive upload)\n",
    "# ============================================================\n",
    "# SKIP if cache is already compressed or loaded from Google Drive\n",
    "\n",
    "import os\n",
    "\n",
    "if os.path.isdir(CACHE_DIR_CHAT):\n",
    "    print(f\"[gzip] Compressing {CACHE_DIR_CHAT}...\")\n",
    "    !tar -zcvf {CACHE_DIR_CHAT}.tgz {CACHE_DIR_CHAT}\n",
    "    compressed_size = os.path.getsize(f\"{CACHE_DIR_CHAT}.tgz\")\n",
    "    print(f\"[gzip] Done: {compressed_size / (1024**3):.2f} GB\")\n",
    "else:\n",
    "    print(f\"[gzip] Directory {CACHE_DIR_CHAT} not found. Skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n4snxtz2q6q",
   "metadata": {},
   "source": [
    "## 4.5) Google Drive Cache Management\n",
    "\n",
    "**Workflow for KD Cache:**\n",
    "\n",
    "1. **First time setup** (slow): \n",
    "   - Run `precompute_teacher_topk.py` to generate cache\n",
    "   - Run \"SAVE TO GOOGLE DRIVE\" cell to persist\n",
    "   \n",
    "2. **Subsequent sessions** (fast):\n",
    "   - Run \"LOAD FROM GOOGLE DRIVE\" cell to restore cache\n",
    "   - Skip cache generation step\n",
    "\n",
    "The cached KD data (~2-3 GB compressed) contains precomputed teacher logits for knowledge distillation training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7745eb2",
   "metadata": {
    "id": "d7745eb2"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPRESS TEXT CACHE (for Google Drive upload)\n",
    "# ============================================================\n",
    "# SKIP if cache is already compressed or loaded from Google Drive\n",
    "\n",
    "import os\n",
    "\n",
    "if os.path.isdir(CACHE_DIR_TEXT):\n",
    "    print(f\"[gzip] Compressing {CACHE_DIR_TEXT}...\")\n",
    "    !tar -zcvf {CACHE_DIR_TEXT}.tgz {CACHE_DIR_TEXT}\n",
    "    compressed_size = os.path.getsize(f\"{CACHE_DIR_TEXT}.tgz\")\n",
    "    print(f\"[gzip] Done: {compressed_size / (1024**3):.2f} GB\")\n",
    "else:\n",
    "    print(f\"[gzip] Directory {CACHE_DIR_TEXT} not found. Skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vlhuS4N9GbN4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vlhuS4N9GbN4",
    "outputId": "dc7ccf7a-5c3f-44de-e36a-6a5c62b24a06"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD CACHED KD DATA FROM GOOGLE DRIVE (run this cell first!)\n",
    "# ============================================================\n",
    "# Mount Google Drive and copy cached KD data back to local storage\n",
    "# This avoids regenerating the cache every session\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create local cache directory\n",
    "!mkdir -p caches\n",
    "\n",
    "# Choose which cache to load (uncomment the one you need)\n",
    "#CACHE_NAME = \"Q4B_alpaca_chat_think_L128_K32_R256\"  # For Qwen3-4B thinking model\n",
    "CACHE_NAME = \"alpaca_chat_think_L128_K32_R256\"  # For Qwen3-4B thinking model\n",
    "\n",
    "# Copy from Google Drive to local\n",
    "print(f\"[cache] Copying {CACHE_NAME}.tgz from Google Drive...\")\n",
    "!rsync -ah --info=progress2 \\\n",
    "  /content/drive/MyDrive/qwen3_caches/{CACHE_NAME}.tgz \\\n",
    "  caches/\n",
    "\n",
    "# Unzip the cache\n",
    "print(f\"[cache] Extracting {CACHE_NAME}.tgz...\")\n",
    "!tar -xzf caches/{CACHE_NAME}.tgz -C .\n",
    "\n",
    "# Verify extraction\n",
    "import os\n",
    "if os.path.isdir(f\"caches/{CACHE_NAME}\"):\n",
    "    num_shards = len([f for f in os.listdir(f\"caches/{CACHE_NAME}\") if f.startswith(\"shard_\")])\n",
    "    print(f\"[cache] Successfully loaded {CACHE_NAME} with {num_shards} shards\")\n",
    "else:\n",
    "    print(f\"[cache] ERROR: Failed to extract {CACHE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NKhv4eaYs4qR",
   "metadata": {
    "id": "NKhv4eaYs4qR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IIIWEkllwGEA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "IIIWEkllwGEA",
    "outputId": "f3666aa9-120c-470a-e75c-f0a9eb45a665"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE CACHED KD DATA TO GOOGLE DRIVE (run after generating cache)\n",
    "# ============================================================\n",
    "# This saves the generated cache to Google Drive for future sessions\n",
    "# Only run this AFTER you've generated the cache with precompute_teacher_topk.py\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create destination directory\n",
    "!mkdir -p /content/drive/MyDrive/qwen3_caches\n",
    "\n",
    "# Choose which cache to save (should match what you generated)\n",
    "CACHE_NAME = \"alpaca_chat_think_L128_K32_R256\"  # For Qwen3-4B thinking model\n",
    "#CACHE_NAME = \"Q4B_alpaca_chat_think_L128_K32_R256\"  # For Qwen3-4B thinking model\n",
    "\n",
    "import os\n",
    "\n",
    "# Check if cache exists and compress if needed\n",
    "if os.path.isdir(f\"caches/{CACHE_NAME}\"):\n",
    "    # Compress if not already compressed\n",
    "    if not os.path.exists(f\"caches/{CACHE_NAME}.tgz\"):\n",
    "        print(f\"[gzip] Compressing {CACHE_NAME}...\")\n",
    "        !tar -zcvf caches/{CACHE_NAME}.tgz -C caches {CACHE_NAME}\n",
    "    \n",
    "    # Copy to Google Drive\n",
    "    print(f\"[save] Copying {CACHE_NAME}.tgz to Google Drive...\")\n",
    "    !rsync -ah --info=progress2 caches/{CACHE_NAME}.tgz /content/drive/MyDrive/qwen3_caches/\n",
    "    \n",
    "    # Verify\n",
    "    gd_size = os.path.getsize(f\"/content/drive/MyDrive/qwen3_caches/{CACHE_NAME}.tgz\")\n",
    "    print(f\"[save] Saved to Google Drive: {gd_size / (1024**3):.2f} GB\")\n",
    "else:\n",
    "    print(f\"[save] ERROR: Cache directory caches/{CACHE_NAME} not found\")\n",
    "    print(\"[save] Run precompute_teacher_topk.py first to generate the cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AHSsmdhzsBTo",
   "metadata": {
    "id": "AHSsmdhzsBTo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A7UPbdfK3c2x",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A7UPbdfK3c2x",
    "outputId": "c2d4109c-4023-487d-e0ba-e952afef548e"
   },
   "outputs": [],
   "source": [
    "RUN_DIR = \"runs/qwen3_kdqat_stream_q2\"\n",
    "\n",
    "# DISABLED --- NOTE used! see #5 for first QAT step\n",
    "# Construct the command string in Python to ensure variable interpolation\n",
    "command_str = f\"\"\"python scripts/train_qat.py \\\n",
    "  --model_name_or_path {MODEL_NAME} \\\n",
    "  --teacher_model_name_or_path {MODEL_NAME} \\\n",
    "  --distill_weight 1.0 \\\n",
    "  --distill_temperature 2.0 \\\n",
    "  --dataset_name allenai/c4 \\\n",
    "  --dataset_config_name en \\\n",
    "  --dataset_split train \\\n",
    "  --dataset_format text \\\n",
    "  --dataset_text_field text \\\n",
    "  --streaming \\\n",
    "  --shuffle_buffer 10000 \\\n",
    "  --output_dir {RUN_DIR} \\\n",
    "  --device {DEVICE} \\\n",
    "  --amp_dtype {AMP_DTYPE} \\\n",
    "  --param_dtype {PARAM_DTYPE} \\\n",
    "  -q {QUANT_BITS} \\\n",
    "  --max_length 128 \\\n",
    "  --per_device_train_batch_size 1 \\\n",
    "  --gradient_accumulation_steps 16 \\\n",
    "  --learning_rate 5e-6 \\\n",
    "  --warmup_steps 0 \\\n",
    "  --max_steps 50 \\\n",
    "  --skip_lm_head \\\n",
    "  --ema_decay 0 \\\n",
    "  --logging_steps 10 \\\n",
    "  --save_steps 50\"\"\"\n",
    "\n",
    "# Execute the constructed command string\n",
    "!{command_str}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UmgbC-RI3c2x",
   "metadata": {
    "id": "UmgbC-RI3c2x"
   },
   "source": [
    "### (Optional) Resume\n",
    "\n",
    "`--resume_from_checkpoint auto` resolves to `checkpoint_last.pt` if it exists in the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mJqdmXSA3c2x",
   "metadata": {
    "id": "mJqdmXSA3c2x"
   },
   "outputs": [],
   "source": [
    "# !python scripts/train_qat.py ... --output_dir {RUN_DIR} --max_steps 500 --resume_from_checkpoint auto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "R7UZBhau3c2x",
   "metadata": {
    "id": "R7UZBhau3c2x"
   },
   "source": [
    "## 5)  KD-cache: precompute teacher top-k + negatives\n",
    "\n",
    "Cache mode is MPS-friendly:\n",
    "- no teacher model during training\n",
    "- no full-vocab logits\n",
    "\n",
    "If you see good KD loss but bad greedy decoding, increase negative coverage (`--rand_neg`) and/or add hard top-1 terms:\n",
    "- `--hard-top1-weight 0.05`\n",
    "- `--hard-full-top1-weight 0.02`–`0.05`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g4s_PzQW3c2x",
   "metadata": {
    "id": "g4s_PzQW3c2x"
   },
   "source": [
    "### KD-cache QAT training\n",
    "\n",
    "This uses cached teacher signals + candidate softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06LaYj0vPIE7",
   "metadata": {
    "id": "06LaYj0vPIE7"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1Li8Aysa3c2x",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Li8Aysa3c2x",
    "outputId": "26d184d7-2ea7-4389-84e4-765801cb1a19"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STAGE 1: KD-QAT (Conservative - freeze MLP/Attention)\n",
    "# ============================================================\n",
    "# First training stage with frozen output layers for stability\n",
    "\n",
    "%pwd\n",
    "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
    "\n",
    "CACHE_DIR = CACHE_DIR_CHAT\n",
    "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2\"\n",
    "\n",
    "!python scripts/train_qat.py \\\n",
    "  --model_name_or_path {MODEL_NAME} \\\n",
    "  --output_dir {RUN_DIR_CACHE} \\\n",
    "  --device {DEVICE} \\\n",
    "  --amp_dtype {AMP_DTYPE} \\\n",
    "  --param_dtype {PARAM_DTYPE} \\\n",
    "  -q {QUANT_BITS} \\\n",
    "  --max_length 128 \\\n",
    "  --per_device_train_batch_size 1 \\\n",
    "  --gradient_accumulation_steps 4 \\\n",
    "  --learning_rate 5e-6 \\\n",
    "  --warmup_steps 0 \\\n",
    "  --max_steps 1000 \\\n",
    "  --save_steps 3000 \\\n",
    "  --logging_steps 5 \\\n",
    "  --skip_lm_head \\\n",
    "  --ema_decay 0 \\\n",
    "  --kd_cache_dir {CACHE_DIR} \\\n",
    "  --kd_cache_shuffle_files \\\n",
    "  --distill_temperature 2.0 \\\n",
    "  --distill_weight 1.0 \\\n",
    "  --hard-top1-weight 0.05 \\\n",
    "  --hard-full-top1-weight 0.03 \\\n",
    "  --ov-freeze \\\n",
    "  --freeze-last-mlp \\\n",
    "  --freeze-last-mlp-layers 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UKgDA-7OuA8m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UKgDA-7OuA8m",
    "outputId": "e103c99b-f149-4228-cfae-9a0654b32c17"
   },
   "outputs": [],
   "source": [
    "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_2\"\n",
    "!python scripts/run_inference.py \\\n",
    "  --model_name_or_path {MODEL_NAME} \\\n",
    "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
    "  --device {DEVICE} \\\n",
    "  --dtype {DTYPE} \\\n",
    "  -q {QUANT_BITS} \\\n",
    "  --skip_lm_head \\\n",
    "  --prompt \"What capital city of France is?\" \\\n",
    "  --do_sample true \\\n",
    "  --max_new_tokens 64\n",
    "\n",
    "#  --prompt \"What is Capital of france?\" \\\n",
    "#   --prompt \"What is Apple Neural Engine?\" \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexJNkyJyMgv",
   "metadata": {
    "id": "sexJNkyJyMgv"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb19fb17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eb19fb17",
    "outputId": "f0f9da64-8663-4d1f-e050-c579e20f1ff5"
   },
   "outputs": [],
   "source": [
    "# Define source and destination paths\n",
    "SOURCE_FILE = \"runs/qwen3_kdqat_cache_q2_2/qat_state_dict.pt\"\n",
    "DEST_DIR_GD = \"/content/drive/MyDrive/runs/Q4B/q2_2/\"\n",
    "\n",
    "# Ensure the destination directory exists on Google Drive\n",
    "!mkdir -p {DEST_DIR_GD}\n",
    "\n",
    "# Copy the file to Google Drive\n",
    "!cp -v {SOURCE_FILE} {DEST_DIR_GD}\n",
    "print(f\"Copied {SOURCE_FILE} to {DEST_DIR_GD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hbfDaYP5yN6-",
   "metadata": {
    "id": "hbfDaYP5yN6-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JUvQIIDeRUF6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JUvQIIDeRUF6",
    "outputId": "30e018b2-2edf-4269-95c6-fc33537bb356"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STAGE 2: KD-QAT (Unfrozen layers, resume from Stage 1)\n",
    "# ============================================================\n",
    "# Continue training with all layers unfrozen\n",
    "\n",
    "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
    "\n",
    "CACHE_DIR = CACHE_DIR_CHAT\n",
    "INIT_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2\"\n",
    "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_2\"\n",
    "\n",
    "!python scripts/train_qat.py \\\n",
    "  --model_name_or_path {MODEL_NAME} \\\n",
    "  --init_model_state {INIT_DIR_CACHE}/qat_state_dict.pt \\\n",
    "  --output_dir {RUN_DIR_CACHE} \\\n",
    "  --device {DEVICE} \\\n",
    "  --amp_dtype {AMP_DTYPE} \\\n",
    "  --param_dtype {PARAM_DTYPE} \\\n",
    "  -q {QUANT_BITS} \\\n",
    "  --max_length 128 \\\n",
    "  --per_device_train_batch_size 1 \\\n",
    "  --gradient_accumulation_steps 4 \\\n",
    "  --learning_rate 5e-6 \\\n",
    "  --warmup_steps 0 \\\n",
    "  --max_steps 1000 \\\n",
    "  --save_steps 3000 \\\n",
    "  --logging_steps 10 \\\n",
    "  --skip_lm_head \\\n",
    "  --ema_decay 0 \\\n",
    "  --kd_cache_dir {CACHE_DIR} \\\n",
    "  --kd_cache_shuffle_files \\\n",
    "  --distill_temperature 2.0 \\\n",
    "  --distill_weight 1.0 \\\n",
    "  --ov-freeze \\\n",
    "  --hard-top1-weight 0.02 \\\n",
    "  --hard-full-top1-weight 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qsfHq1gvtpg-",
   "metadata": {
    "id": "qsfHq1gvtpg-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KXQUTgEJfzl4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KXQUTgEJfzl4",
    "outputId": "828b7fad-a610-4afe-8df2-7fd2177279b4"
   },
   "outputs": [],
   "source": [
    "# Stage 3 resume KD-QAT with unfrozen attention and relaxed hard-top/full!\n",
    "\n",
    "CACHE_DIR = \"caches/alpaca_chat_think_both_L128_K32_R256\"\n",
    "INIT_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_2\"\n",
    "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_3\"\n",
    "\n",
    "!python scripts/train_qat.py \\\n",
    "  --model_name_or_path {MODEL_NAME} \\\n",
    "  --init_model_state {INIT_DIR_CACHE}/qat_state_dict.pt \\\n",
    "  --output_dir {RUN_DIR_CACHE} \\\n",
    "  --device {DEVICE} \\\n",
    "  --amp_dtype {AMP_DTYPE} \\\n",
    "  --param_dtype {PARAM_DTYPE} \\\n",
    "  -q {QUANT_BITS} \\\n",
    "  --max_length 128 \\\n",
    "  --per_device_train_batch_size 128 \\\n",
    "  --gradient_accumulation_steps 1 \\\n",
    "  --learning_rate 5e-6 \\\n",
    "  --warmup_steps 0 \\\n",
    "  --max_steps 1000 \\\n",
    "  --save_steps 3000 \\\n",
    "  --logging_steps 10 \\\n",
    "  --skip_lm_head \\\n",
    "  --ema_decay 0 \\\n",
    "  --kd_cache_dir {CACHE_DIR} \\\n",
    "  --kd_cache_shuffle_files \\\n",
    "  --distill_temperature 2.0 \\\n",
    "  --distill_weight 1.0 \\\n",
    "  --hard-top1-weight 0.00 \\\n",
    "  --hard-full-top1-weight 0.0005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q1Zjl_WnHZ4O",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q1Zjl_WnHZ4O",
    "outputId": "70760464-2645-4b89-8a8f-d4467e95a477"
   },
   "outputs": [],
   "source": [
    "# Stage 4 resume KD-QAT with unfrozen attention and relaxed hard-top/full!\n",
    "#   --hard-full-top1-weight 0.0000\n",
    "#   learning_rate 2e-6\n",
    "CACHE_DIR = \"caches/alpaca_chat_think_both_L128_K32_R256\"\n",
    "INIT_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_3\"\n",
    "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_4\"\n",
    "\n",
    "!python scripts/train_qat.py \\\n",
    "  --model_name_or_path {MODEL_NAME} \\\n",
    "  --init_model_state {INIT_DIR_CACHE}/qat_state_dict.pt \\\n",
    "  --output_dir {RUN_DIR_CACHE} \\\n",
    "  --device {DEVICE} \\\n",
    "  --amp_dtype {AMP_DTYPE} \\\n",
    "  --param_dtype {PARAM_DTYPE} \\\n",
    "  -q {QUANT_BITS} \\\n",
    "  --max_length 128 \\\n",
    "  --per_device_train_batch_size 160 \\\n",
    "  --gradient_accumulation_steps 1 \\\n",
    "  --learning_rate 2e-6 \\\n",
    "  --warmup_steps 0 \\\n",
    "  --max_steps 500 \\\n",
    "  --save_steps 3000 \\\n",
    "  --logging_steps 5 \\\n",
    "  --skip_lm_head \\\n",
    "  --ema_decay 0 \\\n",
    "  --kd_cache_dir {CACHE_DIR} \\\n",
    "  --kd_cache_shuffle_files \\\n",
    "  --distill_temperature 2.0 \\\n",
    "  --distill_weight 1.0 \\\n",
    "  --hard-top1-weight 0.00 \\\n",
    "  --hard-full-top1-weight 0.0000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acak6adahpf",
   "metadata": {},
   "source": [
    "## 5.5) Progressive Layer-by-Layer QAT (Experimental)\n",
    "\n",
    "This approach trains one layer at a time with:\n",
    "- **Local reconstruction loss**: MSE between quantized and fp MLP outputs\n",
    "- **Global KD loss**: Cached teacher logits\n",
    "- **Prefix quantized / suffix fp**: Earlier layers stay quantized, later layers use full precision\n",
    "\n",
    "### Recommended Training Order (most stable first):\n",
    "\n",
    "1. **E2E f-only** (Option 1): Train ALL f parameters at once\n",
    "   - Skip progressive passes, just run Pass 4\n",
    "   - Most stable, fastest validation\n",
    "   \n",
    "2. **Progressive f-only** (Option 2): Layer-by-layer f-param training\n",
    "   - Uses `--train_f_only` flag\n",
    "   - Disable local loss with `--local_weight 0.0`\n",
    "   \n",
    "3. **Full progressive** (Option 3): Train weights + f per layer\n",
    "   - Most aggressive, may show instability at later layers\n",
    "\n",
    "### GPU Configuration:\n",
    "\n",
    "| GPU | Recommended batch_size |\n",
    "|-----|------------------------|\n",
    "| T4 (15GB) | 2-4 |\n",
    "| V100 (32GB) | 4-8 |\n",
    "| A100 (40GB) | 8-16 |\n",
    "| A100 (80GB) / H100 | 16-32 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scxwylalctd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Progressive QAT Config ----\n",
    "# Adjust batch_size for your GPU (A100: 8-16, V100: 4-8, T4: 2-4)\n",
    "BATCH_SIZE = 8                # Increase for faster instances (A100/H100)\n",
    "STEPS_PER_LAYER_MLP = 50      # Steps per MLP layer (Pass 1 + Pass 3)\n",
    "STEPS_PER_LAYER_ATTN = 30     # Steps per attention layer (Pass 2)\n",
    "E2E_STEPS = 500               # E2E quantizer tuning steps (Pass 4)\n",
    "LOCAL_WEIGHT = 0.3            # Local reconstruction loss weight\n",
    "GLOBAL_WEIGHT = 1.0           # Global KD loss weight\n",
    "LOCAL_TOKEN_SAMPLES = 128     # Tokens to sample for local loss\n",
    "MAX_GRAD_NORM = 1.0           # Gradient clipping (important for 2-bit)\n",
    "\n",
    "# Learning rates\n",
    "LR_PROGRESSIVE = 5e-6         # Learning rate for progressive passes\n",
    "LR_E2E = 1e-4                 # Learning rate for E2E f-only tuning\n",
    "\n",
    "# Output directories\n",
    "RUN_DIR_E2E_FONLY = \"runs/e2e_f_only\"\n",
    "RUN_DIR_PROGRESSIVE_FONLY = \"runs/progressive_f_only\"\n",
    "RUN_DIR_PROGRESSIVE = \"runs/progressive_qat_v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q9f32hx0qo",
   "metadata": {},
   "source": [
    "### Option 1: E2E f-only Training (Recommended First)\n",
    "\n",
    "**Most stable approach** - trains ALL `_f_param` (quantization scales) simultaneously.\n",
    "Skip all progressive layer-by-layer passes and go straight to Pass 4.\n",
    "\n",
    "This is recommended when:\n",
    "- Progressive layer-by-layer shows instability (local loss hitting 10.0)\n",
    "- You want to validate the infrastructure works before trying progressive\n",
    "- You have limited time and want the fastest path to a working checkpoint\n",
    "\n",
    "The `f` parameter is the learnable quantization scale from Apple-style quantization:\n",
    "- Actual scale `s = softplus(f)` ensures positivity\n",
    "- Training only `f` keeps weights frozen - more stable for ultra-low-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vh3iifkgpj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E2E f-only: Skip ALL progressive passes, train all f parameters at once\n",
    "# This is the simplest and most stable approach\n",
    "\n",
    "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
    "\n",
    "!python scripts/train_qat_progressive.py \\\n",
    "  --model_name_or_path {MODEL_NAME} \\\n",
    "  --output_dir {RUN_DIR_E2E_FONLY} \\\n",
    "  --device {DEVICE} \\\n",
    "  --amp_dtype {AMP_DTYPE} \\\n",
    "  --param_dtype {PARAM_DTYPE} \\\n",
    "  -q {QUANT_BITS} \\\n",
    "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
    "  --batch_size {BATCH_SIZE} \\\n",
    "  --skip_mlp_pass \\\n",
    "  --skip_attention_pass \\\n",
    "  --skip_mlp_refinement \\\n",
    "  --e2e_steps {E2E_STEPS} \\\n",
    "  --e2e_learning_rate {LR_E2E} \\\n",
    "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
    "  --logging_steps 10 \\\n",
    "  --skip_lm_head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vab9x1zlqhs",
   "metadata": {},
   "source": [
    "### Option 2: Progressive f-only Training\n",
    "\n",
    "Layer-by-layer training but only trains `_f_param` (quantization scales), not weights.\n",
    "More stable than full progressive training, but may still see instability at later layers.\n",
    "\n",
    "Use `--train_f_only` flag to freeze weights and only train quantization scales per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oqxrx3c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progressive f-only: Layer-by-layer, but only train quantization scales\n",
    "# Use --train_f_only for more stable training\n",
    "\n",
    "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
    "\n",
    "!python scripts/train_qat_progressive.py \\\n",
    "  --model_name_or_path {MODEL_NAME} \\\n",
    "  --output_dir {RUN_DIR_PROGRESSIVE_FONLY} \\\n",
    "  --device {DEVICE} \\\n",
    "  --amp_dtype {AMP_DTYPE} \\\n",
    "  --param_dtype {PARAM_DTYPE} \\\n",
    "  -q {QUANT_BITS} \\\n",
    "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
    "  --batch_size {BATCH_SIZE} \\\n",
    "  --train_f_only \\\n",
    "  --steps_per_layer_mlp {STEPS_PER_LAYER_MLP} \\\n",
    "  --steps_per_layer_attn {STEPS_PER_LAYER_ATTN} \\\n",
    "  --e2e_steps {E2E_STEPS} \\\n",
    "  --local_weight 0.0 \\\n",
    "  --global_weight {GLOBAL_WEIGHT} \\\n",
    "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
    "  --learning_rate {LR_PROGRESSIVE} \\\n",
    "  --e2e_learning_rate {LR_E2E} \\\n",
    "  --logging_steps 10 \\\n",
    "  --skip_lm_head \\\n",
    "  --skip_mlp_refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0gsxkcc4",
   "metadata": {},
   "source": [
    "### Option 3: Full Progressive Training (weights + f)\n",
    "\n",
    "Full layer-by-layer training with weights and quantization scales.\n",
    "Most aggressive but potentially unstable for ultra-low-bit (2-bit).\n",
    "\n",
    "**Training Order (3-pass v3):**\n",
    "1. **Pass 1**: Train MLP layers (local reconstruction + global KD)\n",
    "2. **Pass 2**: Train attention layers (global KD only)\n",
    "3. **Pass 3**: MLP refinement (addresses MLP-attention coupling)\n",
    "4. **Pass 4**: E2E quantizer-only tuning (f-param only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7qzyw0rd18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Progressive: MLP pass + E2E f-only (skip attention/refinement for v1)\n",
    "# For full 3-pass training, remove --skip_attention_pass and --skip_mlp_refinement\n",
    "\n",
    "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
    "\n",
    "!python scripts/train_qat_progressive.py \\\n",
    "  --model_name_or_path {MODEL_NAME} \\\n",
    "  --output_dir {RUN_DIR_PROGRESSIVE} \\\n",
    "  --device {DEVICE} \\\n",
    "  --amp_dtype {AMP_DTYPE} \\\n",
    "  --param_dtype {PARAM_DTYPE} \\\n",
    "  -q {QUANT_BITS} \\\n",
    "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
    "  --batch_size {BATCH_SIZE} \\\n",
    "  --steps_per_layer_mlp {STEPS_PER_LAYER_MLP} \\\n",
    "  --e2e_steps {E2E_STEPS} \\\n",
    "  --local_weight {LOCAL_WEIGHT} \\\n",
    "  --global_weight {GLOBAL_WEIGHT} \\\n",
    "  --local_token_samples {LOCAL_TOKEN_SAMPLES} \\\n",
    "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
    "  --learning_rate {LR_PROGRESSIVE} \\\n",
    "  --e2e_learning_rate {LR_E2E} \\\n",
    "  --logging_steps 10 \\\n",
    "  --skip_lm_head \\\n",
    "  --skip_attention_pass \\\n",
    "  --skip_mlp_refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qiaq60t466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v3: Full 3-pass progressive training\n",
    "# MLP -> Attention -> MLP refinement -> E2E f-only\n",
    "# WARNING: May show instability at later layers for 2-bit\n",
    "\n",
    "RUN_DIR_PROGRESSIVE_V3 = \"runs/progressive_qat_v3\"\n",
    "\n",
    "!python scripts/train_qat_progressive.py \\\n",
    "  --model_name_or_path {MODEL_NAME} \\\n",
    "  --output_dir {RUN_DIR_PROGRESSIVE_V3} \\\n",
    "  --device {DEVICE} \\\n",
    "  --amp_dtype {AMP_DTYPE} \\\n",
    "  --param_dtype {PARAM_DTYPE} \\\n",
    "  -q {QUANT_BITS} \\\n",
    "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
    "  --batch_size {BATCH_SIZE} \\\n",
    "  --steps_per_layer_mlp {STEPS_PER_LAYER_MLP} \\\n",
    "  --steps_per_layer_attn {STEPS_PER_LAYER_ATTN} \\\n",
    "  --e2e_steps {E2E_STEPS} \\\n",
    "  --local_weight {LOCAL_WEIGHT} \\\n",
    "  --global_weight {GLOBAL_WEIGHT} \\\n",
    "  --local_token_samples {LOCAL_TOKEN_SAMPLES} \\\n",
    "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
    "  --learning_rate {LR_PROGRESSIVE} \\\n",
    "  --e2e_learning_rate {LR_E2E} \\\n",
    "  --logging_steps 10 \\\n",
    "  --skip_lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ix3xjm2h0xq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot per-layer training progress\n",
    "# Change PLOT_RUN to visualize different runs\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Choose which run to visualize\n",
    "PLOT_RUN = RUN_DIR_E2E_FONLY  # or RUN_DIR_PROGRESSIVE, RUN_DIR_PROGRESSIVE_V3\n",
    "\n",
    "csv_path = f\"{PLOT_RUN}/loss_per_layer.csv\"\n",
    "if not os.path.exists(csv_path):\n",
    "    print(f\"Loss CSV not found at {csv_path}\")\n",
    "    print(\"Run training first or check the path.\")\n",
    "else:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    # Pass 1: MLP training (local loss)\n",
    "    mlp_df = df[(df['pass'] == 1) & (df['component'] == 'mlp')]\n",
    "    if not mlp_df.empty and 'local' in mlp_df.columns:\n",
    "        for layer in mlp_df['layer'].unique():\n",
    "            layer_df = mlp_df[mlp_df['layer'] == layer]\n",
    "            axes[0, 0].plot(layer_df['step'], layer_df['local'], label=f'L{layer}', alpha=0.7)\n",
    "        axes[0, 0].set_title('Pass 1: MLP Local Loss per Layer')\n",
    "        axes[0, 0].set_xlabel('Step')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend(ncol=4, fontsize=6)\n",
    "    else:\n",
    "        axes[0, 0].set_title('Pass 1: MLP Local Loss (skipped or no local loss)')\n",
    "\n",
    "    # Pass 1: MLP global loss\n",
    "    if not mlp_df.empty and 'global' in mlp_df.columns:\n",
    "        for layer in mlp_df['layer'].unique():\n",
    "            layer_df = mlp_df[mlp_df['layer'] == layer]\n",
    "            axes[0, 1].plot(layer_df['step'], layer_df['global'], label=f'L{layer}', alpha=0.7)\n",
    "        axes[0, 1].set_title('Pass 1: MLP Global KD Loss per Layer')\n",
    "        axes[0, 1].set_xlabel('Step')\n",
    "        axes[0, 1].set_ylabel('Loss')\n",
    "        axes[0, 1].legend(ncol=4, fontsize=6)\n",
    "    else:\n",
    "        axes[0, 1].set_title('Pass 1: MLP Global Loss (skipped)')\n",
    "\n",
    "    # Pass 2: Attention training\n",
    "    attn_df = df[(df['pass'] == 2) & (df['component'] == 'attn')]\n",
    "    if not attn_df.empty and 'global' in attn_df.columns:\n",
    "        for layer in attn_df['layer'].unique():\n",
    "            layer_df = attn_df[attn_df['layer'] == layer]\n",
    "            axes[1, 0].plot(layer_df['step'], layer_df['global'], label=f'L{layer}', alpha=0.7)\n",
    "        axes[1, 0].set_title('Pass 2: Attention Global KD Loss per Layer')\n",
    "        axes[1, 0].set_xlabel('Step')\n",
    "        axes[1, 0].set_ylabel('Loss')\n",
    "        axes[1, 0].legend(ncol=4, fontsize=6)\n",
    "    else:\n",
    "        axes[1, 0].set_title('Pass 2: Attention (skipped)')\n",
    "        axes[1, 0].text(0.5, 0.5, 'Not run', ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "\n",
    "    # Pass 4: E2E f-only tuning\n",
    "    e2e_df = df[(df['pass'] == 4)]\n",
    "    if not e2e_df.empty and 'global' in e2e_df.columns:\n",
    "        axes[1, 1].plot(e2e_df['step'], e2e_df['global'], 'b-', linewidth=2)\n",
    "        axes[1, 1].set_title('Pass 4: E2E f-only Tuning')\n",
    "        axes[1, 1].set_xlabel('Step')\n",
    "        axes[1, 1].set_ylabel('Global KD Loss')\n",
    "    else:\n",
    "        axes[1, 1].set_title('Pass 4: E2E (not yet run)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{PLOT_RUN}/loss_per_layer.png\", dpi=150)\n",
    "    plt.show()\n",
    "    print(f\"Saved to {PLOT_RUN}/loss_per_layer.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qgtw9rd3cr",
   "metadata": {},
   "source": [
    "### Inference Check: Progressive QAT Results\n",
    "\n",
    "Test the progressive QAT checkpoint with a quick inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4coakmebsik",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference with progressive QAT checkpoint\n",
    "# Change RUN_DIR to test different runs:\n",
    "#   RUN_DIR_E2E_FONLY, RUN_DIR_PROGRESSIVE_FONLY, RUN_DIR_PROGRESSIVE\n",
    "\n",
    "TEST_RUN = RUN_DIR_E2E_FONLY  # Change this to test other runs\n",
    "\n",
    "!python scripts/run_inference.py \\\n",
    "  --model_name_or_path {MODEL_NAME} \\\n",
    "  --qat_checkpoint {TEST_RUN}/qat_state_dict.pt \\\n",
    "  --device {DEVICE} \\\n",
    "  --dtype {DTYPE} \\\n",
    "  -q {QUANT_BITS} \\\n",
    "  --skip_lm_head \\\n",
    "  --prompt \"What is the capital of France?\" \\\n",
    "  --do_sample true \\\n",
    "  --max_new_tokens 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Yq8GHX0o3c2y",
   "metadata": {
    "id": "Yq8GHX0o3c2y"
   },
   "source": [
    "## 6) Stage B: LoRA recovery\n",
    "\n",
    "Two options:\n",
    "- **SFT LoRA** (Alpaca-style instruction tuning)\n",
    "- **Cached KD-LoRA** (preserve teacher distribution; no new “skills”)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B1yimj8aS11y",
   "metadata": {
    "id": "B1yimj8aS11y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LcGFKyxO3c2y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LcGFKyxO3c2y",
    "outputId": "163cdb4c-62cf-4b8a-c440-988249c1ffa0"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STAGE B: LoRA Recovery (Cached KD-LoRA)\n",
    "# ============================================================\n",
    "# Train LoRA adapters on top of QAT checkpoint\n",
    "\n",
    "CACHE_DIR = CACHE_DIR_CHAT\n",
    "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2\"\n",
    "\n",
    "LORA_DIM = 32\n",
    "LORA_RUN_KD = f\"runs/qwen3_lora_recovery_cached_r{LORA_DIM}\"\n",
    "\n",
    "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
    "\n",
    "!python scripts/train_lora_recovery.py \\\n",
    "  --model_name_or_path {MODEL_NAME} \\\n",
    "  --qat_checkpoint {RUN_DIR_CACHE}/qat_state_dict.pt \\\n",
    "  --output_dir {LORA_RUN_KD} \\\n",
    "  --device {DEVICE} \\\n",
    "  --amp_dtype {AMP_DTYPE} \\\n",
    "  --param_dtype {PARAM_DTYPE} \\\n",
    "  -q {QUANT_BITS} \\\n",
    "  --per_device_train_batch_size 16 \\\n",
    "  --gradient_accumulation_steps 2 \\\n",
    "  --learning_rate 1e-5 \\\n",
    "  --warmup_steps 0 \\\n",
    "  --max_steps 1000 \\\n",
    "  --save_steps 3000 \\\n",
    "  --logging_steps 2 \\\n",
    "  --skip_lm_head \\\n",
    "  --lora_r {LORA_DIM} \\\n",
    "  --lora_alpha {LORA_DIM} \\\n",
    "  --lora_dropout 0.0 \\\n",
    "  --kd_cache_dir {CACHE_DIR} \\\n",
    "  --kd_cache_shuffle_files \\\n",
    "  --distill_temperature 2.0 \\\n",
    "  --distill_weight 1.0 \\\n",
    "  --hard-top1-weight 0.02 \\\n",
    "  --hard-full-top1-weight 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Defule6L3c2y",
   "metadata": {
    "id": "Defule6L3c2y"
   },
   "source": [
    "## 7) Plot loss\n",
    "\n",
    "In Colab, use `--no_show` + `--save` then display the PNG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QM1RNfDh3c2y",
   "metadata": {
    "id": "QM1RNfDh3c2y"
   },
   "outputs": [],
   "source": [
    "!python scripts/plot_loss.py --run_dir {RUN_DIR} --source csv --no_show --save {RUN_DIR}/loss.png\n",
    "from PIL import Image\n",
    "display(Image.open(f\"{RUN_DIR}/loss.png\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p3bOUiMt3c2y",
   "metadata": {
    "id": "p3bOUiMt3c2y"
   },
   "source": [
    "## 8) Inference sanity checks\n",
    "\n",
    "Greedy decode (`--do_sample false`) and keep outputs short (`--max_new_tokens 16`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aA_cJHvT3c2y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aA_cJHvT3c2y",
    "outputId": "86a58a24-0dc1-4cef-c507-4ea378203f82"
   },
   "outputs": [],
   "source": [
    "RUN_DIR = \"runs/qwen3_kdqat_cache_q2\"\n",
    "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
    "\n",
    "!python scripts/run_inference.py \\\n",
    "  --model_name_or_path {MODEL_NAME} \\\n",
    "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
    "  --lora_checkpoint \"runs/qwen3_lora_recovery_cached_r32/lora_only_state_dict.pt\" \\\n",
    "  --device {DEVICE} \\\n",
    "  --dtype {DTYPE} \\\n",
    "  -q {QUANT_BITS} \\\n",
    "  --skip_lm_head \\\n",
    "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
    "  --prompt \"What is capital of France?\" \\\n",
    "  --do_sample false \\\n",
    "  --enable_thinking true \\\n",
    "  --max_new_tokens 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EJlu4sOZCNpY",
   "metadata": {
    "id": "EJlu4sOZCNpY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cD985HdXlm0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1cD985HdXlm0",
    "outputId": "927291c4-1309-42a5-b857-f9b5608a862a"
   },
   "outputs": [],
   "source": [
    "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_2\"\n",
    "!python scripts/run_inference.py \\\n",
    "  --model_name_or_path {MODEL_NAME} \\\n",
    "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
    "  --lora_checkpoint \"runs/qwen3_lora_recovery_cached_r64/lora_only_state_dict.pt\" \\\n",
    "  --device {DEVICE} \\\n",
    "  --dtype {DTYPE} \\\n",
    "  -q {QUANT_BITS} \\\n",
    "  --skip_lm_head \\\n",
    "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
    "  --prompt \"What is Apple Neural Engine?\" \\\n",
    "  --do_sample false \\\n",
    "  --enable_thinking true \\\n",
    "  --max_new_tokens 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xZ6aSDAa3c2y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xZ6aSDAa3c2y",
    "outputId": "c1a67a91-e1a6-4553-b0f9-17d5fb5f7949"
   },
   "outputs": [],
   "source": [
    "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_2\"\n",
    "!python scripts/run_inference.py \\\n",
    "  --model_name_or_path {MODEL_NAME} \\\n",
    "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
    "  --lora_checkpoint \"runs/qwen3_lora_recovery_cached_r64/lora_only_state_dict.pt\" \\\n",
    "  --device {DEVICE} \\\n",
    "  --dtype {DTYPE} \\\n",
    "  -q {QUANT_BITS} \\\n",
    "  --skip_lm_head \\\n",
    "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
    "  --prompt \"2+2=\" \\\n",
    "  --do_sample false \\\n",
    "  --enable_thinking true \\\n",
    "  --max_new_tokens 90\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZOIo97VlyAbh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZOIo97VlyAbh",
    "outputId": "59872cb3-0827-44a0-df78-f53d1ac048a4"
   },
   "outputs": [],
   "source": [
    "LORA_DIM = 64\n",
    "RUN_DIR = \"runs/qwen3_kdqat_cache_q2\"\n",
    "!python scripts/run_inference.py \\\n",
    "  --model_name_or_path {MODEL_NAME} \\\n",
    "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
    "  --device {DEVICE} \\\n",
    "  --dtype {DTYPE} \\\n",
    "  -q {QUANT_BITS} \\\n",
    "  --skip_lm_head \\\n",
    "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
    "  --prompt \"What is capital of France?\" \\\n",
    "  --do_sample true \\\n",
    "  --max_new_tokens 64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ef_Q8P0i3c2y",
   "metadata": {
    "id": "Ef_Q8P0i3c2y"
   },
   "source": [
    "## 9) Optional: snap weights to the exact grid\n",
    "\n",
    "This produces a float checkpoint with weights snapped to the N-bit codebook (not bitpacked)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jk_qSZIs3c2y",
   "metadata": {
    "id": "jk_qSZIs3c2y"
   },
   "outputs": [],
   "source": [
    "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_2\"\n",
    "!python scripts/hard_quantize_checkpoint.py \\\n",
    "  --model_name_or_path {MODEL_NAME} \\\n",
    "  --qat_checkpoint {RUN_DIR}/checkpoint_last.pt \\\n",
    "  --output_path {RUN_DIR}/hard_quant_full_state_dict.pt \\\n",
    "  -q {QUANT_BITS} \\\n",
    "  --skip_lm_head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7HnE7IFS3dK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7HnE7IFS3dK",
    "outputId": "2a19fa5f-28b0-4db8-f3a9-12e39aa64150"
   },
   "outputs": [],
   "source": [
    "%cd qwen3_apple_style_2bit_qat_lora\n",
    "%ls -l runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beQD2eyDWWnb",
   "metadata": {
    "id": "beQD2eyDWWnb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00ef006975b84f8c87312dbe1425f921": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "063d57549bdc4b90946e8173797a7900": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "10a9646f4bbd484ab15d21032401363a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "21cbc3b64c344853af951223c7c4e955": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2c06627c65d445a69f0683f64826f597": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "33619148b8e846778394442403694063": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_00ef006975b84f8c87312dbe1425f921",
      "placeholder": "​",
      "style": "IPY_MODEL_10a9646f4bbd484ab15d21032401363a",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "5060a342ef4d42898688600637345d44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_c7636c01eecf453bac5d94ce00dd7d4e",
      "style": "IPY_MODEL_6e2e98db599b4999bd7aacab2eaad671",
      "tooltip": ""
     }
    },
    "6e2e98db599b4999bd7aacab2eaad671": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "93680907491a4699b13afdfa674f12bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "96c08bce8465490e84ed970275181395": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a53a1357d67848c49b047de5845ba284": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_063d57549bdc4b90946e8173797a7900",
      "placeholder": "​",
      "style": "IPY_MODEL_93680907491a4699b13afdfa674f12bf",
      "value": ""
     }
    },
    "afc730ace4234ac5a577a5a5220c6a4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c5b39aec0c1b4bfcb670602c50af7ec2",
       "IPY_MODEL_a53a1357d67848c49b047de5845ba284",
       "IPY_MODEL_e38abc2fa8264921844ca4a71863ca4b",
       "IPY_MODEL_5060a342ef4d42898688600637345d44",
       "IPY_MODEL_33619148b8e846778394442403694063"
      ],
      "layout": "IPY_MODEL_2c06627c65d445a69f0683f64826f597"
     }
    },
    "bcd8b12c61574d908255a9fad3a4bffc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5b39aec0c1b4bfcb670602c50af7ec2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bcd8b12c61574d908255a9fad3a4bffc",
      "placeholder": "​",
      "style": "IPY_MODEL_21cbc3b64c344853af951223c7c4e955",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "c7636c01eecf453bac5d94ce00dd7d4e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d76812431e314faeafb996c7a8a84507": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e38abc2fa8264921844ca4a71863ca4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_d76812431e314faeafb996c7a8a84507",
      "style": "IPY_MODEL_96c08bce8465490e84ed970275181395",
      "value": true
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
