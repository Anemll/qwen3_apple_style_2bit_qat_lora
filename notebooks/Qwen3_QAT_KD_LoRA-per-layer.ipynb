{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anemll/qwen3_apple_style_2bit_qat_lora/blob/main/notebooks/Qwen3_QAT_KD_LoRA-per-layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rRgC0uK43c2v",
      "metadata": {
        "id": "rRgC0uK43c2v"
      },
      "source": [
        "# Qwen3-0.6B — Apple-style QAT (2-bit / 4-bit) + KD + LoRA recovery\n",
        "\n",
        "This notebook mirrors the structure of common “phone deployment” notebooks, but uses **this repo’s** pipeline:\n",
        "\n",
        "- **Stage A (recommended default):** KD-QAT on plain text (C4 streaming) or KD-cache QAT\n",
        "- **Stage B:** LoRA recovery (either SFT or cached KD-LoRA)\n",
        "- Plot `loss.csv`\n",
        "- Run inference sanity checks\n",
        "\n",
        "Notes:\n",
        "- Qwen3 requires `transformers>=4.51.0`.\n",
        "- For disk usage: C4 is huge; prefer `--streaming` unless you explicitly want to download.\n",
        "- Bitwidth: use `-q 2` (default) or `-q 4` (less aggressive). Checkpoints persist the bitwidth per layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i6Pb9Kki3c2w",
      "metadata": {
        "id": "i6Pb9Kki3c2w"
      },
      "source": [
        "## 0) Setup (Colab / local)\n",
        "\n",
        "If you’re in Colab, clone the repo. If you’re already in the repo directory locally, you can skip this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "UXlLPdtkGM35",
      "metadata": {
        "id": "UXlLPdtkGM35"
      },
      "outputs": [],
      "source": [
        "# ---- Config (edit these) ----\n",
        "#MODEL_NAME = 'Qwen/Qwen3-4B-Thinking-2507'\n",
        "MODEL_NAME = 'Qwen/Qwen3-0.6B'\n",
        "TEACHER_NAME = MODEL_NAME\n",
        "QUANT_BITS = 4  # 2 or 4\n",
        "DEVICE = 'auto'\n",
        "AMP_DTYPE = 'auto'\n",
        "PARAM_DTYPE = 'auto'\n",
        "DTYPE = 'auto'\n",
        "\n",
        "# Cache dirs\n",
        "CACHE_DIR_CHAT = 'caches/alpaca_chat_think_L128_K32_R256'\n",
        "CACHE_DIR_TEXT = 'caches/c4_qwen3_L64_K32_R256'\n",
        "#CACHE_DIR_CHAT = 'caches/Q4B_alpaca_chat_think_L128_K32_R256'\n",
        "#CACHE_DIR_TEXT = 'caches/Q4B_c4_qwen3_L64_K32_R256'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "u2vonfu23c2w",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u2vonfu23c2w",
        "outputId": "7439d3b5-ec70-49bc-a1dc-c47b4b2374ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'qwen3_apple_style_2bit_qat_lora' already exists and is not an empty directory.\n",
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (4/4), 565 bytes | 282.00 KiB/s, done.\n",
            "From https://github.com/Anemll/qwen3_apple_style_2bit_qat_lora\n",
            "   f1836c4..b9c2c0d  main       -> origin/main\n",
            "Updating f1836c4..b9c2c0d\n",
            "Fast-forward\n",
            " scripts/train_qat_progressive.py | 4 \u001b[32m++++\u001b[m\n",
            " 1 file changed, 4 insertions(+)\n",
            "HEAD is now at b9c2c0d Implement early backtrack cleanup in QAT training process\n"
          ]
        }
      ],
      "source": [
        "# Colab-only:\n",
        "%cd /content/\n",
        "!git clone https://github.com/Anemll/qwen3_apple_style_2bit_qat_lora\n",
        "%cd qwen3_apple_style_2bit_qat_lora\n",
        "!git fetch\n",
        "!git pull\n",
        "!git reset --hard HEAD\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZUmiISSL3c2w",
      "metadata": {
        "id": "ZUmiISSL3c2w"
      },
      "source": [
        "## 1) Install dependencies (uv)\n",
        "\n",
        "This repo is set up to work with `uv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "ACRjVuVa3c2x",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACRjVuVa3c2x",
        "outputId": "6ad351ae-0f68-4e6c-d45b-25a4b6a89d6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m7 packages\u001b[0m \u001b[2min 95ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m62 packages\u001b[0m \u001b[2min 120ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 738ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 0.37ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 0.80ms\u001b[0m\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mqat-lora\u001b[0m\u001b[2m==0.0.0 (from file:///content/qwen3_apple_style_2bit_qat_lora)\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mqat-lora\u001b[0m\u001b[2m==0.0.0 (from file:///content/qwen3_apple_style_2bit_qat_lora/qwen3_apple_style_2bit_qat_lora)\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip -q install uv\n",
        "!uv pip install -r requirements.txt\n",
        "!uv pip install -e .\n",
        "# plotting\n",
        "!uv pip install -q matplotlib\n",
        "!uv pip install -q plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yfoLPbBk3c2x",
      "metadata": {
        "id": "yfoLPbBk3c2x"
      },
      "source": [
        "## 2) Optional: Hugging Face login\n",
        "\n",
        "If you hit gated model/dataset errors, log in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wLMWX7E23c2x",
      "metadata": {
        "id": "wLMWX7E23c2x"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()  # paste token when prompted\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fA37ilf33c2x",
      "metadata": {
        "id": "fA37ilf33c2x"
      },
      "source": [
        "## 3) Quick environment check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "VUCXehkU3c2x",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUCXehkU3c2x",
        "outputId": "0377d1f6-830e-429e-c7e4-65de61b795ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch 2.9.0+cu126\n",
            "transformers 4.57.3\n",
            "cuda True\n",
            "mps False\n"
          ]
        }
      ],
      "source": [
        "import torch, transformers\n",
        "print('torch', torch.__version__)\n",
        "print('transformers', transformers.__version__)\n",
        "print('cuda', torch.cuda.is_available())\n",
        "print('mps', torch.backends.mps.is_available())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GCXUYtVC3c2x",
      "metadata": {
        "id": "GCXUYtVC3c2x"
      },
      "source": [
        "## 4) Stage A ((Optional)): KD-QAT on streaming C4\n",
        "\n",
        "This preserves the base model’s behavior under low-bit fake-quant weights.\n",
        "\n",
        "Tips:\n",
        "- Start with a small run (`--max_steps 50`) to validate the pipeline.\n",
        "- Use `-q 4` if 2-bit is too unstable; 4-bit is less aggressive.\n",
        "- On MPS, prefer `--ema_decay 0` for KD-QAT.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jERGktjwjz29",
      "metadata": {
        "id": "jERGktjwjz29"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TpT02cskrs6D",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TpT02cskrs6D",
        "outputId": "5e5648b6-a34d-4ada-85c9-2292e31b316f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[device] cuda | dtype=torch.bfloat16\n",
            "tokenizer_config.json: 9.73kB [00:00, 39.0MB/s]\n",
            "vocab.json: 2.78MB [00:00, 65.7MB/s]\n",
            "merges.txt: 1.67MB [00:00, 132MB/s]\n",
            "tokenizer.json: 100% 11.4M/11.4M [00:00<00:00, 20.4MB/s]\n",
            "config.json: 100% 726/726 [00:00<00:00, 6.09MB/s]\n",
            "2025-12-24 01:22:39.394668: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-24 01:22:39.417464: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766539359.443242    8009 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766539359.449385    8009 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766539359.463433    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766539359.463456    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766539359.463459    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766539359.463461    8009 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 01:22:39.467717: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "model.safetensors: 100% 1.50G/1.50G [00:01<00:00, 935MB/s] \n",
            "generation_config.json: 100% 239/239 [00:00<00:00, 2.03MB/s]\n",
            "README.md: 7.47kB [00:00, 30.6MB/s]\n",
            "data/train-00000-of-00001-a09b74b3ef9c3b(…): 100% 24.2M/24.2M [00:00<00:00, 34.7MB/s]\n",
            "Generating train split: 100% 52002/52002 [00:00<00:00, 336912.65 examples/s]\n",
            "[write] shard_00000.pt | N=512\n",
            "[write] shard_00001.pt | N=512\n",
            "[write] shard_00002.pt | N=512\n",
            "[write] shard_00003.pt | N=512\n",
            "[write] shard_00004.pt | N=512\n",
            "[write] shard_00005.pt | N=512\n",
            "[write] shard_00006.pt | N=512\n",
            "[write] shard_00007.pt | N=512\n",
            "[write] shard_00008.pt | N=512\n",
            "[write] shard_00009.pt | N=512\n",
            "[write] shard_00010.pt | N=512\n",
            "[write] shard_00011.pt | N=512\n",
            "[write] shard_00012.pt | N=512\n",
            "[write] shard_00013.pt | N=512\n",
            "[write] shard_00014.pt | N=512\n",
            "[write] shard_00015.pt | N=512\n",
            "[write] shard_00016.pt | N=512\n",
            "[write] shard_00017.pt | N=512\n",
            "[write] shard_00018.pt | N=512\n",
            "[write] shard_00019.pt | N=512\n",
            "[write] shard_00020.pt | N=512\n",
            "[write] shard_00021.pt | N=512\n",
            "[write] shard_00022.pt | N=512\n",
            "[write] shard_00023.pt | N=512\n",
            "[write] shard_00024.pt | N=512\n",
            "[write] shard_00025.pt | N=512\n",
            "[write] shard_00026.pt | N=512\n",
            "[write] shard_00027.pt | N=512\n",
            "[write] shard_00028.pt | N=512\n",
            "[write] shard_00029.pt | N=512\n",
            "[write] shard_00030.pt | N=512\n",
            "[write] shard_00031.pt | N=512\n",
            "[write] shard_00032.pt | N=512\n",
            "[write] shard_00033.pt | N=512\n",
            "[write] shard_00034.pt | N=512\n",
            "[write] shard_00035.pt | N=512\n",
            "[write] shard_00036.pt | N=512\n",
            "[write] shard_00037.pt | N=512\n",
            "[write] shard_00038.pt | N=512\n",
            "[write] shard_00039.pt | N=32\n",
            "Done. Cached sequences=20000 | out_dir=caches/alpaca_chat_think_L128_K32_R256\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# GENERATE THINKING DATASET (Alpaca chat format)\n",
        "# ============================================================\n",
        "# SKIP THIS CELL if you already have the cache on Google Drive!\n",
        "# Use the \"LOAD FROM GOOGLE DRIVE\" cell instead.\n",
        "\n",
        "!python scripts/precompute_teacher_topk.py \\\n",
        "  --teacher_model_name_or_path {MODEL_NAME} \\\n",
        "  --dataset_name tatsu-lab/alpaca \\\n",
        "  --dataset_split train \\\n",
        "  --dataset_format alpaca_chat \\\n",
        "  --enable_thinking true \\\n",
        "  --max_length 128 \\\n",
        "  --topk 32 \\\n",
        "  --rand_neg 256 \\\n",
        "  --num_sequences 20000 \\\n",
        "  --batch_size 1 \\\n",
        "  --shard_size 512 \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  --output_dir {CACHE_DIR_CHAT}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rlxJ3X8a3c2x",
      "metadata": {
        "id": "rlxJ3X8a3c2x"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# GENERATE TEXT DATASET (C4 streaming)\n",
        "# ============================================================\n",
        "# SKIP THIS CELL if you already have the cache on Google Drive!\n",
        "# Use the \"LOAD FROM GOOGLE DRIVE\" cell instead.\n",
        "\n",
        "import os\n",
        "\n",
        "CACHE_DIR = CACHE_DIR_TEXT\n",
        "\n",
        "if not os.path.isdir(CACHE_DIR):\n",
        "    print(f\"[cache] {CACHE_DIR} not found -> generating cache...\")\n",
        "\n",
        "    !python scripts/precompute_teacher_topk.py \\\n",
        "      --teacher_model_name_or_path {MODEL_NAME} \\\n",
        "      --dataset_name allenai/c4 \\\n",
        "      --dataset_config_name en \\\n",
        "      --dataset_split train \\\n",
        "      --dataset_text_field text \\\n",
        "      --streaming \\\n",
        "      --shuffle_buffer 10000 \\\n",
        "      --max_length 64 \\\n",
        "      --topk 32 \\\n",
        "      --rand_neg 256 \\\n",
        "      --num_sequences 2000 \\\n",
        "      --batch_size 1 \\\n",
        "      --shard_size 512 \\\n",
        "      --device {DEVICE} \\\n",
        "      --dtype {DTYPE} \\\n",
        "      --output_dir {CACHE_DIR}\n",
        "\n",
        "else:\n",
        "    print(f\"[cache] {CACHE_DIR} already exists -> skipping generation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Nt45qTnlKO7a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt45qTnlKO7a",
        "outputId": "be3ace25-b2da-42ec-df47-30b79fc479a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[gzip] Compressing caches/alpaca_chat_think_L128_K32_R256...\n",
            "caches/alpaca_chat_think_L128_K32_R256/\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00018.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00011.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00033.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00031.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00036.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00038.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00030.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00000.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00032.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00010.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00039.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00008.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00005.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00025.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/meta.json\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00015.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00028.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00016.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00009.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00021.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00037.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00001.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00035.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00029.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00003.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00006.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00014.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00007.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00002.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00013.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00024.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00034.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00022.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00020.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00004.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00012.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00019.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00027.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00026.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00017.pt\n",
            "caches/alpaca_chat_think_L128_K32_R256/shard_00023.pt\n",
            "[gzip] Done: 2.84 GB\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# COMPRESS CHAT CACHE (for Google Drive upload)\n",
        "# ============================================================\n",
        "# SKIP if cache is already compressed or loaded from Google Drive\n",
        "\n",
        "import os\n",
        "\n",
        "if os.path.isdir(CACHE_DIR_CHAT):\n",
        "    print(f\"[gzip] Compressing {CACHE_DIR_CHAT}...\")\n",
        "    !tar -zcvf {CACHE_DIR_CHAT}.tgz {CACHE_DIR_CHAT}\n",
        "    compressed_size = os.path.getsize(f\"{CACHE_DIR_CHAT}.tgz\")\n",
        "    print(f\"[gzip] Done: {compressed_size / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(f\"[gzip] Directory {CACHE_DIR_CHAT} not found. Skipping.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IIIWEkllwGEA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IIIWEkllwGEA",
        "outputId": "ef280f39-3d15-42f0-d0e1-41d6b31cb05b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[save] Copying alpaca_chat_think_L128_K32_R256.tgz to Google Drive...\n",
            "          3.05G 100%  427.75MB/s    0:00:06 (xfr#1, to-chk=0/1)\n",
            "[save] Saved to Google Drive: 2.84 GB\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# SAVE CACHED KD DATA TO GOOGLE DRIVE (run after generating cache)\n",
        "# ============================================================\n",
        "# This saves the generated cache to Google Drive for future sessions\n",
        "# Only run this AFTER you've generated the cache with precompute_teacher_topk.py\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create destination directory\n",
        "!mkdir -p /content/drive/MyDrive/qwen3_caches\n",
        "\n",
        "# Choose which cache to save (should match what you generated)\n",
        "CACHE_NAME = \"alpaca_chat_think_L128_K32_R256\"  # For Qwen3-4B thinking model\n",
        "#CACHE_NAME = \"Q4B_alpaca_chat_think_L128_K32_R256\"  # For Qwen3-4B thinking model\n",
        "\n",
        "import os\n",
        "\n",
        "# Check if cache exists and compress if needed\n",
        "if os.path.isdir(f\"caches/{CACHE_NAME}\"):\n",
        "    # Compress if not already compressed\n",
        "    if not os.path.exists(f\"caches/{CACHE_NAME}.tgz\"):\n",
        "        print(f\"[gzip] Compressing {CACHE_NAME}...\")\n",
        "        !tar -zcvf caches/{CACHE_NAME}.tgz -C caches {CACHE_NAME}\n",
        "\n",
        "    # Copy to Google Drive\n",
        "    print(f\"[save] Copying {CACHE_NAME}.tgz to Google Drive...\")\n",
        "    !rsync -ah --info=progress2 caches/{CACHE_NAME}.tgz /content/drive/MyDrive/qwen3_caches/\n",
        "\n",
        "    # Verify\n",
        "    gd_size = os.path.getsize(f\"/content/drive/MyDrive/qwen3_caches/{CACHE_NAME}.tgz\")\n",
        "    print(f\"[save] Saved to Google Drive: {gd_size / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(f\"[save] ERROR: Cache directory caches/{CACHE_NAME} not found\")\n",
        "    print(\"[save] Run precompute_teacher_topk.py first to generate the cache\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n4snxtz2q6q",
      "metadata": {
        "id": "n4snxtz2q6q"
      },
      "source": [
        "## 4.5) Google Drive Cache Management\n",
        "\n",
        "**Workflow for KD Cache:**\n",
        "\n",
        "1. **First time setup** (slow):\n",
        "   - Run `precompute_teacher_topk.py` to generate cache\n",
        "   - Run \"SAVE TO GOOGLE DRIVE\" cell to persist\n",
        "   \n",
        "2. **Subsequent sessions** (fast):\n",
        "   - Run \"LOAD FROM GOOGLE DRIVE\" cell to restore cache\n",
        "   - Skip cache generation step\n",
        "\n",
        "The cached KD data (~2-3 GB compressed) contains precomputed teacher logits for knowledge distillation training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7745eb2",
      "metadata": {
        "id": "d7745eb2"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# COMPRESS TEXT CACHE (for Google Drive upload)\n",
        "# ============================================================\n",
        "# SKIP if cache is already compressed or loaded from Google Drive\n",
        "\n",
        "import os\n",
        "\n",
        "if os.path.isdir(CACHE_DIR_TEXT):\n",
        "    print(f\"[gzip] Compressing {CACHE_DIR_TEXT}...\")\n",
        "    !tar -zcvf {CACHE_DIR_TEXT}.tgz {CACHE_DIR_TEXT}\n",
        "    compressed_size = os.path.getsize(f\"{CACHE_DIR_TEXT}.tgz\")\n",
        "    print(f\"[gzip] Done: {compressed_size / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(f\"[gzip] Directory {CACHE_DIR_TEXT} not found. Skipping.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vlhuS4N9GbN4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlhuS4N9GbN4",
        "outputId": "974e0da4-8e12-4c8c-f4c4-d62be4bae769"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[cache] Copying alpaca_chat_think_L128_K32_R256.tgz from Google Drive...\n",
            "rsync: [sender] link_stat \"/content/drive/MyDrive/qwen3_caches/alpaca_chat_think_L128_K32_R256.tgz\" failed: No such file or directory (2)\n",
            "              0 100%    0.00kB/s    0:00:00 (xfr#0, to-chk=0/0)\n",
            "rsync error: some files/attrs were not transferred (see previous errors) (code 23) at main.c(1338) [sender=3.2.7]\n",
            "[cache] Extracting alpaca_chat_think_L128_K32_R256.tgz...\n",
            "tar (child): caches/alpaca_chat_think_L128_K32_R256.tgz: Cannot open: No such file or directory\n",
            "tar (child): Error is not recoverable: exiting now\n",
            "tar: Child returned status 2\n",
            "tar: Error is not recoverable: exiting now\n",
            "[cache] ERROR: Failed to extract alpaca_chat_think_L128_K32_R256\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# LOAD CACHED KD DATA FROM GOOGLE DRIVE (run this cell first!)\n",
        "# ============================================================\n",
        "# Mount Google Drive and copy cached KD data back to local storage\n",
        "# This avoids regenerating the cache every session\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create local cache directory\n",
        "!mkdir -p caches\n",
        "\n",
        "# Choose which cache to load (uncomment the one you need)\n",
        "#CACHE_NAME = \"Q4B_alpaca_chat_think_L128_K32_R256\"  # For Qwen3-4B thinking model\n",
        "CACHE_NAME = \"alpaca_chat_think_L128_K32_R256\"  # For Qwen3-4B thinking model\n",
        "\n",
        "# Copy from Google Drive to local\n",
        "print(f\"[cache] Copying {CACHE_NAME}.tgz from Google Drive...\")\n",
        "!rsync -ah --info=progress2 \\\n",
        "  /content/drive/MyDrive/qwen3_caches/{CACHE_NAME}.tgz \\\n",
        "  caches/\n",
        "\n",
        "# Unzip the cache\n",
        "print(f\"[cache] Extracting {CACHE_NAME}.tgz...\")\n",
        "!tar -xzf caches/{CACHE_NAME}.tgz -C .\n",
        "\n",
        "# Verify extraction\n",
        "import os\n",
        "if os.path.isdir(f\"caches/{CACHE_NAME}\"):\n",
        "    num_shards = len([f for f in os.listdir(f\"caches/{CACHE_NAME}\") if f.startswith(\"shard_\")])\n",
        "    print(f\"[cache] Successfully loaded {CACHE_NAME} with {num_shards} shards\")\n",
        "else:\n",
        "    print(f\"[cache] ERROR: Failed to extract {CACHE_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NKhv4eaYs4qR",
      "metadata": {
        "id": "NKhv4eaYs4qR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AHSsmdhzsBTo",
      "metadata": {
        "id": "AHSsmdhzsBTo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A7UPbdfK3c2x",
      "metadata": {
        "id": "A7UPbdfK3c2x"
      },
      "outputs": [],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_stream_q2\"\n",
        "\n",
        "# DISABLED --- NOTE used! see #5 for first QAT step\n",
        "# Construct the command string in Python to ensure variable interpolation\n",
        "command_str = f\"\"\"python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --teacher_model_name_or_path {MODEL_NAME} \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --dataset_name allenai/c4 \\\n",
        "  --dataset_config_name en \\\n",
        "  --dataset_split train \\\n",
        "  --dataset_format text \\\n",
        "  --dataset_text_field text \\\n",
        "  --streaming \\\n",
        "  --shuffle_buffer 10000 \\\n",
        "  --output_dir {RUN_DIR} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 1 \\\n",
        "  --gradient_accumulation_steps 16 \\\n",
        "  --learning_rate 5e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 50 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --logging_steps 10 \\\n",
        "  --save_steps 50\"\"\"\n",
        "\n",
        "# Execute the constructed command string\n",
        "!{command_str}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UmgbC-RI3c2x",
      "metadata": {
        "id": "UmgbC-RI3c2x"
      },
      "source": [
        "### (Optional) Resume\n",
        "\n",
        "`--resume_from_checkpoint auto` resolves to `checkpoint_last.pt` if it exists in the output directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mJqdmXSA3c2x",
      "metadata": {
        "id": "mJqdmXSA3c2x"
      },
      "outputs": [],
      "source": [
        "# !python scripts/train_qat.py ... --output_dir {RUN_DIR} --max_steps 500 --resume_from_checkpoint auto\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R7UZBhau3c2x",
      "metadata": {
        "id": "R7UZBhau3c2x"
      },
      "source": [
        "## 5)  KD-cache: precompute teacher top-k + negatives\n",
        "\n",
        "Cache mode is MPS-friendly:\n",
        "- no teacher model during training\n",
        "- no full-vocab logits\n",
        "\n",
        "If you see good KD loss but bad greedy decoding, increase negative coverage (`--rand_neg`) and/or add hard top-1 terms:\n",
        "- `--hard-top1-weight 0.05`\n",
        "- `--hard-full-top1-weight 0.02`–`0.05`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g4s_PzQW3c2x",
      "metadata": {
        "id": "g4s_PzQW3c2x"
      },
      "source": [
        "### KD-cache QAT training\n",
        "\n",
        "This uses cached teacher signals + candidate softmax."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06LaYj0vPIE7",
      "metadata": {
        "id": "06LaYj0vPIE7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1Li8Aysa3c2x",
      "metadata": {
        "id": "1Li8Aysa3c2x"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STAGE 1: KD-QAT (Conservative - freeze MLP/Attention)\n",
        "# ============================================================\n",
        "# First training stage with frozen output layers for stability\n",
        "\n",
        "%pwd\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "CACHE_DIR = CACHE_DIR_CHAT\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2\"\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --output_dir {RUN_DIR_CACHE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 1 \\\n",
        "  --gradient_accumulation_steps 4 \\\n",
        "  --learning_rate 5e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 1000 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 5 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.05 \\\n",
        "  --hard-full-top1-weight 0.03 \\\n",
        "  --ov-freeze \\\n",
        "  --freeze-last-mlp \\\n",
        "  --freeze-last-mlp-layers 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UKgDA-7OuA8m",
      "metadata": {
        "id": "UKgDA-7OuA8m"
      },
      "outputs": [],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --prompt \"What capital city of France is?\" \\\n",
        "  --do_sample true \\\n",
        "  --max_new_tokens 64\n",
        "\n",
        "#  --prompt \"What is Capital of france?\" \\\n",
        "#   --prompt \"What is Apple Neural Engine?\" \\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sexJNkyJyMgv",
      "metadata": {
        "id": "sexJNkyJyMgv"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb19fb17",
      "metadata": {
        "id": "eb19fb17"
      },
      "outputs": [],
      "source": [
        "# Define source and destination paths\n",
        "SOURCE_FILE = \"runs/qwen3_kdqat_cache_q2_2/qat_state_dict.pt\"\n",
        "DEST_DIR_GD = \"/content/drive/MyDrive/runs/Q4B/q2_2/\"\n",
        "\n",
        "# Ensure the destination directory exists on Google Drive\n",
        "!mkdir -p {DEST_DIR_GD}\n",
        "\n",
        "# Copy the file to Google Drive\n",
        "!cp -v {SOURCE_FILE} {DEST_DIR_GD}\n",
        "print(f\"Copied {SOURCE_FILE} to {DEST_DIR_GD}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hbfDaYP5yN6-",
      "metadata": {
        "id": "hbfDaYP5yN6-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JUvQIIDeRUF6",
      "metadata": {
        "id": "JUvQIIDeRUF6"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STAGE 2: KD-QAT (Unfrozen layers, resume from Stage 1)\n",
        "# ============================================================\n",
        "# Continue training with all layers unfrozen\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "CACHE_DIR = CACHE_DIR_CHAT\n",
        "INIT_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2\"\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_DIR_CACHE}/qat_state_dict.pt \\\n",
        "  --output_dir {RUN_DIR_CACHE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 1 \\\n",
        "  --gradient_accumulation_steps 4 \\\n",
        "  --learning_rate 5e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 1000 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --ov-freeze \\\n",
        "  --hard-top1-weight 0.02 \\\n",
        "  --hard-full-top1-weight 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qsfHq1gvtpg-",
      "metadata": {
        "id": "qsfHq1gvtpg-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KXQUTgEJfzl4",
      "metadata": {
        "id": "KXQUTgEJfzl4"
      },
      "outputs": [],
      "source": [
        "# Stage 3 resume KD-QAT with unfrozen attention and relaxed hard-top/full!\n",
        "\n",
        "CACHE_DIR = \"caches/alpaca_chat_think_both_L128_K32_R256\"\n",
        "INIT_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_3\"\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_DIR_CACHE}/qat_state_dict.pt \\\n",
        "  --output_dir {RUN_DIR_CACHE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 128 \\\n",
        "  --gradient_accumulation_steps 1 \\\n",
        "  --learning_rate 5e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 1000 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.00 \\\n",
        "  --hard-full-top1-weight 0.0005\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q1Zjl_WnHZ4O",
      "metadata": {
        "id": "Q1Zjl_WnHZ4O"
      },
      "outputs": [],
      "source": [
        "# Stage 4 resume KD-QAT with unfrozen attention and relaxed hard-top/full!\n",
        "#   --hard-full-top1-weight 0.0000\n",
        "#   learning_rate 2e-6\n",
        "CACHE_DIR = \"caches/alpaca_chat_think_both_L128_K32_R256\"\n",
        "INIT_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_3\"\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2_4\"\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_DIR_CACHE}/qat_state_dict.pt \\\n",
        "  --output_dir {RUN_DIR_CACHE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size 160 \\\n",
        "  --gradient_accumulation_steps 1 \\\n",
        "  --learning_rate 2e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 500 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 5 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.00 \\\n",
        "  --hard-full-top1-weight 0.0000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acak6adahpf",
      "metadata": {
        "id": "acak6adahpf"
      },
      "source": [
        "## 5.5) Progressive Layer-by-Layer QAT (Experimental)\n",
        "\n",
        "This approach trains one layer at a time with:\n",
        "- **Local reconstruction loss**: MSE between quantized and fp MLP outputs\n",
        "- **Global KD loss**: Cached teacher logits\n",
        "- **Prefix quantized / suffix fp**: Earlier layers stay quantized, later layers use full precision\n",
        "\n",
        "### Recommended Training Order (most stable first):\n",
        "\n",
        "1. **E2E f-only** (Option 1): Train ALL f parameters at once\n",
        "   - Skip progressive passes, just run Pass 4\n",
        "   - Most stable, fastest validation\n",
        "   \n",
        "2. **Progressive f-only** (Option 2): Layer-by-layer f-param training\n",
        "   - Uses `--train_f_only` flag\n",
        "   - Disable local loss with `--local_weight 0.0`\n",
        "   \n",
        "3. **Full progressive** (Option 3): Train weights + f per layer\n",
        "   - Most aggressive, may show instability at later layers\n",
        "\n",
        "### GPU Configuration:\n",
        "\n",
        "| GPU | Recommended batch_size |\n",
        "|-----|------------------------|\n",
        "| T4 (15GB) | 2-4 |\n",
        "| V100 (32GB) | 4-8 |\n",
        "| A100 (40GB) | 8-16 |\n",
        "| A100 (80GB) / H100 | 16-32 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "scxwylalctd",
      "metadata": {
        "id": "scxwylalctd"
      },
      "outputs": [],
      "source": [
        "# ---- Progressive QAT Config ----\n",
        "# Adjust batch_size for your GPU (A100: 8-16, V100: 4-8, T4: 2-4)\n",
        "BATCH_SIZE = 96                # Increase for faster instances (A100/H100)\n",
        "STEPS_PER_LAYER_MLP = 100      # Steps per MLP layer (Pass 1 + Pass 3)\n",
        "STEPS_PER_LAYER_ATTN = 30     # Steps per attention layer (Pass 2)\n",
        "E2E_STEPS = 500               # E2E quantizer tuning steps (Pass 4)\n",
        "LOCAL_WEIGHT = 0.3            # Local reconstruction loss weight\n",
        "GLOBAL_WEIGHT = 1.0           # Global KD loss weight\n",
        "LOCAL_TOKEN_SAMPLES = 128     # Tokens to sample for local loss\n",
        "MAX_GRAD_NORM = 1.0           # Gradient clipping (important for 2-bit)\n",
        "\n",
        "# Learning rates\n",
        "LR_PROGRESSIVE = 5e-6         # Learning rate for progressive passes\n",
        "LR_E2E = 1e-4                 # Learning rate for E2E f-only tuning\n",
        "\n",
        "# Output directories\n",
        "RUN_DIR_E2E_FONLY = \"runs/e2e_f_only\"\n",
        "RUN_DIR_PROGRESSIVE_FONLY = \"runs/progressive_f_only\"\n",
        "RUN_DIR_PROGRESSIVE = \"runs/progressive_qat_v1\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q9f32hx0qo",
      "metadata": {
        "id": "q9f32hx0qo"
      },
      "source": [
        "### Option 1: E2E f-only Training (Recommended First)\n",
        "\n",
        "**Most stable approach** - trains ALL `_f_param` (quantization scales) simultaneously.\n",
        "Skip all progressive layer-by-layer passes and go straight to Pass 4.\n",
        "\n",
        "This is recommended when:\n",
        "- Progressive layer-by-layer shows instability (local loss hitting 10.0)\n",
        "- You want to validate the infrastructure works before trying progressive\n",
        "- You have limited time and want the fastest path to a working checkpoint\n",
        "\n",
        "The `f` parameter is the learnable quantization scale from Apple-style quantization:\n",
        "- Actual scale `s = softplus(f)` ensures positivity\n",
        "- Training only `f` keeps weights frozen - more stable for ultra-low-bit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vh3iifkgpj",
      "metadata": {
        "id": "vh3iifkgpj",
        "outputId": "4cc06f1d-23aa-417e-92b9-5aaea6dba2d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "[device] cuda | amp_dtype=torch.bfloat16 | param_dtype=torch.bfloat16\n",
            "[model] Loading Qwen/Qwen3-0.6B\n",
            "2025-12-24 01:53:17.534006: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-24 01:53:17.556130: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766541197.581856   16151 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766541197.587401   16151 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766541197.602015   16151 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766541197.602044   16151 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766541197.602048   16151 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766541197.602051   16151 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 01:53:17.606358: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[qat] weight_bits=4\n",
            "[kd-cache] Loading from caches/alpaca_chat_think_L128_K32_R256\n",
            "[model] 28 transformer layers\n",
            "\n",
            "============================================================\n",
            "PASS 4: E2E Quantizer Tuning (f-only)\n",
            "============================================================\n",
            "  Trainable f parameters: 196\n",
            "  step 0: loss=10.4815\n",
            "  step 10: loss=10.6626\n",
            "  step 20: loss=10.5577\n",
            "  step 30: loss=10.5951\n",
            "  step 40: loss=10.7329\n",
            "  step 50: loss=10.4800\n",
            "  step 60: loss=10.5936\n",
            "  step 70: loss=10.5929\n",
            "  step 80: loss=10.5984\n",
            "  step 90: loss=10.5669\n",
            "  step 100: loss=10.6184\n",
            "  step 110: loss=10.6343\n",
            "  step 120: loss=10.4301\n",
            "  step 130: loss=10.4968\n",
            "  step 140: loss=10.5011\n",
            "  step 150: loss=10.5783\n",
            "  step 160: loss=10.6029\n",
            "  step 170: loss=10.4627\n",
            "  step 180: loss=10.6370\n",
            "  step 190: loss=10.5549\n",
            "  step 200: loss=10.7009\n",
            "  step 210: loss=10.6632\n",
            "  step 220: loss=10.4980\n",
            "  step 230: loss=10.6916\n",
            "  step 240: loss=10.5702\n",
            "  step 250: loss=10.5969\n",
            "  step 260: loss=10.5019\n",
            "  step 270: loss=10.6430\n",
            "  step 280: loss=10.4538\n",
            "  step 290: loss=10.5254\n",
            "  step 300: loss=10.3753\n",
            "  step 310: loss=10.5633\n",
            "  step 320: loss=10.2545\n",
            "  step 330: loss=10.5237\n",
            "  step 340: loss=10.4550\n",
            "  step 350: loss=10.3413\n"
          ]
        }
      ],
      "source": [
        "# E2E f-only: Skip ALL progressive passes, train all f parameters at once\n",
        "# This is the simplest and most stable approach\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/train_qat_progressive.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --output_dir {RUN_DIR_E2E_FONLY} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --skip_mlp_pass \\\n",
        "  --skip_attention_pass \\\n",
        "  --skip_mlp_refinement \\\n",
        "  --e2e_steps {E2E_STEPS} \\\n",
        "  --e2e_learning_rate {LR_E2E} \\\n",
        "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vab9x1zlqhs",
      "metadata": {
        "id": "vab9x1zlqhs"
      },
      "source": [
        "### Option 2: Progressive f-only Training\n",
        "\n",
        "Layer-by-layer training but only trains `_f_param` (quantization scales), not weights.\n",
        "More stable than full progressive training, but may still see instability at later layers.\n",
        "\n",
        "Use `--train_f_only` flag to freeze weights and only train quantization scales per layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oqxrx3c2fe",
      "metadata": {
        "id": "oqxrx3c2fe"
      },
      "outputs": [],
      "source": [
        "# Progressive f-only: Layer-by-layer, but only train quantization scales\n",
        "# Use --train_f_only for more stable training\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/train_qat_progressive.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --output_dir {RUN_DIR_PROGRESSIVE_FONLY} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --train_f_only \\\n",
        "  --steps_per_layer_mlp {STEPS_PER_LAYER_MLP} \\\n",
        "  --steps_per_layer_attn {STEPS_PER_LAYER_ATTN} \\\n",
        "  --e2e_steps {E2E_STEPS} \\\n",
        "  --local_weight 0.0 \\\n",
        "  --global_weight {GLOBAL_WEIGHT} \\\n",
        "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
        "  --learning_rate {LR_PROGRESSIVE} \\\n",
        "  --e2e_learning_rate {LR_E2E} \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --skip_mlp_refinement"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e0gsxkcc4",
      "metadata": {
        "id": "8e0gsxkcc4"
      },
      "source": [
        "**bold text**### Option 3: Full Progressive Training (weights + f)\n",
        "\n",
        "Full layer-by-layer training with weights and quantization scales.\n",
        "Most aggressive but potentially unstable for ultra-low-bit (2-bit).\n",
        "\n",
        "**Training Order (3-pass v3):**\n",
        "1. **Pass 1**: Train MLP layers (local reconstruction + global KD)\n",
        "2. **Pass 2**: Train attention layers (global KD only)\n",
        "3. **Pass 3**: MLP refinement (addresses MLP-attention coupling)\n",
        "4. **Pass 4**: E2E quantizer-only tuning (f-param only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7qzyw0rd18e",
      "metadata": {
        "id": "7qzyw0rd18e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74d666d4-059a-4d7c-d488-a64ca476a24d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "[device] cuda | amp_dtype=torch.bfloat16 | param_dtype=torch.bfloat16\n",
            "[model] Loading Qwen/Qwen3-0.6B\n",
            "2025-12-24 04:34:20.719705: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-24 04:34:20.740968: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766550860.766209   60409 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766550860.771759   60409 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766550860.786357   60409 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766550860.786382   60409 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766550860.786385   60409 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766550860.786387   60409 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-24 04:34:20.790679: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[qat] weight_bits=4\n",
            "[kd-cache] Loading from caches/alpaca_chat_think_L128_K32_R256\n",
            "[model] 28 transformer layers\n",
            "[training] batch_size=96 steps_per_mlp=100 e2e_steps=500\n",
            "\n",
            "============================================================\n",
            "PASS 1: Progressive MLP Training\n",
            "============================================================\n",
            "\n",
            "--- Layer 0/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0481 global=0.0372\n",
            "  step 10: local=0.0469 global=0.0328\n",
            "  step 20: local=0.0454 global=0.0298\n",
            "  step 30: local=0.0467 global=0.0285\n",
            "  step 40: local=0.0489 global=0.0318\n",
            "  step 50: local=0.0458 global=0.0245\n",
            "  step 60: local=0.0475 global=0.0238\n",
            "  step 70: local=0.0458 global=0.0246\n",
            "  step 80: local=0.0464 global=0.0240\n",
            "  step 90: local=0.0422 global=0.0218\n",
            "\n",
            "--- Layer 1/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0744 global=0.1233\n",
            "  step 10: local=0.0736 global=0.0724\n",
            "  step 20: local=0.0697 global=0.0499\n",
            "  step 30: local=0.0713 global=0.0513\n",
            "  step 40: local=0.0698 global=0.0515\n",
            "  step 50: local=0.0710 global=0.0563\n",
            "  step 60: local=0.0664 global=0.0413\n",
            "  step 70: local=0.0660 global=0.0491\n",
            "  step 80: local=0.0622 global=0.0609\n",
            "  step 90: local=0.0615 global=0.0450\n",
            "\n",
            "--- Layer 2/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "\n",
            "  [EARLY BACKTRACK] Layer 2 started at 6.2703 (>0.4317 = 0.0432 * 10.0)\n",
            "  [EARLY BACKTRACK] Aborting layer 2, going back to retrain layer 1 (backtrack 1/5)\n",
            "\n",
            "--- Layer 1/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0702 global=0.0409\n",
            "  step 10: local=0.0639 global=0.0390\n",
            "  step 20: local=0.0611 global=0.0385\n",
            "  step 30: local=0.0628 global=0.0422\n",
            "  step 40: local=0.0629 global=0.0385\n",
            "  step 50: local=0.0598 global=0.0388\n",
            "  step 60: local=0.0602 global=0.0343\n",
            "  step 70: local=0.0604 global=0.0395\n",
            "  step 80: local=0.0597 global=0.0347\n",
            "  step 90: local=0.0574 global=0.0351\n",
            "\n",
            "--- Layer 2/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "\n",
            "  [EARLY BACKTRACK] Layer 2 started at 5.3491 (>0.3858 = 0.0386 * 10.0)\n",
            "  [EARLY BACKTRACK] Aborting layer 2, going back to retrain layer 1 (backtrack 2/5)\n",
            "\n",
            "--- Layer 1/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0587 global=0.0327\n",
            "  step 10: local=0.0546 global=0.0358\n",
            "  step 20: local=0.0576 global=0.0318\n",
            "  step 30: local=0.0550 global=0.0313\n",
            "  step 40: local=0.0633 global=0.0341\n",
            "  step 50: local=0.0581 global=0.0295\n",
            "  step 60: local=0.0561 global=0.0304\n",
            "  step 70: local=0.0529 global=0.0326\n",
            "  step 80: local=0.0557 global=0.0317\n",
            "  step 90: local=0.0554 global=0.0329\n",
            "\n",
            "--- Layer 2/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "\n",
            "  [EARLY BACKTRACK] Layer 2 started at 4.7834 (>0.3155 = 0.0315 * 10.0)\n",
            "  [EARLY BACKTRACK] Aborting layer 2, going back to retrain layer 1 (backtrack 3/5)\n",
            "\n",
            "--- Layer 1/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0550 global=0.0334\n",
            "  step 10: local=0.0527 global=0.0337\n",
            "  step 20: local=0.0534 global=0.0339\n",
            "  step 30: local=0.0563 global=0.0327\n",
            "  step 40: local=0.0511 global=0.0299\n",
            "  step 50: local=0.0510 global=0.0279\n",
            "  step 60: local=0.0500 global=0.0351\n",
            "  step 70: local=0.0495 global=0.0286\n",
            "  step 80: local=0.0511 global=0.0282\n",
            "  step 90: local=0.0514 global=0.0277\n",
            "\n",
            "--- Layer 2/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "\n",
            "  [EARLY BACKTRACK] Layer 2 started at 5.3207 (>0.2991 = 0.0299 * 10.0)\n",
            "  [EARLY BACKTRACK] Aborting layer 2, going back to retrain layer 1 (backtrack 4/5)\n",
            "\n",
            "--- Layer 1/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0510 global=0.0354\n",
            "  step 10: local=0.0528 global=0.0310\n",
            "  step 20: local=0.0492 global=0.0301\n",
            "  step 30: local=0.0518 global=0.0308\n",
            "  step 40: local=0.0516 global=0.0279\n",
            "  step 50: local=0.0514 global=0.0303\n",
            "  step 60: local=0.0486 global=0.0345\n",
            "  step 70: local=0.0487 global=0.0289\n",
            "  step 80: local=0.0480 global=0.0311\n",
            "  step 90: local=0.0494 global=0.0308\n",
            "\n",
            "--- Layer 2/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "\n",
            "  [EARLY BACKTRACK] Layer 2 started at 4.8576 (>0.2749 = 0.0275 * 10.0)\n",
            "  [EARLY BACKTRACK] Aborting layer 2, going back to retrain layer 1 (backtrack 5/5)\n",
            "\n",
            "--- Layer 1/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0497 global=0.0293\n",
            "  step 10: local=0.0482 global=0.0284\n",
            "  step 20: local=0.0478 global=0.0265\n",
            "  step 30: local=0.0493 global=0.0294\n",
            "  step 40: local=0.0499 global=0.0268\n",
            "  step 50: local=0.0470 global=0.0305\n",
            "  step 60: local=0.0496 global=0.0270\n",
            "  step 70: local=0.0486 global=0.0253\n",
            "  step 80: local=0.0493 global=0.0260\n",
            "  step 90: local=0.0465 global=0.0273\n",
            "\n",
            "--- Layer 2/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.2676 global=4.8024\n",
            "  step 10: local=0.1221 global=3.7455\n",
            "  step 20: local=0.1212 global=3.1065\n",
            "  step 30: local=0.1176 global=2.5528\n",
            "  step 40: local=0.2698 global=2.2769\n",
            "  step 50: local=0.2688 global=1.9006\n",
            "  step 60: local=0.2705 global=1.7958\n",
            "  step 70: local=0.1238 global=1.6791\n",
            "  step 80: local=0.1258 global=1.5806\n",
            "  step 90: local=0.2663 global=1.5908\n",
            "  Layer 2 not converged (global=1.5905 > 0.4), repeating...\n",
            "\n",
            "--- Layer 2/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1203 global=1.4879\n",
            "  step 10: local=0.2609 global=1.3769\n",
            "  step 20: local=0.1242 global=1.3598\n",
            "  step 30: local=0.1240 global=1.3353\n",
            "  step 40: local=0.1205 global=1.2681\n",
            "  step 50: local=0.2602 global=1.3117\n",
            "  step 60: local=0.1144 global=1.2996\n",
            "  step 70: local=0.2568 global=1.3520\n",
            "  step 80: local=0.1169 global=1.2530\n",
            "  step 90: local=0.1172 global=1.1707\n",
            "  Layer 2 not converged (global=1.2648 > 0.4), repeating...\n",
            "\n",
            "--- Layer 2/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.2551 global=1.2065\n",
            "  step 10: local=0.2624 global=1.2304\n",
            "  step 20: local=0.2645 global=1.1829\n",
            "  step 30: local=0.1185 global=1.2034\n",
            "  step 40: local=0.1174 global=1.2865\n",
            "  step 50: local=0.2585 global=1.2655\n",
            "  step 60: local=0.1103 global=1.2701\n",
            "  step 70: local=0.2594 global=1.2588\n",
            "  step 80: local=0.1183 global=1.2487\n",
            "  step 90: local=0.2606 global=1.2121\n",
            "  Layer 2 not converged (global=1.2719 > 0.4), repeating...\n",
            "\n",
            "--- Layer 2/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.2578 global=1.3270\n",
            "  step 10: local=0.1218 global=1.2679\n",
            "  step 20: local=0.2607 global=1.1446\n",
            "  step 30: local=0.2539 global=1.2178\n",
            "  step 40: local=0.1153 global=1.2069\n",
            "  step 50: local=0.2607 global=1.2704\n",
            "  step 60: local=0.2631 global=1.2368\n",
            "  step 70: local=0.2533 global=1.2090\n",
            "  step 80: local=0.2576 global=1.2562\n",
            "  step 90: local=0.2525 global=1.2157\n",
            "  Layer 2 not converged (global=1.1591 > 0.4), repeating...\n",
            "\n",
            "--- Layer 2/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.2586 global=1.2098\n",
            "  step 10: local=0.1147 global=1.3042\n",
            "  step 20: local=0.2559 global=1.1909\n",
            "  step 30: local=0.2620 global=1.1301\n",
            "  step 40: local=0.1159 global=1.1102\n",
            "  step 50: local=0.1089 global=1.1150\n",
            "  step 60: local=0.2567 global=1.1869\n",
            "  step 70: local=0.2565 global=1.2057\n",
            "  step 80: local=0.2644 global=1.1380\n",
            "  step 90: local=0.1095 global=1.3225\n",
            "  Layer 2 not converged (global=1.1783 > 0.4), repeating...\n",
            "\n",
            "--- Layer 2/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.2573 global=1.1957\n",
            "  step 10: local=0.2565 global=1.1494\n",
            "  step 20: local=0.1088 global=1.2093\n",
            "  step 30: local=0.1154 global=1.1278\n",
            "  step 40: local=0.2517 global=1.2289\n",
            "  step 50: local=0.2590 global=1.1491\n",
            "  step 60: local=0.1180 global=1.2397\n",
            "  step 70: local=0.2569 global=1.2324\n",
            "  step 80: local=0.2540 global=1.1612\n",
            "  step 90: local=0.1100 global=1.1919\n",
            "  Layer 2 not converged (global=1.1515 > 0.4), repeating...\n",
            "\n",
            "--- Layer 2/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1121 global=1.1515\n",
            "  step 10: local=0.2580 global=1.2639\n",
            "  step 20: local=0.2563 global=1.0767\n",
            "  step 30: local=0.2547 global=1.1828\n",
            "  step 40: local=0.2580 global=1.1780\n",
            "  step 50: local=0.2573 global=1.1802\n",
            "  step 60: local=0.2631 global=1.1366\n",
            "  step 70: local=0.2547 global=1.1820\n",
            "  step 80: local=0.2535 global=1.2279\n",
            "  step 90: local=0.2576 global=1.2854\n",
            "  Layer 2 not converged (global=1.2049 > 0.4), repeating...\n",
            "\n",
            "--- Layer 2/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1128 global=1.1520\n",
            "  step 10: local=0.2581 global=1.1663\n",
            "  step 20: local=0.2537 global=1.2514\n",
            "  step 30: local=0.1100 global=1.1778\n",
            "  step 40: local=0.2540 global=1.1849\n",
            "  step 50: local=0.2504 global=1.1547\n",
            "  step 60: local=0.2626 global=1.1118\n",
            "  step 70: local=0.2602 global=1.2304\n",
            "  step 80: local=0.2573 global=1.2226\n",
            "  step 90: local=0.2610 global=1.2224\n",
            "  Layer 2 not converged (global=1.1921 > 0.4), repeating...\n",
            "\n",
            "--- Layer 2/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.2597 global=1.2645\n",
            "  step 10: local=0.2668 global=1.2053\n",
            "  step 20: local=0.2610 global=1.0779\n",
            "  step 30: local=0.2573 global=1.2133\n",
            "  step 40: local=0.2563 global=1.1886\n",
            "  step 50: local=0.2523 global=1.2982\n",
            "  step 60: local=0.2589 global=1.2367\n",
            "  step 70: local=0.2590 global=1.1879\n",
            "  step 80: local=0.2569 global=1.1632\n",
            "  step 90: local=0.2624 global=1.2812\n",
            "  Layer 2 not converged (global=1.2884 > 0.4), repeating...\n",
            "\n",
            "--- Layer 2/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1140 global=1.1639\n",
            "  step 10: local=0.1132 global=1.1549\n",
            "  step 20: local=0.2545 global=1.2010\n",
            "  step 30: local=0.1119 global=1.1234\n",
            "  step 40: local=0.1145 global=1.1585\n",
            "  step 50: local=0.2529 global=1.2489\n",
            "  step 60: local=0.2617 global=1.0931\n",
            "  step 70: local=0.2528 global=1.1705\n",
            "  step 80: local=0.2548 global=1.1324\n",
            "  step 90: local=0.2565 global=1.2122\n",
            "  Layer 2 not converged (global=1.1979 > 0.4), repeating...\n",
            "\n",
            "--- Layer 2/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.2584 global=1.1612\n",
            "  step 10: local=0.1151 global=1.1240\n",
            "  step 20: local=0.1122 global=1.1926\n",
            "  step 30: local=0.2571 global=1.1989\n",
            "  step 40: local=0.2545 global=1.2167\n",
            "  step 50: local=0.2551 global=1.1569\n",
            "  step 60: local=0.1102 global=1.1808\n",
            "  step 70: local=0.1160 global=1.1823\n",
            "  step 80: local=0.2569 global=1.2067\n",
            "  step 90: local=0.1150 global=1.1500\n",
            "  Layer 2 not converged (global=1.2454 > 0.4), repeating...\n",
            "\n",
            "--- Layer 2/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1106 global=1.1636\n",
            "  step 10: local=0.1120 global=1.2626\n",
            "  step 20: local=0.1143 global=1.0990\n",
            "  step 30: local=0.2579 global=1.2084\n",
            "  step 40: local=0.1123 global=1.1282\n",
            "  step 50: local=0.2555 global=1.1901\n",
            "  step 60: local=0.1108 global=1.1559\n",
            "  step 70: local=0.2603 global=1.1619\n",
            "  step 80: local=0.2575 global=1.1982\n",
            "  step 90: local=0.2571 global=1.1355\n",
            "  Layer 2 not converged (global=1.1847 > 0.4), repeating...\n",
            "\n",
            "--- Layer 2/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.2546 global=1.1229\n",
            "  step 10: local=0.2516 global=1.1790\n",
            "  step 20: local=0.1098 global=1.1599\n",
            "  step 30: local=0.1127 global=1.1671\n",
            "  step 40: local=0.1101 global=1.2075\n",
            "  step 50: local=0.1094 global=1.2004\n",
            "  step 60: local=0.2531 global=1.1176\n",
            "  step 70: local=0.1091 global=1.0840\n",
            "  step 80: local=0.1152 global=1.1999\n",
            "  step 90: local=0.2599 global=1.2705\n",
            "  Layer 2 not converged (global=1.1657 > 0.4), repeating...\n",
            "\n",
            "--- Layer 2/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1104 global=1.1964\n",
            "  step 10: local=0.1156 global=1.1727\n",
            "  step 20: local=0.2510 global=1.2385\n",
            "  step 30: local=0.2518 global=1.1912\n",
            "  step 40: local=0.1138 global=1.1175\n",
            "  step 50: local=0.2556 global=1.1456\n",
            "  step 60: local=0.2617 global=1.1151\n",
            "  step 70: local=0.2625 global=1.1411\n",
            "  step 80: local=0.1099 global=1.1720\n",
            "  step 90: local=0.2574 global=1.1901\n",
            "  Layer 2 not converged (global=1.1242 > 0.4), repeating...\n",
            "\n",
            "--- Layer 2/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1106 global=1.1409\n",
            "  step 10: local=0.2605 global=1.1380\n",
            "  step 20: local=0.1109 global=1.1198\n",
            "  step 30: local=0.2542 global=1.2484\n",
            "  step 40: local=0.1145 global=1.2300\n",
            "  step 50: local=0.2531 global=1.1500\n",
            "  step 60: local=0.1099 global=1.2161\n",
            "  step 70: local=0.1089 global=1.2168\n",
            "  step 80: local=0.1110 global=1.0831\n",
            "  step 90: local=0.2560 global=1.1486\n",
            "  Layer 2 not converged (global=1.1707 > 0.4), repeating...\n",
            "\n",
            "--- Layer 2/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.2518 global=1.2822\n",
            "  step 10: local=0.1173 global=1.2571\n",
            "  step 20: local=0.2568 global=1.1232\n",
            "  step 30: local=0.2522 global=1.3775\n",
            "  step 40: local=0.2565 global=1.1685\n",
            "  step 50: local=0.1164 global=1.1407\n",
            "  step 60: local=0.1115 global=1.2179\n",
            "  step 70: local=0.2525 global=1.1289\n",
            "  step 80: local=0.1174 global=1.2697\n",
            "  step 90: local=0.1193 global=1.2455\n",
            "  Layer 2 not converged (global=1.1258 > 0.4), repeating...\n",
            "\n",
            "--- Layer 2/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.2595 global=1.1634\n",
            "  step 10: local=0.2615 global=1.2114\n",
            "  step 20: local=0.1158 global=1.1453\n",
            "  step 30: local=0.2559 global=1.2265\n",
            "  step 40: local=0.2602 global=1.1622\n",
            "  step 50: local=0.1075 global=1.1875\n",
            "  step 60: local=0.2585 global=1.1718\n",
            "  step 70: local=0.2618 global=1.2097\n",
            "  step 80: local=0.1149 global=1.1571\n",
            "  step 90: local=0.2537 global=1.2310\n",
            "  Layer 2 not converged (global=1.2115 > 0.4), repeating...\n",
            "\n",
            "--- Layer 2/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.2617 global=1.2459\n",
            "  step 10: local=0.2541 global=1.2947\n",
            "  step 20: local=0.1068 global=1.2175\n",
            "  step 30: local=0.2519 global=1.1861\n",
            "  step 40: local=0.1133 global=1.1746\n",
            "  step 50: local=0.1050 global=1.1385\n",
            "  step 60: local=0.1035 global=1.2240\n",
            "  step 70: local=0.2601 global=1.1477\n",
            "  step 80: local=0.2534 global=1.2035\n",
            "  step 90: local=0.1085 global=1.1954\n",
            "  Layer 2 not converged (global=1.1306 > 0.4), repeating...\n",
            "\n",
            "--- Layer 2/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1092 global=1.1843\n",
            "  step 10: local=0.1192 global=1.2799\n",
            "  step 20: local=0.1139 global=1.1965\n",
            "  step 30: local=0.2561 global=1.1501\n",
            "  step 40: local=0.1130 global=1.1104\n",
            "  step 50: local=0.2574 global=1.1156\n",
            "  step 60: local=0.2538 global=1.1487\n",
            "  step 70: local=0.2636 global=1.1891\n",
            "  step 80: local=0.1113 global=1.1061\n",
            "  step 90: local=0.1070 global=1.2240\n",
            "  Layer 2 not converged (global=1.1560 > 0.4), repeating...\n",
            "\n",
            "--- Layer 2/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.1087 global=1.1656\n",
            "  step 10: local=0.2591 global=1.2440\n",
            "  step 20: local=0.1155 global=1.1164\n",
            "  step 30: local=0.1187 global=1.0903\n",
            "  step 40: local=0.2601 global=1.1144\n",
            "  step 50: local=0.2583 global=1.1607\n",
            "  step 60: local=0.2591 global=1.1581\n",
            "  step 70: local=0.2608 global=1.1248\n",
            "  step 80: local=0.1118 global=1.1338\n",
            "  step 90: local=0.2511 global=1.2759\n",
            "  [WARN] Layer 2 did not converge after 20 repeats (global=1.2015)\n",
            "\n",
            "--- Layer 3/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0590 global=1.1839\n",
            "  step 10: local=0.0559 global=1.2475\n",
            "  step 20: local=0.0556 global=1.1558\n",
            "  step 30: local=0.0560 global=1.2227\n",
            "  step 40: local=0.0542 global=1.1892\n",
            "  step 50: local=0.0567 global=1.1185\n",
            "  step 60: local=0.0568 global=1.1782\n",
            "  step 70: local=0.0545 global=1.1952\n",
            "  step 80: local=0.0560 global=1.1770\n",
            "  step 90: local=0.0573 global=1.1114\n",
            "  Layer 3 not converged (global=1.1249 > 0.4), repeating...\n",
            "\n",
            "--- Layer 3/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0534 global=1.1372\n",
            "  step 10: local=0.0548 global=1.1377\n",
            "  step 20: local=0.0548 global=1.1509\n",
            "  step 30: local=0.0549 global=1.0772\n",
            "  step 40: local=0.0544 global=1.0873\n",
            "  step 50: local=0.0546 global=1.1068\n",
            "  step 60: local=0.0564 global=1.0972\n",
            "  step 70: local=0.0572 global=1.1107\n",
            "  step 80: local=0.0570 global=1.1099\n",
            "  step 90: local=0.0560 global=1.0727\n",
            "  Layer 3 not converged (global=1.0748 > 0.4), repeating...\n",
            "\n",
            "--- Layer 3/27 MLP (repeat 3/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0564 global=1.0403\n",
            "  step 10: local=0.0571 global=1.0884\n",
            "  step 20: local=0.0576 global=1.0660\n",
            "  step 30: local=0.0566 global=1.0852\n",
            "  step 40: local=0.0531 global=1.0557\n",
            "  step 50: local=0.0555 global=1.1019\n",
            "  step 60: local=0.0575 global=1.0648\n",
            "  step 70: local=0.0587 global=1.0313\n",
            "  step 80: local=0.0588 global=1.0508\n",
            "  step 90: local=0.0623 global=1.0430\n",
            "  Layer 3 not converged (global=1.0879 > 0.4), repeating...\n",
            "\n",
            "--- Layer 3/27 MLP (repeat 4/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0590 global=1.0248\n",
            "  step 10: local=0.0604 global=0.9962\n",
            "  step 20: local=0.0593 global=1.0556\n",
            "  step 30: local=0.0629 global=1.0647\n",
            "  step 40: local=0.0647 global=1.0714\n",
            "  step 50: local=0.0654 global=1.0121\n",
            "  step 60: local=0.0649 global=1.0496\n",
            "  step 70: local=0.0635 global=1.0394\n",
            "  step 80: local=0.0672 global=1.0222\n",
            "  step 90: local=0.0671 global=1.0182\n",
            "  Layer 3 not converged (global=0.9471 > 0.4), repeating...\n",
            "\n",
            "--- Layer 3/27 MLP (repeat 5/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0681 global=1.0024\n",
            "  step 10: local=0.0683 global=0.9992\n",
            "  step 20: local=0.0665 global=1.0472\n",
            "  step 30: local=0.0692 global=1.0310\n",
            "  step 40: local=0.0679 global=1.0198\n",
            "  step 50: local=0.0706 global=0.9632\n",
            "  step 60: local=0.0687 global=1.0056\n",
            "  step 70: local=0.0694 global=1.0482\n",
            "  step 80: local=0.0696 global=1.0018\n",
            "  step 90: local=0.0696 global=1.0069\n",
            "  Layer 3 not converged (global=1.1236 > 0.4), repeating...\n",
            "\n",
            "--- Layer 3/27 MLP (repeat 6/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0669 global=1.0282\n",
            "  step 10: local=0.0690 global=0.9941\n",
            "  step 20: local=0.0677 global=0.9596\n",
            "  step 30: local=0.0666 global=0.9538\n",
            "  step 40: local=0.0696 global=1.0142\n",
            "  step 50: local=0.0697 global=1.0128\n",
            "  step 60: local=0.0706 global=0.9710\n",
            "  step 70: local=0.0707 global=1.0752\n",
            "  step 80: local=0.0692 global=0.9872\n",
            "  step 90: local=0.0723 global=0.9688\n",
            "  Layer 3 not converged (global=1.0182 > 0.4), repeating...\n",
            "\n",
            "--- Layer 3/27 MLP (repeat 7/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0691 global=1.0603\n",
            "  step 10: local=0.0688 global=0.9970\n",
            "  step 20: local=0.0674 global=1.0522\n",
            "  step 30: local=0.0652 global=1.0077\n",
            "  step 40: local=0.0687 global=1.0057\n",
            "  step 50: local=0.0675 global=0.9911\n",
            "  step 60: local=0.0686 global=0.9799\n",
            "  step 70: local=0.0670 global=1.0232\n",
            "  step 80: local=0.0672 global=0.9868\n",
            "  step 90: local=0.0702 global=1.0386\n",
            "  Layer 3 not converged (global=0.9793 > 0.4), repeating...\n",
            "\n",
            "--- Layer 3/27 MLP (repeat 8/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0668 global=0.9396\n",
            "  step 10: local=0.0677 global=1.0323\n",
            "  step 20: local=0.0689 global=0.9792\n",
            "  step 30: local=0.0686 global=1.0355\n",
            "  step 40: local=0.0692 global=0.9820\n",
            "  step 50: local=0.0690 global=1.0215\n",
            "  step 60: local=0.0686 global=1.0470\n",
            "  step 70: local=0.0679 global=1.0287\n",
            "  step 80: local=0.0674 global=0.9726\n",
            "  step 90: local=0.0678 global=0.9986\n",
            "  Layer 3 not converged (global=1.0225 > 0.4), repeating...\n",
            "\n",
            "--- Layer 3/27 MLP (repeat 9/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0648 global=1.0788\n",
            "  step 10: local=0.0674 global=0.9938\n",
            "  step 20: local=0.0689 global=1.0056\n",
            "  step 30: local=0.0686 global=1.0227\n",
            "  step 40: local=0.0674 global=0.9558\n",
            "  step 50: local=0.0687 global=1.0145\n",
            "  step 60: local=0.0690 global=1.0324\n",
            "  step 70: local=0.0650 global=1.0265\n",
            "  step 80: local=0.0652 global=1.0309\n",
            "  step 90: local=0.0678 global=0.9771\n",
            "  Layer 3 not converged (global=0.9662 > 0.4), repeating...\n",
            "\n",
            "--- Layer 3/27 MLP (repeat 10/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0655 global=0.9771\n",
            "  step 10: local=0.0686 global=1.0127\n",
            "  step 20: local=0.0681 global=0.9741\n",
            "  step 30: local=0.0658 global=1.0367\n",
            "  step 40: local=0.0656 global=0.9957\n",
            "  step 50: local=0.0677 global=0.9998\n",
            "  step 60: local=0.0693 global=0.9872\n",
            "  step 70: local=0.0658 global=0.9487\n",
            "  step 80: local=0.0660 global=0.9697\n",
            "  step 90: local=0.0675 global=0.9650\n",
            "  Layer 3 not converged (global=0.9933 > 0.4), repeating...\n",
            "\n",
            "--- Layer 3/27 MLP (repeat 11/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0678 global=1.0078\n",
            "  step 10: local=0.0658 global=0.9846\n",
            "  step 20: local=0.0651 global=1.0115\n",
            "  step 30: local=0.0671 global=1.0034\n",
            "  step 40: local=0.0672 global=0.9421\n",
            "  step 50: local=0.0684 global=1.0050\n",
            "  step 60: local=0.0664 global=0.9373\n",
            "  step 70: local=0.0669 global=1.0087\n",
            "  step 80: local=0.0666 global=0.9938\n",
            "  step 90: local=0.0677 global=0.9340\n",
            "  Layer 3 not converged (global=0.9865 > 0.4), repeating...\n",
            "\n",
            "--- Layer 3/27 MLP (repeat 12/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0678 global=0.9986\n",
            "  step 10: local=0.0664 global=1.0171\n",
            "  step 20: local=0.0676 global=1.0152\n",
            "  step 30: local=0.0666 global=0.9523\n",
            "  step 40: local=0.0665 global=0.9866\n",
            "  step 50: local=0.0657 global=0.9943\n",
            "  step 60: local=0.0680 global=1.0077\n",
            "  step 70: local=0.0666 global=0.9478\n",
            "  step 80: local=0.0693 global=0.9572\n",
            "  step 90: local=0.0648 global=0.9780\n",
            "  Layer 3 not converged (global=0.9511 > 0.4), repeating...\n",
            "\n",
            "--- Layer 3/27 MLP (repeat 13/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0664 global=0.9706\n",
            "  step 10: local=0.0673 global=0.9943\n",
            "  step 20: local=0.0641 global=1.0032\n",
            "  step 30: local=0.0667 global=0.9591\n",
            "  step 40: local=0.0679 global=0.9310\n",
            "  step 50: local=0.0663 global=0.9829\n",
            "  step 60: local=0.0639 global=0.9687\n",
            "  step 70: local=0.0638 global=0.9910\n",
            "  step 80: local=0.0651 global=0.9699\n",
            "  step 90: local=0.0648 global=1.0142\n",
            "  Layer 3 not converged (global=0.9670 > 0.4), repeating...\n",
            "\n",
            "--- Layer 3/27 MLP (repeat 14/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0655 global=0.9807\n",
            "  step 10: local=0.0660 global=0.9566\n",
            "  step 20: local=0.0674 global=0.9770\n",
            "  step 30: local=0.0643 global=0.9671\n",
            "  step 40: local=0.0667 global=0.9580\n",
            "  step 50: local=0.0638 global=0.9335\n",
            "  step 60: local=0.0634 global=0.9923\n",
            "  step 70: local=0.0646 global=1.0084\n",
            "  step 80: local=0.0675 global=1.0192\n",
            "  step 90: local=0.0681 global=0.9608\n",
            "  Layer 3 not converged (global=0.9565 > 0.4), repeating...\n",
            "\n",
            "--- Layer 3/27 MLP (repeat 15/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0661 global=1.0021\n",
            "  step 10: local=0.0630 global=0.9961\n",
            "  step 20: local=0.0645 global=0.9832\n",
            "  step 30: local=0.0643 global=0.9797\n",
            "  step 40: local=0.0649 global=0.9659\n",
            "  step 50: local=0.0662 global=0.9642\n",
            "  step 60: local=0.0638 global=1.0129\n",
            "  step 70: local=0.0670 global=0.9972\n",
            "  step 80: local=0.0629 global=0.9878\n",
            "  step 90: local=0.0663 global=0.9336\n",
            "  Layer 3 not converged (global=1.0129 > 0.4), repeating...\n",
            "\n",
            "--- Layer 3/27 MLP (repeat 16/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0649 global=0.9756\n",
            "  step 10: local=0.0635 global=1.0203\n",
            "  step 20: local=0.0658 global=0.9744\n",
            "  step 30: local=0.0667 global=0.9807\n",
            "  step 40: local=0.0675 global=0.9999\n",
            "  step 50: local=0.0650 global=0.9712\n",
            "  step 60: local=0.0658 global=0.9382\n",
            "  step 70: local=0.0652 global=0.9315\n",
            "  step 80: local=0.0664 global=0.9943\n",
            "  step 90: local=0.0651 global=0.9907\n",
            "  Layer 3 not converged (global=0.9497 > 0.4), repeating...\n",
            "\n",
            "--- Layer 3/27 MLP (repeat 17/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0636 global=0.9511\n",
            "  step 10: local=0.0667 global=1.0547\n",
            "  step 20: local=0.0659 global=0.9680\n",
            "  step 30: local=0.0666 global=0.9519\n",
            "  step 40: local=0.0639 global=1.0381\n",
            "  step 50: local=0.0653 global=0.9790\n",
            "  step 60: local=0.0674 global=1.0339\n",
            "  step 70: local=0.0651 global=0.9906\n",
            "  step 80: local=0.0655 global=0.9878\n",
            "  step 90: local=0.0671 global=0.9754\n",
            "  Layer 3 not converged (global=0.9660 > 0.4), repeating...\n",
            "\n",
            "--- Layer 3/27 MLP (repeat 18/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0647 global=0.9648\n",
            "  step 10: local=0.0660 global=1.0072\n",
            "  step 20: local=0.0653 global=0.9714\n",
            "  step 30: local=0.0634 global=1.0225\n",
            "  step 40: local=0.0665 global=0.9250\n",
            "  step 50: local=0.0637 global=1.0169\n",
            "  step 60: local=0.0643 global=0.9644\n",
            "  step 70: local=0.0665 global=1.0213\n",
            "  step 80: local=0.0637 global=0.9682\n",
            "  step 90: local=0.0656 global=1.0088\n",
            "  Layer 3 not converged (global=0.9816 > 0.4), repeating...\n",
            "\n",
            "--- Layer 3/27 MLP (repeat 19/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0634 global=1.0338\n",
            "  step 10: local=0.0652 global=1.0173\n",
            "  step 20: local=0.0656 global=0.9604\n",
            "  step 30: local=0.0667 global=0.9861\n",
            "  step 40: local=0.0626 global=1.0650\n",
            "  step 50: local=0.0639 global=0.9796\n",
            "  step 60: local=0.0658 global=0.9934\n",
            "  step 70: local=0.0659 global=1.0114\n",
            "  step 80: local=0.0650 global=0.9452\n",
            "  step 90: local=0.0631 global=1.0023\n",
            "  Layer 3 not converged (global=0.9569 > 0.4), repeating...\n",
            "\n",
            "--- Layer 3/27 MLP (repeat 20/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0658 global=1.0219\n",
            "  step 10: local=0.0633 global=1.0159\n",
            "  step 20: local=0.0653 global=1.0217\n",
            "  step 30: local=0.0664 global=0.9669\n",
            "  step 40: local=0.0649 global=0.9662\n",
            "  step 50: local=0.0656 global=1.0018\n",
            "  step 60: local=0.0648 global=0.9636\n",
            "  step 70: local=0.0643 global=1.0252\n",
            "  step 80: local=0.0666 global=0.9857\n",
            "  step 90: local=0.0656 global=0.9900\n",
            "  [WARN] Layer 3 did not converge after 20 repeats (global=0.9730)\n",
            "\n",
            "--- Layer 4/27 MLP ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0491 global=0.9888\n",
            "  step 10: local=0.0498 global=0.9473\n",
            "  step 20: local=0.0515 global=0.9637\n",
            "  step 30: local=0.0498 global=0.9595\n",
            "  step 40: local=0.0499 global=1.0023\n",
            "  step 50: local=0.0495 global=0.9771\n",
            "  step 60: local=0.0495 global=0.9985\n",
            "  step 70: local=0.0487 global=0.9877\n",
            "  step 80: local=0.0478 global=0.9262\n",
            "  step 90: local=0.0486 global=0.9878\n",
            "  Layer 4 not converged (global=0.9416 > 0.4), repeating...\n",
            "\n",
            "--- Layer 4/27 MLP (repeat 2/20) ---\n",
            "  Trainable params: 9,437,187\n",
            "  step 0: local=0.0481 global=0.9190\n",
            "  step 10: local=0.0474 global=0.9857\n",
            "  step 20: local=0.0480 global=0.9718\n",
            "  step 30: local=0.0480 global=0.9112\n",
            "  step 40: local=0.0469 global=0.9735\n",
            "  step 50: local=0.0442 global=0.9878\n",
            "  step 60: local=0.0484 global=0.9822\n",
            "  step 70: local=0.0487 global=0.9189\n",
            "  step 80: local=0.0476 global=0.9545\n"
          ]
        }
      ],
      "source": [
        "# Full Progressive: MLP pass + E2E f-only (skip attention/refinement for v1)\n",
        "# For full 3-pass training, remove --skip_attention_pass and --skip_mlp_refinement\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/train_qat_progressive.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --output_dir {RUN_DIR_PROGRESSIVE} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --steps_per_layer_mlp {STEPS_PER_LAYER_MLP} \\\n",
        "  --e2e_steps {E2E_STEPS} \\\n",
        "  --local_weight {LOCAL_WEIGHT} \\\n",
        "  --global_weight {GLOBAL_WEIGHT} \\\n",
        "  --local_token_samples {LOCAL_TOKEN_SAMPLES} \\\n",
        "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
        "  --learning_rate {LR_PROGRESSIVE} \\\n",
        "  --e2e_learning_rate {LR_E2E} \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --skip_attention_pass \\\n",
        "  --max_layer_repeats 20 \\\n",
        "  --max_backtrack 5 \\\n",
        "  --layer_converge_threshold 0.4 \\\n",
        "  --skip_mlp_refinement"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# SAVE PROGRESSIVE QAT CHECKPOINT TO GOOGLE DRIVE\n",
        "# ============================================================\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive if not already mounted\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Source directory\n",
        "RUN_NAME = \"progressive_qat_v1\"\n",
        "RUN_DIR = f\"runs/{RUN_NAME}\"\n",
        "\n",
        "# Destination on Google Drive\n",
        "DEST_DIR_GD = \"/content/drive/MyDrive/qwen3_runs/\"\n",
        "!mkdir -p {DEST_DIR_GD}\n",
        "\n",
        "# Check if run directory exists and has content\n",
        "if os.path.isdir(RUN_DIR) and os.listdir(RUN_DIR):\n",
        "    # Compress the run directory\n",
        "    print(f\"[archive] Compressing {RUN_DIR}...\")\n",
        "    !tar -zcvf {RUN_NAME}.tgz -C runs {RUN_NAME}\n",
        "\n",
        "    # Copy to Google Drive\n",
        "    print(f\"[save] Copying {RUN_NAME}.tgz to Google Drive...\")\n",
        "    !rsync -ah --info=progress2 {RUN_NAME}.tgz {DEST_DIR_GD}\n",
        "\n",
        "    # Verify\n",
        "    gd_size = os.path.getsize(f\"{DEST_DIR_GD}/{RUN_NAME}.tgz\")\n",
        "    print(f\"[save] Saved to Google Drive: {gd_size / (1024**3):.2f} GB\")\n",
        "\n",
        "    # Cleanup local archive (optional)\n",
        "    # !rm {RUN_NAME}.tgz\n",
        "else:\n",
        "    print(f\"[save] ERROR: {RUN_DIR} is empty or doesn't exist\")\n",
        "    print(\"[save] Run progressive training first\")\n"
      ],
      "metadata": {
        "id": "dBFCVCZjwBo_"
      },
      "id": "dBFCVCZjwBo_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qiaq60t466e",
      "metadata": {
        "id": "qiaq60t466e"
      },
      "outputs": [],
      "source": [
        "# v3: Full 3-pass progressive training\n",
        "# MLP -> Attention -> MLP refinement -> E2E f-only\n",
        "# WARNING: May show instability at later layers for 2-bit\n",
        "\n",
        "RUN_DIR_PROGRESSIVE_V3 = \"runs/progressive_qat_v3\"\n",
        "\n",
        "!python scripts/train_qat_progressive.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --output_dir {RUN_DIR_PROGRESSIVE_V3} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
        "  --batch_size {BATCH_SIZE} \\\n",
        "  --steps_per_layer_mlp {STEPS_PER_LAYER_MLP} \\\n",
        "  --steps_per_layer_attn {STEPS_PER_LAYER_ATTN} \\\n",
        "  --e2e_steps {E2E_STEPS} \\\n",
        "  --local_weight {LOCAL_WEIGHT} \\\n",
        "  --global_weight {GLOBAL_WEIGHT} \\\n",
        "  --local_token_samples {LOCAL_TOKEN_SAMPLES} \\\n",
        "  --max_grad_norm {MAX_GRAD_NORM} \\\n",
        "  --learning_rate {LR_PROGRESSIVE} \\\n",
        "  --e2e_learning_rate {LR_E2E} \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ix3xjm2h0xq",
      "metadata": {
        "id": "ix3xjm2h0xq"
      },
      "outputs": [],
      "source": [
        "# Plot per-layer training progress\n",
        "# Change PLOT_RUN to visualize different runs\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Choose which run to visualize\n",
        "PLOT_RUN = RUN_DIR_E2E_FONLY  # or RUN_DIR_PROGRESSIVE, RUN_DIR_PROGRESSIVE_V3\n",
        "\n",
        "csv_path = f\"{PLOT_RUN}/loss_per_layer.csv\"\n",
        "if not os.path.exists(csv_path):\n",
        "    print(f\"Loss CSV not found at {csv_path}\")\n",
        "    print(\"Run training first or check the path.\")\n",
        "else:\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "    # Pass 1: MLP training (local loss)\n",
        "    mlp_df = df[(df['pass'] == 1) & (df['component'] == 'mlp')]\n",
        "    if not mlp_df.empty and 'local' in mlp_df.columns:\n",
        "        for layer in mlp_df['layer'].unique():\n",
        "            layer_df = mlp_df[mlp_df['layer'] == layer]\n",
        "            axes[0, 0].plot(layer_df['step'], layer_df['local'], label=f'L{layer}', alpha=0.7)\n",
        "        axes[0, 0].set_title('Pass 1: MLP Local Loss per Layer')\n",
        "        axes[0, 0].set_xlabel('Step')\n",
        "        axes[0, 0].set_ylabel('Loss')\n",
        "        axes[0, 0].legend(ncol=4, fontsize=6)\n",
        "    else:\n",
        "        axes[0, 0].set_title('Pass 1: MLP Local Loss (skipped or no local loss)')\n",
        "\n",
        "    # Pass 1: MLP global loss\n",
        "    if not mlp_df.empty and 'global' in mlp_df.columns:\n",
        "        for layer in mlp_df['layer'].unique():\n",
        "            layer_df = mlp_df[mlp_df['layer'] == layer]\n",
        "            axes[0, 1].plot(layer_df['step'], layer_df['global'], label=f'L{layer}', alpha=0.7)\n",
        "        axes[0, 1].set_title('Pass 1: MLP Global KD Loss per Layer')\n",
        "        axes[0, 1].set_xlabel('Step')\n",
        "        axes[0, 1].set_ylabel('Loss')\n",
        "        axes[0, 1].legend(ncol=4, fontsize=6)\n",
        "    else:\n",
        "        axes[0, 1].set_title('Pass 1: MLP Global Loss (skipped)')\n",
        "\n",
        "    # Pass 2: Attention training\n",
        "    attn_df = df[(df['pass'] == 2) & (df['component'] == 'attn')]\n",
        "    if not attn_df.empty and 'global' in attn_df.columns:\n",
        "        for layer in attn_df['layer'].unique():\n",
        "            layer_df = attn_df[attn_df['layer'] == layer]\n",
        "            axes[1, 0].plot(layer_df['step'], layer_df['global'], label=f'L{layer}', alpha=0.7)\n",
        "        axes[1, 0].set_title('Pass 2: Attention Global KD Loss per Layer')\n",
        "        axes[1, 0].set_xlabel('Step')\n",
        "        axes[1, 0].set_ylabel('Loss')\n",
        "        axes[1, 0].legend(ncol=4, fontsize=6)\n",
        "    else:\n",
        "        axes[1, 0].set_title('Pass 2: Attention (skipped)')\n",
        "        axes[1, 0].text(0.5, 0.5, 'Not run', ha='center', va='center', transform=axes[1, 0].transAxes)\n",
        "\n",
        "    # Pass 4: E2E f-only tuning\n",
        "    e2e_df = df[(df['pass'] == 4)]\n",
        "    if not e2e_df.empty and 'global' in e2e_df.columns:\n",
        "        axes[1, 1].plot(e2e_df['step'], e2e_df['global'], 'b-', linewidth=2)\n",
        "        axes[1, 1].set_title('Pass 4: E2E f-only Tuning')\n",
        "        axes[1, 1].set_xlabel('Step')\n",
        "        axes[1, 1].set_ylabel('Global KD Loss')\n",
        "    else:\n",
        "        axes[1, 1].set_title('Pass 4: E2E (not yet run)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{PLOT_RUN}/loss_per_layer.png\", dpi=150)\n",
        "    plt.show()\n",
        "    print(f\"Saved to {PLOT_RUN}/loss_per_layer.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qgtw9rd3cr",
      "metadata": {
        "id": "qgtw9rd3cr"
      },
      "source": [
        "### Inference Check: Progressive QAT Results\n",
        "\n",
        "Test the progressive QAT checkpoint with a quick inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4coakmebsik",
      "metadata": {
        "id": "4coakmebsik"
      },
      "outputs": [],
      "source": [
        "# Test inference with progressive QAT checkpoint\n",
        "# Change RUN_DIR to test different runs:\n",
        "#   RUN_DIR_E2E_FONLY, RUN_DIR_PROGRESSIVE_FONLY, RUN_DIR_PROGRESSIVE\n",
        "\n",
        "TEST_RUN = RUN_DIR_E2E_FONLY  # Change this to test other runs\n",
        "\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {TEST_RUN}/qat_state_dict.pt \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --prompt \"What is the capital of France?\" \\\n",
        "  --do_sample true \\\n",
        "  --max_new_tokens 64"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yq8GHX0o3c2y",
      "metadata": {
        "id": "Yq8GHX0o3c2y"
      },
      "source": [
        "## 6) Stage B: LoRA recovery\n",
        "\n",
        "Two options:\n",
        "- **SFT LoRA** (Alpaca-style instruction tuning)\n",
        "- **Cached KD-LoRA** (preserve teacher distribution; no new “skills”)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B1yimj8aS11y",
      "metadata": {
        "id": "B1yimj8aS11y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LcGFKyxO3c2y",
      "metadata": {
        "id": "LcGFKyxO3c2y"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STAGE B: LoRA Recovery (Cached KD-LoRA)\n",
        "# ============================================================\n",
        "# Train LoRA adapters on top of QAT checkpoint\n",
        "\n",
        "CACHE_DIR = CACHE_DIR_CHAT\n",
        "RUN_DIR_CACHE = \"runs/qwen3_kdqat_cache_q2\"\n",
        "\n",
        "LORA_DIM = 32\n",
        "LORA_RUN_KD = f\"runs/qwen3_lora_recovery_cached_r{LORA_DIM}\"\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/train_lora_recovery.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR_CACHE}/qat_state_dict.pt \\\n",
        "  --output_dir {LORA_RUN_KD} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --gradient_accumulation_steps 2 \\\n",
        "  --learning_rate 1e-5 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 1000 \\\n",
        "  --save_steps 3000 \\\n",
        "  --logging_steps 2 \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} \\\n",
        "  --lora_alpha {LORA_DIM} \\\n",
        "  --lora_dropout 0.0 \\\n",
        "  --kd_cache_dir {CACHE_DIR} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.02 \\\n",
        "  --hard-full-top1-weight 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Defule6L3c2y",
      "metadata": {
        "id": "Defule6L3c2y"
      },
      "source": [
        "## 7) Plot loss\n",
        "\n",
        "In Colab, use `--no_show` + `--save` then display the PNG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QM1RNfDh3c2y",
      "metadata": {
        "id": "QM1RNfDh3c2y"
      },
      "outputs": [],
      "source": [
        "!python scripts/plot_loss.py --run_dir {RUN_DIR} --source csv --no_show --save {RUN_DIR}/loss.png\n",
        "from PIL import Image\n",
        "display(Image.open(f\"{RUN_DIR}/loss.png\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p3bOUiMt3c2y",
      "metadata": {
        "id": "p3bOUiMt3c2y"
      },
      "source": [
        "## 8) Inference sanity checks\n",
        "\n",
        "Greedy decode (`--do_sample false`) and keep outputs short (`--max_new_tokens 16`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aA_cJHvT3c2y",
      "metadata": {
        "id": "aA_cJHvT3c2y"
      },
      "outputs": [],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2\"\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --lora_checkpoint \"runs/qwen3_lora_recovery_cached_r32/lora_only_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
        "  --prompt \"What is capital of France?\" \\\n",
        "  --do_sample false \\\n",
        "  --enable_thinking true \\\n",
        "  --max_new_tokens 40\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EJlu4sOZCNpY",
      "metadata": {
        "id": "EJlu4sOZCNpY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cD985HdXlm0",
      "metadata": {
        "id": "1cD985HdXlm0"
      },
      "outputs": [],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --lora_checkpoint \"runs/qwen3_lora_recovery_cached_r64/lora_only_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
        "  --prompt \"What is Apple Neural Engine?\" \\\n",
        "  --do_sample false \\\n",
        "  --enable_thinking true \\\n",
        "  --max_new_tokens 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xZ6aSDAa3c2y",
      "metadata": {
        "id": "xZ6aSDAa3c2y"
      },
      "outputs": [],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --lora_checkpoint \"runs/qwen3_lora_recovery_cached_r64/lora_only_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
        "  --prompt \"2+2=\" \\\n",
        "  --do_sample false \\\n",
        "  --enable_thinking true \\\n",
        "  --max_new_tokens 90\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZOIo97VlyAbh",
      "metadata": {
        "id": "ZOIo97VlyAbh"
      },
      "outputs": [],
      "source": [
        "LORA_DIM = 64\n",
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2\"\n",
        "!python scripts/run_inference.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}\"/qat_state_dict.pt\" \\\n",
        "  --device {DEVICE} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_DIM} --lora_alpha {LORA_DIM} --lora_dropout 0.0 \\\n",
        "  --prompt \"What is capital of France?\" \\\n",
        "  --do_sample true \\\n",
        "  --max_new_tokens 64\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ef_Q8P0i3c2y",
      "metadata": {
        "id": "Ef_Q8P0i3c2y"
      },
      "source": [
        "## 9) Optional: snap weights to the exact grid\n",
        "\n",
        "This produces a float checkpoint with weights snapped to the N-bit codebook (not bitpacked)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jk_qSZIs3c2y",
      "metadata": {
        "id": "jk_qSZIs3c2y"
      },
      "outputs": [],
      "source": [
        "RUN_DIR = \"runs/qwen3_kdqat_cache_q2_2\"\n",
        "!python scripts/hard_quantize_checkpoint.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_DIR}/checkpoint_last.pt \\\n",
        "  --output_path {RUN_DIR}/hard_quant_full_state_dict.pt \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --skip_lm_head\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7HnE7IFS3dK",
      "metadata": {
        "id": "f7HnE7IFS3dK"
      },
      "outputs": [],
      "source": [
        "%cd qwen3_apple_style_2bit_qat_lora\n",
        "%ls -l runs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beQD2eyDWWnb",
      "metadata": {
        "id": "beQD2eyDWWnb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}