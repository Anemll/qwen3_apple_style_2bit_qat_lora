{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# TPU Training Test - XLA Warmup Verification\n\n**Rev: 1.5 (2025-01-05 16:45) - Fix TPU lock (don't init in notebook)**\n\nQuick test notebook to verify TPU training works without hanging.\n\n**Requirements:**\n- Colab with TPU runtime (Runtime > Change runtime type > TPU)\n- ~4 min total (cache: ~1 min, training: ~3 min)\n\n**What this tests:**\n- XLA warmup precompilation (forward + backward + optimizer)\n- Training loop stability (20 steps)\n- Compilation count (should be 1-3, not 20+)\n\n**Expected output:**\n```\nReplacing with V2 layers... (parallel SVD init with 23 workers)\n...done (10-15s)\n\n[TPU] Warmup: compiling XLA graph... forward... backward... optimizer... done (90s)\nTraining: ....................\n[20/20] loss=X.XXXX ...\n[TPU] XLA compilations: 1\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check torch_xla is installed (but DON'T initialize TPU - that locks the device)\nimport os\nimport sys\n\ntry:\n    import torch_xla\n    print(f'torch_xla installed: {torch_xla.__version__}')\n    # Just verify TPU exists without initializing\n    tpu_env = os.environ.get('TPU_NAME', os.environ.get('COLAB_TPU_ADDR', 'not set'))\n    print(f'TPU environment: {tpu_env}')\n    print('TPU will be initialized by training script')\nexcept ImportError:\n    print('Installing torch_xla...')\n    !pip install torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html -q\n    import torch_xla\n    print(f'Installed torch_xla: {torch_xla.__version__}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/content')\n",
    "\n",
    "if os.path.exists('qwen3_apple_style_2bit_qat_lora'):\n",
    "    print('Repo exists, pulling latest...')\n",
    "    !cd qwen3_apple_style_2bit_qat_lora && git fetch && git reset --hard origin/main\n",
    "else:\n",
    "    print('Cloning repo...')\n",
    "    !git clone https://github.com/anemll/qwen3_apple_style_2bit_qat_lora.git\n",
    "\n",
    "os.chdir('/content/qwen3_apple_style_2bit_qat_lora')\n",
    "print(f'Working directory: {os.getcwd()}')\n",
    "\n",
    "# Show latest commit\n",
    "!git log --oneline -3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets accelerate jinja2>=3.1.0 -q\n",
    "print('Dependencies installed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Minimal Test Cache\n",
    "\n",
    "Small cache for quick testing (1K sequences, 64 tokens each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%time\n\nCACHE_DIR = 'caches/test_L64_K64_N1K'\n\nif os.path.exists(f'{CACHE_DIR}/meta.json'):\n    print(f'Cache exists: {CACHE_DIR}')\n    !cat {CACHE_DIR}/meta.json\nelse:\n    print('Generating test cache (1K sequences from Alpaca - fast download)...')\n    # Use Alpaca (52K examples, ~25MB) instead of OpenHermes (1M examples, 2GB)\n    !python scripts/precompute_teacher_topk.py \\\n        --output_dir {CACHE_DIR} \\\n        --teacher_model_name_or_path Qwen/Qwen3-0.6B \\\n        --dataset_name tatsu-lab/alpaca \\\n        --dataset_format alpaca \\\n        --max_length 64 \\\n        --topk 64 \\\n        --num_sequences 1000 \\\n        --batch_size 32 \\\n        --shard_size 500 \\\n        --dtype bf16\n    print('\\nCache generated!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Training Test (20 steps)\n",
    "\n",
    "This tests:\n",
    "1. XLA warmup phase (forward + backward + optimizer compilation)\n",
    "2. First few training steps\n",
    "3. No hang at optimizer step\n",
    "\n",
    "**Expected output:**\n",
    "- Warmup phase: ~60-120s (one-time compilation)\n",
    "- Step 1-20: Fast (< 1s per step after warmup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%time\n\n# Note: Layer init now uses parallel SVD (auto-detects CPU cores)\n# Remove --fast-init to get better initial loss with proper SVD initialization\n!python scripts/train_v2_simple.py \\\n    --from-scratch \\\n    --cache-dir caches/test_L64_K64_N1K \\\n    --output-dir runs/tpu_test \\\n    --config q4_r32 \\\n    --max-steps 20 \\\n    --batch-size 4 \\\n    --accumulation-steps 2 \\\n    --lr 3e-5 \\\n    --hard-top1 0.2 \\\n    --temperature 2.0 \\\n    --warmup-steps 5 \\\n    --tpu"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success Criteria\n",
    "\n",
    "The test passes if:\n",
    "1. Warmup completes with \"optimizer... done\"\n",
    "2. Training runs 20 steps without hanging\n",
    "3. Loss values are printed every 5 steps\n",
    "\n",
    "If it hangs after \"[TPU] Warmup: compiling XLA graph...\", the fix didn't work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Test Multi-Step Training\n",
    "\n",
    "If the quick test passes, try a longer run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Uncomment to run longer test (100 steps)\n# %%time\n# !python scripts/train_v2_simple.py \\\n#     --from-scratch \\\n#     --cache-dir caches/test_L64_K64_N1K \\\n#     --output-dir runs/tpu_test_100 \\\n#     --config q4_r32 \\\n#     --max-steps 100 \\\n#     --batch-size 8 \\\n#     --accumulation-steps 4 \\\n#     --lr 3e-5 \\\n#     --warmup-steps 10 \\\n#     --tpu"
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}