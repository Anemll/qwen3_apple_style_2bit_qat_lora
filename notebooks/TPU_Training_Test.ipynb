{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPU Training Test - XLA Warmup Verification\n",
    "\n",
    "Quick test notebook to verify TPU training works without hanging.\n",
    "\n",
    "**Requirements:**\n",
    "- Colab with TPU runtime (Runtime > Change runtime type > TPU)\n",
    "- ~5 min for cache generation + training test\n",
    "\n",
    "**What this tests:**\n",
    "- XLA compilation (forward + backward + optimizer)\n",
    "- Warmup phase precompilation\n",
    "- Training loop stability (20 steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check TPU\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Install torch_xla if needed\n",
    "try:\n",
    "    import torch_xla\n",
    "    print(f'torch_xla already installed: {torch_xla.__version__}')\n",
    "except ImportError:\n",
    "    print('Installing torch_xla...')\n",
    "    !pip install torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html -q\n",
    "    import torch_xla\n",
    "    print(f'Installed torch_xla: {torch_xla.__version__}')\n",
    "\n",
    "# Verify TPU\n",
    "import torch_xla.core.xla_model as xm\n",
    "device = xm.xla_device()\n",
    "print(f'TPU device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/content')\n",
    "\n",
    "if os.path.exists('qwen3_apple_style_2bit_qat_lora'):\n",
    "    print('Repo exists, pulling latest...')\n",
    "    !cd qwen3_apple_style_2bit_qat_lora && git fetch && git reset --hard origin/main\n",
    "else:\n",
    "    print('Cloning repo...')\n",
    "    !git clone https://github.com/anemll/qwen3_apple_style_2bit_qat_lora.git\n",
    "\n",
    "os.chdir('/content/qwen3_apple_style_2bit_qat_lora')\n",
    "print(f'Working directory: {os.getcwd()}')\n",
    "\n",
    "# Show latest commit\n",
    "!git log --oneline -3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets accelerate jinja2>=3.1.0 -q\n",
    "print('Dependencies installed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Minimal Test Cache\n",
    "\n",
    "Small cache for quick testing (1K sequences, 64 tokens each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "CACHE_DIR = 'caches/test_L64_K64_N1K'\n",
    "\n",
    "if os.path.exists(f'{CACHE_DIR}/meta.json'):\n",
    "    print(f'Cache exists: {CACHE_DIR}')\n",
    "    !cat {CACHE_DIR}/meta.json\n",
    "else:\n",
    "    print('Generating test cache (1K sequences)...')\n",
    "    !python scripts/precompute_teacher_topk.py \\\n",
    "        --output_dir {CACHE_DIR} \\\n",
    "        --teacher_model_name_or_path Qwen/Qwen3-0.6B \\\n",
    "        --dataset_name teknium/OpenHermes-2.5 \\\n",
    "        --dataset_format openhermes \\\n",
    "        --max_length 64 \\\n",
    "        --topk 64 \\\n",
    "        --num_sequences 1000 \\\n",
    "        --batch_size 32 \\\n",
    "        --shard_size 500 \\\n",
    "        --dtype bf16\n",
    "    print('\\nCache generated!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Training Test (20 steps)\n",
    "\n",
    "This tests:\n",
    "1. XLA warmup phase (forward + backward + optimizer compilation)\n",
    "2. First few training steps\n",
    "3. No hang at optimizer step\n",
    "\n",
    "**Expected output:**\n",
    "- Warmup phase: ~60-120s (one-time compilation)\n",
    "- Step 1-20: Fast (< 1s per step after warmup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%time\n\n!python scripts/train_v2_simple.py \\\n    --from-scratch \\\n    --cache-dir caches/test_L64_K64_N1K \\\n    --output-dir runs/tpu_test \\\n    --config q4_r32 \\\n    --max-steps 20 \\\n    --batch-size 4 \\\n    --accumulation-steps 2 \\\n    --lr 3e-5 \\\n    --hard-top1 0.2 \\\n    --temperature 2.0 \\\n    --warmup-steps 5 \\\n    --tpu"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success Criteria\n",
    "\n",
    "The test passes if:\n",
    "1. Warmup completes with \"optimizer... done\"\n",
    "2. Training runs 20 steps without hanging\n",
    "3. Loss values are printed every 5 steps\n",
    "\n",
    "If it hangs after \"[TPU] Warmup: compiling XLA graph...\", the fix didn't work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Test Multi-Step Training\n",
    "\n",
    "If the quick test passes, try a longer run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run longer test (100 steps)\n",
    "# %%time\n",
    "# !python scripts/train_v2_simple.py \\\n",
    "#     --from-scratch \\\n",
    "#     --cache-dir caches/test_L64_K64_N1K \\\n",
    "#     --output-dir runs/tpu_test_100 \\\n",
    "#     --config q4_r32 \\\n",
    "#     --max-steps 100 \\\n",
    "#     --batch-size 8 \\\n",
    "#     --accumulation-steps 4 \\\n",
    "#     --lr 3e-5 \\\n",
    "#     --warmup-steps 10 \\\n",
    "#     --logging-steps 20 \\\n",
    "#     --tpu"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}