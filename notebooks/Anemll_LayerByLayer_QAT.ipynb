{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anemll-Style Layer-by-Layer QAT\n",
    "\n",
    "This notebook implements layer-by-layer QAT training using `AnemllQATLinear` with:\n",
    "- Groupwise LUT quantization\n",
    "- Low-rank scale factors (A @ B)\n",
    "- KD cache for distillation\n",
    "\n",
    "## Pipeline:\n",
    "1. Load model and replace linears with AnemllQATLinear\n",
    "2. Layer-by-layer QAT (freeze all but current layer)\n",
    "3. End-to-end refinement\n",
    "4. (Optional) LoRA recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GOOGLE DRIVE PATHS (STANDARD)\n",
    "# ============================================================\n",
    "\n",
    "# Checkpoints/runs go here\n",
    "GD_RUNS = '/content/drive/MyDrive/qwen3_runs'\n",
    "\n",
    "# KD caches go here\n",
    "GD_CACHES = '/content/drive/MyDrive/qwen3_caches'\n",
    "\n",
    "# Local directories (on Colab VM)\n",
    "LOCAL_RUNS = 'runs'\n",
    "LOCAL_CACHES = 'caches'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo if needed\n",
    "!git clone https://github.com/anemll/qwen3_apple_style_2bit_qat_lora.git || (cd qwen3_apple_style_2bit_qat_lora && git pull)\n",
    "%cd qwen3_apple_style_2bit_qat_lora\n",
    "# to allow updates\n",
    "!git fetch\n",
    "!git pull\n",
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers accelerate safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD KD CACHE FROM GOOGLE DRIVE\n",
    "# ============================================================\n",
    "\n",
    "CACHE_NAME = 'alpaca_chat_think_both_L128_K32_R256'\n",
    "CACHE_TGZ = f'{CACHE_NAME}.tgz'\n",
    "\n",
    "!mkdir -p {LOCAL_CACHES}\n",
    "\n",
    "# Check if cache exists locally\n",
    "import os\n",
    "cache_local_path = f'{LOCAL_CACHES}/{CACHE_NAME}'\n",
    "if not os.path.exists(cache_local_path):\n",
    "    print(f'Extracting {CACHE_TGZ} from Google Drive...')\n",
    "    !tar -xzf {GD_CACHES}/{CACHE_TGZ} -C {LOCAL_CACHES}/\n",
    "else:\n",
    "    print(f'Cache already exists at {cache_local_path}')\n",
    "\n",
    "!ls -la {cache_local_path}/ | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "\n",
    "# Model\n",
    "MODEL_ID = 'Qwen/Qwen3-0.6B'\n",
    "\n",
    "# Quantization config (4-bit with groupwise LUT)\n",
    "LUT_SIZE = 16        # 4-bit = 16 levels\n",
    "GROUP_SIZE = 32      # Group size for scales\n",
    "SCALE_RANK = 4       # Low-rank for A @ B scales\n",
    "\n",
    "# Attention quantization (same params)\n",
    "ATTN_LUT_SIZE = 16\n",
    "ATTN_GROUP_SIZE = 32\n",
    "ATTN_SCALE_RANK = 8\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUM = 4\n",
    "LR = 2e-5\n",
    "EPOCHS_PER_LAYER = 1\n",
    "\n",
    "# KD params\n",
    "DISTILL_TEMP = 2.0\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DTYPE = torch.bfloat16\n",
    "\n",
    "print(f'Device: {DEVICE}, dtype: {DTYPE}')\n",
    "print(f'Quant config: lut={LUT_SIZE}, group={GROUP_SIZE}, rank={SCALE_RANK}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD MODEL\n",
    "# ============================================================\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(f'Loading {MODEL_ID}...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=DTYPE,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "print(f'Loaded. Parameters: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# REPLACE LINEARS WITH AnemllQATLinear\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from qat_lora.ane_qat_linear import AnemllQATLinear, AnemllQuantConfig\n",
    "import re\n",
    "import torch.nn as nn\n",
    "\n",
    "def replace_linear_with_anemll(\n",
    "    model: nn.Module,\n",
    "    mlp_config: AnemllQuantConfig,\n",
    "    attn_config: AnemllQuantConfig = None,\n",
    "    quantize_attn: bool = True,\n",
    "):\n",
    "    \"\"\"Replace MLP and optionally attention linears with AnemllQATLinear.\"\"\"\n",
    "    mlp_pattern = re.compile(r'\\.mlp\\.(gate_proj|up_proj|down_proj)$')\n",
    "    attn_pattern = re.compile(r'\\.self_attn\\.(q_proj|k_proj|v_proj|o_proj)$')\n",
    "    \n",
    "    replacements = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if not isinstance(module, nn.Linear):\n",
    "            continue\n",
    "        if isinstance(module, AnemllQATLinear):\n",
    "            continue\n",
    "        \n",
    "        # Check pattern\n",
    "        is_mlp = mlp_pattern.search(name)\n",
    "        is_attn = attn_pattern.search(name)\n",
    "        \n",
    "        if is_mlp:\n",
    "            cfg = mlp_config\n",
    "        elif is_attn and quantize_attn and attn_config:\n",
    "            cfg = attn_config\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Create replacement\n",
    "        new_module = AnemllQATLinear.from_linear(module, config=cfg)\n",
    "        \n",
    "        # Find parent\n",
    "        parts = name.rsplit('.', 1)\n",
    "        if len(parts) == 2:\n",
    "            parent_name, attr = parts\n",
    "            parent = dict(model.named_modules())[parent_name]\n",
    "        else:\n",
    "            parent = model\n",
    "            attr = name\n",
    "        \n",
    "        replacements.append((parent, attr, new_module, name))\n",
    "    \n",
    "    # Apply\n",
    "    for parent, attr, new_module, name in replacements:\n",
    "        setattr(parent, attr, new_module)\n",
    "        print(f'  [replaced] {name}')\n",
    "    \n",
    "    return len(replacements)\n",
    "\n",
    "# Create configs\n",
    "mlp_config = AnemllQuantConfig(\n",
    "    lut_size=LUT_SIZE,\n",
    "    group_size=GROUP_SIZE,\n",
    "    scale_rank=SCALE_RANK,\n",
    "    learnable_lut=False,\n",
    ")\n",
    "\n",
    "attn_config = AnemllQuantConfig(\n",
    "    lut_size=ATTN_LUT_SIZE,\n",
    "    group_size=ATTN_GROUP_SIZE,\n",
    "    scale_rank=ATTN_SCALE_RANK,\n",
    "    learnable_lut=False,\n",
    ")\n",
    "\n",
    "print('Replacing linear layers...')\n",
    "count = replace_linear_with_anemll(model, mlp_config, attn_config, quantize_attn=True)\n",
    "print(f'\\nReplaced {count} layers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INITIAL KD LOSS (before training)\n",
    "# ============================================================\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "\n",
    "def compute_kd_loss_batch(model, batch, device, temperature=2.0):\n",
    "    \"\"\"Compute KD loss for a batch using memory-efficient approach.\"\"\"\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch.get('attention_mask')\n",
    "    if attention_mask is not None:\n",
    "        attention_mask = attention_mask.to(device)\n",
    "    \n",
    "    topk_idx = batch['topk_idx'].to(device).long()\n",
    "    topk_logits = batch['topk_logits'].to(device).float()\n",
    "    \n",
    "    # Get hidden states (not full logits)\n",
    "    with torch.no_grad():\n",
    "        out = model.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            use_cache=False,\n",
    "            return_dict=True,\n",
    "        )\n",
    "    hidden = out.last_hidden_state[:, :-1, :]  # [B, S, H]\n",
    "    B, S, H = hidden.shape\n",
    "    \n",
    "    # Only compute logits for top-k\n",
    "    K = topk_idx.size(-1)\n",
    "    seq_len = min(S, topk_idx.size(1))\n",
    "    \n",
    "    h = hidden[:, :seq_len, :].reshape(B * seq_len, H)\n",
    "    idx = topk_idx[:, :seq_len, :].reshape(B * seq_len, K)\n",
    "    \n",
    "    w = model.lm_head.weight[idx]  # [N, K, H]\n",
    "    student_topk = torch.einsum('nh,nkh->nk', h, w).view(B, seq_len, K)\n",
    "    \n",
    "    # KL divergence with temperature\n",
    "    t_logits = topk_logits[:, :seq_len, :]\n",
    "    teacher_probs = F.softmax(t_logits / temperature, dim=-1)\n",
    "    student_log_probs = F.log_softmax(student_topk / temperature, dim=-1)\n",
    "    kl = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean') * (temperature ** 2)\n",
    "    \n",
    "    return kl\n",
    "\n",
    "def evaluate_kd_loss(model, cache_dir, device, num_samples=40, temperature=2.0):\n",
    "    \"\"\"Evaluate KD loss on cache samples.\"\"\"\n",
    "    cache_path = Path(cache_dir)\n",
    "    files = sorted(cache_path.glob('*.pt'))[:num_samples]\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for f in files:\n",
    "            data = torch.load(f, map_location='cpu', weights_only=True)\n",
    "            \n",
    "            # Make batch\n",
    "            batch = {\n",
    "                'input_ids': data['input_ids'].unsqueeze(0) if data['input_ids'].dim() == 1 else data['input_ids'],\n",
    "                'attention_mask': data.get('attention_mask'),\n",
    "                'topk_idx': data['topk_idx'].unsqueeze(0) if data['topk_idx'].dim() == 2 else data['topk_idx'],\n",
    "                'topk_logits': data['topk_logits'].unsqueeze(0) if data['topk_logits'].dim() == 2 else data['topk_logits'],\n",
    "            }\n",
    "            if batch['attention_mask'] is not None and batch['attention_mask'].dim() == 1:\n",
    "                batch['attention_mask'] = batch['attention_mask'].unsqueeze(0)\n",
    "            \n",
    "            loss = compute_kd_loss_batch(model, batch, device, temperature)\n",
    "            total_loss += loss.item()\n",
    "            count += 1\n",
    "    \n",
    "    return total_loss / max(1, count)\n",
    "\n",
    "print('Computing initial KD loss...')\n",
    "initial_loss = evaluate_kd_loss(model, cache_local_path, DEVICE, num_samples=40)\n",
    "print(f'Initial KD Loss: {initial_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LAYER-BY-LAYER QAT TRAINING\n",
    "# ============================================================\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "import time\n",
    "\n",
    "class KDCacheDataset(Dataset):\n",
    "    \"\"\"Dataset that loads KD cache files.\"\"\"\n",
    "    def __init__(self, cache_dir):\n",
    "        self.files = sorted(Path(cache_dir).glob('*.pt'))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.load(self.files[idx], map_location='cpu', weights_only=True)\n",
    "        return {\n",
    "            'input_ids': data['input_ids'],\n",
    "            'attention_mask': data.get('attention_mask', torch.ones_like(data['input_ids'])),\n",
    "            'topk_idx': data['topk_idx'],\n",
    "            'topk_logits': data['topk_logits'],\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([b['input_ids'] for b in batch]),\n",
    "        'attention_mask': torch.stack([b['attention_mask'] for b in batch]),\n",
    "        'topk_idx': torch.stack([b['topk_idx'] for b in batch]),\n",
    "        'topk_logits': torch.stack([b['topk_logits'] for b in batch]),\n",
    "    }\n",
    "\n",
    "def get_layer_modules(model, layer_idx):\n",
    "    \"\"\"Get all AnemllQATLinear modules in a specific layer.\"\"\"\n",
    "    layer = model.model.layers[layer_idx]\n",
    "    modules = []\n",
    "    for name, m in layer.named_modules():\n",
    "        if isinstance(m, AnemllQATLinear):\n",
    "            modules.append((f'layers.{layer_idx}.{name}', m))\n",
    "    return modules\n",
    "\n",
    "def freeze_all_except_layer(model, layer_idx):\n",
    "    \"\"\"Freeze all parameters except the specified layer's AnemllQATLinear weights.\"\"\"\n",
    "    # Freeze everything\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    # Unfreeze layer's quantized weights (NOT scales yet - freeze A, B)\n",
    "    layer_modules = get_layer_modules(model, layer_idx)\n",
    "    trainable = 0\n",
    "    for name, m in layer_modules:\n",
    "        # Only train the main weight, keep scales frozen for now\n",
    "        m.weight.requires_grad = True\n",
    "        trainable += m.weight.numel()\n",
    "        # Keep scale_A, scale_B, lut frozen\n",
    "        if m.scale_A is not None:\n",
    "            m.scale_A.requires_grad = False\n",
    "        if m.scale_B is not None:\n",
    "            m.scale_B.requires_grad = False\n",
    "    \n",
    "    return trainable\n",
    "\n",
    "def train_layer(model, layer_idx, dataloader, device, lr=2e-5, epochs=1, grad_accum=4):\n",
    "    \"\"\"Train a single layer.\"\"\"\n",
    "    trainable = freeze_all_except_layer(model, layer_idx)\n",
    "    print(f'\\n=== Layer {layer_idx} === ({trainable:,} trainable params)')\n",
    "    \n",
    "    # Get trainable params\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = AdamW(params, lr=lr)\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    steps = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            loss = compute_kd_loss_batch(model, batch, device, DISTILL_TEMP)\n",
    "            loss = loss / grad_accum\n",
    "            loss.backward()\n",
    "            \n",
    "            if (i + 1) % grad_accum == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                steps += 1\n",
    "                total_loss += loss.item() * grad_accum\n",
    "                \n",
    "                if steps % 10 == 0:\n",
    "                    avg = total_loss / steps\n",
    "                    print(f'  Step {steps}, Loss: {avg:.4f}')\n",
    "    \n",
    "    # Final eval\n",
    "    model.eval()\n",
    "    eval_loss = evaluate_kd_loss(model, cache_local_path, device, num_samples=20)\n",
    "    print(f'  Layer {layer_idx} done. Eval Loss: {eval_loss:.4f}')\n",
    "    \n",
    "    return eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RUN LAYER-BY-LAYER TRAINING\n",
    "# ============================================================\n",
    "\n",
    "# Create dataloader\n",
    "dataset = KDCacheDataset(cache_local_path)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "print(f'Dataset: {len(dataset)} samples')\n",
    "print(f'Batches per epoch: {len(dataloader)}')\n",
    "\n",
    "# Get number of layers\n",
    "num_layers = len(model.model.layers)\n",
    "print(f'Number of layers: {num_layers}')\n",
    "\n",
    "# Train layer by layer\n",
    "layer_losses = []\n",
    "t0 = time.time()\n",
    "\n",
    "for layer_idx in range(num_layers):\n",
    "    loss = train_layer(\n",
    "        model, layer_idx, dataloader, DEVICE,\n",
    "        lr=LR, epochs=EPOCHS_PER_LAYER, grad_accum=GRAD_ACCUM\n",
    "    )\n",
    "    layer_losses.append(loss)\n",
    "\n",
    "print(f'\\nLayer-by-layer training complete in {time.time() - t0:.1f}s')\n",
    "print(f'Final losses: {[f\"{l:.4f}\" for l in layer_losses]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUATE AFTER LAYER-BY-LAYER\n",
    "# ============================================================\n",
    "\n",
    "model.eval()\n",
    "post_layer_loss = evaluate_kd_loss(model, cache_local_path, DEVICE, num_samples=40)\n",
    "print(f'Initial KD Loss: {initial_loss:.4f}')\n",
    "print(f'After Layer-by-Layer: {post_layer_loss:.4f}')\n",
    "print(f'Improvement: {initial_loss - post_layer_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE CHECKPOINT\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "\n",
    "RUN_NAME = 'anemll_q4_layer_by_layer_v1'\n",
    "SAVE_DIR = f'{LOCAL_RUNS}/{RUN_NAME}'\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Save state dict\n",
    "torch.save(model.state_dict(), f'{SAVE_DIR}/model_state_dict.pt')\n",
    "\n",
    "# Save config\n",
    "import json\n",
    "config = {\n",
    "    'model_id': MODEL_ID,\n",
    "    'lut_size': LUT_SIZE,\n",
    "    'group_size': GROUP_SIZE,\n",
    "    'scale_rank': SCALE_RANK,\n",
    "    'attn_lut_size': ATTN_LUT_SIZE,\n",
    "    'attn_group_size': ATTN_GROUP_SIZE,\n",
    "    'attn_scale_rank': ATTN_SCALE_RANK,\n",
    "    'initial_kd_loss': initial_loss,\n",
    "    'post_layer_loss': post_layer_loss,\n",
    "    'layer_losses': layer_losses,\n",
    "}\n",
    "with open(f'{SAVE_DIR}/config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f'Saved to {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# UPLOAD TO GOOGLE DRIVE\n",
    "# ============================================================\n",
    "\n",
    "!tar -czvf {RUN_NAME}.tgz -C {LOCAL_RUNS} {RUN_NAME}\n",
    "!cp {RUN_NAME}.tgz {GD_RUNS}/\n",
    "print(f'Uploaded to {GD_RUNS}/{RUN_NAME}.tgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST INFERENCE\n",
    "# ============================================================\n",
    "\n",
    "def run_inference(model, tokenizer, prompt, max_new_tokens=128):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "        {'role': 'user', 'content': prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text, return_tensors='pt').to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "    \n",
    "    return tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "# Test\n",
    "model.eval()\n",
    "prompt = 'What is the capital of France?'\n",
    "response = run_inference(model, tokenizer, prompt)\n",
    "print(f'Prompt: {prompt}')\n",
    "print(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After layer-by-layer training, you can:\n",
    "\n",
    "1. **End-to-end refinement** - Unfreeze all layers and train together\n",
    "2. **Train scales (A, B)** - Unfreeze scale_A, scale_B parameters\n",
    "3. **LoRA recovery** - Add LoRA adapters to recover quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
