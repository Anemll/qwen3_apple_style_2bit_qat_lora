{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqWTWF5EFCuV"
      },
      "source": [
        "# Anemll-Style Layer-by-Layer QAT\n",
        "\n",
        "This notebook implements layer-by-layer QAT training using `AnemllQATLinear` with:\n",
        "- Groupwise LUT quantization\n",
        "- Low-rank scale factors (A @ B)\n",
        "- KD cache for distillation\n",
        "\n",
        "## Pipeline:\n",
        "1. Load model and replace linears with AnemllQATLinear\n",
        "2. Layer-by-layer QAT (freeze all but current layer)\n",
        "3. End-to-end refinement\n",
        "4. (Optional) LoRA recovery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Kfh54i9XFCuW"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# GOOGLE DRIVE PATHS (STANDARD)\n",
        "# ============================================================\n",
        "\n",
        "# Checkpoints/runs go here\n",
        "GD_RUNS = '/content/drive/MyDrive/qwen3_runs'\n",
        "\n",
        "# KD caches go here\n",
        "GD_CACHES = '/content/drive/MyDrive/qwen3_caches'\n",
        "\n",
        "# Local directories (on Colab VM)\n",
        "LOCAL_RUNS = 'runs'\n",
        "LOCAL_CACHES = 'caches'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Tw1k9-anFCuW",
        "outputId": "644d6577-3ce4-447f-f6a6-3a14bedf943d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GITUB"
      ],
      "metadata": {
        "id": "7X9uxbCPPykd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HtdvB50DFCuW",
        "outputId": "a4c53fae-0a27-4ba8-aa52-eed9910e1c12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'qwen3_apple_style_2bit_qat_lora' already exists and is not an empty directory.\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (4/4), 723 bytes | 723.00 KiB/s, done.\n",
            "From https://github.com/anemll/qwen3_apple_style_2bit_qat_lora\n",
            "   d5970d9..91f80e2  main       -> origin/main\n",
            "Updating d5970d9..91f80e2\n",
            "Fast-forward\n",
            " qat_lora/layer_qat.py | 8 \u001b[32m++++++\u001b[m\u001b[31m--\u001b[m\n",
            " 1 file changed, 6 insertions(+), 2 deletions(-)\n",
            "/content/qwen3_apple_style_2bit_qat_lora/qwen3_apple_style_2bit_qat_lora\n",
            "Already up to date.\n",
            "HEAD is now at 91f80e2 Refactor KL divergence computation in compute_kd_loss_batch for improved clarity and accuracy\n"
          ]
        }
      ],
      "source": [
        "# Clone repo if needed\n",
        "!git clone https://github.com/anemll/qwen3_apple_style_2bit_qat_lora.git || (cd qwen3_apple_style_2bit_qat_lora && git pull)\n",
        "%cd qwen3_apple_style_2bit_qat_lora\n",
        "# to allow updates\n",
        "!git fetch\n",
        "!git pull\n",
        "!git reset --hard HEAD\n",
        "import sys\n",
        "[sys.modules.pop(k) for k in list(sys.modules) if k.startswith('qat_lora')]\n",
        "\n",
        "from qat_lora import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "1dJ5vGK9FCuW"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers accelerate safetensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hgqQOvUpFCuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88f45e15-b330-4fd8-f9df-6b72aa08c832"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting alpaca_chat_think_both_L128_K32_R256.tgz from Google Drive...\n",
            "total 4298968\n",
            "drwx------ 2 root root      4096 Dec 18 00:00 .\n",
            "drwxr-xr-x 3 root root      4096 Dec 26 08:04 ..\n",
            "-rw------- 1 root root       421 Dec 18 00:15 meta.json\n",
            "-rw------- 1 root root 112692165 Dec 18 00:15 shard_00000.pt\n",
            "-rw------- 1 root root 112692165 Dec 18 00:15 shard_00001.pt\n",
            "-rw------- 1 root root 112692165 Dec 18 00:15 shard_00002.pt\n",
            "-rw------- 1 root root 112692165 Dec 18 00:15 shard_00003.pt\n",
            "-rw------- 1 root root 112692165 Dec 18 00:16 shard_00004.pt\n",
            "-rw------- 1 root root 112692165 Dec 18 00:16 shard_00005.pt\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# LOAD KD CACHE FROM GOOGLE DRIVE\n",
        "# ============================================================\n",
        "\n",
        "CACHE_NAME = 'alpaca_chat_think_both_L128_K32_R256'\n",
        "CACHE_TGZ = f'{CACHE_NAME}.tgz'\n",
        "\n",
        "!mkdir -p {LOCAL_CACHES}\n",
        "\n",
        "# Check if cache exists locally\n",
        "import os\n",
        "cache_local_path = f'{LOCAL_CACHES}/{CACHE_NAME}'\n",
        "if not os.path.exists(cache_local_path):\n",
        "    print(f'Extracting {CACHE_TGZ} from Google Drive...')\n",
        "    !tar -xzf {GD_CACHES}/{CACHE_TGZ} -C {LOCAL_CACHES}/\n",
        "else:\n",
        "    print(f'Cache already exists at {cache_local_path}')\n",
        "\n",
        "!ls -la {cache_local_path}/ | head -10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "EJ239_eUFCuW",
        "outputId": "3a887cb8-6521-4b1c-c652-59ff2e0dc9a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quality: q4_a4\n",
            "Device: cuda, dtype: torch.bfloat16\n",
            "Quant config: lut=16, group=32, rank=4\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================\n",
        "\n",
        "import torch\n",
        "\n",
        "# Model\n",
        "MODEL_ID = 'Qwen/Qwen3-0.6B'\n",
        "\n",
        "# Quantization config (4-bit with groupwise LUT)\n",
        "LUT_BITS = 4\n",
        "LUT_SIZE = 2**LUT_BITS\n",
        "GROUP_SIZE = 32      # Group size for scales\n",
        "SCALE_RANK = 4       # Low-rank for A @ B scales\n",
        "\n",
        "# Attention quantization (same params)\n",
        "ATTN_LUT_BITS = 4\n",
        "ATTN_LUT_SIZE = 2**ATTN_LUT_BITS\n",
        "ATTN_GROUP_SIZE = 32\n",
        "ATTN_SCALE_RANK = 8\n",
        "\n",
        "# Training\n",
        "BATCH_SIZE = 4\n",
        "GRAD_ACCUM = 4\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    BATCH_SIZE=64\n",
        "    GRAD_ACCUM=1\n",
        "\n",
        "LR = 2e-5\n",
        "EPOCHS_PER_LAYER = 1\n",
        "\n",
        "# KD params\n",
        "DISTILL_TEMP = 2.0\n",
        "\n",
        "# Device\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DTYPE = torch.bfloat16\n",
        "\n",
        "\n",
        "QUAL = f'q{LUT_BITS}_a{ATTN_LUT_BITS}'\n",
        "\n",
        "print(f'Quality: {QUAL}')\n",
        "\n",
        "print(f'Device: {DEVICE}, dtype: {DTYPE}')\n",
        "print(f'Quant config: lut={LUT_SIZE}, group={GROUP_SIZE}, rank={SCALE_RANK}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Extracting LOCAL CACHE\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Verify drive is mounted and cache exists\n",
        "if not os.path.exists('/content/drive/MyDrive'):\n",
        "    print('Google Drive not mounted! Mounting now...')\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "if not os.path.exists(cache_local_path):\n",
        "    print(f'Cache not found at {cache_local_path}')\n",
        "    print(f'Extracting from Google Drive...')\n",
        "    os.makedirs(LOCAL_CACHES, exist_ok=True)\n",
        "    !tar -xzf {GD_CACHES}/{CACHE_TGZ} -C {LOCAL_CACHES}/\n",
        "\n",
        "# Verify cache exists now\n",
        "assert os.path.exists(cache_local_path), f'Cache still not found at {cache_local_path}'\n",
        "cache_files = list(Path(cache_local_path).glob('*.pt'))\n",
        "print(f'Cache ready: {len(cache_files)} files in {cache_local_path}')"
      ],
      "metadata": {
        "id": "r-V8ZhsWGl1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "28436f15-94bd-4329-cfb1-bebc62073a9e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'cache_local_path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4186119626.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_local_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Cache not found at {cache_local_path}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Extracting from Google Drive...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cache_local_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5e5kQrkxFCuX",
        "outputId": "dac6a6f0-0eab-4df7-fd1a-721fe33c58e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Qwen/Qwen3-0.6B...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded. Parameters: 596,049,920\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# LOAD MODEL\n",
        "# ============================================================\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "print(f'Loading {MODEL_ID}...')\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=DTYPE,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "print(f'Loaded. Parameters: {sum(p.numel() for p in model.parameters()):,}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "n29f0NexFCuX",
        "outputId": "800c94df-904e-44c8-8d83-04e8745b215c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking model structure...\n",
            "  Found Linear: model.layers.0.self_attn.q_proj\n",
            "  Found Linear: model.layers.0.self_attn.k_proj\n",
            "  Found Linear: model.layers.0.self_attn.v_proj\n",
            "  Found Linear: model.layers.0.self_attn.o_proj\n",
            "  Found Linear: model.layers.0.mlp.gate_proj\n",
            "Total Linear modules: 197\n",
            "\n",
            "Replacing linear layers...\n",
            "  [replaced] model.layers.0.self_attn.q_proj\n",
            "  [replaced] model.layers.0.self_attn.k_proj\n",
            "  [replaced] model.layers.0.self_attn.v_proj\n",
            "  [replaced] model.layers.0.self_attn.o_proj\n",
            "  [replaced] model.layers.0.mlp.gate_proj\n",
            "  [replaced] model.layers.0.mlp.up_proj\n",
            "  [replaced] model.layers.0.mlp.down_proj\n",
            "  [replaced] model.layers.1.self_attn.q_proj\n",
            "  [replaced] model.layers.1.self_attn.k_proj\n",
            "  [replaced] model.layers.1.self_attn.v_proj\n",
            "  [replaced] model.layers.1.self_attn.o_proj\n",
            "  [replaced] model.layers.1.mlp.gate_proj\n",
            "  [replaced] model.layers.1.mlp.up_proj\n",
            "  [replaced] model.layers.1.mlp.down_proj\n",
            "  [replaced] model.layers.2.self_attn.q_proj\n",
            "  [replaced] model.layers.2.self_attn.k_proj\n",
            "  [replaced] model.layers.2.self_attn.v_proj\n",
            "  [replaced] model.layers.2.self_attn.o_proj\n",
            "  [replaced] model.layers.2.mlp.gate_proj\n",
            "  [replaced] model.layers.2.mlp.up_proj\n",
            "  [replaced] model.layers.2.mlp.down_proj\n",
            "  [replaced] model.layers.3.self_attn.q_proj\n",
            "  [replaced] model.layers.3.self_attn.k_proj\n",
            "  [replaced] model.layers.3.self_attn.v_proj\n",
            "  [replaced] model.layers.3.self_attn.o_proj\n",
            "  [replaced] model.layers.3.mlp.gate_proj\n",
            "  [replaced] model.layers.3.mlp.up_proj\n",
            "  [replaced] model.layers.3.mlp.down_proj\n",
            "  [replaced] model.layers.4.self_attn.q_proj\n",
            "  [replaced] model.layers.4.self_attn.k_proj\n",
            "  [replaced] model.layers.4.self_attn.v_proj\n",
            "  [replaced] model.layers.4.self_attn.o_proj\n",
            "  [replaced] model.layers.4.mlp.gate_proj\n",
            "  [replaced] model.layers.4.mlp.up_proj\n",
            "  [replaced] model.layers.4.mlp.down_proj\n",
            "  [replaced] model.layers.5.self_attn.q_proj\n",
            "  [replaced] model.layers.5.self_attn.k_proj\n",
            "  [replaced] model.layers.5.self_attn.v_proj\n",
            "  [replaced] model.layers.5.self_attn.o_proj\n",
            "  [replaced] model.layers.5.mlp.gate_proj\n",
            "  [replaced] model.layers.5.mlp.up_proj\n",
            "  [replaced] model.layers.5.mlp.down_proj\n",
            "  [replaced] model.layers.6.self_attn.q_proj\n",
            "  [replaced] model.layers.6.self_attn.k_proj\n",
            "  [replaced] model.layers.6.self_attn.v_proj\n",
            "  [replaced] model.layers.6.self_attn.o_proj\n",
            "  [replaced] model.layers.6.mlp.gate_proj\n",
            "  [replaced] model.layers.6.mlp.up_proj\n",
            "  [replaced] model.layers.6.mlp.down_proj\n",
            "  [replaced] model.layers.7.self_attn.q_proj\n",
            "  [replaced] model.layers.7.self_attn.k_proj\n",
            "  [replaced] model.layers.7.self_attn.v_proj\n",
            "  [replaced] model.layers.7.self_attn.o_proj\n",
            "  [replaced] model.layers.7.mlp.gate_proj\n",
            "  [replaced] model.layers.7.mlp.up_proj\n",
            "  [replaced] model.layers.7.mlp.down_proj\n",
            "  [replaced] model.layers.8.self_attn.q_proj\n",
            "  [replaced] model.layers.8.self_attn.k_proj\n",
            "  [replaced] model.layers.8.self_attn.v_proj\n",
            "  [replaced] model.layers.8.self_attn.o_proj\n",
            "  [replaced] model.layers.8.mlp.gate_proj\n",
            "  [replaced] model.layers.8.mlp.up_proj\n",
            "  [replaced] model.layers.8.mlp.down_proj\n",
            "  [replaced] model.layers.9.self_attn.q_proj\n",
            "  [replaced] model.layers.9.self_attn.k_proj\n",
            "  [replaced] model.layers.9.self_attn.v_proj\n",
            "  [replaced] model.layers.9.self_attn.o_proj\n",
            "  [replaced] model.layers.9.mlp.gate_proj\n",
            "  [replaced] model.layers.9.mlp.up_proj\n",
            "  [replaced] model.layers.9.mlp.down_proj\n",
            "  [replaced] model.layers.10.self_attn.q_proj\n",
            "  [replaced] model.layers.10.self_attn.k_proj\n",
            "  [replaced] model.layers.10.self_attn.v_proj\n",
            "  [replaced] model.layers.10.self_attn.o_proj\n",
            "  [replaced] model.layers.10.mlp.gate_proj\n",
            "  [replaced] model.layers.10.mlp.up_proj\n",
            "  [replaced] model.layers.10.mlp.down_proj\n",
            "  [replaced] model.layers.11.self_attn.q_proj\n",
            "  [replaced] model.layers.11.self_attn.k_proj\n",
            "  [replaced] model.layers.11.self_attn.v_proj\n",
            "  [replaced] model.layers.11.self_attn.o_proj\n",
            "  [replaced] model.layers.11.mlp.gate_proj\n",
            "  [replaced] model.layers.11.mlp.up_proj\n",
            "  [replaced] model.layers.11.mlp.down_proj\n",
            "  [replaced] model.layers.12.self_attn.q_proj\n",
            "  [replaced] model.layers.12.self_attn.k_proj\n",
            "  [replaced] model.layers.12.self_attn.v_proj\n",
            "  [replaced] model.layers.12.self_attn.o_proj\n",
            "  [replaced] model.layers.12.mlp.gate_proj\n",
            "  [replaced] model.layers.12.mlp.up_proj\n",
            "  [replaced] model.layers.12.mlp.down_proj\n",
            "  [replaced] model.layers.13.self_attn.q_proj\n",
            "  [replaced] model.layers.13.self_attn.k_proj\n",
            "  [replaced] model.layers.13.self_attn.v_proj\n",
            "  [replaced] model.layers.13.self_attn.o_proj\n",
            "  [replaced] model.layers.13.mlp.gate_proj\n",
            "  [replaced] model.layers.13.mlp.up_proj\n",
            "  [replaced] model.layers.13.mlp.down_proj\n",
            "  [replaced] model.layers.14.self_attn.q_proj\n",
            "  [replaced] model.layers.14.self_attn.k_proj\n",
            "  [replaced] model.layers.14.self_attn.v_proj\n",
            "  [replaced] model.layers.14.self_attn.o_proj\n",
            "  [replaced] model.layers.14.mlp.gate_proj\n",
            "  [replaced] model.layers.14.mlp.up_proj\n",
            "  [replaced] model.layers.14.mlp.down_proj\n",
            "  [replaced] model.layers.15.self_attn.q_proj\n",
            "  [replaced] model.layers.15.self_attn.k_proj\n",
            "  [replaced] model.layers.15.self_attn.v_proj\n",
            "  [replaced] model.layers.15.self_attn.o_proj\n",
            "  [replaced] model.layers.15.mlp.gate_proj\n",
            "  [replaced] model.layers.15.mlp.up_proj\n",
            "  [replaced] model.layers.15.mlp.down_proj\n",
            "  [replaced] model.layers.16.self_attn.q_proj\n",
            "  [replaced] model.layers.16.self_attn.k_proj\n",
            "  [replaced] model.layers.16.self_attn.v_proj\n",
            "  [replaced] model.layers.16.self_attn.o_proj\n",
            "  [replaced] model.layers.16.mlp.gate_proj\n",
            "  [replaced] model.layers.16.mlp.up_proj\n",
            "  [replaced] model.layers.16.mlp.down_proj\n",
            "  [replaced] model.layers.17.self_attn.q_proj\n",
            "  [replaced] model.layers.17.self_attn.k_proj\n",
            "  [replaced] model.layers.17.self_attn.v_proj\n",
            "  [replaced] model.layers.17.self_attn.o_proj\n",
            "  [replaced] model.layers.17.mlp.gate_proj\n",
            "  [replaced] model.layers.17.mlp.up_proj\n",
            "  [replaced] model.layers.17.mlp.down_proj\n",
            "  [replaced] model.layers.18.self_attn.q_proj\n",
            "  [replaced] model.layers.18.self_attn.k_proj\n",
            "  [replaced] model.layers.18.self_attn.v_proj\n",
            "  [replaced] model.layers.18.self_attn.o_proj\n",
            "  [replaced] model.layers.18.mlp.gate_proj\n",
            "  [replaced] model.layers.18.mlp.up_proj\n",
            "  [replaced] model.layers.18.mlp.down_proj\n",
            "  [replaced] model.layers.19.self_attn.q_proj\n",
            "  [replaced] model.layers.19.self_attn.k_proj\n",
            "  [replaced] model.layers.19.self_attn.v_proj\n",
            "  [replaced] model.layers.19.self_attn.o_proj\n",
            "  [replaced] model.layers.19.mlp.gate_proj\n",
            "  [replaced] model.layers.19.mlp.up_proj\n",
            "  [replaced] model.layers.19.mlp.down_proj\n",
            "  [replaced] model.layers.20.self_attn.q_proj\n",
            "  [replaced] model.layers.20.self_attn.k_proj\n",
            "  [replaced] model.layers.20.self_attn.v_proj\n",
            "  [replaced] model.layers.20.self_attn.o_proj\n",
            "  [replaced] model.layers.20.mlp.gate_proj\n",
            "  [replaced] model.layers.20.mlp.up_proj\n",
            "  [replaced] model.layers.20.mlp.down_proj\n",
            "  [replaced] model.layers.21.self_attn.q_proj\n",
            "  [replaced] model.layers.21.self_attn.k_proj\n",
            "  [replaced] model.layers.21.self_attn.v_proj\n",
            "  [replaced] model.layers.21.self_attn.o_proj\n",
            "  [replaced] model.layers.21.mlp.gate_proj\n",
            "  [replaced] model.layers.21.mlp.up_proj\n",
            "  [replaced] model.layers.21.mlp.down_proj\n",
            "  [replaced] model.layers.22.self_attn.q_proj\n",
            "  [replaced] model.layers.22.self_attn.k_proj\n",
            "  [replaced] model.layers.22.self_attn.v_proj\n",
            "  [replaced] model.layers.22.self_attn.o_proj\n",
            "  [replaced] model.layers.22.mlp.gate_proj\n",
            "  [replaced] model.layers.22.mlp.up_proj\n",
            "  [replaced] model.layers.22.mlp.down_proj\n",
            "  [replaced] model.layers.23.self_attn.q_proj\n",
            "  [replaced] model.layers.23.self_attn.k_proj\n",
            "  [replaced] model.layers.23.self_attn.v_proj\n",
            "  [replaced] model.layers.23.self_attn.o_proj\n",
            "  [replaced] model.layers.23.mlp.gate_proj\n",
            "  [replaced] model.layers.23.mlp.up_proj\n",
            "  [replaced] model.layers.23.mlp.down_proj\n",
            "  [replaced] model.layers.24.self_attn.q_proj\n",
            "  [replaced] model.layers.24.self_attn.k_proj\n",
            "  [replaced] model.layers.24.self_attn.v_proj\n",
            "  [replaced] model.layers.24.self_attn.o_proj\n",
            "  [replaced] model.layers.24.mlp.gate_proj\n",
            "  [replaced] model.layers.24.mlp.up_proj\n",
            "  [replaced] model.layers.24.mlp.down_proj\n",
            "  [replaced] model.layers.25.self_attn.q_proj\n",
            "  [replaced] model.layers.25.self_attn.k_proj\n",
            "  [replaced] model.layers.25.self_attn.v_proj\n",
            "  [replaced] model.layers.25.self_attn.o_proj\n",
            "  [replaced] model.layers.25.mlp.gate_proj\n",
            "  [replaced] model.layers.25.mlp.up_proj\n",
            "  [replaced] model.layers.25.mlp.down_proj\n",
            "  [replaced] model.layers.26.self_attn.q_proj\n",
            "  [replaced] model.layers.26.self_attn.k_proj\n",
            "  [replaced] model.layers.26.self_attn.v_proj\n",
            "  [replaced] model.layers.26.self_attn.o_proj\n",
            "  [replaced] model.layers.26.mlp.gate_proj\n",
            "  [replaced] model.layers.26.mlp.up_proj\n",
            "  [replaced] model.layers.26.mlp.down_proj\n",
            "  [replaced] model.layers.27.self_attn.q_proj\n",
            "  [replaced] model.layers.27.self_attn.k_proj\n",
            "  [replaced] model.layers.27.self_attn.v_proj\n",
            "  [replaced] model.layers.27.self_attn.o_proj\n",
            "  [replaced] model.layers.27.mlp.gate_proj\n",
            "  [replaced] model.layers.27.mlp.up_proj\n",
            "  [replaced] model.layers.27.mlp.down_proj\n",
            "\n",
            "Replaced 196 layers\n",
            "\n",
            "Verification: 0 AnemllQATLinear modules in model\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# REPLACE LINEARS WITH AnemllQATLinear\n",
        "# ============================================================\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "# Force reimport to get latest code\n",
        "import importlib\n",
        "import qat_lora\n",
        "importlib.reload(qat_lora)\n",
        "import qat_lora.ane_qat_linear as ane_module\n",
        "importlib.reload(ane_module)\n",
        "import qat_lora.layer_qat as layer_module\n",
        "importlib.reload(layer_module)\n",
        "\n",
        "from qat_lora import AnemllQuantConfig, replace_linear_with_anemll\n",
        "\n",
        "# Debug: Check what modules exist in the model\n",
        "print(\"Checking model structure...\")\n",
        "import torch.nn as nn\n",
        "linear_count = 0\n",
        "for name, m in model.named_modules():\n",
        "    if isinstance(m, nn.Linear):\n",
        "        linear_count += 1\n",
        "        if linear_count <= 5:\n",
        "            print(f\"  Found Linear: {name}\")\n",
        "print(f\"Total Linear modules: {linear_count}\")\n",
        "\n",
        "# Create configs\n",
        "mlp_config = AnemllQuantConfig(\n",
        "    lut_size=LUT_SIZE,\n",
        "    group_size=GROUP_SIZE,\n",
        "    scale_rank=SCALE_RANK,\n",
        "    learnable_lut=False,\n",
        ")\n",
        "\n",
        "attn_config = AnemllQuantConfig(\n",
        "    lut_size=ATTN_LUT_SIZE,\n",
        "    group_size=ATTN_GROUP_SIZE,\n",
        "    scale_rank=ATTN_SCALE_RANK,\n",
        "    learnable_lut=False,\n",
        ")\n",
        "\n",
        "print('\\nReplacing linear layers...')\n",
        "count = replace_linear_with_anemll(\n",
        "    model,\n",
        "    mlp_config=mlp_config,\n",
        "    attn_config=attn_config,\n",
        "    quantize_attn=True,\n",
        "    quantize_lm_head=False,\n",
        ")\n",
        "\n",
        "# Verify replacement worked\n",
        "from qat_lora import AnemllQATLinear\n",
        "qat_count = sum(1 for _, m in model.named_modules() if isinstance(m, AnemllQATLinear))\n",
        "print(f\"\\nVerification: {qat_count} AnemllQATLinear modules in model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "D_gOyY1qFCuX",
        "outputId": "c6fcb612-6372-4401-a0b1-828a6a910c2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer QAT utilities imported from qat_lora\n",
            "\n",
            "Verifying gradient flow...\n",
            "ERROR: No AnemllQATLinear modules found! Replacement failed.\n",
            "\n",
            "Computing initial KD loss...\n",
            "Initial KD Loss: 1.0885\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# IMPORT LAYER-BY-LAYER QAT UTILITIES & VERIFY GRADIENTS\n",
        "# ============================================================\n",
        "\n",
        "from qat_lora import (\n",
        "    evaluate_kd_loss,\n",
        "    train_all_layers,\n",
        "    AnemllQATLinear,\n",
        ")\n",
        "\n",
        "print('Layer QAT utilities imported from qat_lora')\n",
        "\n",
        "# Verify gradient flow works\n",
        "print('\\nVerifying gradient flow...')\n",
        "layer0 = model.model.layers[0]\n",
        "test_module = None\n",
        "for name, m in layer0.named_modules():\n",
        "    if isinstance(m, AnemllQATLinear):\n",
        "        test_module = m\n",
        "        break\n",
        "\n",
        "if test_module is None:\n",
        "    print(\"ERROR: No AnemllQATLinear modules found! Replacement failed.\")\n",
        "else:\n",
        "    # Test gradient flow\n",
        "    test_module.weight.requires_grad = True\n",
        "    x = torch.randn(1, 10, test_module.in_features, device=DEVICE, dtype=DTYPE)\n",
        "    y = test_module(x)\n",
        "    loss = y.sum()\n",
        "    try:\n",
        "        loss.backward()\n",
        "        if test_module.weight.grad is not None:\n",
        "            print(f\"  Gradient OK: weight.grad.shape = {test_module.weight.grad.shape}\")\n",
        "            test_module.weight.grad = None  # Clear for actual training\n",
        "        else:\n",
        "            print(\"  ERROR: weight.grad is None after backward!\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ERROR during backward: {e}\")\n",
        "\n",
        "# Compute initial KD loss\n",
        "print('\\nComputing initial KD loss...')\n",
        "initial_loss = evaluate_kd_loss(model, cache_local_path, DEVICE, num_samples=40, temperature=DISTILL_TEMP)\n",
        "print(f'Initial KD Loss: {initial_loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SCALE OPTIMIZATION** (Weights Frozen)\n",
        "\n",
        "After layer-by-layer QAT on weights, optimize the per-weight scales (A @ B) to further reduce quantization error.\n",
        "\n",
        "- Weights are **frozen**\n",
        "- Only `scale_A` and `scale_B` are trained\n",
        "- Much fewer parameters → can use higher learning rate"
      ],
      "metadata": {
        "id": "9B4uAJDNfRgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# LAYER-BY-LAYER SCALE OPTIMIZATION\n",
        "# ============================================================\n",
        "# Freeze weights, only train scale_A and scale_B tensors\n",
        "# Higher LR since fewer parameters\n",
        "\n",
        "SCALE_LR = 1e-3  # Higher LR for scales (fewer params)\n",
        "SCALE_EPOCHS = 2  # More epochs since scales have less capacity\n",
        "\n",
        "\n",
        "print('Starting scale-only layer-by-layer optimization...')\n",
        "print(f'LR: {SCALE_LR}, Epochs per layer: {SCALE_EPOCHS}')\n",
        "\n",
        "# Get loss before scale optimization\n",
        "pre_scale_loss = evaluate_kd_loss(model, cache_local_path, DEVICE, num_samples=40)\n",
        "print(f'KD Loss before scale optimization: {pre_scale_loss:.4f}')\n",
        "\n",
        "# Train scales layer-by-layer\n",
        "scale_losses = train_all_layers(\n",
        "    model=model,\n",
        "    cache_dir=cache_local_path,\n",
        "    device=DEVICE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    lr=SCALE_LR,\n",
        "    epochs_per_layer=SCALE_EPOCHS,\n",
        "    grad_accum=GRAD_ACCUM,\n",
        "    temperature=DISTILL_TEMP,\n",
        "    train_weights=False,  # Freeze weights\n",
        "    train_scales=True,    # Train scales only\n",
        "    local_weight=0.5,\n",
        "    global_weight=0.5,\n",
        "    verbose=True,\n",
        "    steps_per_layer=100,\n",
        ")\n",
        "\n",
        "# Evaluate after scale optimization\n",
        "post_scale_loss = evaluate_kd_loss(model, cache_local_path, DEVICE, num_samples=40)\n",
        "print(f'\\n=== Scale Optimization Results ===')\n",
        "print(f'Before: {pre_scale_loss:.4f}')\n",
        "print(f'After:  {post_scale_loss:.4f}')\n",
        "print(f'Improvement: {pre_scale_loss - post_scale_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ1kq2a9fRgs",
        "outputId": "84a4cabf-6962-421a-b259-41f9beea1f2e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting scale-only layer-by-layer optimization...\n",
            "LR: 0.001, Epochs per layer: 2\n",
            "KD Loss before scale optimization: 1.0885\n",
            "Training 28 layers (mode=scales only)...\n",
            "Cache: caches/alpaca_chat_think_both_L128_K32_R256\n",
            "Batch size: 64, Grad accum: 1\n",
            "LR: 0.001, Steps per layer: 100\n",
            "\n",
            "[Initial Global KD Loss]: 1.0885\n",
            "\n",
            "=== Layer 0 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 1.0345\n",
            "  step 10: local=0.0443 global=0.8040 (3.5s)\n",
            "  step 20: local=0.0697 global=0.8055 (6.8s)\n",
            "  step 30: local=0.0735 global=0.7204 (10.1s)\n",
            "  step 40: local=0.0670 global=0.6938 (13.3s)\n",
            "  step 50: local=0.0644 global=0.7117 (16.7s)\n",
            "  step 60: local=0.0652 global=0.6485 (19.9s)\n",
            "  step 70: local=0.0666 global=0.6576 (23.2s)\n",
            "  step 80: local=0.0649 global=0.6751 (26.4s)\n",
            "  step 90: local=0.0672 global=0.6351 (29.8s)\n",
            "  step 100: local=0.0656 global=0.6617 (33.0s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0278 -> 0.0656 (Δ=-0.0378)\n",
            "  [Global Loss]:  0.9705 -> 0.6617 (Δ=0.3088)\n",
            "  [Eval KD]:      1.0345 -> 0.6805 (Δ=0.3540, 34.2%)\n",
            "  [Time]:         37.3s\n",
            "\n",
            "[Progress: 1/28] Elapsed: 0:37, ETA: 16:46\n",
            "\n",
            "=== Layer 1 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 0.6805\n",
            "  step 10: local=0.0377 global=0.6446 (3.3s)\n",
            "  step 20: local=0.0468 global=0.6478 (6.5s)\n",
            "  step 30: local=0.0549 global=0.6087 (9.7s)\n",
            "  step 40: local=0.0532 global=0.6958 (12.9s)\n",
            "  step 50: local=0.0531 global=0.6198 (16.1s)\n",
            "  step 60: local=0.0590 global=0.5949 (19.3s)\n",
            "  step 70: local=0.0570 global=0.5929 (22.5s)\n",
            "  step 80: local=0.0579 global=0.6141 (25.7s)\n",
            "  step 90: local=0.0541 global=0.5895 (29.0s)\n",
            "  step 100: local=0.0572 global=0.6229 (32.2s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0362 -> 0.0572 (Δ=-0.0210)\n",
            "  [Global Loss]:  0.6138 -> 0.6229 (Δ=-0.0091)\n",
            "  [Eval KD]:      0.6805 -> 0.6117 (Δ=0.0688, 10.1%)\n",
            "  [Time]:         36.5s\n",
            "\n",
            "[Progress: 2/28] Elapsed: 1:13, ETA: 15:58\n",
            "\n",
            "=== Layer 2 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 0.6117\n",
            "  step 10: local=0.0458 global=0.4965 (3.2s)\n",
            "  step 20: local=0.0548 global=0.5067 (6.4s)\n",
            "  step 30: local=0.0510 global=0.4931 (9.5s)\n",
            "  step 40: local=0.0416 global=0.4655 (12.6s)\n",
            "  step 50: local=0.0426 global=0.4662 (15.8s)\n",
            "  step 60: local=0.0434 global=0.4586 (18.9s)\n",
            "  step 70: local=0.0540 global=0.4526 (22.0s)\n",
            "  step 80: local=0.0433 global=0.4428 (25.2s)\n",
            "  step 90: local=0.0458 global=0.4648 (28.3s)\n",
            "  step 100: local=0.0465 global=0.4627 (31.5s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0381 -> 0.0465 (Δ=-0.0084)\n",
            "  [Global Loss]:  0.5937 -> 0.4627 (Δ=0.1310)\n",
            "  [Eval KD]:      0.6117 -> 0.4545 (Δ=0.1572, 25.7%)\n",
            "  [Time]:         35.7s\n",
            "\n",
            "[Progress: 3/28] Elapsed: 1:49, ETA: 15:11\n",
            "\n",
            "=== Layer 3 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 0.4545\n",
            "  step 10: local=0.0387 global=0.4403 (3.1s)\n",
            "  step 20: local=0.0445 global=0.4225 (6.2s)\n",
            "  step 30: local=0.0442 global=0.4565 (9.3s)\n",
            "  step 40: local=0.0434 global=0.4213 (12.4s)\n",
            "  step 50: local=0.0448 global=0.4398 (15.5s)\n",
            "  step 60: local=0.0445 global=0.4204 (18.6s)\n",
            "  step 70: local=0.0434 global=0.4404 (21.6s)\n",
            "  step 80: local=0.0449 global=0.4712 (24.7s)\n",
            "  step 90: local=0.0439 global=0.4462 (27.9s)\n",
            "  step 100: local=0.0442 global=0.4250 (30.9s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0388 -> 0.0442 (Δ=-0.0054)\n",
            "  [Global Loss]:  0.4216 -> 0.4250 (Δ=-0.0034)\n",
            "  [Eval KD]:      0.4545 -> 0.4467 (Δ=0.0077, 1.7%)\n",
            "  [Time]:         35.1s\n",
            "\n",
            "[Progress: 4/28] Elapsed: 2:24, ETA: 14:27\n",
            "\n",
            "=== Layer 4 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 0.4467\n",
            "  step 10: local=0.0367 global=0.4377 (3.1s)\n",
            "  step 20: local=0.0407 global=0.4413 (6.1s)\n",
            "  step 30: local=0.0436 global=0.4406 (9.1s)\n",
            "  step 40: local=0.0429 global=0.4978 (12.1s)\n",
            "  step 50: local=0.0422 global=0.4390 (15.2s)\n",
            "  step 60: local=0.0434 global=0.4396 (18.2s)\n",
            "  step 70: local=0.0425 global=0.4347 (21.2s)\n",
            "  step 80: local=0.0429 global=0.4249 (24.2s)\n",
            "  step 90: local=0.0423 global=0.4185 (27.3s)\n",
            "  step 100: local=0.0428 global=0.4054 (30.3s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0358 -> 0.0428 (Δ=-0.0070)\n",
            "  [Global Loss]:  0.4449 -> 0.4054 (Δ=0.0396)\n",
            "  [Eval KD]:      0.4467 -> 0.4457 (Δ=0.0010, 0.2%)\n",
            "  [Time]:         34.5s\n",
            "\n",
            "[Progress: 5/28] Elapsed: 2:59, ETA: 13:43\n",
            "\n",
            "=== Layer 5 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 0.4457\n",
            "  step 10: local=0.0378 global=0.4352 (3.0s)\n",
            "  step 20: local=0.0416 global=0.4222 (6.0s)\n",
            "  step 30: local=0.0428 global=0.3875 (8.9s)\n",
            "  step 40: local=0.0424 global=0.4293 (11.9s)\n",
            "  step 50: local=0.0423 global=0.4086 (14.9s)\n",
            "  step 60: local=0.0437 global=0.4131 (17.8s)\n",
            "  step 70: local=0.0440 global=0.4117 (20.8s)\n",
            "  step 80: local=0.0449 global=0.4051 (23.8s)\n",
            "  step 90: local=0.0441 global=0.4459 (26.8s)\n",
            "  step 100: local=0.0443 global=0.4400 (29.7s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0375 -> 0.0443 (Δ=-0.0067)\n",
            "  [Global Loss]:  0.3873 -> 0.4400 (Δ=-0.0526)\n",
            "  [Eval KD]:      0.4457 -> 0.4344 (Δ=0.0113, 2.5%)\n",
            "  [Time]:         33.9s\n",
            "\n",
            "[Progress: 6/28] Elapsed: 3:32, ETA: 13:00\n",
            "\n",
            "=== Layer 6 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 0.4344\n",
            "  step 10: local=0.0391 global=0.4150 (2.9s)\n",
            "  step 20: local=0.0419 global=0.3985 (5.8s)\n",
            "  step 30: local=0.0425 global=0.4113 (8.7s)\n",
            "  step 40: local=0.0433 global=0.3979 (11.6s)\n",
            "  step 50: local=0.0422 global=0.4069 (14.6s)\n",
            "  step 60: local=0.0440 global=0.3982 (17.4s)\n",
            "  step 70: local=0.0435 global=0.3879 (20.3s)\n",
            "  step 80: local=0.0444 global=0.4095 (23.2s)\n",
            "  step 90: local=0.0429 global=0.3944 (26.2s)\n",
            "  step 100: local=0.0449 global=0.4055 (29.1s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0361 -> 0.0449 (Δ=-0.0088)\n",
            "  [Global Loss]:  0.4278 -> 0.4055 (Δ=0.0223)\n",
            "  [Eval KD]:      0.4344 -> 0.4310 (Δ=0.0033, 0.8%)\n",
            "  [Time]:         33.3s\n",
            "\n",
            "[Progress: 7/28] Elapsed: 4:06, ETA: 12:18\n",
            "\n",
            "=== Layer 7 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 0.4310\n",
            "  step 10: local=0.0386 global=0.4268 (2.9s)\n",
            "  step 20: local=0.0432 global=0.4171 (5.7s)\n",
            "  step 30: local=0.0426 global=0.3967 (8.5s)\n",
            "  step 40: local=0.0420 global=0.3893 (11.4s)\n",
            "  step 50: local=0.0433 global=0.4100 (14.2s)\n",
            "  step 60: local=0.0416 global=0.4061 (17.1s)\n",
            "  step 70: local=0.0433 global=0.4246 (19.9s)\n",
            "  step 80: local=0.0426 global=0.3729 (22.7s)\n",
            "  step 90: local=0.0421 global=0.4013 (25.6s)\n",
            "  step 100: local=0.0423 global=0.3877 (28.4s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0378 -> 0.0423 (Δ=-0.0045)\n",
            "  [Global Loss]:  0.4000 -> 0.3877 (Δ=0.0122)\n",
            "  [Eval KD]:      0.4310 -> 0.4221 (Δ=0.0089, 2.1%)\n",
            "  [Time]:         32.6s\n",
            "\n",
            "[Progress: 8/28] Elapsed: 4:38, ETA: 11:37\n",
            "\n",
            "=== Layer 8 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 0.4221\n",
            "  step 10: local=0.0438 global=0.4084 (2.8s)\n",
            "  step 20: local=0.0465 global=0.4198 (5.6s)\n",
            "  step 30: local=0.0469 global=0.3905 (8.3s)\n",
            "  step 40: local=0.0472 global=0.4369 (11.1s)\n",
            "  step 50: local=0.0480 global=0.4117 (13.9s)\n",
            "  step 60: local=0.0481 global=0.4093 (16.7s)\n",
            "  step 70: local=0.0462 global=0.4054 (19.4s)\n",
            "  step 80: local=0.0475 global=0.4009 (22.2s)\n",
            "  step 90: local=0.0497 global=0.3904 (25.0s)\n",
            "  step 100: local=0.0475 global=0.4272 (27.7s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0414 -> 0.0475 (Δ=-0.0061)\n",
            "  [Global Loss]:  0.3937 -> 0.4272 (Δ=-0.0334)\n",
            "  [Eval KD]:      0.4221 -> 0.4210 (Δ=0.0012, 0.3%)\n",
            "  [Time]:         31.9s\n",
            "\n",
            "[Progress: 9/28] Elapsed: 5:10, ETA: 10:55\n",
            "\n",
            "=== Layer 9 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 0.4210\n",
            "  step 10: local=0.0425 global=0.4090 (2.8s)\n",
            "  step 20: local=0.0459 global=0.3998 (5.5s)\n",
            "  step 30: local=0.0443 global=0.4217 (8.2s)\n",
            "  step 40: local=0.0453 global=0.3892 (10.9s)\n",
            "  step 50: local=0.0477 global=0.3979 (13.6s)\n",
            "  step 60: local=0.0470 global=0.4328 (16.3s)\n",
            "  step 70: local=0.0465 global=0.3867 (19.0s)\n",
            "  step 80: local=0.0458 global=0.3816 (21.7s)\n",
            "  step 90: local=0.0462 global=0.3871 (24.4s)\n",
            "  step 100: local=0.0452 global=0.3975 (27.1s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0402 -> 0.0452 (Δ=-0.0050)\n",
            "  [Global Loss]:  0.3901 -> 0.3975 (Δ=-0.0074)\n",
            "  [Eval KD]:      0.4210 -> 0.4177 (Δ=0.0032, 0.8%)\n",
            "  [Time]:         31.4s\n",
            "\n",
            "[Progress: 10/28] Elapsed: 5:42, ETA: 10:15\n",
            "\n",
            "=== Layer 10 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 0.4177\n",
            "  step 10: local=0.0464 global=0.3906 (2.7s)\n",
            "  step 20: local=0.0476 global=0.3594 (5.3s)\n",
            "  step 30: local=0.0464 global=0.3971 (8.0s)\n",
            "  step 40: local=0.0479 global=0.3962 (10.6s)\n",
            "  step 50: local=0.0468 global=0.4086 (13.3s)\n",
            "  step 60: local=0.0465 global=0.3921 (16.0s)\n",
            "  step 70: local=0.0445 global=0.4122 (18.6s)\n",
            "  step 80: local=0.0438 global=0.4183 (21.2s)\n",
            "  step 90: local=0.0437 global=0.3847 (23.9s)\n",
            "  step 100: local=0.0452 global=0.3734 (26.6s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0500 -> 0.0452 (Δ=0.0049)\n",
            "  [Global Loss]:  0.3987 -> 0.3734 (Δ=0.0253)\n",
            "  [Eval KD]:      0.4177 -> 0.4184 (Δ=-0.0006, -0.1%)\n",
            "  [Time]:         30.8s\n",
            "\n",
            "[Progress: 11/28] Elapsed: 6:12, ETA: 9:36\n",
            "\n",
            "=== Layer 11 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 0.4184\n",
            "  step 10: local=0.0449 global=0.3936 (2.6s)\n",
            "  step 20: local=0.0485 global=0.4199 (5.2s)\n",
            "  step 30: local=0.0465 global=0.3939 (7.8s)\n",
            "  step 40: local=0.0478 global=0.4567 (10.4s)\n",
            "  step 50: local=0.0479 global=0.4247 (13.0s)\n",
            "  step 60: local=0.0477 global=0.4133 (15.6s)\n",
            "  step 70: local=0.0481 global=0.3994 (18.2s)\n",
            "  step 80: local=0.0478 global=0.3918 (20.8s)\n",
            "  step 90: local=0.0476 global=0.4093 (23.4s)\n",
            "  step 100: local=0.0460 global=0.3993 (26.0s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0443 -> 0.0460 (Δ=-0.0017)\n",
            "  [Global Loss]:  0.3810 -> 0.3993 (Δ=-0.0183)\n",
            "  [Eval KD]:      0.4184 -> 0.4113 (Δ=0.0070, 1.7%)\n",
            "  [Time]:         30.2s\n",
            "\n",
            "[Progress: 12/28] Elapsed: 6:43, ETA: 8:57\n",
            "\n",
            "=== Layer 12 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 0.4113\n",
            "  step 10: local=0.0460 global=0.3933 (2.6s)\n",
            "  step 20: local=0.0500 global=0.4264 (5.1s)\n",
            "  step 30: local=0.0504 global=0.4013 (7.6s)\n",
            "  step 40: local=0.0501 global=0.3662 (10.1s)\n",
            "  step 50: local=0.0500 global=0.3848 (12.7s)\n",
            "  step 60: local=0.0512 global=0.4048 (15.2s)\n",
            "  step 70: local=0.0507 global=0.4040 (17.7s)\n",
            "  step 80: local=0.0503 global=0.4118 (20.3s)\n",
            "  step 90: local=0.0517 global=0.4084 (22.8s)\n",
            "  step 100: local=0.0495 global=0.4226 (25.3s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0434 -> 0.0495 (Δ=-0.0062)\n",
            "  [Global Loss]:  0.3594 -> 0.4226 (Δ=-0.0632)\n",
            "  [Eval KD]:      0.4113 -> 0.4169 (Δ=-0.0055, -1.3%)\n",
            "  [Time]:         29.6s\n",
            "\n",
            "[Progress: 13/28] Elapsed: 7:12, ETA: 8:19\n",
            "\n",
            "=== Layer 13 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 0.4169\n",
            "  step 10: local=0.0454 global=0.4280 (2.5s)\n",
            "  step 20: local=0.0505 global=0.4181 (5.0s)\n",
            "  step 30: local=0.0502 global=0.3885 (7.4s)\n",
            "  step 40: local=0.0490 global=0.3821 (9.9s)\n",
            "  step 50: local=0.0505 global=0.3976 (12.4s)\n",
            "  step 60: local=0.0494 global=0.4119 (14.8s)\n",
            "  step 70: local=0.0497 global=0.3897 (17.3s)\n",
            "  step 80: local=0.0520 global=0.3979 (19.7s)\n",
            "  step 90: local=0.0503 global=0.3782 (22.2s)\n",
            "  step 100: local=0.0507 global=0.3744 (24.7s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0455 -> 0.0507 (Δ=-0.0052)\n",
            "  [Global Loss]:  0.3766 -> 0.3744 (Δ=0.0022)\n",
            "  [Eval KD]:      0.4169 -> 0.4201 (Δ=-0.0032, -0.8%)\n",
            "  [Time]:         28.9s\n",
            "\n",
            "[Progress: 14/28] Elapsed: 7:41, ETA: 7:41\n",
            "\n",
            "=== Layer 14 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 0.4201\n",
            "  step 10: local=0.0471 global=0.3996 (2.5s)\n",
            "  step 20: local=0.0508 global=0.3948 (4.8s)\n",
            "  step 30: local=0.0496 global=0.3987 (7.2s)\n",
            "  step 40: local=0.0495 global=0.3873 (9.6s)\n",
            "  step 50: local=0.0495 global=0.4026 (12.1s)\n",
            "  step 60: local=0.0492 global=0.4054 (14.5s)\n",
            "  step 70: local=0.0502 global=0.3881 (16.9s)\n",
            "  step 80: local=0.0515 global=0.3978 (19.2s)\n",
            "  step 90: local=0.0505 global=0.3908 (21.7s)\n",
            "  step 100: local=0.0497 global=0.3808 (24.1s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0479 -> 0.0497 (Δ=-0.0019)\n",
            "  [Global Loss]:  0.3823 -> 0.3808 (Δ=0.0014)\n",
            "  [Eval KD]:      0.4201 -> 0.4197 (Δ=0.0004, 0.1%)\n",
            "  [Time]:         28.3s\n",
            "\n",
            "[Progress: 15/28] Elapsed: 8:09, ETA: 7:04\n",
            "\n",
            "=== Layer 15 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 0.4197\n",
            "  step 10: local=0.0498 global=0.3931 (2.4s)\n",
            "  step 20: local=0.0539 global=0.4055 (4.7s)\n",
            "  step 30: local=0.0533 global=0.3865 (7.0s)\n",
            "  step 40: local=0.0533 global=0.3818 (9.4s)\n",
            "  step 50: local=0.0540 global=0.3788 (11.8s)\n",
            "  step 60: local=0.0535 global=0.3887 (14.1s)\n",
            "  step 70: local=0.0517 global=0.3652 (16.4s)\n",
            "  step 80: local=0.0535 global=0.4063 (18.7s)\n",
            "  step 90: local=0.0544 global=0.3786 (21.1s)\n",
            "  step 100: local=0.0541 global=0.4104 (23.5s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0532 -> 0.0541 (Δ=-0.0009)\n",
            "  [Global Loss]:  0.3723 -> 0.4104 (Δ=-0.0381)\n",
            "  [Eval KD]:      0.4197 -> 0.4177 (Δ=0.0019, 0.5%)\n",
            "  [Time]:         27.6s\n",
            "\n",
            "[Progress: 16/28] Elapsed: 8:37, ETA: 6:28\n",
            "\n",
            "=== Layer 16 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 0.4177\n",
            "  step 10: local=0.0547 global=0.3905 (2.3s)\n",
            "  step 20: local=0.0553 global=0.3817 (4.6s)\n",
            "  step 30: local=0.0534 global=0.4217 (6.9s)\n",
            "  step 40: local=0.0523 global=0.3870 (9.1s)\n",
            "  step 50: local=0.0511 global=0.3881 (11.5s)\n",
            "  step 60: local=0.0535 global=0.4127 (13.7s)\n",
            "  step 70: local=0.0521 global=0.3872 (16.0s)\n",
            "  step 80: local=0.0533 global=0.4144 (18.3s)\n",
            "  step 90: local=0.0530 global=0.3808 (20.6s)\n",
            "  step 100: local=0.0525 global=0.4181 (22.8s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0617 -> 0.0525 (Δ=0.0092)\n",
            "  [Global Loss]:  0.4224 -> 0.4181 (Δ=0.0043)\n",
            "  [Eval KD]:      0.4177 -> 0.4223 (Δ=-0.0046, -1.1%)\n",
            "  [Time]:         27.0s\n",
            "\n",
            "[Progress: 17/28] Elapsed: 9:04, ETA: 5:52\n",
            "\n",
            "=== Layer 17 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 0.4223\n",
            "  step 10: local=0.0410 global=0.4066 (2.3s)\n",
            "  step 20: local=0.0478 global=0.3880 (4.5s)\n",
            "  step 30: local=0.0523 global=0.3826 (6.7s)\n",
            "  step 40: local=0.0507 global=0.3913 (8.9s)\n",
            "  step 50: local=0.0511 global=0.3738 (11.1s)\n",
            "  step 60: local=0.0497 global=0.3916 (13.4s)\n",
            "  step 70: local=0.0503 global=0.4054 (15.6s)\n",
            "  step 80: local=0.0505 global=0.3789 (17.8s)\n",
            "  step 90: local=0.0510 global=0.4003 (20.0s)\n",
            "  step 100: local=0.0510 global=0.4019 (22.2s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0425 -> 0.0510 (Δ=-0.0085)\n",
            "  [Global Loss]:  0.4123 -> 0.4019 (Δ=0.0104)\n",
            "  [Eval KD]:      0.4223 -> 0.4263 (Δ=-0.0040, -0.9%)\n",
            "  [Time]:         26.5s\n",
            "\n",
            "[Progress: 18/28] Elapsed: 9:30, ETA: 5:17\n",
            "\n",
            "=== Layer 18 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 0.4263\n",
            "  step 10: local=0.0460 global=0.4233 (2.2s)\n",
            "  step 20: local=0.0504 global=0.3845 (4.3s)\n",
            "  step 30: local=0.0505 global=0.3889 (6.5s)\n",
            "  step 40: local=0.0505 global=0.3844 (8.6s)\n",
            "  step 50: local=0.0506 global=0.3680 (10.8s)\n",
            "  step 60: local=0.0501 global=0.3870 (13.0s)\n",
            "  step 70: local=0.0495 global=0.3967 (15.1s)\n",
            "  step 80: local=0.0505 global=0.3932 (17.3s)\n",
            "  step 90: local=0.0494 global=0.3721 (19.4s)\n",
            "  step 100: local=0.0503 global=0.4110 (21.6s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0442 -> 0.0503 (Δ=-0.0061)\n",
            "  [Global Loss]:  0.4054 -> 0.4110 (Δ=-0.0056)\n",
            "  [Eval KD]:      0.4263 -> 0.4206 (Δ=0.0057, 1.3%)\n",
            "  [Time]:         25.7s\n",
            "\n",
            "[Progress: 19/28] Elapsed: 9:56, ETA: 4:42\n",
            "\n",
            "=== Layer 19 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 0.4206\n",
            "  step 10: local=0.0500 global=0.4013 (2.1s)\n",
            "  step 20: local=0.0536 global=0.3833 (4.2s)\n",
            "  step 30: local=0.0514 global=0.3864 (6.3s)\n",
            "  step 40: local=0.0535 global=0.3737 (8.4s)\n",
            "  step 50: local=0.0516 global=0.4032 (10.5s)\n",
            "  step 60: local=0.0519 global=0.4032 (12.6s)\n",
            "  step 70: local=0.0532 global=0.3719 (14.7s)\n",
            "  step 80: local=0.0523 global=0.3998 (16.8s)\n",
            "  step 90: local=0.0535 global=0.3826 (18.9s)\n",
            "  step 100: local=0.0538 global=0.3446 (21.0s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0555 -> 0.0538 (Δ=0.0017)\n",
            "  [Global Loss]:  0.4404 -> 0.3446 (Δ=0.0958)\n",
            "  [Eval KD]:      0.4206 -> 0.4197 (Δ=0.0009, 0.2%)\n",
            "  [Time]:         25.2s\n",
            "\n",
            "[Progress: 20/28] Elapsed: 10:21, ETA: 4:08\n",
            "\n",
            "=== Layer 20 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 0.4197\n",
            "  step 10: local=0.0426 global=0.3913 (2.1s)\n",
            "  step 20: local=0.0432 global=0.4271 (4.1s)\n",
            "  step 30: local=0.0448 global=0.3835 (6.1s)\n",
            "  step 40: local=0.0451 global=0.3800 (8.1s)\n",
            "  step 50: local=0.0443 global=0.3858 (10.2s)\n",
            "  step 60: local=0.0464 global=0.3805 (12.2s)\n",
            "  step 70: local=0.0451 global=0.3837 (14.3s)\n",
            "  step 80: local=0.0459 global=0.4285 (16.3s)\n",
            "  step 90: local=0.0453 global=0.4105 (18.3s)\n",
            "  step 100: local=0.0451 global=0.3808 (20.4s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0448 -> 0.0451 (Δ=-0.0003)\n",
            "  [Global Loss]:  0.3873 -> 0.3808 (Δ=0.0066)\n",
            "  [Eval KD]:      0.4197 -> 0.4223 (Δ=-0.0026, -0.6%)\n",
            "  [Time]:         24.6s\n",
            "\n",
            "[Progress: 21/28] Elapsed: 10:46, ETA: 3:35\n",
            "\n",
            "=== Layer 21 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 0.4223\n",
            "  step 10: local=0.0384 global=0.3831 (2.0s)\n",
            "  step 20: local=0.0414 global=0.4008 (4.0s)\n",
            "  step 30: local=0.0427 global=0.3605 (5.9s)\n",
            "  step 40: local=0.0438 global=0.3798 (7.9s)\n",
            "  step 50: local=0.0419 global=0.4174 (9.9s)\n",
            "  step 60: local=0.0421 global=0.3800 (11.9s)\n",
            "  step 70: local=0.0435 global=0.3911 (13.8s)\n",
            "  step 80: local=0.0440 global=0.3764 (15.8s)\n",
            "  step 90: local=0.0432 global=0.3747 (17.8s)\n",
            "  step 100: local=0.0441 global=0.3880 (19.7s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0370 -> 0.0441 (Δ=-0.0071)\n",
            "  [Global Loss]:  0.3837 -> 0.3880 (Δ=-0.0042)\n",
            "  [Eval KD]:      0.4223 -> 0.4161 (Δ=0.0061, 1.5%)\n",
            "  [Time]:         24.0s\n",
            "\n",
            "[Progress: 22/28] Elapsed: 11:10, ETA: 3:02\n",
            "\n",
            "=== Layer 22 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 0.4161\n",
            "  step 10: local=0.0337 global=0.3777 (2.0s)\n",
            "  step 20: local=0.0376 global=0.3797 (3.8s)\n",
            "  step 30: local=0.0377 global=0.3981 (5.7s)\n",
            "  step 40: local=0.0381 global=0.3819 (7.6s)\n",
            "  step 50: local=0.0378 global=0.3791 (9.6s)\n",
            "  step 60: local=0.0385 global=0.3886 (11.5s)\n",
            "  step 70: local=0.0380 global=0.4135 (13.4s)\n",
            "  step 80: local=0.0373 global=0.3686 (15.3s)\n",
            "  step 90: local=0.0374 global=0.3651 (17.2s)\n",
            "  step 100: local=0.0386 global=0.3737 (19.1s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0315 -> 0.0386 (Δ=-0.0071)\n",
            "  [Global Loss]:  0.3913 -> 0.3737 (Δ=0.0176)\n",
            "  [Eval KD]:      0.4161 -> 0.4128 (Δ=0.0033, 0.8%)\n",
            "  [Time]:         23.3s\n",
            "\n",
            "[Progress: 23/28] Elapsed: 11:33, ETA: 2:30\n",
            "\n",
            "=== Layer 23 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 0.4128\n",
            "  step 10: local=0.0352 global=0.4115 (1.9s)\n",
            "  step 20: local=0.0388 global=0.4006 (3.7s)\n",
            "  step 30: local=0.0400 global=0.3933 (5.6s)\n",
            "  step 40: local=0.0404 global=0.3949 (7.4s)\n",
            "  step 50: local=0.0404 global=0.3975 (9.3s)\n",
            "  step 60: local=0.0390 global=0.4239 (11.1s)\n",
            "  step 70: local=0.0398 global=0.3716 (12.9s)\n",
            "  step 80: local=0.0377 global=0.3823 (14.8s)\n",
            "  step 90: local=0.0385 global=0.3946 (16.7s)\n",
            "  step 100: local=0.0384 global=0.3906 (18.5s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0318 -> 0.0384 (Δ=-0.0066)\n",
            "  [Global Loss]:  0.3867 -> 0.3906 (Δ=-0.0039)\n",
            "  [Eval KD]:      0.4128 -> 0.4034 (Δ=0.0094, 2.3%)\n",
            "  [Time]:         22.7s\n",
            "\n",
            "[Progress: 24/28] Elapsed: 11:56, ETA: 1:59\n",
            "\n",
            "=== Layer 24 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 0.4034\n",
            "  step 10: local=0.0290 global=0.3677 (1.8s)\n",
            "  step 20: local=0.0304 global=0.3940 (3.6s)\n",
            "  step 30: local=0.0313 global=0.3873 (5.4s)\n",
            "  step 40: local=0.0299 global=0.3833 (7.1s)\n",
            "  step 50: local=0.0323 global=0.4020 (8.9s)\n",
            "  step 60: local=0.0298 global=0.3991 (10.7s)\n",
            "  step 70: local=0.0329 global=0.3972 (12.5s)\n",
            "  step 80: local=0.0326 global=0.3871 (14.2s)\n",
            "  step 90: local=0.0326 global=0.3789 (16.0s)\n",
            "  step 100: local=0.0325 global=0.3433 (17.8s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0289 -> 0.0325 (Δ=-0.0036)\n",
            "  [Global Loss]:  0.3620 -> 0.3433 (Δ=0.0188)\n",
            "  [Eval KD]:      0.4034 -> 0.4061 (Δ=-0.0027, -0.7%)\n",
            "  [Time]:         21.9s\n",
            "\n",
            "[Progress: 25/28] Elapsed: 12:18, ETA: 1:28\n",
            "\n",
            "=== Layer 25 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 0.4061\n",
            "  step 10: local=0.0346 global=0.3689 (1.8s)\n",
            "  step 20: local=0.0354 global=0.3864 (3.5s)\n",
            "  step 30: local=0.0352 global=0.3788 (5.2s)\n",
            "  step 40: local=0.0365 global=0.3598 (6.9s)\n",
            "  step 50: local=0.0375 global=0.3883 (8.6s)\n",
            "  step 60: local=0.0381 global=0.3722 (10.3s)\n",
            "  step 70: local=0.0399 global=0.3753 (12.0s)\n",
            "  step 80: local=0.0405 global=0.3703 (13.7s)\n",
            "  step 90: local=0.0366 global=0.3903 (15.4s)\n",
            "  step 100: local=0.0384 global=0.4003 (17.1s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0418 -> 0.0384 (Δ=0.0034)\n",
            "  [Global Loss]:  0.3999 -> 0.4003 (Δ=-0.0004)\n",
            "  [Eval KD]:      0.4061 -> 0.4024 (Δ=0.0037, 0.9%)\n",
            "  [Time]:         21.3s\n",
            "\n",
            "[Progress: 26/28] Elapsed: 12:39, ETA: 0:58\n",
            "\n",
            "=== Layer 26 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 0.4024\n",
            "  step 10: local=0.0415 global=0.3935 (1.7s)\n",
            "  step 20: local=0.0505 global=0.4211 (3.4s)\n",
            "  step 30: local=0.0447 global=0.3788 (5.0s)\n",
            "  step 40: local=0.0432 global=0.3735 (6.7s)\n",
            "  step 50: local=0.0484 global=0.3594 (8.4s)\n",
            "  step 60: local=0.0462 global=0.4196 (10.0s)\n",
            "  step 70: local=0.0465 global=0.3846 (11.6s)\n",
            "  step 80: local=0.0470 global=0.3763 (13.3s)\n",
            "  step 90: local=0.0454 global=0.3864 (15.0s)\n",
            "  step 100: local=0.0463 global=0.3853 (16.6s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0483 -> 0.0463 (Δ=0.0020)\n",
            "  [Global Loss]:  0.3639 -> 0.3853 (Δ=-0.0214)\n",
            "  [Eval KD]:      0.4024 -> 0.3967 (Δ=0.0057, 1.4%)\n",
            "  [Time]:         20.8s\n",
            "\n",
            "[Progress: 27/28] Elapsed: 13:00, ETA: 0:28\n",
            "\n",
            "=== Layer 27 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 0.3967\n",
            "  step 10: local=0.0266 global=0.3913 (1.6s)\n",
            "  step 20: local=0.0606 global=0.3795 (3.2s)\n",
            "  step 30: local=0.0696 global=0.3957 (4.8s)\n",
            "  step 40: local=0.0559 global=0.4035 (6.4s)\n",
            "  step 50: local=0.0254 global=0.3610 (8.0s)\n",
            "  step 60: local=0.0310 global=0.3939 (9.6s)\n",
            "  step 70: local=0.0315 global=0.3551 (11.2s)\n",
            "  step 80: local=0.0202 global=0.3843 (12.8s)\n",
            "  step 90: local=0.0278 global=0.3819 (14.4s)\n",
            "  step 100: local=0.0195 global=0.3906 (16.0s)\n",
            "  ---\n",
            "  [Local Loss]:   0.9088 -> 0.0195 (Δ=0.8892)\n",
            "  [Global Loss]:  0.3652 -> 0.3906 (Δ=-0.0254)\n",
            "  [Eval KD]:      0.3967 -> 0.3968 (Δ=-0.0001, -0.0%)\n",
            "  [Time]:         20.2s\n",
            "\n",
            "============================================================\n",
            "LAYER-BY-LAYER TRAINING COMPLETE\n",
            "============================================================\n",
            "Total time:    13:24 (804.8s)\n",
            "Avg per layer: 28.6s\n",
            "\n",
            "[Initial Global KD Loss]: 1.0885\n",
            "[Final Global KD Loss]:   0.3684\n",
            "[Total Improvement]:      0.7200 (66.2%)\n",
            "\n",
            "Per-layer summary:\n",
            " Layer  Eval Before   Eval After     Eval Δ  Local 1st Local Last  Global 1st  Global Last     Time\n",
            "---------------------------------------------------------------------------------------------------------\n",
            "     0       1.0345       0.6805    +0.3540     0.0278     0.0656      0.9705       0.6617    37.3s\n",
            "     1       0.6805       0.6117    +0.0688     0.0362     0.0572      0.6138       0.6229    36.5s\n",
            "     2       0.6117       0.4545    +0.1572     0.0381     0.0465      0.5937       0.4627    35.7s\n",
            "     3       0.4545       0.4467    +0.0077     0.0388     0.0442      0.4216       0.4250    35.1s\n",
            "     4       0.4467       0.4457    +0.0010     0.0358     0.0428      0.4449       0.4054    34.5s\n",
            "     5       0.4457       0.4344    +0.0113     0.0375     0.0443      0.3873       0.4400    33.9s\n",
            "     6       0.4344       0.4310    +0.0033     0.0361     0.0449      0.4278       0.4055    33.3s\n",
            "     7       0.4310       0.4221    +0.0089     0.0378     0.0423      0.4000       0.3877    32.6s\n",
            "     8       0.4221       0.4210    +0.0012     0.0414     0.0475      0.3937       0.4272    31.9s\n",
            "     9       0.4210       0.4177    +0.0032     0.0402     0.0452      0.3901       0.3975    31.4s\n",
            "    10       0.4177       0.4184    -0.0006     0.0500     0.0452      0.3987       0.3734    30.8s\n",
            "    11       0.4184       0.4113    +0.0070     0.0443     0.0460      0.3810       0.3993    30.2s\n",
            "    12       0.4113       0.4169    -0.0055     0.0434     0.0495      0.3594       0.4226    29.6s\n",
            "    13       0.4169       0.4201    -0.0032     0.0455     0.0507      0.3766       0.3744    28.9s\n",
            "    14       0.4201       0.4197    +0.0004     0.0479     0.0497      0.3823       0.3808    28.3s\n",
            "    15       0.4197       0.4177    +0.0019     0.0532     0.0541      0.3723       0.4104    27.6s\n",
            "    16       0.4177       0.4223    -0.0046     0.0617     0.0525      0.4224       0.4181    27.0s\n",
            "    17       0.4223       0.4263    -0.0040     0.0425     0.0510      0.4123       0.4019    26.5s\n",
            "    18       0.4263       0.4206    +0.0057     0.0442     0.0503      0.4054       0.4110    25.7s\n",
            "    19       0.4206       0.4197    +0.0009     0.0555     0.0538      0.4404       0.3446    25.2s\n",
            "    20       0.4197       0.4223    -0.0026     0.0448     0.0451      0.3873       0.3808    24.6s\n",
            "    21       0.4223       0.4161    +0.0061     0.0370     0.0441      0.3837       0.3880    24.0s\n",
            "    22       0.4161       0.4128    +0.0033     0.0315     0.0386      0.3913       0.3737    23.3s\n",
            "    23       0.4128       0.4034    +0.0094     0.0318     0.0384      0.3867       0.3906    22.7s\n",
            "    24       0.4034       0.4061    -0.0027     0.0289     0.0325      0.3620       0.3433    21.9s\n",
            "    25       0.4061       0.4024    +0.0037     0.0418     0.0384      0.3999       0.4003    21.3s\n",
            "    26       0.4024       0.3967    +0.0057     0.0483     0.0463      0.3639       0.3853    20.8s\n",
            "    27       0.3967       0.3968    -0.0001     0.9088     0.0195      0.3652       0.3906    20.2s\n",
            "\n",
            "=== Scale Optimization Results ===\n",
            "Before: 1.0885\n",
            "After:  0.3684\n",
            "Improvement: 0.7200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RUN** LAYER-BY-LAYER TRAINING"
      ],
      "metadata": {
        "id": "o2veFRWaQEkB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "gupQzmm5FCuX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01bdc6cd-46fb-4f0b-8cf7-8f6157ad969c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training 28 layers (mode=weights)...\n",
            "Cache: caches/alpaca_chat_think_both_L128_K32_R256\n",
            "Batch size: 64, Grad accum: 1\n",
            "LR: 2e-05, Steps per layer: 100\n",
            "\n",
            "[Initial Global KD Loss]: 0.3684\n",
            "\n",
            "=== Layer 0 === (15,728,640 trainable params, mode=weights)\n",
            "  [Global KD Loss BEFORE]: 0.3965\n",
            "  step 10: local=0.0610 global=0.3522 (3.3s)\n",
            "  step 20: local=0.0628 global=0.3898 (6.6s)\n",
            "  step 30: local=0.0555 global=0.3610 (9.8s)\n",
            "  step 40: local=0.0573 global=0.3725 (13.1s)\n",
            "  step 50: local=0.0543 global=0.3390 (16.4s)\n",
            "  step 60: local=0.0529 global=0.3771 (19.7s)\n",
            "  step 70: local=0.0562 global=0.3892 (23.0s)\n",
            "  step 80: local=0.0556 global=0.3794 (26.2s)\n",
            "  step 90: local=0.0520 global=0.3474 (29.5s)\n",
            "  step 100: local=0.0498 global=0.3814 (32.8s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0648 -> 0.0498 (Δ=0.0150)\n",
            "  [Global Loss]:  0.3766 -> 0.3814 (Δ=-0.0048)\n",
            "  [Eval KD]:      0.3965 -> 0.3729 (Δ=0.0235, 5.9%)\n",
            "  [Time]:         37.1s\n",
            "\n",
            "[Progress: 1/28] Elapsed: 0:37, ETA: 16:41\n",
            "\n",
            "=== Layer 1 === (15,728,640 trainable params, mode=weights)\n",
            "  [Global KD Loss BEFORE]: 0.3728\n",
            "  step 10: local=0.0520 global=0.3674 (3.3s)\n",
            "  step 20: local=0.0552 global=0.3675 (6.5s)\n",
            "  step 30: local=0.0541 global=0.3530 (9.7s)\n",
            "  step 40: local=0.0580 global=0.3980 (12.9s)\n",
            "  step 50: local=0.0529 global=0.3387 (16.1s)\n",
            "  step 60: local=0.0468 global=0.3653 (19.3s)\n",
            "  step 70: local=0.0489 global=0.3540 (22.5s)\n",
            "  step 80: local=0.0463 global=0.3683 (25.7s)\n",
            "  step 90: local=0.0584 global=0.3936 (29.0s)\n",
            "  step 100: local=0.0540 global=0.4044 (32.2s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0611 -> 0.0540 (Δ=0.0071)\n",
            "  [Global Loss]:  0.3484 -> 0.4044 (Δ=-0.0560)\n",
            "  [Eval KD]:      0.3728 -> 0.3688 (Δ=0.0040, 1.1%)\n",
            "  [Time]:         36.4s\n",
            "\n",
            "[Progress: 2/28] Elapsed: 1:13, ETA: 15:55\n",
            "\n",
            "=== Layer 2 === (15,728,640 trainable params, mode=weights)\n",
            "  [Global KD Loss BEFORE]: 0.3695\n",
            "  step 10: local=0.0440 global=0.3548 (3.2s)\n",
            "  step 20: local=0.0424 global=0.3135 (6.3s)\n",
            "  step 30: local=0.0506 global=0.3662 (9.5s)\n",
            "  step 40: local=0.0415 global=0.3573 (12.6s)\n",
            "  step 50: local=0.0418 global=0.3770 (15.8s)\n",
            "  step 60: local=0.0423 global=0.3678 (18.9s)\n",
            "  step 70: local=0.0380 global=0.3567 (22.1s)\n",
            "  step 80: local=0.0375 global=0.3526 (25.2s)\n",
            "  step 90: local=0.0433 global=0.3416 (28.4s)\n",
            "  step 100: local=0.0372 global=0.3519 (31.6s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0455 -> 0.0372 (Δ=0.0083)\n",
            "  [Global Loss]:  0.3661 -> 0.3519 (Δ=0.0142)\n",
            "  [Eval KD]:      0.3695 -> 0.3676 (Δ=0.0019, 0.5%)\n",
            "  [Time]:         35.8s\n",
            "\n",
            "[Progress: 3/28] Elapsed: 1:49, ETA: 15:10\n",
            "\n",
            "=== Layer 3 === (15,728,640 trainable params, mode=weights)\n",
            "  [Global KD Loss BEFORE]: 0.3675\n",
            "  step 10: local=0.0412 global=0.3553 (3.1s)\n",
            "  step 20: local=0.0408 global=0.3500 (6.2s)\n",
            "  step 30: local=0.0400 global=0.3354 (9.3s)\n",
            "  step 40: local=0.0368 global=0.3454 (12.4s)\n",
            "  step 50: local=0.0396 global=0.3063 (15.5s)\n",
            "  step 60: local=0.0398 global=0.3853 (18.6s)\n",
            "  step 70: local=0.0382 global=0.3320 (21.7s)\n",
            "  step 80: local=0.0364 global=0.3506 (24.7s)\n",
            "  step 90: local=0.0364 global=0.3439 (27.8s)\n",
            "  step 100: local=0.0351 global=0.3392 (30.9s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0450 -> 0.0351 (Δ=0.0098)\n",
            "  [Global Loss]:  0.3536 -> 0.3392 (Δ=0.0145)\n",
            "  [Eval KD]:      0.3675 -> 0.3586 (Δ=0.0089, 2.4%)\n",
            "  [Time]:         35.2s\n",
            "\n",
            "[Progress: 4/28] Elapsed: 2:24, ETA: 14:26\n",
            "\n",
            "=== Layer 4 === (15,728,640 trainable params, mode=weights)\n",
            "  [Global KD Loss BEFORE]: 0.3585\n",
            "  step 10: local=0.0430 global=0.3544 (3.1s)\n",
            "  step 20: local=0.0424 global=0.3789 (6.1s)\n",
            "  step 30: local=0.0414 global=0.3794 (9.1s)\n",
            "  step 40: local=0.0396 global=0.3164 (12.1s)\n",
            "  step 50: local=0.0385 global=0.3321 (15.2s)\n",
            "  step 60: local=0.0366 global=0.3606 (18.2s)\n",
            "  step 70: local=0.0381 global=0.3535 (21.2s)\n",
            "  step 80: local=0.0358 global=0.3453 (24.2s)\n",
            "  step 90: local=0.0363 global=0.3376 (27.3s)\n",
            "  step 100: local=0.0382 global=0.3506 (30.3s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0451 -> 0.0382 (Δ=0.0069)\n",
            "  [Global Loss]:  0.3301 -> 0.3506 (Δ=-0.0204)\n",
            "  [Eval KD]:      0.3585 -> 0.3572 (Δ=0.0013, 0.4%)\n",
            "  [Time]:         34.6s\n",
            "\n",
            "[Progress: 5/28] Elapsed: 2:59, ETA: 13:43\n",
            "\n",
            "=== Layer 5 === (15,728,640 trainable params, mode=weights)\n",
            "  [Global KD Loss BEFORE]: 0.3573\n",
            "  step 10: local=0.0422 global=0.3337 (3.0s)\n",
            "  step 20: local=0.0417 global=0.3547 (6.0s)\n",
            "  step 30: local=0.0401 global=0.3241 (8.9s)\n",
            "  step 40: local=0.0395 global=0.3425 (11.9s)\n",
            "  step 50: local=0.0376 global=0.3351 (14.9s)\n",
            "  step 60: local=0.0393 global=0.3430 (17.8s)\n",
            "  step 70: local=0.0364 global=0.3542 (20.8s)\n",
            "  step 80: local=0.0408 global=0.3243 (23.8s)\n",
            "  step 90: local=0.0381 global=0.3471 (26.8s)\n",
            "  step 100: local=0.0382 global=0.3259 (29.7s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0443 -> 0.0382 (Δ=0.0061)\n",
            "  [Global Loss]:  0.3018 -> 0.3259 (Δ=-0.0241)\n",
            "  [Eval KD]:      0.3573 -> 0.3515 (Δ=0.0058, 1.6%)\n",
            "  [Time]:         34.0s\n",
            "\n",
            "[Progress: 6/28] Elapsed: 3:32, ETA: 13:00\n",
            "\n",
            "=== Layer 6 === (15,728,640 trainable params, mode=weights)\n",
            "  [Global KD Loss BEFORE]: 0.3513\n",
            "  step 10: local=0.0432 global=0.3579 (3.0s)\n",
            "  step 20: local=0.0426 global=0.3413 (5.8s)\n",
            "  step 30: local=0.0412 global=0.3447 (8.7s)\n",
            "  step 40: local=0.0417 global=0.3432 (11.6s)\n",
            "  step 50: local=0.0401 global=0.4031 (14.6s)\n",
            "  step 60: local=0.0402 global=0.3483 (17.5s)\n",
            "  step 70: local=0.0402 global=0.3245 (20.4s)\n",
            "  step 80: local=0.0394 global=0.3387 (23.3s)\n",
            "  step 90: local=0.0375 global=0.3463 (26.2s)\n",
            "  step 100: local=0.0378 global=0.3520 (29.1s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0451 -> 0.0378 (Δ=0.0073)\n",
            "  [Global Loss]:  0.3526 -> 0.3520 (Δ=0.0006)\n",
            "  [Eval KD]:      0.3513 -> 0.3512 (Δ=0.0001, 0.0%)\n",
            "  [Time]:         33.3s\n",
            "\n",
            "[Progress: 7/28] Elapsed: 4:06, ETA: 12:18\n",
            "\n",
            "=== Layer 7 === (15,728,640 trainable params, mode=weights)\n",
            "  [Global KD Loss BEFORE]: 0.3514\n",
            "  step 10: local=0.0420 global=0.3601 (2.9s)\n",
            "  step 20: local=0.0432 global=0.3458 (5.7s)\n",
            "  step 30: local=0.0387 global=0.3312 (8.5s)\n",
            "  step 40: local=0.0384 global=0.3299 (11.4s)\n",
            "  step 50: local=0.0378 global=0.3328 (14.3s)\n",
            "  step 60: local=0.0396 global=0.3502 (17.1s)\n",
            "  step 70: local=0.0399 global=0.3354 (19.9s)\n",
            "  step 80: local=0.0358 global=0.3276 (22.7s)\n",
            "  step 90: local=0.0368 global=0.3264 (25.6s)\n",
            "  step 100: local=0.0387 global=0.3792 (28.5s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0427 -> 0.0387 (Δ=0.0040)\n",
            "  [Global Loss]:  0.3402 -> 0.3792 (Δ=-0.0391)\n",
            "  [Eval KD]:      0.3514 -> 0.3468 (Δ=0.0046, 1.3%)\n",
            "  [Time]:         32.7s\n",
            "\n",
            "[Progress: 8/28] Elapsed: 4:38, ETA: 11:37\n",
            "\n",
            "=== Layer 8 === (15,728,640 trainable params, mode=weights)\n",
            "  [Global KD Loss BEFORE]: 0.3469\n",
            "  step 10: local=0.0452 global=0.3300 (2.8s)\n",
            "  step 20: local=0.0437 global=0.3251 (5.6s)\n",
            "  step 30: local=0.0432 global=0.3198 (8.4s)\n",
            "  step 40: local=0.0413 global=0.3376 (11.1s)\n",
            "  step 50: local=0.0412 global=0.3126 (14.0s)\n",
            "  step 60: local=0.0384 global=0.3583 (16.7s)\n",
            "  step 70: local=0.0393 global=0.3278 (19.5s)\n",
            "  step 80: local=0.0397 global=0.3250 (22.3s)\n",
            "  step 90: local=0.0397 global=0.3490 (25.1s)\n",
            "  step 100: local=0.0390 global=0.3248 (27.8s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0466 -> 0.0390 (Δ=0.0077)\n",
            "  [Global Loss]:  0.3370 -> 0.3248 (Δ=0.0122)\n",
            "  [Eval KD]:      0.3469 -> 0.3445 (Δ=0.0023, 0.7%)\n",
            "  [Time]:         32.1s\n",
            "\n",
            "[Progress: 9/28] Elapsed: 5:11, ETA: 10:56\n",
            "\n",
            "=== Layer 9 === (15,728,640 trainable params, mode=weights)\n",
            "  [Global KD Loss BEFORE]: 0.3446\n",
            "  step 10: local=0.0457 global=0.3071 (2.8s)\n",
            "  step 20: local=0.0440 global=0.3303 (5.5s)\n",
            "  step 30: local=0.0432 global=0.3486 (8.2s)\n",
            "  step 40: local=0.0404 global=0.3400 (10.9s)\n",
            "  step 50: local=0.0413 global=0.3624 (13.6s)\n",
            "  step 60: local=0.0408 global=0.3478 (16.4s)\n",
            "  step 70: local=0.0391 global=0.3136 (19.1s)\n",
            "  step 80: local=0.0384 global=0.3215 (21.8s)\n",
            "  step 90: local=0.0378 global=0.3368 (24.5s)\n",
            "  step 100: local=0.0362 global=0.3300 (27.2s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0467 -> 0.0362 (Δ=0.0105)\n",
            "  [Global Loss]:  0.2878 -> 0.3300 (Δ=-0.0423)\n",
            "  [Eval KD]:      0.3446 -> 0.3440 (Δ=0.0006, 0.2%)\n",
            "  [Time]:         31.5s\n",
            "\n",
            "[Progress: 10/28] Elapsed: 5:42, ETA: 10:16\n",
            "\n",
            "=== Layer 10 === (15,728,640 trainable params, mode=weights)\n",
            "  [Global KD Loss BEFORE]: 0.3441\n",
            "  step 10: local=0.0446 global=0.3193 (2.7s)\n",
            "  step 20: local=0.0432 global=0.3171 (5.4s)\n",
            "  step 30: local=0.0425 global=0.3119 (8.0s)\n",
            "  step 40: local=0.0407 global=0.3271 (10.6s)\n",
            "  step 50: local=0.0406 global=0.3166 (13.3s)\n",
            "  step 60: local=0.0388 global=0.3159 (16.0s)\n",
            "  step 70: local=0.0375 global=0.3363 (18.6s)\n",
            "  step 80: local=0.0397 global=0.3268 (21.3s)\n",
            "  step 90: local=0.0366 global=0.3202 (23.9s)\n",
            "  step 100: local=0.0380 global=0.3437 (26.6s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0462 -> 0.0380 (Δ=0.0082)\n",
            "  [Global Loss]:  0.3466 -> 0.3437 (Δ=0.0029)\n",
            "  [Eval KD]:      0.3441 -> 0.3414 (Δ=0.0027, 0.8%)\n",
            "  [Time]:         30.8s\n",
            "\n",
            "[Progress: 11/28] Elapsed: 6:13, ETA: 9:36\n",
            "\n",
            "=== Layer 11 === (15,728,640 trainable params, mode=weights)\n",
            "  [Global KD Loss BEFORE]: 0.3416\n",
            "  step 10: local=0.0465 global=0.3175 (2.6s)\n",
            "  step 20: local=0.0459 global=0.3320 (5.2s)\n",
            "  step 30: local=0.0407 global=0.3286 (7.8s)\n",
            "  step 40: local=0.0432 global=0.3786 (10.4s)\n",
            "  step 50: local=0.0424 global=0.3280 (13.0s)\n",
            "  step 60: local=0.0418 global=0.3659 (15.6s)\n",
            "  step 70: local=0.0411 global=0.3346 (18.2s)\n",
            "  step 80: local=0.0401 global=0.3252 (20.8s)\n",
            "  step 90: local=0.0401 global=0.3327 (23.4s)\n",
            "  step 100: local=0.0375 global=0.3104 (26.0s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0481 -> 0.0375 (Δ=0.0106)\n",
            "  [Global Loss]:  0.3111 -> 0.3104 (Δ=0.0006)\n",
            "  [Eval KD]:      0.3416 -> 0.3373 (Δ=0.0043, 1.3%)\n",
            "  [Time]:         30.2s\n",
            "\n",
            "[Progress: 12/28] Elapsed: 6:43, ETA: 8:57\n",
            "\n",
            "=== Layer 12 === (15,728,640 trainable params, mode=weights)\n",
            "  [Global KD Loss BEFORE]: 0.3372\n",
            "  step 10: local=0.0472 global=0.3238 (2.6s)\n",
            "  step 20: local=0.0461 global=0.3306 (5.1s)\n",
            "  step 30: local=0.0443 global=0.3105 (7.6s)\n",
            "  step 40: local=0.0431 global=0.3264 (10.1s)\n",
            "  step 50: local=0.0423 global=0.3192 (12.7s)\n",
            "  step 60: local=0.0415 global=0.3114 (15.2s)\n",
            "  step 70: local=0.0399 global=0.3102 (17.8s)\n",
            "  step 80: local=0.0404 global=0.3253 (20.3s)\n",
            "  step 90: local=0.0400 global=0.3499 (22.8s)\n",
            "  step 100: local=0.0408 global=0.3092 (25.4s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0486 -> 0.0408 (Δ=0.0078)\n",
            "  [Global Loss]:  0.3021 -> 0.3092 (Δ=-0.0071)\n",
            "  [Eval KD]:      0.3372 -> 0.3351 (Δ=0.0021, 0.6%)\n",
            "  [Time]:         29.5s\n",
            "\n",
            "[Progress: 13/28] Elapsed: 7:13, ETA: 8:19\n",
            "\n",
            "=== Layer 13 === (15,728,640 trainable params, mode=weights)\n",
            "  [Global KD Loss BEFORE]: 0.3350\n",
            "  step 10: local=0.0473 global=0.3031 (2.5s)\n",
            "  step 20: local=0.0465 global=0.3024 (5.0s)\n",
            "  step 30: local=0.0457 global=0.3393 (7.4s)\n",
            "  step 40: local=0.0447 global=0.3080 (9.9s)\n",
            "  step 50: local=0.0422 global=0.2915 (12.4s)\n",
            "  step 60: local=0.0411 global=0.3211 (14.9s)\n",
            "  step 70: local=0.0427 global=0.3315 (17.3s)\n",
            "  step 80: local=0.0390 global=0.3159 (19.8s)\n",
            "  step 90: local=0.0412 global=0.3261 (22.3s)\n",
            "  step 100: local=0.0403 global=0.2868 (24.7s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0499 -> 0.0403 (Δ=0.0095)\n",
            "  [Global Loss]:  0.3006 -> 0.2868 (Δ=0.0139)\n",
            "  [Eval KD]:      0.3350 -> 0.3313 (Δ=0.0037, 1.1%)\n",
            "  [Time]:         29.0s\n",
            "\n",
            "[Progress: 14/28] Elapsed: 7:41, ETA: 7:41\n",
            "\n",
            "=== Layer 14 === (15,728,640 trainable params, mode=weights)\n",
            "  [Global KD Loss BEFORE]: 0.3313\n",
            "  step 10: local=0.0500 global=0.3384 (2.5s)\n",
            "  step 20: local=0.0486 global=0.3234 (4.8s)\n",
            "  step 30: local=0.0458 global=0.3252 (7.2s)\n",
            "  step 40: local=0.0442 global=0.3393 (9.6s)\n",
            "  step 50: local=0.0441 global=0.3147 (12.1s)\n",
            "  step 60: local=0.0417 global=0.3451 (14.5s)\n",
            "  step 70: local=0.0414 global=0.3312 (16.9s)\n",
            "  step 80: local=0.0404 global=0.3155 (19.3s)\n",
            "  step 90: local=0.0429 global=0.2792 (21.7s)\n",
            "  step 100: local=0.0394 global=0.3210 (24.1s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0511 -> 0.0394 (Δ=0.0118)\n",
            "  [Global Loss]:  0.3086 -> 0.3210 (Δ=-0.0124)\n",
            "  [Eval KD]:      0.3313 -> 0.3301 (Δ=0.0013, 0.4%)\n",
            "  [Time]:         28.3s\n",
            "\n",
            "[Progress: 15/28] Elapsed: 8:10, ETA: 7:04\n",
            "\n",
            "=== Layer 15 === (15,728,640 trainable params, mode=weights)\n",
            "  [Global KD Loss BEFORE]: 0.3303\n",
            "  step 10: local=0.0500 global=0.3217 (2.4s)\n",
            "  step 20: local=0.0483 global=0.3071 (4.7s)\n",
            "  step 30: local=0.0481 global=0.3428 (7.1s)\n",
            "  step 40: local=0.0443 global=0.3088 (9.4s)\n",
            "  step 50: local=0.0443 global=0.2988 (11.8s)\n",
            "  step 60: local=0.0446 global=0.3105 (14.1s)\n",
            "  step 70: local=0.0432 global=0.3127 (16.4s)\n",
            "  step 80: local=0.0412 global=0.3079 (18.8s)\n",
            "  step 90: local=0.0433 global=0.3192 (21.2s)\n",
            "  step 100: local=0.0419 global=0.3135 (23.5s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0547 -> 0.0419 (Δ=0.0127)\n",
            "  [Global Loss]:  0.3257 -> 0.3135 (Δ=0.0123)\n",
            "  [Eval KD]:      0.3303 -> 0.3254 (Δ=0.0049, 1.5%)\n",
            "  [Time]:         27.7s\n",
            "\n",
            "[Progress: 16/28] Elapsed: 8:37, ETA: 6:28\n",
            "\n",
            "=== Layer 16 === (15,728,640 trainable params, mode=weights)\n",
            "  [Global KD Loss BEFORE]: 0.3254\n",
            "  step 10: local=0.0509 global=0.3106 (2.3s)\n",
            "  step 20: local=0.0479 global=0.3342 (4.6s)\n",
            "  step 30: local=0.0480 global=0.3107 (6.9s)\n",
            "  step 40: local=0.0469 global=0.2966 (9.1s)\n",
            "  step 50: local=0.0490 global=0.3163 (11.5s)\n",
            "  step 60: local=0.0471 global=0.3015 (13.7s)\n",
            "  step 70: local=0.0441 global=0.3307 (16.0s)\n",
            "  step 80: local=0.0441 global=0.3091 (18.2s)\n",
            "  step 90: local=0.0439 global=0.3182 (20.5s)\n",
            "  step 100: local=0.0424 global=0.3035 (22.8s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0523 -> 0.0424 (Δ=0.0098)\n",
            "  [Global Loss]:  0.3261 -> 0.3035 (Δ=0.0226)\n",
            "  [Eval KD]:      0.3254 -> 0.3220 (Δ=0.0034, 1.0%)\n",
            "  [Time]:         27.0s\n",
            "\n",
            "[Progress: 17/28] Elapsed: 9:04, ETA: 5:52\n",
            "\n",
            "=== Layer 17 === (15,728,640 trainable params, mode=weights)\n",
            "  [Global KD Loss BEFORE]: 0.3218\n",
            "  step 10: local=0.0484 global=0.3225 (2.3s)\n",
            "  step 20: local=0.0474 global=0.3224 (4.5s)\n",
            "  step 30: local=0.0471 global=0.3058 (6.7s)\n",
            "  step 40: local=0.0438 global=0.3076 (8.9s)\n",
            "  step 50: local=0.0439 global=0.3138 (11.1s)\n",
            "  step 60: local=0.0420 global=0.2874 (13.3s)\n",
            "  step 70: local=0.0420 global=0.3082 (15.5s)\n",
            "  step 80: local=0.0424 global=0.3394 (17.7s)\n",
            "  step 90: local=0.0400 global=0.2731 (19.9s)\n",
            "  step 100: local=0.0387 global=0.3079 (22.1s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0500 -> 0.0387 (Δ=0.0112)\n",
            "  [Global Loss]:  0.3056 -> 0.3079 (Δ=-0.0023)\n",
            "  [Eval KD]:      0.3218 -> 0.3085 (Δ=0.0133, 4.1%)\n",
            "  [Time]:         26.3s\n",
            "\n",
            "[Progress: 18/28] Elapsed: 9:31, ETA: 5:17\n",
            "\n",
            "=== Layer 18 === (15,728,640 trainable params, mode=weights)\n",
            "  [Global KD Loss BEFORE]: 0.3085\n",
            "  step 10: local=0.0477 global=0.3003 (2.2s)\n",
            "  step 20: local=0.0452 global=0.3113 (4.3s)\n",
            "  step 30: local=0.0440 global=0.2858 (6.5s)\n",
            "  step 40: local=0.0428 global=0.2963 (8.6s)\n",
            "  step 50: local=0.0413 global=0.2925 (10.8s)\n",
            "  step 60: local=0.0421 global=0.2668 (13.0s)\n",
            "  step 70: local=0.0393 global=0.3006 (15.1s)\n",
            "  step 80: local=0.0412 global=0.3016 (17.3s)\n",
            "  step 90: local=0.0417 global=0.3114 (19.5s)\n",
            "  step 100: local=0.0422 global=0.3392 (21.6s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0488 -> 0.0422 (Δ=0.0066)\n",
            "  [Global Loss]:  0.2845 -> 0.3392 (Δ=-0.0547)\n",
            "  [Eval KD]:      0.3085 -> 0.3048 (Δ=0.0037, 1.2%)\n",
            "  [Time]:         25.9s\n",
            "\n",
            "[Progress: 19/28] Elapsed: 9:57, ETA: 4:42\n",
            "\n",
            "=== Layer 19 === (15,728,640 trainable params, mode=weights)\n",
            "  [Global KD Loss BEFORE]: 0.3048\n",
            "  step 10: local=0.0519 global=0.3137 (2.1s)\n",
            "  step 20: local=0.0494 global=0.2895 (4.2s)\n",
            "  step 30: local=0.0491 global=0.3017 (6.3s)\n",
            "  step 40: local=0.0472 global=0.2906 (8.4s)\n",
            "  step 50: local=0.0453 global=0.2937 (10.5s)\n",
            "  step 60: local=0.0465 global=0.3057 (12.6s)\n",
            "  step 70: local=0.0456 global=0.2806 (14.7s)\n",
            "  step 80: local=0.0443 global=0.3013 (16.8s)\n",
            "  step 90: local=0.0441 global=0.2868 (18.9s)\n",
            "  step 100: local=0.0419 global=0.2841 (21.0s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0527 -> 0.0419 (Δ=0.0108)\n",
            "  [Global Loss]:  0.2939 -> 0.2841 (Δ=0.0098)\n",
            "  [Eval KD]:      0.3048 -> 0.3029 (Δ=0.0019, 0.6%)\n",
            "  [Time]:         25.2s\n",
            "\n",
            "[Progress: 20/28] Elapsed: 10:22, ETA: 4:08\n",
            "\n",
            "=== Layer 20 === (15,728,640 trainable params, mode=weights)\n",
            "  [Global KD Loss BEFORE]: 0.3025\n",
            "  step 10: local=0.0437 global=0.2900 (2.1s)\n",
            "  step 20: local=0.0432 global=0.2609 (4.1s)\n",
            "  step 30: local=0.0427 global=0.2984 (6.1s)\n",
            "  step 40: local=0.0414 global=0.2718 (8.1s)\n",
            "  step 50: local=0.0404 global=0.3167 (10.2s)\n",
            "  step 60: local=0.0393 global=0.2849 (12.2s)\n",
            "  step 70: local=0.0396 global=0.3020 (14.2s)\n",
            "  step 80: local=0.0377 global=0.2897 (16.3s)\n",
            "  step 90: local=0.0390 global=0.2867 (18.3s)\n",
            "  step 100: local=0.0373 global=0.3082 (20.4s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0473 -> 0.0373 (Δ=0.0100)\n",
            "  [Global Loss]:  0.2842 -> 0.3082 (Δ=-0.0240)\n",
            "  [Eval KD]:      0.3025 -> 0.2991 (Δ=0.0034, 1.1%)\n",
            "  [Time]:         24.5s\n",
            "\n",
            "[Progress: 21/28] Elapsed: 10:46, ETA: 3:35\n",
            "\n",
            "=== Layer 21 === (15,728,640 trainable params, mode=weights)\n",
            "  [Global KD Loss BEFORE]: 0.2993\n",
            "  step 10: local=0.0429 global=0.2817 (2.0s)\n",
            "  step 20: local=0.0409 global=0.2825 (4.0s)\n",
            "  step 30: local=0.0419 global=0.2894 (5.9s)\n",
            "  step 40: local=0.0368 global=0.2980 (7.9s)\n",
            "  step 50: local=0.0395 global=0.3040 (9.9s)\n",
            "  step 60: local=0.0376 global=0.2929 (11.9s)\n",
            "  step 70: local=0.0381 global=0.2975 (13.8s)\n",
            "  step 80: local=0.0365 global=0.2909 (15.8s)\n",
            "  step 90: local=0.0347 global=0.3023 (17.8s)\n",
            "  step 100: local=0.0376 global=0.3006 (19.7s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0427 -> 0.0376 (Δ=0.0051)\n",
            "  [Global Loss]:  0.3050 -> 0.3006 (Δ=0.0044)\n",
            "  [Eval KD]:      0.2993 -> 0.2964 (Δ=0.0028, 0.9%)\n",
            "  [Time]:         23.9s\n",
            "\n",
            "[Progress: 22/28] Elapsed: 11:10, ETA: 3:02\n",
            "\n",
            "=== Layer 22 === (15,728,640 trainable params, mode=weights)\n",
            "  [Global KD Loss BEFORE]: 0.2964\n",
            "  step 10: local=0.0373 global=0.2888 (2.0s)\n",
            "  step 20: local=0.0350 global=0.2778 (3.8s)\n",
            "  step 30: local=0.0380 global=0.2943 (5.7s)\n",
            "  step 40: local=0.0363 global=0.2945 (7.6s)\n",
            "  step 50: local=0.0360 global=0.2578 (9.6s)\n",
            "  step 60: local=0.0353 global=0.2972 (11.5s)\n",
            "  step 70: local=0.0326 global=0.2895 (13.4s)\n",
            "  step 80: local=0.0314 global=0.2932 (15.3s)\n",
            "  step 90: local=0.0318 global=0.2657 (17.2s)\n",
            "  step 100: local=0.0315 global=0.2843 (19.1s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0373 -> 0.0315 (Δ=0.0058)\n",
            "  [Global Loss]:  0.2867 -> 0.2843 (Δ=0.0024)\n",
            "  [Eval KD]:      0.2964 -> 0.2950 (Δ=0.0014, 0.5%)\n",
            "  [Time]:         23.4s\n",
            "\n",
            "[Progress: 23/28] Elapsed: 11:34, ETA: 2:30\n",
            "\n",
            "=== Layer 23 === (15,728,640 trainable params, mode=weights)\n",
            "  [Global KD Loss BEFORE]: 0.2949\n",
            "  step 10: local=0.0387 global=0.3023 (1.9s)\n",
            "  step 20: local=0.0375 global=0.3110 (3.7s)\n",
            "  step 30: local=0.0401 global=0.3215 (5.6s)\n",
            "  step 40: local=0.0359 global=0.3206 (7.4s)\n",
            "  step 50: local=0.0349 global=0.3119 (9.3s)\n",
            "  step 60: local=0.0335 global=0.2743 (11.2s)\n",
            "  step 70: local=0.0341 global=0.2775 (13.0s)\n",
            "  step 80: local=0.0338 global=0.3105 (14.8s)\n",
            "  step 90: local=0.0334 global=0.2838 (16.7s)\n",
            "  step 100: local=0.0333 global=0.3003 (18.6s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0391 -> 0.0333 (Δ=0.0058)\n",
            "  [Global Loss]:  0.2833 -> 0.3003 (Δ=-0.0170)\n",
            "  [Eval KD]:      0.2949 -> 0.2931 (Δ=0.0017, 0.6%)\n",
            "  [Time]:         22.8s\n",
            "\n",
            "[Progress: 24/28] Elapsed: 11:56, ETA: 1:59\n",
            "\n",
            "=== Layer 24 === (15,728,640 trainable params, mode=weights)\n",
            "  [Global KD Loss BEFORE]: 0.2930\n",
            "  step 10: local=0.0338 global=0.2801 (1.8s)\n",
            "  step 20: local=0.0297 global=0.2800 (3.6s)\n",
            "  step 30: local=0.0300 global=0.2972 (5.4s)\n",
            "  step 40: local=0.0298 global=0.3107 (7.2s)\n",
            "  step 50: local=0.0286 global=0.2743 (9.0s)\n",
            "  step 60: local=0.0294 global=0.2993 (10.8s)\n",
            "  step 70: local=0.0295 global=0.2785 (12.5s)\n",
            "  step 80: local=0.0276 global=0.2862 (14.3s)\n",
            "  step 90: local=0.0289 global=0.3117 (16.1s)\n",
            "  step 100: local=0.0267 global=0.2838 (17.9s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0327 -> 0.0267 (Δ=0.0060)\n",
            "  [Global Loss]:  0.2946 -> 0.2838 (Δ=0.0108)\n",
            "  [Eval KD]:      0.2930 -> 0.2892 (Δ=0.0038, 1.3%)\n",
            "  [Time]:         22.2s\n",
            "\n",
            "[Progress: 25/28] Elapsed: 12:19, ETA: 1:28\n",
            "\n",
            "=== Layer 25 === (15,728,640 trainable params, mode=weights)\n",
            "  [Global KD Loss BEFORE]: 0.2892\n",
            "  step 10: local=0.0385 global=0.2725 (1.8s)\n",
            "  step 20: local=0.0389 global=0.2832 (3.5s)\n",
            "  step 30: local=0.0377 global=0.2934 (5.2s)\n",
            "  step 40: local=0.0368 global=0.2891 (6.9s)\n",
            "  step 50: local=0.0342 global=0.2804 (8.7s)\n",
            "  step 60: local=0.0342 global=0.2718 (10.4s)\n",
            "  step 70: local=0.0330 global=0.2755 (12.1s)\n",
            "  step 80: local=0.0346 global=0.2779 (13.8s)\n",
            "  step 90: local=0.0316 global=0.2944 (15.6s)\n",
            "  step 100: local=0.0313 global=0.2840 (17.3s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0386 -> 0.0313 (Δ=0.0073)\n",
            "  [Global Loss]:  0.2896 -> 0.2840 (Δ=0.0056)\n",
            "  [Eval KD]:      0.2892 -> 0.2873 (Δ=0.0019, 0.7%)\n",
            "  [Time]:         21.5s\n",
            "\n",
            "[Progress: 26/28] Elapsed: 12:40, ETA: 0:58\n",
            "\n",
            "=== Layer 26 === (15,728,640 trainable params, mode=weights)\n",
            "  [Global KD Loss BEFORE]: 0.2874\n",
            "  step 10: local=0.0459 global=0.3066 (1.7s)\n",
            "  step 20: local=0.0444 global=0.2873 (3.4s)\n",
            "  step 30: local=0.0401 global=0.3006 (5.0s)\n",
            "  step 40: local=0.0408 global=0.2837 (6.7s)\n",
            "  step 50: local=0.0382 global=0.2681 (8.4s)\n",
            "  step 60: local=0.0410 global=0.2959 (10.0s)\n",
            "  step 70: local=0.0371 global=0.2724 (11.7s)\n",
            "  step 80: local=0.0377 global=0.3206 (13.3s)\n",
            "  step 90: local=0.0354 global=0.2685 (15.0s)\n",
            "  step 100: local=0.0347 global=0.2806 (16.7s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0479 -> 0.0347 (Δ=0.0132)\n",
            "  [Global Loss]:  0.3003 -> 0.2806 (Δ=0.0197)\n",
            "  [Eval KD]:      0.2874 -> 0.2859 (Δ=0.0015, 0.5%)\n",
            "  [Time]:         20.9s\n",
            "\n",
            "[Progress: 27/28] Elapsed: 13:01, ETA: 0:28\n",
            "\n",
            "=== Layer 27 === (15,728,640 trainable params, mode=weights)\n",
            "  [Global KD Loss BEFORE]: 0.2858\n",
            "  step 10: local=0.0191 global=0.2757 (1.7s)\n",
            "  step 20: local=0.0230 global=0.2966 (3.2s)\n",
            "  step 30: local=0.0205 global=0.2697 (4.8s)\n",
            "  step 40: local=0.0148 global=0.2678 (6.4s)\n",
            "  step 50: local=0.0230 global=0.2727 (8.0s)\n",
            "  step 60: local=0.0148 global=0.2773 (9.6s)\n",
            "  step 70: local=0.0153 global=0.2875 (11.2s)\n",
            "  step 80: local=0.0157 global=0.3163 (12.7s)\n",
            "  step 90: local=0.0138 global=0.2856 (14.3s)\n",
            "  step 100: local=0.0164 global=0.3103 (15.9s)\n",
            "  ---\n",
            "  [Local Loss]:   0.0216 -> 0.0164 (Δ=0.0052)\n",
            "  [Global Loss]:  0.2897 -> 0.3103 (Δ=-0.0206)\n",
            "  [Eval KD]:      0.2858 -> 0.2823 (Δ=0.0035, 1.2%)\n",
            "  [Time]:         20.2s\n",
            "\n",
            "============================================================\n",
            "LAYER-BY-LAYER TRAINING COMPLETE\n",
            "============================================================\n",
            "Total time:    13:25 (805.8s)\n",
            "Avg per layer: 28.6s\n",
            "\n",
            "[Initial Global KD Loss]: 0.3684\n",
            "[Final Global KD Loss]:   0.2653\n",
            "[Total Improvement]:      0.1031 (28.0%)\n",
            "\n",
            "Per-layer summary:\n",
            " Layer  Eval Before   Eval After     Eval Δ  Local 1st Local Last  Global 1st  Global Last     Time\n",
            "---------------------------------------------------------------------------------------------------------\n",
            "     0       0.3965       0.3729    +0.0235     0.0648     0.0498      0.3766       0.3814    37.1s\n",
            "     1       0.3728       0.3688    +0.0040     0.0611     0.0540      0.3484       0.4044    36.4s\n",
            "     2       0.3695       0.3676    +0.0019     0.0455     0.0372      0.3661       0.3519    35.8s\n",
            "     3       0.3675       0.3586    +0.0089     0.0450     0.0351      0.3536       0.3392    35.2s\n",
            "     4       0.3585       0.3572    +0.0013     0.0451     0.0382      0.3301       0.3506    34.6s\n",
            "     5       0.3573       0.3515    +0.0058     0.0443     0.0382      0.3018       0.3259    34.0s\n",
            "     6       0.3513       0.3512    +0.0001     0.0451     0.0378      0.3526       0.3520    33.3s\n",
            "     7       0.3514       0.3468    +0.0046     0.0427     0.0387      0.3402       0.3792    32.7s\n",
            "     8       0.3469       0.3445    +0.0023     0.0466     0.0390      0.3370       0.3248    32.1s\n",
            "     9       0.3446       0.3440    +0.0006     0.0467     0.0362      0.2878       0.3300    31.5s\n",
            "    10       0.3441       0.3414    +0.0027     0.0462     0.0380      0.3466       0.3437    30.8s\n",
            "    11       0.3416       0.3373    +0.0043     0.0481     0.0375      0.3111       0.3104    30.2s\n",
            "    12       0.3372       0.3351    +0.0021     0.0486     0.0408      0.3021       0.3092    29.5s\n",
            "    13       0.3350       0.3313    +0.0037     0.0499     0.0403      0.3006       0.2868    29.0s\n",
            "    14       0.3313       0.3301    +0.0013     0.0511     0.0394      0.3086       0.3210    28.3s\n",
            "    15       0.3303       0.3254    +0.0049     0.0547     0.0419      0.3257       0.3135    27.7s\n",
            "    16       0.3254       0.3220    +0.0034     0.0523     0.0424      0.3261       0.3035    27.0s\n",
            "    17       0.3218       0.3085    +0.0133     0.0500     0.0387      0.3056       0.3079    26.3s\n",
            "    18       0.3085       0.3048    +0.0037     0.0488     0.0422      0.2845       0.3392    25.9s\n",
            "    19       0.3048       0.3029    +0.0019     0.0527     0.0419      0.2939       0.2841    25.2s\n",
            "    20       0.3025       0.2991    +0.0034     0.0473     0.0373      0.2842       0.3082    24.5s\n",
            "    21       0.2993       0.2964    +0.0028     0.0427     0.0376      0.3050       0.3006    23.9s\n",
            "    22       0.2964       0.2950    +0.0014     0.0373     0.0315      0.2867       0.2843    23.4s\n",
            "    23       0.2949       0.2931    +0.0017     0.0391     0.0333      0.2833       0.3003    22.8s\n",
            "    24       0.2930       0.2892    +0.0038     0.0327     0.0267      0.2946       0.2838    22.2s\n",
            "    25       0.2892       0.2873    +0.0019     0.0386     0.0313      0.2896       0.2840    21.5s\n",
            "    26       0.2874       0.2859    +0.0015     0.0479     0.0347      0.3003       0.2806    20.9s\n",
            "    27       0.2858       0.2823    +0.0035     0.0216     0.0164      0.2897       0.3103    20.2s\n"
          ]
        }
      ],
      "source": [
        "# Train all layers using the imported function\n",
        "layer_losses = train_all_layers(\n",
        "    model=model,\n",
        "    cache_dir=cache_local_path,\n",
        "    device=DEVICE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    lr=LR,\n",
        "    epochs_per_layer=EPOCHS_PER_LAYER,\n",
        "    grad_accum=GRAD_ACCUM,\n",
        "    temperature=DISTILL_TEMP,\n",
        "    train_scales=False,  # Keep scales frozen for now\n",
        "    verbose=True,\n",
        "    steps_per_layer=100,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "uv6rcQvuFCuX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "765eba8a-0030-4eec-e928-d4a2e3b70d78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial KD Loss: 1.0885\n",
            "After Layer-by-Layer: 0.2653\n",
            "Improvement: 0.8232\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# EVALUATE AFTER LAYER-BY-LAYER\n",
        "# ============================================================\n",
        "\n",
        "model.eval()\n",
        "post_layer_loss = evaluate_kd_loss(model, cache_local_path, DEVICE, num_samples=40)\n",
        "print(f'Initial KD Loss: {initial_loss:.4f}')\n",
        "print(f'After Layer-by-Layer: {post_layer_loss:.4f}')\n",
        "print(f'Improvement: {initial_loss - post_layer_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "lxSBHcq6FCuX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90a71c51-96ac-43d9-a837-a0b542a30edb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved to runs/anemll_q4_layer_by_layer_v1\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# SAVE CHECKPOINT\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "\n",
        "RUN_NAME = f'anemll_{QUAL}_layer_by_layer_v1'\n",
        "SAVE_DIR = f'{LOCAL_RUNS}/{RUN_NAME}'\n",
        "\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Save state dict\n",
        "torch.save(model.state_dict(), f'{SAVE_DIR}/model_state_dict.pt')\n",
        "\n",
        "# Save config\n",
        "import json\n",
        "config = {\n",
        "    'model_id': MODEL_ID,\n",
        "    'lut_size': LUT_SIZE,\n",
        "    'group_size': GROUP_SIZE,\n",
        "    'scale_rank': SCALE_RANK,\n",
        "    'attn_lut_size': ATTN_LUT_SIZE,\n",
        "    'attn_group_size': ATTN_GROUP_SIZE,\n",
        "    'attn_scale_rank': ATTN_SCALE_RANK,\n",
        "    'initial_kd_loss': initial_loss,\n",
        "    'post_layer_loss': post_layer_loss,\n",
        "    'layer_losses': layer_losses,\n",
        "}\n",
        "with open(f'{SAVE_DIR}/config.json', 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(f'Saved to {SAVE_DIR}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "R4wh-UVDFCuX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b20ed80d-5766-48b3-9e81-da1dff6e74bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anemll_q4_layer_by_layer_v1/\n",
            "anemll_q4_layer_by_layer_v1/config.json\n",
            "anemll_q4_layer_by_layer_v1/model_state_dict.pt\n",
            "Uploaded to /content/drive/MyDrive/qwen3_runs/anemll_q4_layer_by_layer_v1.tgz\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# UPLOAD TO GOOGLE DRIVE\n",
        "# ============================================================\n",
        "\n",
        "!tar -czvf {RUN_NAME}.tgz -C {LOCAL_RUNS} {RUN_NAME}\n",
        "!cp {RUN_NAME}.tgz {GD_RUNS}/\n",
        "print(f'Uploaded to {GD_RUNS}/{RUN_NAME}.tgz')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **INFERENCE OPTIMIZATION**\n",
        "\n",
        "Before running inference, freeze all layers to precompute quantized weights.\n",
        "This avoids recomputing `LUT[indices] * (scale_A @ scale_B)` on every forward pass."
      ],
      "metadata": {
        "id": "fqafgf8KfRgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# FREEZE MODEL FOR FAST INFERENCE\n",
        "# ============================================================\n",
        "# Precompute quantized weights once for all layers\n",
        "# This avoids recomputing LUT[indices] * (scale_A @ scale_B) per token\n",
        "\n",
        "from qat_lora import freeze_model_for_inference, unfreeze_model_for_training\n",
        "\n",
        "print('Freezing model for inference...')\n",
        "num_frozen = freeze_model_for_inference(model, verbose=False)\n",
        "print(f'Frozen {num_frozen} layers')\n",
        "\n",
        "# To resume training later:\n",
        "# unfreeze_model_for_training(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWZ2KP18fRgs",
        "outputId": "7bd29b13-d8ce-4887-f758-3edf9321ac1c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing model for inference...\n",
            "Frozen 196 layers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Ugmqo-MHFCuX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66ff9b82-65cf-46bf-b8df-fd89898c8005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: What is the capital of France?\n",
            "Response: <think>\n",
            "<think>\n",
            "Okay, the user is asking about the capital of France. Let me think about this.\n",
            "\n",
            "First, I need to recall the capital of France. France is a country in Europe, and I know that the capital is Paris. But wait, I should double-check to make sure. Let me think again. \n",
            "\n",
            "Paris is the capital of France, right? Yes, that's correct. I don't see any other city with the same capital. \n",
            "\n",
            "Is there any chance that the capital is another city? For example, maybe a city in the south or somewhere else? I don't think so. The capital is always Paris. \n",
            "\n",
            "So, the answer is Paris. Let me make sure I'm not making a mistake here. \n",
            "\n",
            "I think I'm confident now. The capital of France is Paris. \n",
            "\n",
            "**Final Answer**\n",
            "</think>\n",
            "\n",
            "The capital of France is **Paris**.\n",
            "--------------------------------------------------\n",
            "Prompt: What is Apple Neural Engine?\n",
            "Response: <think>\n",
            "<think>\n",
            "Okay, the user asked about Apple Neural Engine. Let me start by recalling what I know. Apple has been developing AI and machine learning models for various applications. The Neural Engine is a part of their AI framework, probably related to deep learning. I need to explain it in a simple way. Maybe start by mentioning that it's a powerful tool for AI and machine learning. Then, explain that it's used in different areas like image recognition, natural language processing, and more. Also, mention that it's part of Apple's broader AI ecosystem. Make sure to keep it clear and easy to understand.\n",
            "</think>\n",
            "\n",
            "Apple Neural Engine is a powerful tool that powers Apple's AI and machine learning models. It's designed to run on Apple's devices and is used in a variety of applications, such as image recognition, natural language processing, and more. It's part of Apple's broader AI ecosystem, allowing developers to create AI models that can perform complex tasks like text generation, speech recognition, and more.\n",
            "--------------------------------------------------\n",
            "Prompt: Explain quantum mechanics\n",
            "Response: <think>\n",
            "<think>\n",
            "Okay, the user asked me to explain quantum mechanics. Let me start by recalling what I know about the basics of quantum mechanics. From what I remember, it's a branch of physics that deals with the behavior of particles at very small scales, like atoms and electrons. \n",
            "\n",
            "I should explain the concepts like particles, wave-particle duality, and the concept of quantum numbers. Maybe I can compare it to how particles behave in everyday life, like how a ball moves or how light behaves. \n",
            "\n",
            "I need to make sure I'm accurate and not making up too much. Let me check some key points: particles have both particle and wave properties, they can exist in multiple states at once, and the wave-particle duality is a fundamental concept. \n",
            "\n",
            "Also, I should mention the famous equations and models, like the Schrödinger's equation and the Heisenberg's uncertainty principle. But I should keep it simple and avoid complex terms. \n",
            "\n",
            "Wait, the user might be a student or someone new to quantum mechanics, so I should present it in a clear and easy way. Maybe start with the basic idea of particles and their properties, then move to the wave-particle duality, and then the equations. \n",
            "\n",
            "I should also mention the historical context, like Einstein's contributions, to give a sense of where the field started. But I should keep it concise. Let me structure it step by step, starting with the basics, then moving to the wave-particle duality, and finally the equations. \n",
            "\n",
            "Is there anything else I should add? Maybe include some examples or diagrams to help the user visualize? But I should stick to the information I have. Alright, I think that's a good approach.\n",
            "</think>\n",
            "\n",
            "Quantum mechanics is a fundamental part of modern physics that deals with the behavior of particles at very small scales. Here's a simple explanation:\n",
            "\n",
            "1. **Particles have both particle and wave properties**: At the atomic and subatomic levels, particles (like electrons, protons, and neutrons) have both particle and wave properties. This means they can exist in multiple states at once, like being in a superposition of different energy levels.\n",
            "\n",
            "2. **Wave-particle duality**: One of the most important concepts in quantum mechanics is the **wave-particle duality**, which means that particles can exhibit both wave-like and particle-like behavior. For example, a particle can be detected as a wave, but it can also be a particle, like a ball or a ball in a box.\n",
            "\n",
            "3. **Quantum numbers**: These are the basic properties of particles, such as their mass, charge, and position. These numbers are used to describe the state of a particle in a quantum system.\n",
            "\n",
            "4. **Schrödinger's Equation**: This equation describes how a particle's position and momentum change over time in a quantum system. It's a mathematical model that shows how particles behave in a quantum world.\n",
            "\n",
            "5. **Heisenberg's Uncertainty Principle**: This principle states that the product of the uncertainties in a particle's position and momentum is always greater than or equal to a certain value, which is a fundamental aspect of quantum mechanics.\n",
            "\n",
            "In short, quantum mechanics is the study of how particles behave at the smallest scales, and it's a key part of our understanding of the universe.\n",
            "--------------------------------------------------\n",
            "Prompt: What is speed of light\n",
            "Response: <think>\n",
            "Okay, the user asked, \"What is speed of light?\" I need to provide an answer. Let me think about this.\n",
            "\n",
            "First, I should recall the known speed of light. From physics, I know that light travels at a constant speed, approximately 3.0000000000000001 meters per second. But I should check if there's any other sources or if there's any other information I might have. Wait, the user might be interested in the exact value, or maybe they're talking about a different context. For example, in the context of physics, the speed of light is a fundamental constant. In other contexts, like in the universe, it might be different. But I should also consider if there's any other information I might have. For example, in the context of the universe, the speed of light is a constant, but in the context of the universe, it might be different. But I don't have that information. \n",
            "\n",
            "Wait, the user might be asking about the speed of light in a specific context, like in the universe, or maybe in the context of the universe. For example, in the universe, the speed of light is a constant, but in the context of the universe, it might be different. But I don't have that information. \n",
            "\n",
            "Alternatively, maybe the user is asking about the speed of light in a different unit or in a different context. For example, in the context of the universe, the speed of light is a constant, but in the context of the universe, it might be different. \n",
            "\n",
            "Wait, the user might be asking about the speed of light in a different context, like in the context of the universe. For example, in the context of the universe, the speed of light is a constant, but in the context of the universe, it might be different. \n",
            "\n",
            "Alternatively, maybe the user is asking about the speed of light in a different unit or in a different context. For example, in the context of the universe, the speed of light is a constant, but in the context of the universe, it might be different. \n",
            "\n",
            "Wait, I need to make sure I'm not missing any information. Let me think again. \n",
            "\n",
            "The user asked, \"What is speed of light?\" The answer should be the known speed of light, which is approximately 3.0000000000000001 meters per second. I should also mention that in the context of the universe, the speed of light is a constant, but in other contexts, like in the context of the universe, it might be different. \n",
            "\n",
            "But I should also check if there's any other information I might have. For example, if the user is asking about the speed of light in a different context, like in the context of the universe, or maybe in the context of the universe, it might be different. \n",
            "\n",
            "Alternatively, maybe the user is asking about the speed of light in a different unit or in a different context. For example, if the user is asking about the speed of light in the context of the universe, then the answer would be the known speed of light. \n",
            "\n",
            "Alternatively, maybe the user is asking about the speed of light in a different context, like in the context of the universe, and the answer would be the known speed of light. \n",
            "\n",
            "Wait, I need to be careful here. Let me think again. \n",
            "\n",
            "The user asked, \"What is speed of light?\" The answer should be the known speed of light, which is approximately 3.0000000000000001 meters per second. I should also mention that in the context of the universe, the speed of light is a constant, but in other contexts, like in the context of the universe, it might be different. \n",
            "\n",
            "But I should also check if there's any other information I might have. For example, if the user is asking about the speed of light in a different context, like in the context of the universe, then the answer would be the known speed of light. \n",
            "\n",
            "Alternatively, maybe the user is asking about the speed of light in a different context, like in the context of the universe, and the answer would be the known speed of light. \n",
            "\n",
            "Wait, I need to be precise. Let me think again. \n",
            "\n",
            "The user asked, \"What is speed of light?\" The answer should be the known speed of light, which is approximately 3.0000000000000001 meters per second. I should also mention that in the context of the universe, the speed of light is a constant, but in other contexts, like in the context of the universe, it might be different. \n",
            "\n",
            "But I should also check if there's any other information I might have. For example, if the user is asking about the speed of light in a different context, like in the context of the universe,\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# ============================================================\n",
        "# TEST INFERENCE\n",
        "# ============================================================\n",
        "\n",
        "def run_inference(model, tokenizer, prompt, max_new_tokens=128):\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
        "        {'role': 'user', 'content': prompt}\n",
        "    ]\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(text, return_tensors='pt').to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "\n",
        "    return tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "# List of prompts to test\n",
        "prompts = [\n",
        "    'What is the capital of France?',\n",
        "    'What is Apple Neural Engine?',\n",
        "    'Explain quantum mechanics',\n",
        "    'What is speed of light'\n",
        "]\n",
        "\n",
        "model.eval() # Set model to evaluation mode once\n",
        "\n",
        "for prompt in prompts:\n",
        "    response = run_inference(model, tokenizer, prompt,max_new_tokens=1024)\n",
        "    print(f'Prompt: {prompt}')\n",
        "    print(f'Response: {response}')\n",
        "    print('-' * 50) # Separator for readability\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKGYuTbyFCuX"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "After layer-by-layer training, you can:\n",
        "\n",
        "1. **End-to-end refinement** - Unfreeze all layers and train together\n",
        "2. **Train scales (A, B)** - Unfreeze scale_A, scale_B parameters\n",
        "3. **LoRA recovery** - Add LoRA adapters to recover quality"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}