{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anemll-Style Layer-by-Layer QAT\n",
    "\n",
    "This notebook implements layer-by-layer QAT training using `AnemllQATLinear` with:\n",
    "- Groupwise LUT quantization\n",
    "- Low-rank scale factors (A @ B)\n",
    "- KD cache for distillation\n",
    "\n",
    "## Pipeline:\n",
    "1. Load model and replace linears with AnemllQATLinear\n",
    "2. Layer-by-layer QAT (freeze all but current layer)\n",
    "3. End-to-end refinement\n",
    "4. (Optional) LoRA recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GOOGLE DRIVE PATHS (STANDARD)\n",
    "# ============================================================\n",
    "\n",
    "# Checkpoints/runs go here\n",
    "GD_RUNS = '/content/drive/MyDrive/qwen3_runs'\n",
    "\n",
    "# KD caches go here\n",
    "GD_CACHES = '/content/drive/MyDrive/qwen3_caches'\n",
    "\n",
    "# Local directories (on Colab VM)\n",
    "LOCAL_RUNS = 'runs'\n",
    "LOCAL_CACHES = 'caches'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo if needed\n",
    "!git clone https://github.com/anemll/qwen3_apple_style_2bit_qat_lora.git || (cd qwen3_apple_style_2bit_qat_lora && git pull)\n",
    "%cd qwen3_apple_style_2bit_qat_lora\n",
    "# to allow updates\n",
    "!git fetch\n",
    "!git pull\n",
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers accelerate safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD KD CACHE FROM GOOGLE DRIVE\n",
    "# ============================================================\n",
    "\n",
    "CACHE_NAME = 'alpaca_chat_think_both_L128_K32_R256'\n",
    "CACHE_TGZ = f'{CACHE_NAME}.tgz'\n",
    "\n",
    "!mkdir -p {LOCAL_CACHES}\n",
    "\n",
    "# Check if cache exists locally\n",
    "import os\n",
    "cache_local_path = f'{LOCAL_CACHES}/{CACHE_NAME}'\n",
    "if not os.path.exists(cache_local_path):\n",
    "    print(f'Extracting {CACHE_TGZ} from Google Drive...')\n",
    "    !tar -xzf {GD_CACHES}/{CACHE_TGZ} -C {LOCAL_CACHES}/\n",
    "else:\n",
    "    print(f'Cache already exists at {cache_local_path}')\n",
    "\n",
    "!ls -la {cache_local_path}/ | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "\n",
    "# Model\n",
    "MODEL_ID = 'Qwen/Qwen3-0.6B'\n",
    "\n",
    "# Quantization config (4-bit with groupwise LUT)\n",
    "LUT_SIZE = 16        # 4-bit = 16 levels\n",
    "GROUP_SIZE = 32      # Group size for scales\n",
    "SCALE_RANK = 4       # Low-rank for A @ B scales\n",
    "\n",
    "# Attention quantization (same params)\n",
    "ATTN_LUT_SIZE = 16\n",
    "ATTN_GROUP_SIZE = 32\n",
    "ATTN_SCALE_RANK = 8\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUM = 4\n",
    "LR = 2e-5\n",
    "EPOCHS_PER_LAYER = 1\n",
    "\n",
    "# KD params\n",
    "DISTILL_TEMP = 2.0\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DTYPE = torch.bfloat16\n",
    "\n",
    "print(f'Device: {DEVICE}, dtype: {DTYPE}')\n",
    "print(f'Quant config: lut={LUT_SIZE}, group={GROUP_SIZE}, rank={SCALE_RANK}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD MODEL\n",
    "# ============================================================\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(f'Loading {MODEL_ID}...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=DTYPE,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "print(f'Loaded. Parameters: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# REPLACE LINEARS WITH AnemllQATLinear\n# ============================================================\n\nimport sys\nsys.path.insert(0, '.')\n\nfrom qat_lora import AnemllQuantConfig, replace_linear_with_anemll\n\n# Create configs\nmlp_config = AnemllQuantConfig(\n    lut_size=LUT_SIZE,\n    group_size=GROUP_SIZE,\n    scale_rank=SCALE_RANK,\n    learnable_lut=False,\n)\n\nattn_config = AnemllQuantConfig(\n    lut_size=ATTN_LUT_SIZE,\n    group_size=ATTN_GROUP_SIZE,\n    scale_rank=ATTN_SCALE_RANK,\n    learnable_lut=False,\n)\n\nprint('Replacing linear layers...')\ncount = replace_linear_with_anemll(\n    model,\n    mlp_config=mlp_config,\n    attn_config=attn_config,\n    quantize_attn=True,\n    quantize_lm_head=False,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# IMPORT LAYER-BY-LAYER QAT UTILITIES\n# ============================================================\n\nfrom qat_lora import (\n    evaluate_kd_loss,\n    train_all_layers,\n)\n\nprint('Layer QAT utilities imported from qat_lora')\n\n# Compute initial KD loss\nprint('\\nComputing initial KD loss...')\ninitial_loss = evaluate_kd_loss(model, cache_local_path, DEVICE, num_samples=40, temperature=DISTILL_TEMP)\nprint(f'Initial KD Loss: {initial_loss:.4f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# RUN LAYER-BY-LAYER TRAINING\n# ============================================================\n\nimport os\nfrom pathlib import Path\n\n# Verify drive is mounted and cache exists\nif not os.path.exists('/content/drive/MyDrive'):\n    print('Google Drive not mounted! Mounting now...')\n    from google.colab import drive\n    drive.mount('/content/drive')\n\nif not os.path.exists(cache_local_path):\n    print(f'Cache not found at {cache_local_path}')\n    print(f'Extracting from Google Drive...')\n    os.makedirs(LOCAL_CACHES, exist_ok=True)\n    !tar -xzf {GD_CACHES}/{CACHE_TGZ} -C {LOCAL_CACHES}/\n\n# Verify cache exists now\nassert os.path.exists(cache_local_path), f'Cache still not found at {cache_local_path}'\ncache_files = list(Path(cache_local_path).glob('*.pt'))\nprint(f'Cache ready: {len(cache_files)} files in {cache_local_path}')\n\n# Train all layers using the imported function\nlayer_losses = train_all_layers(\n    model=model,\n    cache_dir=cache_local_path,\n    device=DEVICE,\n    batch_size=BATCH_SIZE,\n    lr=LR,\n    epochs_per_layer=EPOCHS_PER_LAYER,\n    grad_accum=GRAD_ACCUM,\n    temperature=DISTILL_TEMP,\n    train_scales=False,  # Keep scales frozen for now\n    verbose=True,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUATE AFTER LAYER-BY-LAYER\n",
    "# ============================================================\n",
    "\n",
    "model.eval()\n",
    "post_layer_loss = evaluate_kd_loss(model, cache_local_path, DEVICE, num_samples=40)\n",
    "print(f'Initial KD Loss: {initial_loss:.4f}')\n",
    "print(f'After Layer-by-Layer: {post_layer_loss:.4f}')\n",
    "print(f'Improvement: {initial_loss - post_layer_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE CHECKPOINT\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "\n",
    "RUN_NAME = 'anemll_q4_layer_by_layer_v1'\n",
    "SAVE_DIR = f'{LOCAL_RUNS}/{RUN_NAME}'\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Save state dict\n",
    "torch.save(model.state_dict(), f'{SAVE_DIR}/model_state_dict.pt')\n",
    "\n",
    "# Save config\n",
    "import json\n",
    "config = {\n",
    "    'model_id': MODEL_ID,\n",
    "    'lut_size': LUT_SIZE,\n",
    "    'group_size': GROUP_SIZE,\n",
    "    'scale_rank': SCALE_RANK,\n",
    "    'attn_lut_size': ATTN_LUT_SIZE,\n",
    "    'attn_group_size': ATTN_GROUP_SIZE,\n",
    "    'attn_scale_rank': ATTN_SCALE_RANK,\n",
    "    'initial_kd_loss': initial_loss,\n",
    "    'post_layer_loss': post_layer_loss,\n",
    "    'layer_losses': layer_losses,\n",
    "}\n",
    "with open(f'{SAVE_DIR}/config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f'Saved to {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# UPLOAD TO GOOGLE DRIVE\n",
    "# ============================================================\n",
    "\n",
    "!tar -czvf {RUN_NAME}.tgz -C {LOCAL_RUNS} {RUN_NAME}\n",
    "!cp {RUN_NAME}.tgz {GD_RUNS}/\n",
    "print(f'Uploaded to {GD_RUNS}/{RUN_NAME}.tgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST INFERENCE\n",
    "# ============================================================\n",
    "\n",
    "def run_inference(model, tokenizer, prompt, max_new_tokens=128):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "        {'role': 'user', 'content': prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text, return_tensors='pt').to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "    \n",
    "    return tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "# Test\n",
    "model.eval()\n",
    "prompt = 'What is the capital of France?'\n",
    "response = run_inference(model, tokenizer, prompt)\n",
    "print(f'Prompt: {prompt}')\n",
    "print(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After layer-by-layer training, you can:\n",
    "\n",
    "1. **End-to-end refinement** - Unfreeze all layers and train together\n",
    "2. **Train scales (A, B)** - Unfreeze scale_A, scale_B parameters\n",
    "3. **LoRA recovery** - Add LoRA adapters to recover quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}