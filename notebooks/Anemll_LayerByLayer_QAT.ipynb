{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqWTWF5EFCuV"
   },
   "source": "# Anemll-Style Layer-by-Layer QAT\n\nThis notebook implements layer-by-layer QAT training using `AnemllQATLinear` with:\n- Groupwise LUT quantization\n- Low-rank scale factors (A @ B)\n- KD cache for distillation\n- **Hard label loss** for improved convergence\n\n## Pipeline:\n1. Load model and replace linears with AnemllQATLinear\n2. Layer-by-layer scale optimization (weights frozen)\n3. Layer-by-layer weight training (with hard label loss)\n4. End-to-end refinement\n5. (Optional) LoRA recovery\n\n## Distillation Options:\n- `temperature`: KL divergence temperature (default: 2.0)\n- `hard_top1_weight`: Hard label top-1 loss weight (recommended: 0.1 for weights, 0.0 for scales)\n- `hard_full_weight`: Hard label full vocab loss weight (optional)"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Kfh54i9XFCuW"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GOOGLE DRIVE PATHS (STANDARD)\n",
    "# ============================================================\n",
    "\n",
    "# Checkpoints/runs go here\n",
    "GD_RUNS = '/content/drive/MyDrive/qwen3_runs'\n",
    "\n",
    "# KD caches go here\n",
    "GD_CACHES = '/content/drive/MyDrive/qwen3_caches'\n",
    "\n",
    "# Local directories (on Colab VM)\n",
    "LOCAL_RUNS = 'runs'\n",
    "LOCAL_CACHES = 'caches'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Tw1k9-anFCuW",
    "outputId": "644d6577-3ce4-447f-f6a6-3a14bedf943d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GITUB"
   ],
   "metadata": {
    "id": "7X9uxbCPPykd"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HtdvB50DFCuW",
    "outputId": "a4c53fae-0a27-4ba8-aa52-eed9910e1c12",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fatal: destination path 'qwen3_apple_style_2bit_qat_lora' already exists and is not an empty directory.\n",
      "remote: Enumerating objects: 7, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
      "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects: 100% (4/4), 723 bytes | 723.00 KiB/s, done.\n",
      "From https://github.com/anemll/qwen3_apple_style_2bit_qat_lora\n",
      "   d5970d9..91f80e2  main       -> origin/main\n",
      "Updating d5970d9..91f80e2\n",
      "Fast-forward\n",
      " qat_lora/layer_qat.py | 8 \u001b[32m++++++\u001b[m\u001b[31m--\u001b[m\n",
      " 1 file changed, 6 insertions(+), 2 deletions(-)\n",
      "/content/qwen3_apple_style_2bit_qat_lora/qwen3_apple_style_2bit_qat_lora\n",
      "Already up to date.\n",
      "HEAD is now at 91f80e2 Refactor KL divergence computation in compute_kd_loss_batch for improved clarity and accuracy\n"
     ]
    }
   ],
   "source": [
    "# Clone repo if needed\n",
    "!git clone https://github.com/anemll/qwen3_apple_style_2bit_qat_lora.git || (cd qwen3_apple_style_2bit_qat_lora && git pull)\n",
    "%cd qwen3_apple_style_2bit_qat_lora\n",
    "# to allow updates\n",
    "!git fetch\n",
    "!git pull\n",
    "!git reset --hard HEAD\n",
    "import sys\n",
    "[sys.modules.pop(k) for k in list(sys.modules) if k.startswith('qat_lora')]\n",
    "\n",
    "from qat_lora import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "1dJ5vGK9FCuW"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers accelerate safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hgqQOvUpFCuW",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "88f45e15-b330-4fd8-f9df-6b72aa08c832"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting alpaca_chat_think_both_L128_K32_R256.tgz from Google Drive...\n",
      "total 4298968\n",
      "drwx------ 2 root root      4096 Dec 18 00:00 .\n",
      "drwxr-xr-x 3 root root      4096 Dec 26 08:04 ..\n",
      "-rw------- 1 root root       421 Dec 18 00:15 meta.json\n",
      "-rw------- 1 root root 112692165 Dec 18 00:15 shard_00000.pt\n",
      "-rw------- 1 root root 112692165 Dec 18 00:15 shard_00001.pt\n",
      "-rw------- 1 root root 112692165 Dec 18 00:15 shard_00002.pt\n",
      "-rw------- 1 root root 112692165 Dec 18 00:15 shard_00003.pt\n",
      "-rw------- 1 root root 112692165 Dec 18 00:16 shard_00004.pt\n",
      "-rw------- 1 root root 112692165 Dec 18 00:16 shard_00005.pt\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LOAD KD CACHE FROM GOOGLE DRIVE\n",
    "# ============================================================\n",
    "\n",
    "CACHE_NAME = 'alpaca_chat_think_both_L128_K32_R256'\n",
    "CACHE_TGZ = f'{CACHE_NAME}.tgz'\n",
    "\n",
    "!mkdir -p {LOCAL_CACHES}\n",
    "\n",
    "# Check if cache exists locally\n",
    "import os\n",
    "cache_local_path = f'{LOCAL_CACHES}/{CACHE_NAME}'\n",
    "if not os.path.exists(cache_local_path):\n",
    "    print(f'Extracting {CACHE_TGZ} from Google Drive...')\n",
    "    !tar -xzf {GD_CACHES}/{CACHE_TGZ} -C {LOCAL_CACHES}/\n",
    "else:\n",
    "    print(f'Cache already exists at {cache_local_path}')\n",
    "\n",
    "!ls -la {cache_local_path}/ | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJ239_eUFCuW",
    "outputId": "3a887cb8-6521-4b1c-c652-59ff2e0dc9a3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": "# ============================================================\n# CONFIGURATION\n# ============================================================\n\nimport torch\n\n# Model\nMODEL_ID = 'Qwen/Qwen3-0.6B'\n\n# Quantization config (4-bit with groupwise LUT)\nLUT_BITS = 4\nLUT_SIZE = 2**LUT_BITS\nGROUP_SIZE = 32      # Group size for scales\nSCALE_RANK = 4       # Low-rank for A @ B scales\n\n# Attention quantization (same params)\nATTN_LUT_BITS = 4\nATTN_LUT_SIZE = 2**ATTN_LUT_BITS\nATTN_GROUP_SIZE = 32\nATTN_SCALE_RANK = 8\n\n# Training\nBATCH_SIZE = 4\nGRAD_ACCUM = 4\n\nif torch.cuda.is_available():\n    BATCH_SIZE=64\n    GRAD_ACCUM=1\n\nLR = 2e-5\nEPOCHS_PER_LAYER = 1\n\n# KD / Distillation params\nDISTILL_TEMP = 2.0\nHARD_TOP1_WEIGHT = 0.1    # Hard label top-1 loss (helps convergence)\nHARD_FULL_WEIGHT = 0.0    # Hard label full vocab loss (optional)\n\n# Device\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nDTYPE = torch.bfloat16\n\n\nQUAL = f'q{LUT_BITS}_a{ATTN_LUT_BITS}'\n\nprint(f'Quality: {QUAL}')\n\nprint(f'Device: {DEVICE}, dtype: {DTYPE}')\nprint(f'Quant config: lut={LUT_SIZE}, group={GROUP_SIZE}, rank={SCALE_RANK}')\nprint(f'Distillation: temp={DISTILL_TEMP}, hard_top1={HARD_TOP1_WEIGHT}, hard_full={HARD_FULL_WEIGHT}')"
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================\n",
    "# Extracting LOCAL CACHE\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Verify drive is mounted and cache exists\n",
    "if not os.path.exists('/content/drive/MyDrive'):\n",
    "    print('Google Drive not mounted! Mounting now...')\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "if not os.path.exists(cache_local_path):\n",
    "    print(f'Cache not found at {cache_local_path}')\n",
    "    print(f'Extracting from Google Drive...')\n",
    "    os.makedirs(LOCAL_CACHES, exist_ok=True)\n",
    "    !tar -xzf {GD_CACHES}/{CACHE_TGZ} -C {LOCAL_CACHES}/\n",
    "\n",
    "# Verify cache exists now\n",
    "assert os.path.exists(cache_local_path), f'Cache still not found at {cache_local_path}'\n",
    "cache_files = list(Path(cache_local_path).glob('*.pt'))\n",
    "print(f'Cache ready: {len(cache_files)} files in {cache_local_path}')"
   ],
   "metadata": {
    "id": "r-V8ZhsWGl1d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "outputId": "28436f15-94bd-4329-cfb1-bebc62073a9e"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'cache_local_path' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-4186119626.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_local_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Cache not found at {cache_local_path}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Extracting from Google Drive...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cache_local_path' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5e5kQrkxFCuX",
    "outputId": "dac6a6f0-0eab-4df7-fd1a-721fe33c58e3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Qwen/Qwen3-0.6B...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded. Parameters: 596,049,920\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LOAD MODEL\n",
    "# ============================================================\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(f'Loading {MODEL_ID}...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=DTYPE,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "print(f'Loaded. Parameters: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "n29f0NexFCuX",
    "outputId": "800c94df-904e-44c8-8d83-04e8745b215c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Checking model structure...\n",
      "  Found Linear: model.layers.0.self_attn.q_proj\n",
      "  Found Linear: model.layers.0.self_attn.k_proj\n",
      "  Found Linear: model.layers.0.self_attn.v_proj\n",
      "  Found Linear: model.layers.0.self_attn.o_proj\n",
      "  Found Linear: model.layers.0.mlp.gate_proj\n",
      "Total Linear modules: 197\n",
      "\n",
      "Replacing linear layers...\n",
      "  [replaced] model.layers.0.self_attn.q_proj\n",
      "  [replaced] model.layers.0.self_attn.k_proj\n",
      "  [replaced] model.layers.0.self_attn.v_proj\n",
      "  [replaced] model.layers.0.self_attn.o_proj\n",
      "  [replaced] model.layers.0.mlp.gate_proj\n",
      "  [replaced] model.layers.0.mlp.up_proj\n",
      "  [replaced] model.layers.0.mlp.down_proj\n",
      "  [replaced] model.layers.1.self_attn.q_proj\n",
      "  [replaced] model.layers.1.self_attn.k_proj\n",
      "  [replaced] model.layers.1.self_attn.v_proj\n",
      "  [replaced] model.layers.1.self_attn.o_proj\n",
      "  [replaced] model.layers.1.mlp.gate_proj\n",
      "  [replaced] model.layers.1.mlp.up_proj\n",
      "  [replaced] model.layers.1.mlp.down_proj\n",
      "  [replaced] model.layers.2.self_attn.q_proj\n",
      "  [replaced] model.layers.2.self_attn.k_proj\n",
      "  [replaced] model.layers.2.self_attn.v_proj\n",
      "  [replaced] model.layers.2.self_attn.o_proj\n",
      "  [replaced] model.layers.2.mlp.gate_proj\n",
      "  [replaced] model.layers.2.mlp.up_proj\n",
      "  [replaced] model.layers.2.mlp.down_proj\n",
      "  [replaced] model.layers.3.self_attn.q_proj\n",
      "  [replaced] model.layers.3.self_attn.k_proj\n",
      "  [replaced] model.layers.3.self_attn.v_proj\n",
      "  [replaced] model.layers.3.self_attn.o_proj\n",
      "  [replaced] model.layers.3.mlp.gate_proj\n",
      "  [replaced] model.layers.3.mlp.up_proj\n",
      "  [replaced] model.layers.3.mlp.down_proj\n",
      "  [replaced] model.layers.4.self_attn.q_proj\n",
      "  [replaced] model.layers.4.self_attn.k_proj\n",
      "  [replaced] model.layers.4.self_attn.v_proj\n",
      "  [replaced] model.layers.4.self_attn.o_proj\n",
      "  [replaced] model.layers.4.mlp.gate_proj\n",
      "  [replaced] model.layers.4.mlp.up_proj\n",
      "  [replaced] model.layers.4.mlp.down_proj\n",
      "  [replaced] model.layers.5.self_attn.q_proj\n",
      "  [replaced] model.layers.5.self_attn.k_proj\n",
      "  [replaced] model.layers.5.self_attn.v_proj\n",
      "  [replaced] model.layers.5.self_attn.o_proj\n",
      "  [replaced] model.layers.5.mlp.gate_proj\n",
      "  [replaced] model.layers.5.mlp.up_proj\n",
      "  [replaced] model.layers.5.mlp.down_proj\n",
      "  [replaced] model.layers.6.self_attn.q_proj\n",
      "  [replaced] model.layers.6.self_attn.k_proj\n",
      "  [replaced] model.layers.6.self_attn.v_proj\n",
      "  [replaced] model.layers.6.self_attn.o_proj\n",
      "  [replaced] model.layers.6.mlp.gate_proj\n",
      "  [replaced] model.layers.6.mlp.up_proj\n",
      "  [replaced] model.layers.6.mlp.down_proj\n",
      "  [replaced] model.layers.7.self_attn.q_proj\n",
      "  [replaced] model.layers.7.self_attn.k_proj\n",
      "  [replaced] model.layers.7.self_attn.v_proj\n",
      "  [replaced] model.layers.7.self_attn.o_proj\n",
      "  [replaced] model.layers.7.mlp.gate_proj\n",
      "  [replaced] model.layers.7.mlp.up_proj\n",
      "  [replaced] model.layers.7.mlp.down_proj\n",
      "  [replaced] model.layers.8.self_attn.q_proj\n",
      "  [replaced] model.layers.8.self_attn.k_proj\n",
      "  [replaced] model.layers.8.self_attn.v_proj\n",
      "  [replaced] model.layers.8.self_attn.o_proj\n",
      "  [replaced] model.layers.8.mlp.gate_proj\n",
      "  [replaced] model.layers.8.mlp.up_proj\n",
      "  [replaced] model.layers.8.mlp.down_proj\n",
      "  [replaced] model.layers.9.self_attn.q_proj\n",
      "  [replaced] model.layers.9.self_attn.k_proj\n",
      "  [replaced] model.layers.9.self_attn.v_proj\n",
      "  [replaced] model.layers.9.self_attn.o_proj\n",
      "  [replaced] model.layers.9.mlp.gate_proj\n",
      "  [replaced] model.layers.9.mlp.up_proj\n",
      "  [replaced] model.layers.9.mlp.down_proj\n",
      "  [replaced] model.layers.10.self_attn.q_proj\n",
      "  [replaced] model.layers.10.self_attn.k_proj\n",
      "  [replaced] model.layers.10.self_attn.v_proj\n",
      "  [replaced] model.layers.10.self_attn.o_proj\n",
      "  [replaced] model.layers.10.mlp.gate_proj\n",
      "  [replaced] model.layers.10.mlp.up_proj\n",
      "  [replaced] model.layers.10.mlp.down_proj\n",
      "  [replaced] model.layers.11.self_attn.q_proj\n",
      "  [replaced] model.layers.11.self_attn.k_proj\n",
      "  [replaced] model.layers.11.self_attn.v_proj\n",
      "  [replaced] model.layers.11.self_attn.o_proj\n",
      "  [replaced] model.layers.11.mlp.gate_proj\n",
      "  [replaced] model.layers.11.mlp.up_proj\n",
      "  [replaced] model.layers.11.mlp.down_proj\n",
      "  [replaced] model.layers.12.self_attn.q_proj\n",
      "  [replaced] model.layers.12.self_attn.k_proj\n",
      "  [replaced] model.layers.12.self_attn.v_proj\n",
      "  [replaced] model.layers.12.self_attn.o_proj\n",
      "  [replaced] model.layers.12.mlp.gate_proj\n",
      "  [replaced] model.layers.12.mlp.up_proj\n",
      "  [replaced] model.layers.12.mlp.down_proj\n",
      "  [replaced] model.layers.13.self_attn.q_proj\n",
      "  [replaced] model.layers.13.self_attn.k_proj\n",
      "  [replaced] model.layers.13.self_attn.v_proj\n",
      "  [replaced] model.layers.13.self_attn.o_proj\n",
      "  [replaced] model.layers.13.mlp.gate_proj\n",
      "  [replaced] model.layers.13.mlp.up_proj\n",
      "  [replaced] model.layers.13.mlp.down_proj\n",
      "  [replaced] model.layers.14.self_attn.q_proj\n",
      "  [replaced] model.layers.14.self_attn.k_proj\n",
      "  [replaced] model.layers.14.self_attn.v_proj\n",
      "  [replaced] model.layers.14.self_attn.o_proj\n",
      "  [replaced] model.layers.14.mlp.gate_proj\n",
      "  [replaced] model.layers.14.mlp.up_proj\n",
      "  [replaced] model.layers.14.mlp.down_proj\n",
      "  [replaced] model.layers.15.self_attn.q_proj\n",
      "  [replaced] model.layers.15.self_attn.k_proj\n",
      "  [replaced] model.layers.15.self_attn.v_proj\n",
      "  [replaced] model.layers.15.self_attn.o_proj\n",
      "  [replaced] model.layers.15.mlp.gate_proj\n",
      "  [replaced] model.layers.15.mlp.up_proj\n",
      "  [replaced] model.layers.15.mlp.down_proj\n",
      "  [replaced] model.layers.16.self_attn.q_proj\n",
      "  [replaced] model.layers.16.self_attn.k_proj\n",
      "  [replaced] model.layers.16.self_attn.v_proj\n",
      "  [replaced] model.layers.16.self_attn.o_proj\n",
      "  [replaced] model.layers.16.mlp.gate_proj\n",
      "  [replaced] model.layers.16.mlp.up_proj\n",
      "  [replaced] model.layers.16.mlp.down_proj\n",
      "  [replaced] model.layers.17.self_attn.q_proj\n",
      "  [replaced] model.layers.17.self_attn.k_proj\n",
      "  [replaced] model.layers.17.self_attn.v_proj\n",
      "  [replaced] model.layers.17.self_attn.o_proj\n",
      "  [replaced] model.layers.17.mlp.gate_proj\n",
      "  [replaced] model.layers.17.mlp.up_proj\n",
      "  [replaced] model.layers.17.mlp.down_proj\n",
      "  [replaced] model.layers.18.self_attn.q_proj\n",
      "  [replaced] model.layers.18.self_attn.k_proj\n",
      "  [replaced] model.layers.18.self_attn.v_proj\n",
      "  [replaced] model.layers.18.self_attn.o_proj\n",
      "  [replaced] model.layers.18.mlp.gate_proj\n",
      "  [replaced] model.layers.18.mlp.up_proj\n",
      "  [replaced] model.layers.18.mlp.down_proj\n",
      "  [replaced] model.layers.19.self_attn.q_proj\n",
      "  [replaced] model.layers.19.self_attn.k_proj\n",
      "  [replaced] model.layers.19.self_attn.v_proj\n",
      "  [replaced] model.layers.19.self_attn.o_proj\n",
      "  [replaced] model.layers.19.mlp.gate_proj\n",
      "  [replaced] model.layers.19.mlp.up_proj\n",
      "  [replaced] model.layers.19.mlp.down_proj\n",
      "  [replaced] model.layers.20.self_attn.q_proj\n",
      "  [replaced] model.layers.20.self_attn.k_proj\n",
      "  [replaced] model.layers.20.self_attn.v_proj\n",
      "  [replaced] model.layers.20.self_attn.o_proj\n",
      "  [replaced] model.layers.20.mlp.gate_proj\n",
      "  [replaced] model.layers.20.mlp.up_proj\n",
      "  [replaced] model.layers.20.mlp.down_proj\n",
      "  [replaced] model.layers.21.self_attn.q_proj\n",
      "  [replaced] model.layers.21.self_attn.k_proj\n",
      "  [replaced] model.layers.21.self_attn.v_proj\n",
      "  [replaced] model.layers.21.self_attn.o_proj\n",
      "  [replaced] model.layers.21.mlp.gate_proj\n",
      "  [replaced] model.layers.21.mlp.up_proj\n",
      "  [replaced] model.layers.21.mlp.down_proj\n",
      "  [replaced] model.layers.22.self_attn.q_proj\n",
      "  [replaced] model.layers.22.self_attn.k_proj\n",
      "  [replaced] model.layers.22.self_attn.v_proj\n",
      "  [replaced] model.layers.22.self_attn.o_proj\n",
      "  [replaced] model.layers.22.mlp.gate_proj\n",
      "  [replaced] model.layers.22.mlp.up_proj\n",
      "  [replaced] model.layers.22.mlp.down_proj\n",
      "  [replaced] model.layers.23.self_attn.q_proj\n",
      "  [replaced] model.layers.23.self_attn.k_proj\n",
      "  [replaced] model.layers.23.self_attn.v_proj\n",
      "  [replaced] model.layers.23.self_attn.o_proj\n",
      "  [replaced] model.layers.23.mlp.gate_proj\n",
      "  [replaced] model.layers.23.mlp.up_proj\n",
      "  [replaced] model.layers.23.mlp.down_proj\n",
      "  [replaced] model.layers.24.self_attn.q_proj\n",
      "  [replaced] model.layers.24.self_attn.k_proj\n",
      "  [replaced] model.layers.24.self_attn.v_proj\n",
      "  [replaced] model.layers.24.self_attn.o_proj\n",
      "  [replaced] model.layers.24.mlp.gate_proj\n",
      "  [replaced] model.layers.24.mlp.up_proj\n",
      "  [replaced] model.layers.24.mlp.down_proj\n",
      "  [replaced] model.layers.25.self_attn.q_proj\n",
      "  [replaced] model.layers.25.self_attn.k_proj\n",
      "  [replaced] model.layers.25.self_attn.v_proj\n",
      "  [replaced] model.layers.25.self_attn.o_proj\n",
      "  [replaced] model.layers.25.mlp.gate_proj\n",
      "  [replaced] model.layers.25.mlp.up_proj\n",
      "  [replaced] model.layers.25.mlp.down_proj\n",
      "  [replaced] model.layers.26.self_attn.q_proj\n",
      "  [replaced] model.layers.26.self_attn.k_proj\n",
      "  [replaced] model.layers.26.self_attn.v_proj\n",
      "  [replaced] model.layers.26.self_attn.o_proj\n",
      "  [replaced] model.layers.26.mlp.gate_proj\n",
      "  [replaced] model.layers.26.mlp.up_proj\n",
      "  [replaced] model.layers.26.mlp.down_proj\n",
      "  [replaced] model.layers.27.self_attn.q_proj\n",
      "  [replaced] model.layers.27.self_attn.k_proj\n",
      "  [replaced] model.layers.27.self_attn.v_proj\n",
      "  [replaced] model.layers.27.self_attn.o_proj\n",
      "  [replaced] model.layers.27.mlp.gate_proj\n",
      "  [replaced] model.layers.27.mlp.up_proj\n",
      "  [replaced] model.layers.27.mlp.down_proj\n",
      "\n",
      "Replaced 196 layers\n",
      "\n",
      "Verification: 0 AnemllQATLinear modules in model\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# REPLACE LINEARS WITH AnemllQATLinear\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "# Force reimport to get latest code\n",
    "import importlib\n",
    "import qat_lora\n",
    "importlib.reload(qat_lora)\n",
    "import qat_lora.ane_qat_linear as ane_module\n",
    "importlib.reload(ane_module)\n",
    "import qat_lora.layer_qat as layer_module\n",
    "importlib.reload(layer_module)\n",
    "\n",
    "from qat_lora import AnemllQuantConfig, replace_linear_with_anemll\n",
    "\n",
    "# Debug: Check what modules exist in the model\n",
    "print(\"Checking model structure...\")\n",
    "import torch.nn as nn\n",
    "linear_count = 0\n",
    "for name, m in model.named_modules():\n",
    "    if isinstance(m, nn.Linear):\n",
    "        linear_count += 1\n",
    "        if linear_count <= 5:\n",
    "            print(f\"  Found Linear: {name}\")\n",
    "print(f\"Total Linear modules: {linear_count}\")\n",
    "\n",
    "# Create configs\n",
    "mlp_config = AnemllQuantConfig(\n",
    "    lut_size=LUT_SIZE,\n",
    "    group_size=GROUP_SIZE,\n",
    "    scale_rank=SCALE_RANK,\n",
    "    learnable_lut=False,\n",
    ")\n",
    "\n",
    "attn_config = AnemllQuantConfig(\n",
    "    lut_size=ATTN_LUT_SIZE,\n",
    "    group_size=ATTN_GROUP_SIZE,\n",
    "    scale_rank=ATTN_SCALE_RANK,\n",
    "    learnable_lut=False,\n",
    ")\n",
    "\n",
    "print('\\nReplacing linear layers...')\n",
    "count = replace_linear_with_anemll(\n",
    "    model,\n",
    "    mlp_config=mlp_config,\n",
    "    attn_config=attn_config,\n",
    "    quantize_attn=True,\n",
    "    quantize_lm_head=False,\n",
    ")\n",
    "\n",
    "# Verify replacement worked\n",
    "from qat_lora import AnemllQATLinear\n",
    "qat_count = sum(1 for _, m in model.named_modules() if isinstance(m, AnemllQATLinear))\n",
    "print(f\"\\nVerification: {qat_count} AnemllQATLinear modules in model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "D_gOyY1qFCuX",
    "outputId": "c6fcb612-6372-4401-a0b1-828a6a910c2f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Layer QAT utilities imported from qat_lora\n",
      "\n",
      "Verifying gradient flow...\n",
      "ERROR: No AnemllQATLinear modules found! Replacement failed.\n",
      "\n",
      "Computing initial KD loss...\n",
      "Initial KD Loss: 1.0885\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# IMPORT LAYER-BY-LAYER QAT UTILITIES & VERIFY GRADIENTS\n",
    "# ============================================================\n",
    "\n",
    "from qat_lora import (\n",
    "    evaluate_kd_loss,\n",
    "    train_all_layers,\n",
    "    AnemllQATLinear,\n",
    ")\n",
    "\n",
    "print('Layer QAT utilities imported from qat_lora')\n",
    "\n",
    "# Verify gradient flow works\n",
    "print('\\nVerifying gradient flow...')\n",
    "layer0 = model.model.layers[0]\n",
    "test_module = None\n",
    "for name, m in layer0.named_modules():\n",
    "    if isinstance(m, AnemllQATLinear):\n",
    "        test_module = m\n",
    "        break\n",
    "\n",
    "if test_module is None:\n",
    "    print(\"ERROR: No AnemllQATLinear modules found! Replacement failed.\")\n",
    "else:\n",
    "    # Test gradient flow\n",
    "    test_module.weight.requires_grad = True\n",
    "    x = torch.randn(1, 10, test_module.in_features, device=DEVICE, dtype=DTYPE)\n",
    "    y = test_module(x)\n",
    "    loss = y.sum()\n",
    "    try:\n",
    "        loss.backward()\n",
    "        if test_module.weight.grad is not None:\n",
    "            print(f\"  Gradient OK: weight.grad.shape = {test_module.weight.grad.shape}\")\n",
    "            test_module.weight.grad = None  # Clear for actual training\n",
    "        else:\n",
    "            print(\"  ERROR: weight.grad is None after backward!\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR during backward: {e}\")\n",
    "\n",
    "# Compute initial KD loss\n",
    "print('\\nComputing initial KD loss...')\n",
    "initial_loss = evaluate_kd_loss(model, cache_local_path, DEVICE, num_samples=40, temperature=DISTILL_TEMP)\n",
    "print(f'Initial KD Loss: {initial_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **SCALE OPTIMIZATION** (Weights Frozen)\n",
    "\n",
    "After layer-by-layer QAT on weights, optimize the per-weight scales (A @ B) to further reduce quantization error.\n",
    "\n",
    "- Weights are **frozen**\n",
    "- Only `scale_A` and `scale_B` are trained\n",
    "- Much fewer parameters → can use higher learning rate"
   ],
   "metadata": {
    "id": "9B4uAJDNfRgs"
   }
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# LAYER-BY-LAYER SCALE OPTIMIZATION\n# ============================================================\n# Freeze weights, only train scale_A and scale_B tensors\n# Higher LR since fewer parameters\n# Note: Hard label loss not needed for scale optimization\n\nSCALE_LR = 1e-3  # Higher LR for scales (fewer params)\nSCALE_EPOCHS = 2  # More epochs since scales have less capacity\n\n\nprint('Starting scale-only layer-by-layer optimization...')\nprint(f'LR: {SCALE_LR}, Epochs per layer: {SCALE_EPOCHS}')\n\n# Get loss before scale optimization\npre_scale_loss = evaluate_kd_loss(model, cache_local_path, DEVICE, num_samples=40)\nprint(f'KD Loss before scale optimization: {pre_scale_loss:.4f}')\n\n# Train scales layer-by-layer (no hard label needed for scales)\nscale_losses = train_all_layers(\n    model=model,\n    cache_dir=cache_local_path,\n    device=DEVICE,\n    batch_size=BATCH_SIZE,\n    lr=SCALE_LR,\n    epochs_per_layer=SCALE_EPOCHS,\n    grad_accum=GRAD_ACCUM,\n    temperature=DISTILL_TEMP,\n    train_weights=False,  # Freeze weights\n    train_scales=True,    # Train scales only\n    local_weight=0.5,\n    global_weight=0.5,\n    hard_top1_weight=0.0,  # Not needed for scale optimization\n    hard_full_weight=0.0,\n    verbose=True,\n    steps_per_layer=100,\n)\n\n# Evaluate after scale optimization\npost_scale_loss = evaluate_kd_loss(model, cache_local_path, DEVICE, num_samples=40)\nprint(f'\\n=== Scale Optimization Results ===')\nprint(f'Before: {pre_scale_loss:.4f}')\nprint(f'After:  {post_scale_loss:.4f}')\nprint(f'Improvement: {pre_scale_loss - post_scale_loss:.4f}')",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cQ1kq2a9fRgs",
    "outputId": "84a4cabf-6962-421a-b259-41f9beea1f2e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **RUN** LAYER-BY-LAYER TRAINING"
   ],
   "metadata": {
    "id": "o2veFRWaQEkB"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gupQzmm5FCuX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "01bdc6cd-46fb-4f0b-8cf7-8f6157ad969c"
   },
   "outputs": [],
   "source": "# ============================================================\n# LAYER-BY-LAYER WEIGHT TRAINING\n# ============================================================\n# Train weights with hard label loss for better convergence\n\nprint('Starting layer-by-layer weight training...')\nprint(f'LR: {LR}, Hard label: top1={HARD_TOP1_WEIGHT}, full={HARD_FULL_WEIGHT}')\n\n# Train all layers using the imported function\nlayer_losses = train_all_layers(\n    model=model,\n    cache_dir=cache_local_path,\n    device=DEVICE,\n    batch_size=BATCH_SIZE,\n    lr=LR,\n    epochs_per_layer=EPOCHS_PER_LAYER,\n    grad_accum=GRAD_ACCUM,\n    temperature=DISTILL_TEMP,\n    train_weights=True,   # Train weights\n    train_scales=False,   # Keep scales frozen for now\n    local_weight=0.5,\n    global_weight=0.5,\n    hard_top1_weight=HARD_TOP1_WEIGHT,  # Helps convergence\n    hard_full_weight=HARD_FULL_WEIGHT,\n    verbose=True,\n    steps_per_layer=100,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "uv6rcQvuFCuX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "765eba8a-0030-4eec-e928-d4a2e3b70d78"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initial KD Loss: 1.0885\n",
      "After Layer-by-Layer: 0.2653\n",
      "Improvement: 0.8232\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# EVALUATE AFTER LAYER-BY-LAYER\n",
    "# ============================================================\n",
    "\n",
    "model.eval()\n",
    "post_layer_loss = evaluate_kd_loss(model, cache_local_path, DEVICE, num_samples=40)\n",
    "print(f'Initial KD Loss: {initial_loss:.4f}')\n",
    "print(f'After Layer-by-Layer: {post_layer_loss:.4f}')\n",
    "print(f'Improvement: {initial_loss - post_layer_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "lxSBHcq6FCuX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "90a71c51-96ac-43d9-a837-a0b542a30edb"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved to runs/anemll_q4_layer_by_layer_v1\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SAVE CHECKPOINT\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "\n",
    "RUN_NAME = f'anemll_{QUAL}_layer_by_layer_v1'\n",
    "SAVE_DIR = f'{LOCAL_RUNS}/{RUN_NAME}'\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Save state dict\n",
    "torch.save(model.state_dict(), f'{SAVE_DIR}/model_state_dict.pt')\n",
    "\n",
    "# Save config\n",
    "import json\n",
    "config = {\n",
    "    'model_id': MODEL_ID,\n",
    "    'lut_size': LUT_SIZE,\n",
    "    'group_size': GROUP_SIZE,\n",
    "    'scale_rank': SCALE_RANK,\n",
    "    'attn_lut_size': ATTN_LUT_SIZE,\n",
    "    'attn_group_size': ATTN_GROUP_SIZE,\n",
    "    'attn_scale_rank': ATTN_SCALE_RANK,\n",
    "    'initial_kd_loss': initial_loss,\n",
    "    'post_layer_loss': post_layer_loss,\n",
    "    'layer_losses': layer_losses,\n",
    "}\n",
    "with open(f'{SAVE_DIR}/config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f'Saved to {SAVE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "R4wh-UVDFCuX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b20ed80d-5766-48b3-9e81-da1dff6e74bb"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "anemll_q4_layer_by_layer_v1/\n",
      "anemll_q4_layer_by_layer_v1/config.json\n",
      "anemll_q4_layer_by_layer_v1/model_state_dict.pt\n",
      "Uploaded to /content/drive/MyDrive/qwen3_runs/anemll_q4_layer_by_layer_v1.tgz\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# UPLOAD TO GOOGLE DRIVE\n",
    "# ============================================================\n",
    "\n",
    "!tar -czvf {RUN_NAME}.tgz -C {LOCAL_RUNS} {RUN_NAME}\n",
    "!cp {RUN_NAME}.tgz {GD_RUNS}/\n",
    "print(f'Uploaded to {GD_RUNS}/{RUN_NAME}.tgz')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **INFERENCE OPTIMIZATION**\n",
    "\n",
    "Before running inference, freeze all layers to precompute quantized weights.\n",
    "This avoids recomputing `LUT[indices] * (scale_A @ scale_B)` on every forward pass."
   ],
   "metadata": {
    "id": "fqafgf8KfRgs"
   }
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# FREEZE MODEL FOR FAST INFERENCE\n# ============================================================\n# Precompute quantized weights once for all layers\n# This caches LUT[idx] * scale to avoid recomputation per token\n\nfrom qat_lora import freeze_model_for_inference, unfreeze_model_for_training\n\nprint('Freezing model for inference...')\nnum_frozen = freeze_model_for_inference(model, verbose=False)\nprint(f'Frozen {num_frozen} layers')\n\n# To resume training later:\n# unfreeze_model_for_training(model)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lWZ2KP18fRgs",
    "outputId": "7bd29b13-d8ce-4887-f758-3edf9321ac1c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Ugmqo-MHFCuX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "66ff9b82-65cf-46bf-b8df-fd89898c8005"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prompt: What is the capital of France?\n",
      "Response: <think>\n",
      "<think>\n",
      "Okay, the user is asking about the capital of France. Let me think about this.\n",
      "\n",
      "First, I need to recall the capital of France. France is a country in Europe, and I know that the capital is Paris. But wait, I should double-check to make sure. Let me think again. \n",
      "\n",
      "Paris is the capital of France, right? Yes, that's correct. I don't see any other city with the same capital. \n",
      "\n",
      "Is there any chance that the capital is another city? For example, maybe a city in the south or somewhere else? I don't think so. The capital is always Paris. \n",
      "\n",
      "So, the answer is Paris. Let me make sure I'm not making a mistake here. \n",
      "\n",
      "I think I'm confident now. The capital of France is Paris. \n",
      "\n",
      "**Final Answer**\n",
      "</think>\n",
      "\n",
      "The capital of France is **Paris**.\n",
      "--------------------------------------------------\n",
      "Prompt: What is Apple Neural Engine?\n",
      "Response: <think>\n",
      "<think>\n",
      "Okay, the user asked about Apple Neural Engine. Let me start by recalling what I know. Apple has been developing AI and machine learning models for various applications. The Neural Engine is a part of their AI framework, probably related to deep learning. I need to explain it in a simple way. Maybe start by mentioning that it's a powerful tool for AI and machine learning. Then, explain that it's used in different areas like image recognition, natural language processing, and more. Also, mention that it's part of Apple's broader AI ecosystem. Make sure to keep it clear and easy to understand.\n",
      "</think>\n",
      "\n",
      "Apple Neural Engine is a powerful tool that powers Apple's AI and machine learning models. It's designed to run on Apple's devices and is used in a variety of applications, such as image recognition, natural language processing, and more. It's part of Apple's broader AI ecosystem, allowing developers to create AI models that can perform complex tasks like text generation, speech recognition, and more.\n",
      "--------------------------------------------------\n",
      "Prompt: Explain quantum mechanics\n",
      "Response: <think>\n",
      "<think>\n",
      "Okay, the user asked me to explain quantum mechanics. Let me start by recalling what I know about the basics of quantum mechanics. From what I remember, it's a branch of physics that deals with the behavior of particles at very small scales, like atoms and electrons. \n",
      "\n",
      "I should explain the concepts like particles, wave-particle duality, and the concept of quantum numbers. Maybe I can compare it to how particles behave in everyday life, like how a ball moves or how light behaves. \n",
      "\n",
      "I need to make sure I'm accurate and not making up too much. Let me check some key points: particles have both particle and wave properties, they can exist in multiple states at once, and the wave-particle duality is a fundamental concept. \n",
      "\n",
      "Also, I should mention the famous equations and models, like the Schrödinger's equation and the Heisenberg's uncertainty principle. But I should keep it simple and avoid complex terms. \n",
      "\n",
      "Wait, the user might be a student or someone new to quantum mechanics, so I should present it in a clear and easy way. Maybe start with the basic idea of particles and their properties, then move to the wave-particle duality, and then the equations. \n",
      "\n",
      "I should also mention the historical context, like Einstein's contributions, to give a sense of where the field started. But I should keep it concise. Let me structure it step by step, starting with the basics, then moving to the wave-particle duality, and finally the equations. \n",
      "\n",
      "Is there anything else I should add? Maybe include some examples or diagrams to help the user visualize? But I should stick to the information I have. Alright, I think that's a good approach.\n",
      "</think>\n",
      "\n",
      "Quantum mechanics is a fundamental part of modern physics that deals with the behavior of particles at very small scales. Here's a simple explanation:\n",
      "\n",
      "1. **Particles have both particle and wave properties**: At the atomic and subatomic levels, particles (like electrons, protons, and neutrons) have both particle and wave properties. This means they can exist in multiple states at once, like being in a superposition of different energy levels.\n",
      "\n",
      "2. **Wave-particle duality**: One of the most important concepts in quantum mechanics is the **wave-particle duality**, which means that particles can exhibit both wave-like and particle-like behavior. For example, a particle can be detected as a wave, but it can also be a particle, like a ball or a ball in a box.\n",
      "\n",
      "3. **Quantum numbers**: These are the basic properties of particles, such as their mass, charge, and position. These numbers are used to describe the state of a particle in a quantum system.\n",
      "\n",
      "4. **Schrödinger's Equation**: This equation describes how a particle's position and momentum change over time in a quantum system. It's a mathematical model that shows how particles behave in a quantum world.\n",
      "\n",
      "5. **Heisenberg's Uncertainty Principle**: This principle states that the product of the uncertainties in a particle's position and momentum is always greater than or equal to a certain value, which is a fundamental aspect of quantum mechanics.\n",
      "\n",
      "In short, quantum mechanics is the study of how particles behave at the smallest scales, and it's a key part of our understanding of the universe.\n",
      "--------------------------------------------------\n",
      "Prompt: What is speed of light\n",
      "Response: <think>\n",
      "Okay, the user asked, \"What is speed of light?\" I need to provide an answer. Let me think about this.\n",
      "\n",
      "First, I should recall the known speed of light. From physics, I know that light travels at a constant speed, approximately 3.0000000000000001 meters per second. But I should check if there's any other sources or if there's any other information I might have. Wait, the user might be interested in the exact value, or maybe they're talking about a different context. For example, in the context of physics, the speed of light is a fundamental constant. In other contexts, like in the universe, it might be different. But I should also consider if there's any other information I might have. For example, in the context of the universe, the speed of light is a constant, but in the context of the universe, it might be different. But I don't have that information. \n",
      "\n",
      "Wait, the user might be asking about the speed of light in a specific context, like in the universe, or maybe in the context of the universe. For example, in the universe, the speed of light is a constant, but in the context of the universe, it might be different. But I don't have that information. \n",
      "\n",
      "Alternatively, maybe the user is asking about the speed of light in a different unit or in a different context. For example, in the context of the universe, the speed of light is a constant, but in the context of the universe, it might be different. \n",
      "\n",
      "Wait, the user might be asking about the speed of light in a different context, like in the context of the universe. For example, in the context of the universe, the speed of light is a constant, but in the context of the universe, it might be different. \n",
      "\n",
      "Alternatively, maybe the user is asking about the speed of light in a different unit or in a different context. For example, in the context of the universe, the speed of light is a constant, but in the context of the universe, it might be different. \n",
      "\n",
      "Wait, I need to make sure I'm not missing any information. Let me think again. \n",
      "\n",
      "The user asked, \"What is speed of light?\" The answer should be the known speed of light, which is approximately 3.0000000000000001 meters per second. I should also mention that in the context of the universe, the speed of light is a constant, but in other contexts, like in the context of the universe, it might be different. \n",
      "\n",
      "But I should also check if there's any other information I might have. For example, if the user is asking about the speed of light in a different context, like in the context of the universe, or maybe in the context of the universe, it might be different. \n",
      "\n",
      "Alternatively, maybe the user is asking about the speed of light in a different unit or in a different context. For example, if the user is asking about the speed of light in the context of the universe, then the answer would be the known speed of light. \n",
      "\n",
      "Alternatively, maybe the user is asking about the speed of light in a different context, like in the context of the universe, and the answer would be the known speed of light. \n",
      "\n",
      "Wait, I need to be careful here. Let me think again. \n",
      "\n",
      "The user asked, \"What is speed of light?\" The answer should be the known speed of light, which is approximately 3.0000000000000001 meters per second. I should also mention that in the context of the universe, the speed of light is a constant, but in other contexts, like in the context of the universe, it might be different. \n",
      "\n",
      "But I should also check if there's any other information I might have. For example, if the user is asking about the speed of light in a different context, like in the context of the universe, then the answer would be the known speed of light. \n",
      "\n",
      "Alternatively, maybe the user is asking about the speed of light in a different context, like in the context of the universe, and the answer would be the known speed of light. \n",
      "\n",
      "Wait, I need to be precise. Let me think again. \n",
      "\n",
      "The user asked, \"What is speed of light?\" The answer should be the known speed of light, which is approximately 3.0000000000000001 meters per second. I should also mention that in the context of the universe, the speed of light is a constant, but in other contexts, like in the context of the universe, it might be different. \n",
      "\n",
      "But I should also check if there's any other information I might have. For example, if the user is asking about the speed of light in a different context, like in the context of the universe,\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# ============================================================\n",
    "# TEST INFERENCE\n",
    "# ============================================================\n",
    "\n",
    "def run_inference(model, tokenizer, prompt, max_new_tokens=128):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "        {'role': 'user', 'content': prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text, return_tensors='pt').to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "\n",
    "    return tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "# List of prompts to test\n",
    "prompts = [\n",
    "    'What is the capital of France?',\n",
    "    'What is Apple Neural Engine?',\n",
    "    'Explain quantum mechanics',\n",
    "    'What is speed of light'\n",
    "]\n",
    "\n",
    "model.eval() # Set model to evaluation mode once\n",
    "\n",
    "for prompt in prompts:\n",
    "    response = run_inference(model, tokenizer, prompt,max_new_tokens=1024)\n",
    "    print(f'Prompt: {prompt}')\n",
    "    print(f'Response: {response}')\n",
    "    print('-' * 50) # Separator for readability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKGYuTbyFCuX"
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "After layer-by-layer training, you can:\n",
    "\n",
    "1. **End-to-end refinement** - Unfreeze all layers and train together\n",
    "2. **Train scales (A, B)** - Unfreeze scale_A, scale_B parameters\n",
    "3. **LoRA recovery** - Add LoRA adapters to recover quality"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# **END-TO-END KD-QAT REFINEMENT**\n\nAfter layer-by-layer training, refine the model with all layers unfrozen.\n\nTwo modes:\n1. **Train weights** (scales frozen) - Fine-tune weights globally with hard label loss\n2. **Train scales** (weights frozen) - Optimize scales for better quantization\n\n## Distillation Options\n\n| Parameter | Weight Training | Scale Training |\n|-----------|----------------|----------------|\n| `temperature` | 2.0 | 2.0 |\n| `hard_top1_weight` | 0.1 (recommended) | 0.0 |\n| `hard_full_weight` | 0.0 | 0.0 |\n\nHard label loss helps prevent divergence during weight training.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# END-TO-END KD-QAT: TRAIN WEIGHTS (SCALES FROZEN)\n# ============================================================\n# Use hard label loss for stable weight training\n\nfrom qat_lora import train_e2e, save_checkpoint, load_checkpoint, unfreeze_model_for_training\n\n# Unfreeze for training (clear any cached weights)\nunfreeze_model_for_training(model)\n\nprint('E2E weight training with hard label loss...')\nprint(f'Hard label: top1={HARD_TOP1_WEIGHT}, full={HARD_FULL_WEIGHT}')\n\n# Train weights (scales frozen)\ne2e_weights_result = train_e2e(\n    model=model,\n    cache_dir=cache_local_path,\n    device=DEVICE,\n    max_steps=3000,\n    batch_size=128 if torch.cuda.is_available() else 32,\n    lr=5e-6,\n    temperature=DISTILL_TEMP,\n    train_weights=True,\n    train_scales=False,\n    hard_top1_weight=HARD_TOP1_WEIGHT,  # Helps prevent divergence\n    hard_full_weight=HARD_FULL_WEIGHT,\n    logging_steps=50,\n    eval_steps=500,\n    verbose=True,\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# END-TO-END KD-QAT: TRAIN SCALES (WEIGHTS FROZEN)\n# ============================================================\n# Scale training doesn't need hard label loss\n\n# Train scales (weights frozen) - higher LR since fewer params\ne2e_scales_result = train_e2e(\n    model=model,\n    cache_dir=cache_local_path,\n    device=DEVICE,\n    max_steps=500,\n    batch_size=128 if torch.cuda.is_available() else 32,\n    lr=1e-3,  # Higher LR for scales\n    temperature=DISTILL_TEMP,\n    train_weights=False,\n    train_scales=True,\n    hard_top1_weight=0.0,  # Not needed for scale training\n    hard_full_weight=0.0,\n    logging_steps=20,\n    eval_steps=100,\n    verbose=True,\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# SAVE FINAL CHECKPOINT\n# ============================================================\n\nE2E_RUN_NAME = f'anemll_{QUAL}_e2e_v1'\nE2E_SAVE_DIR = f'{LOCAL_RUNS}/{E2E_RUN_NAME}'\n\n# Save with config\nconfig = {\n    'model_id': MODEL_ID,\n    'lut_size': LUT_SIZE,\n    'group_size': GROUP_SIZE,\n    'scale_rank': SCALE_RANK,\n    'attn_lut_size': ATTN_LUT_SIZE,\n    'attn_group_size': ATTN_GROUP_SIZE,\n    'attn_scale_rank': ATTN_SCALE_RANK,\n    'e2e_weights_result': e2e_weights_result,\n    'e2e_scales_result': e2e_scales_result,\n}\n\nsave_checkpoint(model, E2E_SAVE_DIR, config=config)\n\n# Upload to Google Drive\n!tar -czvf {E2E_RUN_NAME}.tgz -C {LOCAL_RUNS} {E2E_RUN_NAME}\n!cp {E2E_RUN_NAME}.tgz {GD_RUNS}/\nprint(f'\\nUploaded to {GD_RUNS}/{E2E_RUN_NAME}.tgz')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# **EXPORT FOR ANEMLL CONVERTER**\n\nSnap weights to quantized values and export for external tools.\n\nTwo export modes:\n- `store_lut_values=True`: weights = LUT[idx] (normalized in [-1,1]), scales separate\n- `store_lut_values=False`: weights = LUT[idx] * scale (full dequant)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# SNAP WEIGHTS AND EXPORT\n# ============================================================\n# Snap weights to LUT[idx] values for ANEMLL converter\n\nfrom qat_lora import snap_all_weights, export_quantized_model, unfreeze_model_for_training\n\n# First unfreeze to clear cached weights\nunfreeze_model_for_training(model)\n\n# Export quantized representation BEFORE snapping (keeps original weights)\nprint('Exporting quantized model representation...')\nexport_dict = export_quantized_model(model, verbose=True)\n\n# Save export for ANEMLL converter\nEXPORT_DIR = f'{LOCAL_RUNS}/{E2E_RUN_NAME}_export'\nos.makedirs(EXPORT_DIR, exist_ok=True)\ntorch.save(export_dict, f'{EXPORT_DIR}/quantized_model.pt')\nprint(f'\\nSaved export to {EXPORT_DIR}/quantized_model.pt')\n\n# Each layer in export_dict contains:\n# - indices: [out, in] uint8 LUT indices\n# - quantized_weights: [out, in] LUT[idx] values in [-1, 1]\n# - scales: {'scale_A': [out, rank], 'scale_B': [rank, in]} or full [out, in]\n# - lut: [lut_size] values\n# - bias, in_features, out_features, etc.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# SNAP WEIGHTS TO FULL DEQUANT AND TEST\n# ============================================================\n# Snap weights = LUT[idx] * scale, then disable fake_quant for direct use\n\nprint('Snapping weights to full dequantized values (LUT[idx] * scale)...')\nindices = snap_all_weights(model, store_lut_values=False, verbose=True)\n\n# Disable fake quantization - use snapped weights directly\nfor name, module in model.named_modules():\n    if type(module).__name__ == 'AnemllQATLinear':\n        module.enable_fake_quant = False\n\nprint('\\nTesting inference with snapped weights...')\nmodel.eval()\n\n# Quick test\nresponse = run_inference(model, tokenizer, 'What is 2+2?', max_new_tokens=64)\nprint(f'Prompt: What is 2+2?')\nprint(f'Response: {response}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}