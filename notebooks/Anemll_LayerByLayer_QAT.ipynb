{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqWTWF5EFCuV"
      },
      "source": [
        "# Anemll-Style Layer-by-Layer QAT\n",
        "\n",
        "This notebook implements layer-by-layer QAT training using `AnemllQATLinear` with:\n",
        "- Groupwise LUT quantization\n",
        "- Low-rank scale factors (A @ B)\n",
        "- KD cache for distillation\n",
        "\n",
        "## Pipeline:\n",
        "1. Load model and replace linears with AnemllQATLinear\n",
        "2. Layer-by-layer QAT (freeze all but current layer)\n",
        "3. End-to-end refinement\n",
        "4. (Optional) LoRA recovery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Kfh54i9XFCuW"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# GOOGLE DRIVE PATHS (STANDARD)\n",
        "# ============================================================\n",
        "\n",
        "# Checkpoints/runs go here\n",
        "GD_RUNS = '/content/drive/MyDrive/qwen3_runs'\n",
        "\n",
        "# KD caches go here\n",
        "GD_CACHES = '/content/drive/MyDrive/qwen3_caches'\n",
        "\n",
        "# Local directories (on Colab VM)\n",
        "LOCAL_RUNS = 'runs'\n",
        "LOCAL_CACHES = 'caches'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Tw1k9-anFCuW",
        "outputId": "2d1d6bf8-78b6-4320-b67c-37ac60b99aab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GITUB"
      ],
      "metadata": {
        "id": "7X9uxbCPPykd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HtdvB50DFCuW",
        "outputId": "02e16c40-b1d7-480c-fbc7-e8faddf2e438",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'qwen3_apple_style_2bit_qat_lora' already exists and is not an empty directory.\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 6 (delta 5), reused 6 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (6/6), 1.65 KiB | 842.00 KiB/s, done.\n",
            "From https://github.com/anemll/qwen3_apple_style_2bit_qat_lora\n",
            "   ac7f797..a579dfd  main       -> origin/main\n",
            "Updating ac7f797..a579dfd\n",
            "Fast-forward\n",
            " qat_lora/ane_qat_linear.py | 12 \u001b[32m+++++++++\u001b[m\u001b[31m---\u001b[m\n",
            " scripts/test_anemll_qat.py | 43 \u001b[32m+++++++++++++++++++++++++++++++++++++++++++\u001b[m\n",
            " 2 files changed, 52 insertions(+), 3 deletions(-)\n",
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "Already up to date.\n",
            "HEAD is now at a579dfd Enhance AnemllQATLinear and add scale optimization features\n"
          ]
        }
      ],
      "source": [
        "# Clone repo if needed\n",
        "!git clone https://github.com/anemll/qwen3_apple_style_2bit_qat_lora.git || (cd qwen3_apple_style_2bit_qat_lora && git pull)\n",
        "%cd qwen3_apple_style_2bit_qat_lora\n",
        "# to allow updates\n",
        "!git fetch\n",
        "!git pull\n",
        "!git reset --hard HEAD\n",
        "import sys\n",
        "[sys.modules.pop(k) for k in list(sys.modules) if k.startswith('qat_lora')]\n",
        "\n",
        "from qat_lora import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1dJ5vGK9FCuW"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers accelerate safetensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hgqQOvUpFCuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36bdaa75-2466-4525-deea-d7440bd4ac1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cache already exists at caches/alpaca_chat_think_both_L128_K32_R256\n",
            "total 4298972\n",
            "drwx------ 2 root root      4096 Dec 18 00:00 .\n",
            "drwxr-xr-x 3 root root      4096 Dec 26 07:09 ..\n",
            "-rw------- 1 root root       421 Dec 18 00:15 meta.json\n",
            "-rw------- 1 root root 112692165 Dec 18 00:15 shard_00000.pt\n",
            "-rw------- 1 root root 112692165 Dec 18 00:15 shard_00001.pt\n",
            "-rw------- 1 root root 112692165 Dec 18 00:15 shard_00002.pt\n",
            "-rw------- 1 root root 112692165 Dec 18 00:15 shard_00003.pt\n",
            "-rw------- 1 root root 112692165 Dec 18 00:16 shard_00004.pt\n",
            "-rw------- 1 root root 112692165 Dec 18 00:16 shard_00005.pt\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# LOAD KD CACHE FROM GOOGLE DRIVE\n",
        "# ============================================================\n",
        "\n",
        "CACHE_NAME = 'alpaca_chat_think_both_L128_K32_R256'\n",
        "CACHE_TGZ = f'{CACHE_NAME}.tgz'\n",
        "\n",
        "!mkdir -p {LOCAL_CACHES}\n",
        "\n",
        "# Check if cache exists locally\n",
        "import os\n",
        "cache_local_path = f'{LOCAL_CACHES}/{CACHE_NAME}'\n",
        "if not os.path.exists(cache_local_path):\n",
        "    print(f'Extracting {CACHE_TGZ} from Google Drive...')\n",
        "    !tar -xzf {GD_CACHES}/{CACHE_TGZ} -C {LOCAL_CACHES}/\n",
        "else:\n",
        "    print(f'Cache already exists at {cache_local_path}')\n",
        "\n",
        "!ls -la {cache_local_path}/ | head -10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EJ239_eUFCuW",
        "outputId": "423e3627-33fe-481f-8d08-787b0b4d7b39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda, dtype: torch.bfloat16\n",
            "Quant config: lut=16, group=32, rank=4\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================\n",
        "\n",
        "import torch\n",
        "\n",
        "# Model\n",
        "MODEL_ID = 'Qwen/Qwen3-0.6B'\n",
        "\n",
        "# Quantization config (4-bit with groupwise LUT)\n",
        "LUT_SIZE = 16        # 4-bit = 16 levels\n",
        "GROUP_SIZE = 32      # Group size for scales\n",
        "SCALE_RANK = 4       # Low-rank for A @ B scales\n",
        "\n",
        "# Attention quantization (same params)\n",
        "ATTN_LUT_SIZE = 16\n",
        "ATTN_GROUP_SIZE = 32\n",
        "ATTN_SCALE_RANK = 8\n",
        "\n",
        "# Training\n",
        "BATCH_SIZE = 4\n",
        "GRAD_ACCUM = 4\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    BATCH_SIZE=64\n",
        "    GRAD_ACCUM=1\n",
        "\n",
        "LR = 2e-5\n",
        "EPOCHS_PER_LAYER = 1\n",
        "\n",
        "# KD params\n",
        "DISTILL_TEMP = 2.0\n",
        "\n",
        "# Device\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DTYPE = torch.bfloat16\n",
        "\n",
        "print(f'Device: {DEVICE}, dtype: {DTYPE}')\n",
        "print(f'Quant config: lut={LUT_SIZE}, group={GROUP_SIZE}, rank={SCALE_RANK}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Extracting LOCAL CACHE\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Verify drive is mounted and cache exists\n",
        "if not os.path.exists('/content/drive/MyDrive'):\n",
        "    print('Google Drive not mounted! Mounting now...')\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "if not os.path.exists(cache_local_path):\n",
        "    print(f'Cache not found at {cache_local_path}')\n",
        "    print(f'Extracting from Google Drive...')\n",
        "    os.makedirs(LOCAL_CACHES, exist_ok=True)\n",
        "    !tar -xzf {GD_CACHES}/{CACHE_TGZ} -C {LOCAL_CACHES}/\n",
        "\n",
        "# Verify cache exists now\n",
        "assert os.path.exists(cache_local_path), f'Cache still not found at {cache_local_path}'\n",
        "cache_files = list(Path(cache_local_path).glob('*.pt'))\n",
        "print(f'Cache ready: {len(cache_files)} files in {cache_local_path}')"
      ],
      "metadata": {
        "id": "r-V8ZhsWGl1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e61f58e7-40a6-4aab-d5fa-f456d3e65967"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cache ready: 40 files in caches/alpaca_chat_think_both_L128_K32_R256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5e5kQrkxFCuX",
        "outputId": "31b489bb-c97a-49e9-d38d-5e904d2a0189",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Qwen/Qwen3-0.6B...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded. Parameters: 596,049,920\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# LOAD MODEL\n",
        "# ============================================================\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "print(f'Loading {MODEL_ID}...')\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=DTYPE,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "print(f'Loaded. Parameters: {sum(p.numel() for p in model.parameters()):,}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "n29f0NexFCuX",
        "outputId": "afd22094-6901-4f28-ff05-a59b82d87163",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking model structure...\n",
            "  Found Linear: model.layers.0.self_attn.q_proj\n",
            "  Found Linear: model.layers.0.self_attn.k_proj\n",
            "  Found Linear: model.layers.0.self_attn.v_proj\n",
            "  Found Linear: model.layers.0.self_attn.o_proj\n",
            "  Found Linear: model.layers.0.mlp.gate_proj\n",
            "Total Linear modules: 197\n",
            "\n",
            "Replacing linear layers...\n",
            "  [replaced] model.layers.0.self_attn.q_proj\n",
            "  [replaced] model.layers.0.self_attn.k_proj\n",
            "  [replaced] model.layers.0.self_attn.v_proj\n",
            "  [replaced] model.layers.0.self_attn.o_proj\n",
            "  [replaced] model.layers.0.mlp.gate_proj\n",
            "  [replaced] model.layers.0.mlp.up_proj\n",
            "  [replaced] model.layers.0.mlp.down_proj\n",
            "  [replaced] model.layers.1.self_attn.q_proj\n",
            "  [replaced] model.layers.1.self_attn.k_proj\n",
            "  [replaced] model.layers.1.self_attn.v_proj\n",
            "  [replaced] model.layers.1.self_attn.o_proj\n",
            "  [replaced] model.layers.1.mlp.gate_proj\n",
            "  [replaced] model.layers.1.mlp.up_proj\n",
            "  [replaced] model.layers.1.mlp.down_proj\n",
            "  [replaced] model.layers.2.self_attn.q_proj\n",
            "  [replaced] model.layers.2.self_attn.k_proj\n",
            "  [replaced] model.layers.2.self_attn.v_proj\n",
            "  [replaced] model.layers.2.self_attn.o_proj\n",
            "  [replaced] model.layers.2.mlp.gate_proj\n",
            "  [replaced] model.layers.2.mlp.up_proj\n",
            "  [replaced] model.layers.2.mlp.down_proj\n",
            "  [replaced] model.layers.3.self_attn.q_proj\n",
            "  [replaced] model.layers.3.self_attn.k_proj\n",
            "  [replaced] model.layers.3.self_attn.v_proj\n",
            "  [replaced] model.layers.3.self_attn.o_proj\n",
            "  [replaced] model.layers.3.mlp.gate_proj\n",
            "  [replaced] model.layers.3.mlp.up_proj\n",
            "  [replaced] model.layers.3.mlp.down_proj\n",
            "  [replaced] model.layers.4.self_attn.q_proj\n",
            "  [replaced] model.layers.4.self_attn.k_proj\n",
            "  [replaced] model.layers.4.self_attn.v_proj\n",
            "  [replaced] model.layers.4.self_attn.o_proj\n",
            "  [replaced] model.layers.4.mlp.gate_proj\n",
            "  [replaced] model.layers.4.mlp.up_proj\n",
            "  [replaced] model.layers.4.mlp.down_proj\n",
            "  [replaced] model.layers.5.self_attn.q_proj\n",
            "  [replaced] model.layers.5.self_attn.k_proj\n",
            "  [replaced] model.layers.5.self_attn.v_proj\n",
            "  [replaced] model.layers.5.self_attn.o_proj\n",
            "  [replaced] model.layers.5.mlp.gate_proj\n",
            "  [replaced] model.layers.5.mlp.up_proj\n",
            "  [replaced] model.layers.5.mlp.down_proj\n",
            "  [replaced] model.layers.6.self_attn.q_proj\n",
            "  [replaced] model.layers.6.self_attn.k_proj\n",
            "  [replaced] model.layers.6.self_attn.v_proj\n",
            "  [replaced] model.layers.6.self_attn.o_proj\n",
            "  [replaced] model.layers.6.mlp.gate_proj\n",
            "  [replaced] model.layers.6.mlp.up_proj\n",
            "  [replaced] model.layers.6.mlp.down_proj\n",
            "  [replaced] model.layers.7.self_attn.q_proj\n",
            "  [replaced] model.layers.7.self_attn.k_proj\n",
            "  [replaced] model.layers.7.self_attn.v_proj\n",
            "  [replaced] model.layers.7.self_attn.o_proj\n",
            "  [replaced] model.layers.7.mlp.gate_proj\n",
            "  [replaced] model.layers.7.mlp.up_proj\n",
            "  [replaced] model.layers.7.mlp.down_proj\n",
            "  [replaced] model.layers.8.self_attn.q_proj\n",
            "  [replaced] model.layers.8.self_attn.k_proj\n",
            "  [replaced] model.layers.8.self_attn.v_proj\n",
            "  [replaced] model.layers.8.self_attn.o_proj\n",
            "  [replaced] model.layers.8.mlp.gate_proj\n",
            "  [replaced] model.layers.8.mlp.up_proj\n",
            "  [replaced] model.layers.8.mlp.down_proj\n",
            "  [replaced] model.layers.9.self_attn.q_proj\n",
            "  [replaced] model.layers.9.self_attn.k_proj\n",
            "  [replaced] model.layers.9.self_attn.v_proj\n",
            "  [replaced] model.layers.9.self_attn.o_proj\n",
            "  [replaced] model.layers.9.mlp.gate_proj\n",
            "  [replaced] model.layers.9.mlp.up_proj\n",
            "  [replaced] model.layers.9.mlp.down_proj\n",
            "  [replaced] model.layers.10.self_attn.q_proj\n",
            "  [replaced] model.layers.10.self_attn.k_proj\n",
            "  [replaced] model.layers.10.self_attn.v_proj\n",
            "  [replaced] model.layers.10.self_attn.o_proj\n",
            "  [replaced] model.layers.10.mlp.gate_proj\n",
            "  [replaced] model.layers.10.mlp.up_proj\n",
            "  [replaced] model.layers.10.mlp.down_proj\n",
            "  [replaced] model.layers.11.self_attn.q_proj\n",
            "  [replaced] model.layers.11.self_attn.k_proj\n",
            "  [replaced] model.layers.11.self_attn.v_proj\n",
            "  [replaced] model.layers.11.self_attn.o_proj\n",
            "  [replaced] model.layers.11.mlp.gate_proj\n",
            "  [replaced] model.layers.11.mlp.up_proj\n",
            "  [replaced] model.layers.11.mlp.down_proj\n",
            "  [replaced] model.layers.12.self_attn.q_proj\n",
            "  [replaced] model.layers.12.self_attn.k_proj\n",
            "  [replaced] model.layers.12.self_attn.v_proj\n",
            "  [replaced] model.layers.12.self_attn.o_proj\n",
            "  [replaced] model.layers.12.mlp.gate_proj\n",
            "  [replaced] model.layers.12.mlp.up_proj\n",
            "  [replaced] model.layers.12.mlp.down_proj\n",
            "  [replaced] model.layers.13.self_attn.q_proj\n",
            "  [replaced] model.layers.13.self_attn.k_proj\n",
            "  [replaced] model.layers.13.self_attn.v_proj\n",
            "  [replaced] model.layers.13.self_attn.o_proj\n",
            "  [replaced] model.layers.13.mlp.gate_proj\n",
            "  [replaced] model.layers.13.mlp.up_proj\n",
            "  [replaced] model.layers.13.mlp.down_proj\n",
            "  [replaced] model.layers.14.self_attn.q_proj\n",
            "  [replaced] model.layers.14.self_attn.k_proj\n",
            "  [replaced] model.layers.14.self_attn.v_proj\n",
            "  [replaced] model.layers.14.self_attn.o_proj\n",
            "  [replaced] model.layers.14.mlp.gate_proj\n",
            "  [replaced] model.layers.14.mlp.up_proj\n",
            "  [replaced] model.layers.14.mlp.down_proj\n",
            "  [replaced] model.layers.15.self_attn.q_proj\n",
            "  [replaced] model.layers.15.self_attn.k_proj\n",
            "  [replaced] model.layers.15.self_attn.v_proj\n",
            "  [replaced] model.layers.15.self_attn.o_proj\n",
            "  [replaced] model.layers.15.mlp.gate_proj\n",
            "  [replaced] model.layers.15.mlp.up_proj\n",
            "  [replaced] model.layers.15.mlp.down_proj\n",
            "  [replaced] model.layers.16.self_attn.q_proj\n",
            "  [replaced] model.layers.16.self_attn.k_proj\n",
            "  [replaced] model.layers.16.self_attn.v_proj\n",
            "  [replaced] model.layers.16.self_attn.o_proj\n",
            "  [replaced] model.layers.16.mlp.gate_proj\n",
            "  [replaced] model.layers.16.mlp.up_proj\n",
            "  [replaced] model.layers.16.mlp.down_proj\n",
            "  [replaced] model.layers.17.self_attn.q_proj\n",
            "  [replaced] model.layers.17.self_attn.k_proj\n",
            "  [replaced] model.layers.17.self_attn.v_proj\n",
            "  [replaced] model.layers.17.self_attn.o_proj\n",
            "  [replaced] model.layers.17.mlp.gate_proj\n",
            "  [replaced] model.layers.17.mlp.up_proj\n",
            "  [replaced] model.layers.17.mlp.down_proj\n",
            "  [replaced] model.layers.18.self_attn.q_proj\n",
            "  [replaced] model.layers.18.self_attn.k_proj\n",
            "  [replaced] model.layers.18.self_attn.v_proj\n",
            "  [replaced] model.layers.18.self_attn.o_proj\n",
            "  [replaced] model.layers.18.mlp.gate_proj\n",
            "  [replaced] model.layers.18.mlp.up_proj\n",
            "  [replaced] model.layers.18.mlp.down_proj\n",
            "  [replaced] model.layers.19.self_attn.q_proj\n",
            "  [replaced] model.layers.19.self_attn.k_proj\n",
            "  [replaced] model.layers.19.self_attn.v_proj\n",
            "  [replaced] model.layers.19.self_attn.o_proj\n",
            "  [replaced] model.layers.19.mlp.gate_proj\n",
            "  [replaced] model.layers.19.mlp.up_proj\n",
            "  [replaced] model.layers.19.mlp.down_proj\n",
            "  [replaced] model.layers.20.self_attn.q_proj\n",
            "  [replaced] model.layers.20.self_attn.k_proj\n",
            "  [replaced] model.layers.20.self_attn.v_proj\n",
            "  [replaced] model.layers.20.self_attn.o_proj\n",
            "  [replaced] model.layers.20.mlp.gate_proj\n",
            "  [replaced] model.layers.20.mlp.up_proj\n",
            "  [replaced] model.layers.20.mlp.down_proj\n",
            "  [replaced] model.layers.21.self_attn.q_proj\n",
            "  [replaced] model.layers.21.self_attn.k_proj\n",
            "  [replaced] model.layers.21.self_attn.v_proj\n",
            "  [replaced] model.layers.21.self_attn.o_proj\n",
            "  [replaced] model.layers.21.mlp.gate_proj\n",
            "  [replaced] model.layers.21.mlp.up_proj\n",
            "  [replaced] model.layers.21.mlp.down_proj\n",
            "  [replaced] model.layers.22.self_attn.q_proj\n",
            "  [replaced] model.layers.22.self_attn.k_proj\n",
            "  [replaced] model.layers.22.self_attn.v_proj\n",
            "  [replaced] model.layers.22.self_attn.o_proj\n",
            "  [replaced] model.layers.22.mlp.gate_proj\n",
            "  [replaced] model.layers.22.mlp.up_proj\n",
            "  [replaced] model.layers.22.mlp.down_proj\n",
            "  [replaced] model.layers.23.self_attn.q_proj\n",
            "  [replaced] model.layers.23.self_attn.k_proj\n",
            "  [replaced] model.layers.23.self_attn.v_proj\n",
            "  [replaced] model.layers.23.self_attn.o_proj\n",
            "  [replaced] model.layers.23.mlp.gate_proj\n",
            "  [replaced] model.layers.23.mlp.up_proj\n",
            "  [replaced] model.layers.23.mlp.down_proj\n",
            "  [replaced] model.layers.24.self_attn.q_proj\n",
            "  [replaced] model.layers.24.self_attn.k_proj\n",
            "  [replaced] model.layers.24.self_attn.v_proj\n",
            "  [replaced] model.layers.24.self_attn.o_proj\n",
            "  [replaced] model.layers.24.mlp.gate_proj\n",
            "  [replaced] model.layers.24.mlp.up_proj\n",
            "  [replaced] model.layers.24.mlp.down_proj\n",
            "  [replaced] model.layers.25.self_attn.q_proj\n",
            "  [replaced] model.layers.25.self_attn.k_proj\n",
            "  [replaced] model.layers.25.self_attn.v_proj\n",
            "  [replaced] model.layers.25.self_attn.o_proj\n",
            "  [replaced] model.layers.25.mlp.gate_proj\n",
            "  [replaced] model.layers.25.mlp.up_proj\n",
            "  [replaced] model.layers.25.mlp.down_proj\n",
            "  [replaced] model.layers.26.self_attn.q_proj\n",
            "  [replaced] model.layers.26.self_attn.k_proj\n",
            "  [replaced] model.layers.26.self_attn.v_proj\n",
            "  [replaced] model.layers.26.self_attn.o_proj\n",
            "  [replaced] model.layers.26.mlp.gate_proj\n",
            "  [replaced] model.layers.26.mlp.up_proj\n",
            "  [replaced] model.layers.26.mlp.down_proj\n",
            "  [replaced] model.layers.27.self_attn.q_proj\n",
            "  [replaced] model.layers.27.self_attn.k_proj\n",
            "  [replaced] model.layers.27.self_attn.v_proj\n",
            "  [replaced] model.layers.27.self_attn.o_proj\n",
            "  [replaced] model.layers.27.mlp.gate_proj\n",
            "  [replaced] model.layers.27.mlp.up_proj\n",
            "  [replaced] model.layers.27.mlp.down_proj\n",
            "\n",
            "Replaced 196 layers\n",
            "\n",
            "Verification: 0 AnemllQATLinear modules in model\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# REPLACE LINEARS WITH AnemllQATLinear\n",
        "# ============================================================\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "# Force reimport to get latest code\n",
        "import importlib\n",
        "import qat_lora\n",
        "importlib.reload(qat_lora)\n",
        "import qat_lora.ane_qat_linear as ane_module\n",
        "importlib.reload(ane_module)\n",
        "import qat_lora.layer_qat as layer_module\n",
        "importlib.reload(layer_module)\n",
        "\n",
        "from qat_lora import AnemllQuantConfig, replace_linear_with_anemll\n",
        "\n",
        "# Debug: Check what modules exist in the model\n",
        "print(\"Checking model structure...\")\n",
        "import torch.nn as nn\n",
        "linear_count = 0\n",
        "for name, m in model.named_modules():\n",
        "    if isinstance(m, nn.Linear):\n",
        "        linear_count += 1\n",
        "        if linear_count <= 5:\n",
        "            print(f\"  Found Linear: {name}\")\n",
        "print(f\"Total Linear modules: {linear_count}\")\n",
        "\n",
        "# Create configs\n",
        "mlp_config = AnemllQuantConfig(\n",
        "    lut_size=LUT_SIZE,\n",
        "    group_size=GROUP_SIZE,\n",
        "    scale_rank=SCALE_RANK,\n",
        "    learnable_lut=False,\n",
        ")\n",
        "\n",
        "attn_config = AnemllQuantConfig(\n",
        "    lut_size=ATTN_LUT_SIZE,\n",
        "    group_size=ATTN_GROUP_SIZE,\n",
        "    scale_rank=ATTN_SCALE_RANK,\n",
        "    learnable_lut=False,\n",
        ")\n",
        "\n",
        "print('\\nReplacing linear layers...')\n",
        "count = replace_linear_with_anemll(\n",
        "    model,\n",
        "    mlp_config=mlp_config,\n",
        "    attn_config=attn_config,\n",
        "    quantize_attn=True,\n",
        "    quantize_lm_head=False,\n",
        ")\n",
        "\n",
        "# Verify replacement worked\n",
        "from qat_lora import AnemllQATLinear\n",
        "qat_count = sum(1 for _, m in model.named_modules() if isinstance(m, AnemllQATLinear))\n",
        "print(f\"\\nVerification: {qat_count} AnemllQATLinear modules in model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "D_gOyY1qFCuX",
        "outputId": "b547239f-3409-4990-9a21-e69964155aa6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer QAT utilities imported from qat_lora\n",
            "\n",
            "Verifying gradient flow...\n",
            "ERROR: No AnemllQATLinear modules found! Replacement failed.\n",
            "\n",
            "Computing initial KD loss...\n",
            "Initial KD Loss: 138.2384\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# IMPORT LAYER-BY-LAYER QAT UTILITIES & VERIFY GRADIENTS\n",
        "# ============================================================\n",
        "\n",
        "from qat_lora import (\n",
        "    evaluate_kd_loss,\n",
        "    train_all_layers,\n",
        "    AnemllQATLinear,\n",
        ")\n",
        "\n",
        "print('Layer QAT utilities imported from qat_lora')\n",
        "\n",
        "# Verify gradient flow works\n",
        "print('\\nVerifying gradient flow...')\n",
        "layer0 = model.model.layers[0]\n",
        "test_module = None\n",
        "for name, m in layer0.named_modules():\n",
        "    if isinstance(m, AnemllQATLinear):\n",
        "        test_module = m\n",
        "        break\n",
        "\n",
        "if test_module is None:\n",
        "    print(\"ERROR: No AnemllQATLinear modules found! Replacement failed.\")\n",
        "else:\n",
        "    # Test gradient flow\n",
        "    test_module.weight.requires_grad = True\n",
        "    x = torch.randn(1, 10, test_module.in_features, device=DEVICE, dtype=DTYPE)\n",
        "    y = test_module(x)\n",
        "    loss = y.sum()\n",
        "    try:\n",
        "        loss.backward()\n",
        "        if test_module.weight.grad is not None:\n",
        "            print(f\"  Gradient OK: weight.grad.shape = {test_module.weight.grad.shape}\")\n",
        "            test_module.weight.grad = None  # Clear for actual training\n",
        "        else:\n",
        "            print(\"  ERROR: weight.grad is None after backward!\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ERROR during backward: {e}\")\n",
        "\n",
        "# Compute initial KD loss\n",
        "print('\\nComputing initial KD loss...')\n",
        "initial_loss = evaluate_kd_loss(model, cache_local_path, DEVICE, num_samples=40, temperature=DISTILL_TEMP)\n",
        "print(f'Initial KD Loss: {initial_loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RUN** LAYER-BY-LAYER TRAINING"
      ],
      "metadata": {
        "id": "o2veFRWaQEkB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gupQzmm5FCuX"
      },
      "outputs": [],
      "source": [
        "# Train all layers using the imported function\n",
        "layer_losses = train_all_layers(\n",
        "    model=model,\n",
        "    cache_dir=cache_local_path,\n",
        "    device=DEVICE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    lr=LR,\n",
        "    epochs_per_layer=EPOCHS_PER_LAYER,\n",
        "    grad_accum=GRAD_ACCUM,\n",
        "    temperature=DISTILL_TEMP,\n",
        "    train_scales=False,  # Keep scales frozen for now\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SCALE OPTIMIZATION** (Weights Frozen)\n",
        "\n",
        "After layer-by-layer QAT on weights, optimize the per-weight scales (A @ B) to further reduce quantization error.\n",
        "\n",
        "- Weights are **frozen**\n",
        "- Only `scale_A` and `scale_B` are trained\n",
        "- Much fewer parameters â†’ can use higher learning rate"
      ],
      "metadata": {
        "id": "9B4uAJDNfRgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# LAYER-BY-LAYER SCALE OPTIMIZATION\n",
        "# ============================================================\n",
        "# Freeze weights, only train scale_A and scale_B tensors\n",
        "# Higher LR since fewer parameters\n",
        "\n",
        "SCALE_LR = 1e-3  # Higher LR for scales (fewer params)\n",
        "SCALE_EPOCHS = 2  # More epochs since scales have less capacity\n",
        "\n",
        "BATCH_SIZE=64\n",
        "GRAD_ACCUM=1\n",
        "\n",
        "print('Starting scale-only layer-by-layer optimization...')\n",
        "print(f'LR: {SCALE_LR}, Epochs per layer: {SCALE_EPOCHS}')\n",
        "\n",
        "# Get loss before scale optimization\n",
        "pre_scale_loss = evaluate_kd_loss(model, cache_local_path, DEVICE, num_samples=40)\n",
        "print(f'KD Loss before scale optimization: {pre_scale_loss:.4f}')\n",
        "\n",
        "# Train scales layer-by-layer\n",
        "scale_losses = train_all_layers(\n",
        "    model=model,\n",
        "    cache_dir=cache_local_path,\n",
        "    device=DEVICE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    lr=SCALE_LR,\n",
        "    epochs_per_layer=SCALE_EPOCHS,\n",
        "    grad_accum=GRAD_ACCUM,\n",
        "    temperature=DISTILL_TEMP,\n",
        "    train_weights=False,  # Freeze weights\n",
        "    train_scales=True,    # Train scales only\n",
        "    local_weight=0.5,\n",
        "    global_weight=0.5,\n",
        "    verbose=True,\n",
        "    steps_per_layer=10,\n",
        ")\n",
        "\n",
        "# Evaluate after scale optimization\n",
        "post_scale_loss = evaluate_kd_loss(model, cache_local_path, DEVICE, num_samples=40)\n",
        "print(f'\\n=== Scale Optimization Results ===')\n",
        "print(f'Before: {pre_scale_loss:.4f}')\n",
        "print(f'After:  {post_scale_loss:.4f}')\n",
        "print(f'Improvement: {pre_scale_loss - post_scale_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cQ1kq2a9fRgs",
        "outputId": "b83c0c6c-a4a3-4db6-af64-af1dd32f2f5b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting scale-only layer-by-layer optimization...\n",
            "LR: 0.001, Epochs per layer: 2\n",
            "KD Loss before scale optimization: 69.3246\n",
            "Training 28 layers (mode=scales only)...\n",
            "Cache: caches/alpaca_chat_think_both_L128_K32_R256\n",
            "Batch size: 64, Grad accum: 1\n",
            "LR: 0.001, Epochs per layer: 2\n",
            "\n",
            "[Initial Global KD Loss]: 69.3246\n",
            "\n",
            "=== Layer 0 === (131,072 trainable params, mode=scales only)\n",
            "  [Global KD Loss BEFORE]: 72.4513\n",
            "  step 10: local=0.1959 global=68.8231 (3.4s)\n",
            "  step 20: local=0.1918 global=63.0253 (6.6s)\n",
            "  step 30: local=0.2036 global=71.4586 (9.9s)\n",
            "  step 40: local=0.1930 global=67.8574 (13.1s)\n",
            "  step 50: local=0.2019 global=72.4248 (16.5s)\n",
            "  step 60: local=0.2068 global=69.8087 (19.7s)\n",
            "  step 70: local=0.2042 global=71.9635 (23.0s)\n",
            "  step 80: local=0.2104 global=66.1914 (26.2s)\n",
            "  step 90: local=0.2113 global=67.8753 (29.6s)\n",
            "  step 100: local=0.2161 global=71.7947 (32.8s)\n",
            "  step 110: local=0.2099 global=69.3908 (36.1s)\n",
            "  step 120: local=0.2158 global=73.2216 (39.4s)\n",
            "  step 130: local=0.2081 global=67.1825 (42.7s)\n",
            "  step 140: local=0.2195 global=71.9301 (45.9s)\n",
            "  step 150: local=0.2179 global=70.5733 (49.2s)\n",
            "  step 160: local=0.2190 global=67.8695 (52.5s)\n",
            "  step 170: local=0.2263 global=68.8067 (55.8s)\n",
            "  step 180: local=0.2180 global=73.0389 (59.0s)\n",
            "  step 190: local=0.2294 global=68.3435 (62.3s)\n",
            "  step 200: local=0.2281 global=69.8824 (65.6s)\n",
            "  step 210: local=0.2291 global=66.7978 (68.9s)\n",
            "  step 220: local=0.2193 global=68.3200 (72.1s)\n",
            "  step 230: local=0.2300 global=68.9239 (75.4s)\n",
            "  step 240: local=0.2305 global=66.9427 (78.7s)\n",
            "  step 250: local=0.2263 global=68.5032 (82.0s)\n",
            "  step 260: local=0.2247 global=70.3378 (85.2s)\n",
            "  step 270: local=0.2300 global=65.1616 (88.5s)\n",
            "  step 280: local=0.2376 global=69.3734 (91.8s)\n",
            "  step 290: local=0.2475 global=69.2463 (95.1s)\n",
            "  step 300: local=0.2293 global=72.8566 (98.3s)\n",
            "  step 310: local=0.2315 global=71.0460 (101.6s)\n",
            "  step 320: local=0.2342 global=66.5898 (104.8s)\n",
            "  step 330: local=0.2480 global=71.5766 (108.1s)\n",
            "  step 340: local=0.2417 global=71.3440 (111.3s)\n",
            "  step 350: local=0.2330 global=69.6202 (114.6s)\n",
            "  step 360: local=0.2471 global=68.0250 (117.9s)\n",
            "  step 370: local=0.2465 global=72.4144 (121.2s)\n",
            "  step 380: local=0.2461 global=70.2785 (124.4s)\n",
            "  step 390: local=0.2433 global=69.7572 (127.7s)\n",
            "  step 400: local=0.2437 global=72.5224 (131.0s)\n",
            "  step 410: local=0.2494 global=71.5153 (134.3s)\n",
            "  step 420: local=0.2500 global=72.3465 (137.5s)\n",
            "  step 430: local=0.2549 global=75.4079 (140.8s)\n",
            "  step 440: local=0.2521 global=65.7236 (144.1s)\n",
            "  step 450: local=0.2486 global=68.3368 (147.4s)\n",
            "  step 460: local=0.2512 global=70.1044 (150.6s)\n",
            "  step 470: local=0.2508 global=67.5825 (153.9s)\n",
            "  step 480: local=0.2475 global=66.5585 (157.2s)\n",
            "  step 490: local=0.2514 global=71.3392 (160.5s)\n",
            "  step 500: local=0.2548 global=71.7166 (163.7s)\n",
            "  step 510: local=0.2622 global=68.1556 (167.0s)\n",
            "  step 520: local=0.2533 global=67.8259 (170.3s)\n",
            "  step 530: local=0.2555 global=69.9441 (173.6s)\n",
            "  step 540: local=0.2517 global=64.0152 (176.8s)\n",
            "  step 550: local=0.2628 global=71.7055 (180.1s)\n",
            "  step 560: local=0.2587 global=70.2951 (183.4s)\n",
            "  step 570: local=0.2702 global=66.6453 (186.7s)\n",
            "  step 580: local=0.2703 global=68.4575 (189.9s)\n",
            "  step 590: local=0.2612 global=69.2664 (193.2s)\n",
            "  step 600: local=0.2579 global=65.8874 (196.5s)\n",
            "  step 610: local=0.2619 global=68.8299 (199.8s)\n",
            "  step 620: local=0.2527 global=75.7158 (203.0s)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3660245394.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Train scales layer-by-layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m scale_losses = train_all_layers(\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_local_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/qwen3_apple_style_2bit_qat_lora/qat_lora/layer_qat.py\u001b[0m in \u001b[0;36mtrain_all_layers\u001b[0;34m(model, cache_dir, device, batch_size, lr, epochs_per_layer, grad_accum, temperature, train_weights, train_scales, verbose, eval_before, local_weight, global_weight, local_tokens)\u001b[0m\n\u001b[1;32m    799\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\n[Progress: {layer_idx}/{num_layers}] Elapsed: {format_time(elapsed)}, ETA: {format_time(eta_seconds)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m         result = train_layer(\n\u001b[0m\u001b[1;32m    802\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/qwen3_apple_style_2bit_qat_lora/qat_lora/layer_qat.py\u001b[0m in \u001b[0;36mtrain_layer\u001b[0;34m(model, layer_idx, cache_dir, device, batch_size, lr, epochs, grad_accum, temperature, train_weights, train_scales, verbose, eval_before, local_weight, global_weight, local_tokens)\u001b[0m\n\u001b[1;32m    615\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0;31m# Global KD loss (forward pass also captures MLP I/O via hook)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m             \u001b[0mglobal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_kd_loss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m             \u001b[0;31m# Local MLP reconstruction loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/qwen3_apple_style_2bit_qat_lora/qat_lora/layer_qat.py\u001b[0m in \u001b[0;36mcompute_kd_loss_batch\u001b[0;34m(model, batch, device, temperature, no_grad)\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/qwen3_apple_style_2bit_qat_lora/qat_lora/layer_qat.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;31m# Get hidden states (not full logits)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         out = model.model(\n\u001b[0m\u001b[1;32m    248\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m                 \u001b[0;31m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             hidden_states = decoder_layer(\n\u001b[0m\u001b[1;32m    411\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecoder_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/qwen3_apple_style_2bit_qat_lora/qat_lora/ane_qat_linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;31m# --- Step 3: Linear transform ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;31m# --- Step 4: Add LoRA residual (if enabled) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uv6rcQvuFCuX"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# EVALUATE AFTER LAYER-BY-LAYER\n",
        "# ============================================================\n",
        "\n",
        "model.eval()\n",
        "post_layer_loss = evaluate_kd_loss(model, cache_local_path, DEVICE, num_samples=40)\n",
        "print(f'Initial KD Loss: {initial_loss:.4f}')\n",
        "print(f'After Layer-by-Layer: {post_layer_loss:.4f}')\n",
        "print(f'Improvement: {initial_loss - post_layer_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxSBHcq6FCuX"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SAVE CHECKPOINT\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "\n",
        "RUN_NAME = 'anemll_q4_layer_by_layer_v1'\n",
        "SAVE_DIR = f'{LOCAL_RUNS}/{RUN_NAME}'\n",
        "\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Save state dict\n",
        "torch.save(model.state_dict(), f'{SAVE_DIR}/model_state_dict.pt')\n",
        "\n",
        "# Save config\n",
        "import json\n",
        "config = {\n",
        "    'model_id': MODEL_ID,\n",
        "    'lut_size': LUT_SIZE,\n",
        "    'group_size': GROUP_SIZE,\n",
        "    'scale_rank': SCALE_RANK,\n",
        "    'attn_lut_size': ATTN_LUT_SIZE,\n",
        "    'attn_group_size': ATTN_GROUP_SIZE,\n",
        "    'attn_scale_rank': ATTN_SCALE_RANK,\n",
        "    'initial_kd_loss': initial_loss,\n",
        "    'post_layer_loss': post_layer_loss,\n",
        "    'layer_losses': layer_losses,\n",
        "}\n",
        "with open(f'{SAVE_DIR}/config.json', 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(f'Saved to {SAVE_DIR}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4wh-UVDFCuX"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# UPLOAD TO GOOGLE DRIVE\n",
        "# ============================================================\n",
        "\n",
        "!tar -czvf {RUN_NAME}.tgz -C {LOCAL_RUNS} {RUN_NAME}\n",
        "!cp {RUN_NAME}.tgz {GD_RUNS}/\n",
        "print(f'Uploaded to {GD_RUNS}/{RUN_NAME}.tgz')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **INFERENCE OPTIMIZATION**\n",
        "\n",
        "Before running inference, freeze all layers to precompute quantized weights.\n",
        "This avoids recomputing `LUT[indices] * (scale_A @ scale_B)` on every forward pass."
      ],
      "metadata": {
        "id": "fqafgf8KfRgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# FREEZE MODEL FOR FAST INFERENCE\n",
        "# ============================================================\n",
        "# Precompute quantized weights once for all layers\n",
        "# This avoids recomputing LUT[indices] * (scale_A @ scale_B) per token\n",
        "\n",
        "from qat_lora import freeze_model_for_inference, unfreeze_model_for_training\n",
        "\n",
        "print('Freezing model for inference...')\n",
        "num_frozen = freeze_model_for_inference(model, verbose=False)\n",
        "print(f'Frozen {num_frozen} layers')\n",
        "\n",
        "# To resume training later:\n",
        "# unfreeze_model_for_training(model)"
      ],
      "metadata": {
        "id": "lWZ2KP18fRgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ugmqo-MHFCuX"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# TEST INFERENCE\n",
        "# ============================================================\n",
        "\n",
        "def run_inference(model, tokenizer, prompt, max_new_tokens=128):\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
        "        {'role': 'user', 'content': prompt}\n",
        "    ]\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(text, return_tensors='pt').to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "\n",
        "    return tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "# Test\n",
        "model.eval()\n",
        "prompt = 'What is the capital of France?'\n",
        "response = run_inference(model, tokenizer, prompt)\n",
        "print(f'Prompt: {prompt}')\n",
        "print(f'Response: {response}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKGYuTbyFCuX"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "After layer-by-layer training, you can:\n",
        "\n",
        "1. **End-to-end refinement** - Unfreeze all layers and train together\n",
        "2. **Train scales (A, B)** - Unfreeze scale_A, scale_B parameters\n",
        "3. **LoRA recovery** - Add LoRA adapters to recover quality"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}