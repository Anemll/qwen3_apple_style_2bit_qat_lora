{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen3 2-bit QAT - Improved Pipeline v1\n",
    "\n",
    "This notebook implements improved 2-bit QAT training with:\n",
    "\n",
    "**Key Improvements:**\n",
    "1. **MSE Grid-Search f_init** - Optimal scale initialization before training\n",
    "2. **Full 3-Pass Progressive** - MLP + Attention + MLP Refinement + E2E\n",
    "3. **Relaxed Convergence** - 2-bit needs looser thresholds (0.8-1.2 vs 0.4)\n",
    "4. **Lower Learning Rates** - More stable for 2-bit\n",
    "\n",
    "**Starting Point:** 4-bit QAT checkpoint (not from scratch)\n",
    "\n",
    "**Pipeline:**\n",
    "```\n",
    "4-bit checkpoint\n",
    "    |\n",
    "    v\n",
    "[MSE Grid-Search f_init Calibration] <- NEW\n",
    "    |\n",
    "    v\n",
    "Pass 1: MLP L-b-L (local + global KD)\n",
    "    |\n",
    "    v\n",
    "Pass 2: Attention L-b-L (global KD)  <- Previously skipped\n",
    "    |\n",
    "    v\n",
    "Pass 3: MLP Refinement L-b-L        <- Previously skipped\n",
    "    |\n",
    "    v\n",
    "Pass 4: E2E f-only tuning\n",
    "    |\n",
    "    v\n",
    "[Optional: KD-QAT Refinement]\n",
    "    |\n",
    "    v\n",
    "[Optional: LoRA Recovery]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SETUP: Clone repo and install dependencies\n",
    "# ============================================================\n",
    "\n",
    "!git clone https://github.com/anemll/qwen3_apple_style_2bit_qat_lora.git\n",
    "%cd qwen3_apple_style_2bit_qat_lora\n",
    "!pip install -q transformers accelerate datasets sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "MODEL_NAME = 'Qwen/Qwen3-0.6B'\n",
    "\n",
    "# Quantization\n",
    "QUANT_BITS = 2\n",
    "\n",
    "# KD Cache (use K=64 if available, otherwise K=32)\n",
    "# K=64 provides richer teacher signal for 2-bit\n",
    "CACHE_DIR_CHAT = 'caches/alpaca_chat_think_both_L128_K64_R512'\n",
    "# Fallback to K=32 if K=64 not available:\n",
    "# CACHE_DIR_CHAT = 'caches/alpaca_chat_think_both_L128_K32_R256'\n",
    "\n",
    "# 4-bit checkpoint to start from\n",
    "INIT_CHECKPOINT = 'runs/qwen3_kdqat_cache_q2_4/qat_state_dict.pt'\n",
    "\n",
    "# Output directory\n",
    "RUN_DIR = 'runs/progressive_qat_q2_improved_v1'\n",
    "\n",
    "# Training parameters (optimized for 2-bit)\n",
    "BATCH_SIZE = 2\n",
    "STEPS_PER_MLP = 50\n",
    "STEPS_PER_ATTN = 30\n",
    "E2E_STEPS = 200\n",
    "\n",
    "# Lower learning rates for 2-bit stability\n",
    "LEARNING_RATE = 1e-6      # Down from 5e-6 for 4-bit\n",
    "E2E_LEARNING_RATE = 5e-7  # Down from 1e-6 for 4-bit\n",
    "\n",
    "# Relaxed convergence for 2-bit (4 levels is hard!)\n",
    "LAYER_CONVERGE_THRESHOLD = 1.0  # Up from 0.4 for 4-bit\n",
    "MAX_LAYER_REPEATS = 3           # Allow more attempts\n",
    "\n",
    "# Calibration method for f_init\n",
    "CALIBRATE_METHOD = 'mse_grid'  # 'mse_grid', 'newton', or 'percentile'\n",
    "\n",
    "print(f\"Config:\")\n",
    "print(f\"  - Model: {MODEL_NAME}\")\n",
    "print(f\"  - Quant bits: {QUANT_BITS}\")\n",
    "print(f\"  - Init checkpoint: {INIT_CHECKPOINT}\")\n",
    "print(f\"  - Cache: {CACHE_DIR_CHAT}\")\n",
    "print(f\"  - Output: {RUN_DIR}\")\n",
    "print(f\"  - Calibration: {CALIBRATE_METHOD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MOUNT GOOGLE DRIVE\n",
    "# ============================================================\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD 4-BIT CHECKPOINT FROM GOOGLE DRIVE\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "\n",
    "CHECKPOINT_TAR = 'qwen3_kdqat_cache_q2_4.tgz'\n",
    "GD_CHECKPOINT = f'/content/drive/MyDrive/qwen3_caches/{CHECKPOINT_TAR}'\n",
    "\n",
    "if os.path.exists(GD_CHECKPOINT):\n",
    "    print(f\"[load] Extracting 4-bit checkpoint from Google Drive...\")\n",
    "    !mkdir -p runs\n",
    "    !tar -xzf {GD_CHECKPOINT} -C runs/\n",
    "    \n",
    "    # Verify\n",
    "    if os.path.exists(INIT_CHECKPOINT):\n",
    "        print(f\"[load] Checkpoint ready: {INIT_CHECKPOINT}\")\n",
    "    else:\n",
    "        print(f\"[load] ERROR: Expected {INIT_CHECKPOINT} not found\")\n",
    "        !ls -la runs/\n",
    "else:\n",
    "    print(f\"[load] ERROR: {GD_CHECKPOINT} not found\")\n",
    "    print(\"Please upload your 4-bit checkpoint to Google Drive first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD KD CACHE FROM GOOGLE DRIVE\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "\n",
    "# Try K=64 first, fall back to K=32\n",
    "CACHE_OPTIONS = [\n",
    "    'alpaca_chat_think_both_L128_K64_R512',\n",
    "    'alpaca_chat_think_both_L128_K32_R256',\n",
    "]\n",
    "\n",
    "GD_CACHE_DIR = '/content/drive/MyDrive/qwen3_caches'\n",
    "\n",
    "cache_loaded = False\n",
    "for cache_name in CACHE_OPTIONS:\n",
    "    gd_cache_path = f\"{GD_CACHE_DIR}/{cache_name}\"\n",
    "    if os.path.isdir(gd_cache_path):\n",
    "        print(f\"[cache] Found {cache_name}, copying...\")\n",
    "        !mkdir -p caches\n",
    "        !rsync -ah --info=progress2 {gd_cache_path}/ caches/{cache_name}/\n",
    "        CACHE_DIR_CHAT = f'caches/{cache_name}'\n",
    "        cache_loaded = True\n",
    "        break\n",
    "\n",
    "if cache_loaded:\n",
    "    print(f\"[cache] Using: {CACHE_DIR_CHAT}\")\n",
    "else:\n",
    "    print(\"[cache] ERROR: No KD cache found. Generate one first using Generate_KD_Cache_K64_K128.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progressive QAT Training (Full 3-Pass)\n",
    "\n",
    "This runs the complete progressive pipeline:\n",
    "- **Pass 1**: MLP layers (local reconstruction + global KD)\n",
    "- **Pass 2**: Attention layers (global KD only)\n",
    "- **Pass 3**: MLP refinement (fixes MLP-attention coupling)\n",
    "- **Pass 4**: E2E f-only tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PROGRESSIVE QAT TRAINING (Full 3-Pass with MSE Calibration)\n",
    "# ============================================================\n",
    "# This is the main training cell\n",
    "# Expected time: ~2-3 hours on A100 for all 4 passes\n",
    "\n",
    "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
    "\n",
    "!python scripts/train_qat_progressive.py \\\n",
    "  --model_name_or_path {MODEL_NAME} \\\n",
    "  --init_model_state {INIT_CHECKPOINT} \\\n",
    "  --output_dir {RUN_DIR} \\\n",
    "  --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
    "  --quant_bits {QUANT_BITS} \\\n",
    "  --calibrate_f_init {CALIBRATE_METHOD} \\\n",
    "  --skip_lm_head \\\n",
    "  --batch_size {BATCH_SIZE} \\\n",
    "  --steps_per_layer_mlp {STEPS_PER_MLP} \\\n",
    "  --steps_per_layer_attn {STEPS_PER_ATTN} \\\n",
    "  --e2e_steps {E2E_STEPS} \\\n",
    "  --learning_rate {LEARNING_RATE} \\\n",
    "  --e2e_learning_rate {E2E_LEARNING_RATE} \\\n",
    "  --layer_converge_threshold {LAYER_CONVERGE_THRESHOLD} \\\n",
    "  --max_layer_repeats {MAX_LAYER_REPEATS} \\\n",
    "  --logging_steps 10 \\\n",
    "  --device auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CHECK TRAINING RESULTS\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "loss_log = f\"{RUN_DIR}/loss_per_layer.csv\"\n",
    "if os.path.exists(loss_log):\n",
    "    df = pd.read_csv(loss_log)\n",
    "    print(\"Loss summary by pass:\")\n",
    "    print(df.groupby(['pass', 'component'])['global'].agg(['min', 'max', 'mean']))\n",
    "    \n",
    "    # Final loss\n",
    "    final_loss = df['global'].iloc[-1]\n",
    "    print(f\"\\nFinal global loss: {final_loss:.4f}\")\n",
    "else:\n",
    "    print(f\"Loss log not found: {loss_log}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: KD-QAT Refinement\n",
    "\n",
    "If loss is still high (>1.5), run additional KD-QAT refinement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# OPTIONAL: KD-QAT REFINEMENT (if needed)\n",
    "# ============================================================\n",
    "\n",
    "RUN_KDQAT_REFINE = False  # Set to True to run\n",
    "KDQAT_STEPS = 1000\n",
    "KDQAT_LR = 1e-6\n",
    "KDQAT_OUTPUT = 'runs/kdqat_refine_q2_improved_v1'\n",
    "\n",
    "if RUN_KDQAT_REFINE:\n",
    "    !python scripts/train_qat.py \\\n",
    "      --model_name_or_path {MODEL_NAME} \\\n",
    "      --init_model_state {RUN_DIR}/qat_state_dict.pt \\\n",
    "      --output_dir {KDQAT_OUTPUT} \\\n",
    "      --kd_cache_dir {CACHE_DIR_CHAT} \\\n",
    "      --quant_bits {QUANT_BITS} \\\n",
    "      --skip_lm_head \\\n",
    "      --batch_size 2 \\\n",
    "      --max_steps {KDQAT_STEPS} \\\n",
    "      --learning_rate {KDQAT_LR} \\\n",
    "      --logging_steps 50 \\\n",
    "      --save_steps 500 \\\n",
    "      --device auto\n",
    "else:\n",
    "    print(\"KD-QAT refinement skipped. Set RUN_KDQAT_REFINE = True to run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE TO GOOGLE DRIVE\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "\n",
    "SAVE_NAME = 'progressive_qat_q2_improved_v1'\n",
    "GD_DEST = f'/content/drive/MyDrive/qwen3_caches/{SAVE_NAME}.tgz'\n",
    "\n",
    "if os.path.isdir(RUN_DIR):\n",
    "    print(f\"[save] Compressing {RUN_DIR}...\")\n",
    "    !tar -czvf {SAVE_NAME}.tgz -C runs {os.path.basename(RUN_DIR)}\n",
    "    \n",
    "    print(f\"[save] Copying to Google Drive...\")\n",
    "    !cp {SAVE_NAME}.tgz {GD_DEST}\n",
    "    \n",
    "    if os.path.exists(GD_DEST):\n",
    "        size_mb = os.path.getsize(GD_DEST) / (1024*1024)\n",
    "        print(f\"[save] Saved to {GD_DEST} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"[save] ERROR: Failed to save\")\n",
    "else:\n",
    "    print(f\"[save] ERROR: {RUN_DIR} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INFERENCE TEST\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import sys\n",
    "sys.path.append('/content/qwen3_apple_style_2bit_qat_lora')\n",
    "\n",
    "from qat_lora.model_utils import replace_linear_with_qat\n",
    "from qat_lora.quantizer import QATQuantConfig\n",
    "\n",
    "# Load model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16)\n",
    "\n",
    "# Apply QAT structure\n",
    "qc = QATQuantConfig(n_bits=QUANT_BITS)\n",
    "replace_linear_with_qat(model, qc=qc, exclude_regex=r\"(^lm_head$)\", verbose=False)\n",
    "\n",
    "# Load trained weights\n",
    "CHECKPOINT_TO_TEST = f\"{RUN_DIR}/qat_state_dict.pt\"\n",
    "state_dict = torch.load(CHECKPOINT_TO_TEST, map_location='cpu')\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model = model.to('cuda').eval()\n",
    "\n",
    "print(f\"Loaded: {CHECKPOINT_TO_TEST}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GENERATE TEST\n",
    "# ============================================================\n",
    "\n",
    "def generate(prompt, max_new_tokens=100, temperature=0.7):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to('cuda')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test prompts\n",
    "prompts = [\n",
    "    \"What is 2 + 2?\",\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "    \"Write a haiku about programming.\",\n",
    "]\n",
    "\n",
    "for p in prompts:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Prompt: {p}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(generate(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Improvements in this notebook:**\n",
    "\n",
    "| Feature | Previous | This Notebook |\n",
    "|---------|----------|---------------|\n",
    "| f_init calibration | Newton heuristic | MSE Grid-Search |\n",
    "| Progressive passes | MLP only | MLP + Attn + Refine |\n",
    "| Converge threshold | 0.4 | 1.0 (relaxed) |\n",
    "| Learning rate | 5e-6 | 1e-6 (lower) |\n",
    "| Max repeats | 1 | 3 |\n",
    "\n",
    "**If results are still not satisfactory, try:**\n",
    "1. Use K=128 cache (generate with `Generate_KD_Cache_K64_K128.ipynb`)\n",
    "2. Add EMA (future notebook)\n",
    "3. Mixed precision (embed 4-bit, rest 2-bit)\n",
    "4. LoRA with error-initialized weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
