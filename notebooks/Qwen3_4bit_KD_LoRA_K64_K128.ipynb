{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen3 4-bit KD-QAT Refinement + LoRA Recovery\n",
    "\n",
    "This notebook refines a 4-bit QAT checkpoint using K=64 and K=128 KD caches.\n",
    "\n",
    "**Pipeline:**\n",
    "```\n",
    "4-bit QAT checkpoint\n",
    "    |\n",
    "    v\n",
    "Stage 1: KD-QAT with K=64 (warm-up)\n",
    "    |\n",
    "    v\n",
    "Stage 2: KD-QAT with K=128 (polish)\n",
    "    |\n",
    "    v\n",
    "Inference Test\n",
    "    |\n",
    "    v\n",
    "Stage 3: LoRA Recovery\n",
    "    |\n",
    "    v\n",
    "Final Inference Test\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "- Progressive K: K=64 â†’ K=128 for refined teacher signal\n",
    "- Unfrozen attention in later stages\n",
    "- Relaxed hard-top1 weights for better convergence\n",
    "- LoRA recovery for final quality boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SETUP\n",
    "# ============================================================\n",
    "\n",
    "!git clone https://github.com/anemll/qwen3_apple_style_2bit_qat_lora.git\n",
    "%cd qwen3_apple_style_2bit_qat_lora\n",
    "!pip install -q transformers accelerate datasets sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "MODEL_NAME = 'Qwen/Qwen3-0.6B'\n",
    "\n",
    "# Quantization\n",
    "QUANT_BITS = 4\n",
    "\n",
    "# Device settings\n",
    "DEVICE = 'auto'\n",
    "AMP_DTYPE = 'auto'\n",
    "PARAM_DTYPE = 'auto'\n",
    "\n",
    "# KD Caches (progressive: K64 -> K128)\n",
    "CACHE_K64 = 'caches/alpaca_chat_think_both_L128_K64_R512'\n",
    "CACHE_K128 = 'caches/alpaca_chat_think_both_L128_K128_R512'\n",
    "\n",
    "# Input checkpoint (your trained 4-bit QAT)\n",
    "INIT_CHECKPOINT = 'runs/qwen3_kdqat_cache_q4/qat_state_dict.pt'\n",
    "\n",
    "# Output directories\n",
    "RUN_K64 = 'runs/qwen3_q4_kd_k64'\n",
    "RUN_K128 = 'runs/qwen3_q4_kd_k128'\n",
    "RUN_LORA = 'runs/qwen3_q4_lora_recovery'\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 64\n",
    "GRAD_ACCUM = 1\n",
    "\n",
    "print(f\"Config:\")\n",
    "print(f\"  - Model: {MODEL_NAME}\")\n",
    "print(f\"  - Quant bits: {QUANT_BITS}\")\n",
    "print(f\"  - Init checkpoint: {INIT_CHECKPOINT}\")\n",
    "print(f\"  - Caches: K64={CACHE_K64}, K128={CACHE_K128}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MOUNT GOOGLE DRIVE\n",
    "# ============================================================\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD CHECKPOINT FROM GOOGLE DRIVE\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "\n",
    "GD_BASE = '/content/drive/MyDrive/qwen3_caches'\n",
    "\n",
    "# Load 4-bit checkpoint\n",
    "CHECKPOINT_TAR = 'qwen3_kdqat_cache_q4.tgz'  # Adjust name as needed\n",
    "GD_CHECKPOINT = f'{GD_BASE}/{CHECKPOINT_TAR}'\n",
    "\n",
    "if os.path.exists(GD_CHECKPOINT):\n",
    "    print(f\"[load] Extracting checkpoint from Google Drive...\")\n",
    "    !mkdir -p runs\n",
    "    !tar -xzf {GD_CHECKPOINT} -C runs/\n",
    "    print(f\"[load] Done.\")\n",
    "else:\n",
    "    print(f\"[load] Checkpoint not found: {GD_CHECKPOINT}\")\n",
    "    print(\"Please upload your 4-bit checkpoint or adjust the path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD KD CACHES FROM GOOGLE DRIVE\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "\n",
    "GD_BASE = '/content/drive/MyDrive/qwen3_caches'\n",
    "\n",
    "!mkdir -p caches\n",
    "\n",
    "# Load K=64 cache\n",
    "cache_name_64 = 'alpaca_chat_think_both_L128_K64_R512'\n",
    "gd_cache_64 = f\"{GD_BASE}/{cache_name_64}\"\n",
    "if os.path.isdir(gd_cache_64):\n",
    "    print(f\"[cache] Copying K=64 cache...\")\n",
    "    !rsync -ah --info=progress2 {gd_cache_64}/ caches/{cache_name_64}/\n",
    "    print(f\"[cache] K=64 ready: {CACHE_K64}\")\n",
    "else:\n",
    "    print(f\"[cache] K=64 not found at {gd_cache_64}\")\n",
    "\n",
    "# Load K=128 cache\n",
    "cache_name_128 = 'alpaca_chat_think_both_L128_K128_R512'\n",
    "gd_cache_128 = f\"{GD_BASE}/{cache_name_128}\"\n",
    "if os.path.isdir(gd_cache_128):\n",
    "    print(f\"[cache] Copying K=128 cache...\")\n",
    "    !rsync -ah --info=progress2 {gd_cache_128}/ caches/{cache_name_128}/\n",
    "    print(f\"[cache] K=128 ready: {CACHE_K128}\")\n",
    "else:\n",
    "    print(f\"[cache] K=128 not found at {gd_cache_128}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: KD-QAT with K=64\n",
    "\n",
    "Warm-up refinement with K=64 cache. Uses relaxed hard-top1 weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STAGE 1: KD-QAT with K=64\n",
    "# ============================================================\n",
    "\n",
    "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
    "\n",
    "!python scripts/train_qat.py \\\n",
    "  --model_name_or_path {MODEL_NAME} \\\n",
    "  --init_model_state {INIT_CHECKPOINT} \\\n",
    "  --output_dir {RUN_K64} \\\n",
    "  --device {DEVICE} \\\n",
    "  --amp_dtype {AMP_DTYPE} \\\n",
    "  --param_dtype {PARAM_DTYPE} \\\n",
    "  -q {QUANT_BITS} \\\n",
    "  --max_length 128 \\\n",
    "  --per_device_train_batch_size {BATCH_SIZE} \\\n",
    "  --gradient_accumulation_steps {GRAD_ACCUM} \\\n",
    "  --learning_rate 3e-6 \\\n",
    "  --warmup_steps 50 \\\n",
    "  --max_steps 1500 \\\n",
    "  --save_steps 1500 \\\n",
    "  --logging_steps 10 \\\n",
    "  --skip_lm_head \\\n",
    "  --ema_decay 0 \\\n",
    "  --kd_cache_dir {CACHE_K64} \\\n",
    "  --kd_cache_shuffle_files \\\n",
    "  --distill_temperature 2.0 \\\n",
    "  --distill_weight 1.0 \\\n",
    "  --hard-top1-weight 0.01 \\\n",
    "  --hard-full-top1-weight 0.005\n",
    "\n",
    "print(f\"\\n[Stage 1] K=64 refinement complete. Checkpoint: {RUN_K64}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: KD-QAT with K=128 (Polish)\n",
    "\n",
    "Final polish with K=128 cache for maximum teacher signal. Lower learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STAGE 2: KD-QAT with K=128 (Polish)\n",
    "# ============================================================\n",
    "\n",
    "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
    "\n",
    "!python scripts/train_qat.py \\\n",
    "  --model_name_or_path {MODEL_NAME} \\\n",
    "  --init_model_state {RUN_K64}/qat_state_dict.pt \\\n",
    "  --output_dir {RUN_K128} \\\n",
    "  --device {DEVICE} \\\n",
    "  --amp_dtype {AMP_DTYPE} \\\n",
    "  --param_dtype {PARAM_DTYPE} \\\n",
    "  -q {QUANT_BITS} \\\n",
    "  --max_length 128 \\\n",
    "  --per_device_train_batch_size {BATCH_SIZE} \\\n",
    "  --gradient_accumulation_steps {GRAD_ACCUM} \\\n",
    "  --learning_rate 1e-6 \\\n",
    "  --warmup_steps 0 \\\n",
    "  --max_steps 500 \\\n",
    "  --save_steps 500 \\\n",
    "  --logging_steps 10 \\\n",
    "  --skip_lm_head \\\n",
    "  --ema_decay 0 \\\n",
    "  --kd_cache_dir {CACHE_K128} \\\n",
    "  --kd_cache_shuffle_files \\\n",
    "  --distill_temperature 2.0 \\\n",
    "  --distill_weight 1.0 \\\n",
    "  --hard-top1-weight 0.005 \\\n",
    "  --hard-full-top1-weight 0.002\n",
    "\n",
    "print(f\"\\n[Stage 2] K=128 polish complete. Checkpoint: {RUN_K128}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Test (Pre-LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INFERENCE TEST (Pre-LoRA)\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import sys\n",
    "sys.path.append('/content/qwen3_apple_style_2bit_qat_lora')\n",
    "\n",
    "from qat_lora.model_utils import replace_linear_with_qat\n",
    "from qat_lora.quantizer import QATQuantConfig\n",
    "\n",
    "# Load model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16)\n",
    "\n",
    "# Apply QAT structure\n",
    "qc = QATQuantConfig(n_bits=QUANT_BITS)\n",
    "replace_linear_with_qat(model, qc=qc, exclude_regex=r\"(^lm_head$)\", verbose=False)\n",
    "\n",
    "# Load trained weights (K=128 checkpoint)\n",
    "CHECKPOINT_TO_TEST = f\"{RUN_K128}/qat_state_dict.pt\"\n",
    "state_dict = torch.load(CHECKPOINT_TO_TEST, map_location='cpu')\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model = model.to('cuda').eval()\n",
    "\n",
    "print(f\"Loaded: {CHECKPOINT_TO_TEST}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GENERATE TEST (Pre-LoRA)\n",
    "# ============================================================\n",
    "\n",
    "def generate(prompt, max_new_tokens=100, temperature=0.7):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to('cuda')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test prompts\n",
    "prompts = [\n",
    "    \"What is 2 + 2?\",\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "    \"Write a haiku about programming.\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PRE-LORA INFERENCE TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for p in prompts:\n",
    "    print(f\"\\nPrompt: {p}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(generate(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory before LoRA training\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: LoRA Recovery\n",
    "\n",
    "Train LoRA adapters on top of the refined QAT checkpoint to recover any remaining quality loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STAGE 3: LoRA Recovery\n",
    "# ============================================================\n",
    "\n",
    "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
    "\n",
    "LORA_R = 32\n",
    "LORA_ALPHA = 32\n",
    "\n",
    "!python scripts/train_lora_recovery.py \\\n",
    "  --model_name_or_path {MODEL_NAME} \\\n",
    "  --qat_checkpoint {RUN_K128}/qat_state_dict.pt \\\n",
    "  --output_dir {RUN_LORA} \\\n",
    "  --device {DEVICE} \\\n",
    "  --amp_dtype {AMP_DTYPE} \\\n",
    "  --param_dtype {PARAM_DTYPE} \\\n",
    "  -q {QUANT_BITS} \\\n",
    "  --per_device_train_batch_size 16 \\\n",
    "  --gradient_accumulation_steps 2 \\\n",
    "  --learning_rate 1e-5 \\\n",
    "  --warmup_steps 50 \\\n",
    "  --max_steps 2000 \\\n",
    "  --save_steps 2000 \\\n",
    "  --logging_steps 10 \\\n",
    "  --skip_lm_head \\\n",
    "  --lora_r {LORA_R} \\\n",
    "  --lora_alpha {LORA_ALPHA} \\\n",
    "  --lora_dropout 0.0 \\\n",
    "  --kd_cache_dir {CACHE_K128} \\\n",
    "  --kd_cache_shuffle_files \\\n",
    "  --distill_temperature 2.0 \\\n",
    "  --distill_weight 1.0 \\\n",
    "  --hard-top1-weight 0.02 \\\n",
    "  --hard-full-top1-weight 0.01\n",
    "\n",
    "print(f\"\\n[Stage 3] LoRA recovery complete. Checkpoint: {RUN_LORA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Inference Test (With LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL INFERENCE TEST (With LoRA)\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import sys\n",
    "sys.path.append('/content/qwen3_apple_style_2bit_qat_lora')\n",
    "\n",
    "from qat_lora.model_utils import replace_linear_with_qat\n",
    "from qat_lora.quantizer import QATQuantConfig\n",
    "\n",
    "# Load model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16)\n",
    "\n",
    "# Apply QAT structure with LoRA\n",
    "qc = QATQuantConfig(n_bits=QUANT_BITS)\n",
    "replace_linear_with_qat(\n",
    "    model, \n",
    "    qc=qc, \n",
    "    exclude_regex=r\"(^lm_head$)\", \n",
    "    lora_r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Load trained weights (LoRA checkpoint)\n",
    "CHECKPOINT_TO_TEST = f\"{RUN_LORA}/qat_lora_state_dict.pt\"\n",
    "state_dict = torch.load(CHECKPOINT_TO_TEST, map_location='cpu')\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model = model.to('cuda').eval()\n",
    "\n",
    "print(f\"Loaded: {CHECKPOINT_TO_TEST}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GENERATE TEST (With LoRA)\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"POST-LORA INFERENCE TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for p in prompts:\n",
    "    print(f\"\\nPrompt: {p}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(generate(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE ALL CHECKPOINTS TO GOOGLE DRIVE\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "\n",
    "GD_DEST = '/content/drive/MyDrive/qwen3_caches'\n",
    "\n",
    "# Save K=64 checkpoint\n",
    "if os.path.isdir(RUN_K64):\n",
    "    save_name = os.path.basename(RUN_K64)\n",
    "    !tar -czvf {save_name}.tgz -C runs {save_name}\n",
    "    !cp {save_name}.tgz {GD_DEST}/\n",
    "    print(f\"[save] K=64 checkpoint saved to {GD_DEST}/{save_name}.tgz\")\n",
    "\n",
    "# Save K=128 checkpoint\n",
    "if os.path.isdir(RUN_K128):\n",
    "    save_name = os.path.basename(RUN_K128)\n",
    "    !tar -czvf {save_name}.tgz -C runs {save_name}\n",
    "    !cp {save_name}.tgz {GD_DEST}/\n",
    "    print(f\"[save] K=128 checkpoint saved to {GD_DEST}/{save_name}.tgz\")\n",
    "\n",
    "# Save LoRA checkpoint\n",
    "if os.path.isdir(RUN_LORA):\n",
    "    save_name = os.path.basename(RUN_LORA)\n",
    "    !tar -czvf {save_name}.tgz -C runs {save_name}\n",
    "    !cp {save_name}.tgz {GD_DEST}/\n",
    "    print(f\"[save] LoRA checkpoint saved to {GD_DEST}/{save_name}.tgz\")\n",
    "\n",
    "print(\"\\n[save] All checkpoints saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Pipeline completed:**\n",
    "\n",
    "| Stage | Cache | Steps | Output |\n",
    "|-------|-------|-------|--------|\n",
    "| 1. KD-QAT warm-up | K=64 | 1500 | `runs/qwen3_q4_kd_k64` |\n",
    "| 2. KD-QAT polish | K=128 | 500 | `runs/qwen3_q4_kd_k128` |\n",
    "| 3. LoRA recovery | K=128 | 2000 | `runs/qwen3_q4_lora_recovery` |\n",
    "\n",
    "**Expected loss progression:**\n",
    "- Initial 4-bit: ~0.5\n",
    "- After K=64: ~0.4\n",
    "- After K=128: ~0.35\n",
    "- With LoRA: ~0.25-0.3\n",
    "\n",
    "**Next steps:**\n",
    "- Evaluate on downstream benchmarks\n",
    "- Export for deployment (quantize LoRA weights if needed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
