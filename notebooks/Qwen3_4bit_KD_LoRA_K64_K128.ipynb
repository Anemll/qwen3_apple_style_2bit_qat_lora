{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeI4-Wi6YCAB"
      },
      "source": [
        "# Qwen3 4-bit KD-QAT Refinement + LoRA Recovery\n",
        "\n",
        "This notebook refines a 4-bit QAT checkpoint using K=64 and K=128 KD caches.\n",
        "\n",
        "**Pipeline:**\n",
        "```\n",
        "4-bit QAT checkpoint\n",
        "    |\n",
        "    v\n",
        "Stage 1: KD-QAT with K=64 (warm-up)\n",
        "    |\n",
        "    v\n",
        "Stage 2: KD-QAT with K=128 (polish)\n",
        "    |\n",
        "    v\n",
        "Inference Test\n",
        "    |\n",
        "    v\n",
        "Stage 3: LoRA Recovery\n",
        "    |\n",
        "    v\n",
        "Final Inference Test\n",
        "```\n",
        "\n",
        "**Key Features:**\n",
        "- Progressive K: K=64 â†’ K=128 for refined teacher signal\n",
        "- Unfrozen attention in later stages\n",
        "- Relaxed hard-top1 weights for better convergence\n",
        "- LoRA recovery for final quality boost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ExdmDQoxYCAC",
        "outputId": "27cb58f7-32d9-4598-ed86-27aaa4e2323e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'qwen3_apple_style_2bit_qat_lora'...\n",
            "remote: Enumerating objects: 283, done.\u001b[K\n",
            "remote: Counting objects: 100% (283/283), done.\u001b[K\n",
            "remote: Compressing objects: 100% (207/207), done.\u001b[K\n",
            "remote: Total 283 (delta 172), reused 165 (delta 73), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (283/283), 369.92 KiB | 5.52 MiB/s, done.\n",
            "Resolving deltas: 100% (172/172), done.\n",
            "/content/qwen3_apple_style_2bit_qat_lora\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# SETUP\n",
        "# ============================================================\n",
        "\n",
        "!git clone https://github.com/anemll/qwen3_apple_style_2bit_qat_lora.git\n",
        "%cd qwen3_apple_style_2bit_qat_lora\n",
        "!pip install -q transformers accelerate datasets sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "l33lJHcQYCAC",
        "outputId": "7935d00a-a1af-4a73-ce48-3d956c25cbfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config:\n",
            "  - Model: Qwen/Qwen3-0.6B\n",
            "  - Quant bits: 4\n",
            "  - Init checkpoint: runs/qwen3_kdqat_cache_q4/qat_state_dict.pt\n",
            "  - Caches: K64=caches/alpaca_chat_think_both_L128_K64_R512, K128=caches/alpaca_chat_think_both_L128_K128_R512\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CONFIG\n",
        "# ============================================================\n",
        "\n",
        "MODEL_NAME = 'Qwen/Qwen3-0.6B'\n",
        "\n",
        "# Quantization\n",
        "QUANT_BITS = 4\n",
        "\n",
        "# Device settings\n",
        "DEVICE = 'auto'\n",
        "AMP_DTYPE = 'auto'\n",
        "PARAM_DTYPE = 'auto'\n",
        "\n",
        "# KD Caches (progressive: K64 -> K128)\n",
        "CACHE_K64 = 'caches/alpaca_chat_think_both_L128_K64_R512'\n",
        "CACHE_K128 = 'caches/alpaca_chat_think_both_L128_K128_R512'\n",
        "\n",
        "# Input checkpoint (your trained 4-bit QAT)\n",
        "INIT_CHECKPOINT = 'runs/qwen3_kdqat_cache_q4/qat_state_dict.pt'\n",
        "\n",
        "# Output directories\n",
        "RUN_K64 = 'runs/qwen3_q4_kd_k64'\n",
        "RUN_K128 = 'runs/qwen3_q4_kd_k128'\n",
        "RUN_LORA = 'runs/qwen3_q4_lora_recovery'\n",
        "\n",
        "# Training parameters\n",
        "BATCH_SIZE = 64\n",
        "GRAD_ACCUM = 1\n",
        "\n",
        "print(f\"Config:\")\n",
        "print(f\"  - Model: {MODEL_NAME}\")\n",
        "print(f\"  - Quant bits: {QUANT_BITS}\")\n",
        "print(f\"  - Init checkpoint: {INIT_CHECKPOINT}\")\n",
        "print(f\"  - Caches: K64={CACHE_K64}, K128={CACHE_K128}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "511JSw40YCAC",
        "outputId": "e0067e06-f15a-4b42-e91c-32e6fead8f87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# MOUNT GOOGLE DRIVE\n",
        "# ============================================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_UnaC18XYCAC",
        "outputId": "7ca4b6e9-3877-4e02-954d-263fb6a169f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[load] Extracting checkpoint from Google Drive...\n",
            "[load] Done.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# LOAD CHECKPOINT FROM GOOGLE DRIVE\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "\n",
        "GD_BASE = '/content/drive/MyDrive/qwen3_runs'\n",
        "\n",
        "# Load 4-bit checkpoint\n",
        "CHECKPOINT_TAR = 'qwen3_kdqat_cache_q2_4.tgz'  # Adjust name as needed\n",
        "GD_CHECKPOINT = f'{GD_BASE}/{CHECKPOINT_TAR}'\n",
        "\n",
        "if os.path.exists(GD_CHECKPOINT):\n",
        "    print(f\"[load] Extracting checkpoint from Google Drive...\")\n",
        "    !mkdir -p runs\n",
        "    !tar -xzf {GD_CHECKPOINT} -C runs/\n",
        "    print(f\"[load] Done.\")\n",
        "else:\n",
        "    print(f\"[load] Checkpoint not found: {GD_CHECKPOINT}\")\n",
        "    print(\"Please upload your 4-bit checkpoint or adjust the path.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RujAok-1YCAC",
        "outputId": "9272bea7-57e2-44fe-cb8c-9012a16cb1c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[cache] Copying K=64 cache...\n",
            "          8.79G 100%   85.37MB/s    0:01:38 (xfr#21, to-chk=0/22)\n",
            "[cache] K=64 ready: caches/alpaca_chat_think_both_L128_K64_R512\n",
            "[cache] K=128 not found at /content/drive/MyDrive/qwen3_caches/alpaca_chat_think_both_L128_K128_R512\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# LOAD KD CACHES FROM GOOGLE DRIVE\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "\n",
        "GD_BASE = '/content/drive/MyDrive/qwen3_caches'\n",
        "\n",
        "!mkdir -p caches\n",
        "\n",
        "# Load K=64 cache\n",
        "cache_name_64 = 'alpaca_chat_think_both_L128_K64_R512'\n",
        "gd_cache_64 = f\"{GD_BASE}/{cache_name_64}\"\n",
        "if os.path.isdir(gd_cache_64):\n",
        "    print(f\"[cache] Copying K=64 cache...\")\n",
        "    !rsync -ah --info=progress2 {gd_cache_64}/ caches/{cache_name_64}/\n",
        "    print(f\"[cache] K=64 ready: {CACHE_K64}\")\n",
        "else:\n",
        "    print(f\"[cache] K=64 not found at {gd_cache_64}\")\n",
        "\n",
        "# Load K=128 cache\n",
        "cache_name_128 = 'alpaca_chat_think_both_L128_K128_R512'\n",
        "gd_cache_128 = f\"{GD_BASE}/{cache_name_128}\"\n",
        "if os.path.isdir(gd_cache_128):\n",
        "    print(f\"[cache] Copying K=128 cache...\")\n",
        "    !rsync -ah --info=progress2 {gd_cache_128}/ caches/{cache_name_128}/\n",
        "    print(f\"[cache] K=128 ready: {CACHE_K128}\")\n",
        "else:\n",
        "    print(f\"[cache] K=128 not found at {gd_cache_128}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWvbRny1YCAC"
      },
      "source": [
        "## Stage 1: KD-QAT with K=64\n",
        "\n",
        "Warm-up refinement with K=64 cache. Uses relaxed hard-top1 weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFDUgVGCYCAD"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STAGE 1: KD-QAT with K=64\n",
        "# ============================================================\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {INIT_CHECKPOINT} \\\n",
        "  --output_dir {RUN_K64} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size {BATCH_SIZE} \\\n",
        "  --gradient_accumulation_steps {GRAD_ACCUM} \\\n",
        "  --learning_rate 3e-6 \\\n",
        "  --warmup_steps 50 \\\n",
        "  --max_steps 1500 \\\n",
        "  --save_steps 1500 \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_K64} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.01 \\\n",
        "  --hard-full-top1-weight 0.005\n",
        "\n",
        "print(f\"\\n[Stage 1] K=64 refinement complete. Checkpoint: {RUN_K64}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB7Uw-80YCAD"
      },
      "source": [
        "## Stage 2: KD-QAT with K=128 (Polish)\n",
        "\n",
        "Final polish with K=128 cache for maximum teacher signal. Lower learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Djgu8lojYCAD"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STAGE 2: KD-QAT with K=128 (Polish)\n",
        "# ============================================================\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/train_qat.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --init_model_state {RUN_K64}/qat_state_dict.pt \\\n",
        "  --output_dir {RUN_K128} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --max_length 128 \\\n",
        "  --per_device_train_batch_size {BATCH_SIZE} \\\n",
        "  --gradient_accumulation_steps {GRAD_ACCUM} \\\n",
        "  --learning_rate 1e-6 \\\n",
        "  --warmup_steps 0 \\\n",
        "  --max_steps 500 \\\n",
        "  --save_steps 500 \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --ema_decay 0 \\\n",
        "  --kd_cache_dir {CACHE_K128} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.005 \\\n",
        "  --hard-full-top1-weight 0.002\n",
        "\n",
        "print(f\"\\n[Stage 2] K=128 polish complete. Checkpoint: {RUN_K128}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ky2xpvUYCAD"
      },
      "source": [
        "## Inference Test (Pre-LoRA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFwwZJODYCAD"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# INFERENCE TEST (Pre-LoRA)\n",
        "# ============================================================\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import sys\n",
        "sys.path.append('/content/qwen3_apple_style_2bit_qat_lora')\n",
        "\n",
        "from qat_lora.model_utils import replace_linear_with_qat\n",
        "from qat_lora.quantizer import QATQuantConfig\n",
        "\n",
        "# Load model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16)\n",
        "\n",
        "# Apply QAT structure\n",
        "qc = QATQuantConfig(n_bits=QUANT_BITS)\n",
        "replace_linear_with_qat(model, qc=qc, exclude_regex=r\"(^lm_head$)\", verbose=False)\n",
        "\n",
        "# Load trained weights (K=128 checkpoint)\n",
        "CHECKPOINT_TO_TEST = f\"{RUN_K128}/qat_state_dict.pt\"\n",
        "state_dict = torch.load(CHECKPOINT_TO_TEST, map_location='cpu')\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model = model.to('cuda').eval()\n",
        "\n",
        "print(f\"Loaded: {CHECKPOINT_TO_TEST}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cQK9WsfYCAD"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# GENERATE TEST (Pre-LoRA)\n",
        "# ============================================================\n",
        "\n",
        "def generate(prompt, max_new_tokens=100, temperature=0.7):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to('cuda')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.1,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Test prompts\n",
        "prompts = [\n",
        "    \"What is 2 + 2?\",\n",
        "    \"Explain quantum computing in simple terms.\",\n",
        "    \"Write a haiku about programming.\",\n",
        "]\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PRE-LORA INFERENCE TEST\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for p in prompts:\n",
        "    print(f\"\\nPrompt: {p}\")\n",
        "    print(\"-\" * 40)\n",
        "    print(generate(p))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYDYjd0PYCAD"
      },
      "outputs": [],
      "source": [
        "# Free memory before LoRA training\n",
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUroica5YCAD"
      },
      "source": [
        "## Stage 3: LoRA Recovery\n",
        "\n",
        "Train LoRA adapters on top of the refined QAT checkpoint to recover any remaining quality loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MWak5wpYCAD"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STAGE 3: LoRA Recovery\n",
        "# ============================================================\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "LORA_R = 32\n",
        "LORA_ALPHA = 32\n",
        "\n",
        "!python scripts/train_lora_recovery.py \\\n",
        "  --model_name_or_path {MODEL_NAME} \\\n",
        "  --qat_checkpoint {RUN_K128}/qat_state_dict.pt \\\n",
        "  --output_dir {RUN_LORA} \\\n",
        "  --device {DEVICE} \\\n",
        "  --amp_dtype {AMP_DTYPE} \\\n",
        "  --param_dtype {PARAM_DTYPE} \\\n",
        "  -q {QUANT_BITS} \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --gradient_accumulation_steps 2 \\\n",
        "  --learning_rate 1e-5 \\\n",
        "  --warmup_steps 50 \\\n",
        "  --max_steps 2000 \\\n",
        "  --save_steps 2000 \\\n",
        "  --logging_steps 10 \\\n",
        "  --skip_lm_head \\\n",
        "  --lora_r {LORA_R} \\\n",
        "  --lora_alpha {LORA_ALPHA} \\\n",
        "  --lora_dropout 0.0 \\\n",
        "  --kd_cache_dir {CACHE_K128} \\\n",
        "  --kd_cache_shuffle_files \\\n",
        "  --distill_temperature 2.0 \\\n",
        "  --distill_weight 1.0 \\\n",
        "  --hard-top1-weight 0.02 \\\n",
        "  --hard-full-top1-weight 0.01\n",
        "\n",
        "print(f\"\\n[Stage 3] LoRA recovery complete. Checkpoint: {RUN_LORA}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo3G9frYYCAD"
      },
      "source": [
        "## Final Inference Test (With LoRA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgePDXLKYCAD"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# FINAL INFERENCE TEST (With LoRA)\n",
        "# ============================================================\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import sys\n",
        "sys.path.append('/content/qwen3_apple_style_2bit_qat_lora')\n",
        "\n",
        "from qat_lora.model_utils import replace_linear_with_qat\n",
        "from qat_lora.quantizer import QATQuantConfig\n",
        "\n",
        "# Load model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16)\n",
        "\n",
        "# Apply QAT structure with LoRA\n",
        "qc = QATQuantConfig(n_bits=QUANT_BITS)\n",
        "replace_linear_with_qat(\n",
        "    model,\n",
        "    qc=qc,\n",
        "    exclude_regex=r\"(^lm_head$)\",\n",
        "    lora_r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Load trained weights (LoRA checkpoint)\n",
        "CHECKPOINT_TO_TEST = f\"{RUN_LORA}/qat_lora_state_dict.pt\"\n",
        "state_dict = torch.load(CHECKPOINT_TO_TEST, map_location='cpu')\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model = model.to('cuda').eval()\n",
        "\n",
        "print(f\"Loaded: {CHECKPOINT_TO_TEST}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3EIEZ2gYCAD"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# GENERATE TEST (With LoRA)\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"POST-LORA INFERENCE TEST\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for p in prompts:\n",
        "    print(f\"\\nPrompt: {p}\")\n",
        "    print(\"-\" * 40)\n",
        "    print(generate(p))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6CyhefLYCAD"
      },
      "source": [
        "## Save to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnWo0WERYCAD"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SAVE ALL CHECKPOINTS TO GOOGLE DRIVE\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "\n",
        "GD_DEST = '/content/drive/MyDrive/qwen3_caches'\n",
        "\n",
        "# Save K=64 checkpoint\n",
        "if os.path.isdir(RUN_K64):\n",
        "    save_name = os.path.basename(RUN_K64)\n",
        "    !tar -czvf {save_name}.tgz -C runs {save_name}\n",
        "    !cp {save_name}.tgz {GD_DEST}/\n",
        "    print(f\"[save] K=64 checkpoint saved to {GD_DEST}/{save_name}.tgz\")\n",
        "\n",
        "# Save K=128 checkpoint\n",
        "if os.path.isdir(RUN_K128):\n",
        "    save_name = os.path.basename(RUN_K128)\n",
        "    !tar -czvf {save_name}.tgz -C runs {save_name}\n",
        "    !cp {save_name}.tgz {GD_DEST}/\n",
        "    print(f\"[save] K=128 checkpoint saved to {GD_DEST}/{save_name}.tgz\")\n",
        "\n",
        "# Save LoRA checkpoint\n",
        "if os.path.isdir(RUN_LORA):\n",
        "    save_name = os.path.basename(RUN_LORA)\n",
        "    !tar -czvf {save_name}.tgz -C runs {save_name}\n",
        "    !cp {save_name}.tgz {GD_DEST}/\n",
        "    print(f\"[save] LoRA checkpoint saved to {GD_DEST}/{save_name}.tgz\")\n",
        "\n",
        "print(\"\\n[save] All checkpoints saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t191Us6YCAD"
      },
      "source": [
        "## Summary\n",
        "\n",
        "**Pipeline completed:**\n",
        "\n",
        "| Stage | Cache | Steps | Output |\n",
        "|-------|-------|-------|--------|\n",
        "| 1. KD-QAT warm-up | K=64 | 1500 | `runs/qwen3_q4_kd_k64` |\n",
        "| 2. KD-QAT polish | K=128 | 500 | `runs/qwen3_q4_kd_k128` |\n",
        "| 3. LoRA recovery | K=128 | 2000 | `runs/qwen3_q4_lora_recovery` |\n",
        "\n",
        "**Expected loss progression:**\n",
        "- Initial 4-bit: ~0.5\n",
        "- After K=64: ~0.4\n",
        "- After K=128: ~0.35\n",
        "- With LoRA: ~0.25-0.3\n",
        "\n",
        "**Next steps:**\n",
        "- Evaluate on downstream benchmarks\n",
        "- Export for deployment (quantize LoRA weights if needed)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}