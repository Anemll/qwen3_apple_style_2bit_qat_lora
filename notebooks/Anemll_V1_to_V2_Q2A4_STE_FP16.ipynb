{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Q2_A4 V1 to V2 Conversion + STE-FP16 Finetuning\n\n**Configuration:**\n- MLP: 2-bit (LUT=4), rank=32\n- Attention: 4-bit (LUT=16), rank=8\n\nThis notebook:\n1. Loads a trained Q2_A4 V1 checkpoint (loss ~0.38)\n2. Converts V1 scales to V2 format (unit-norm + rank_magnitude)\n3. Finetunes the V2 model with **STE-FP16** (FP32 master weights + FP16 forward)\n4. Exports **FP16 checkpoint** (ANE-ready!)\n\n## STE-FP16 Training:\n- **FP32 master weights**: Stable gradients, no underflow\n- **FP16 forward pass** (via STE): Matches ANE behavior exactly\n- **Export to FP16**: Direct conversion for ANE deployment"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GOOGLE DRIVE PATHS\n",
    "# ============================================================\n",
    "\n",
    "GD_RUNS = '/content/drive/MyDrive/qwen3_runs'\n",
    "GD_CACHES = '/content/drive/MyDrive/qwen3_caches'\n",
    "\n",
    "LOCAL_RUNS = 'runs'\n",
    "LOCAL_CACHES = 'caches'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone/update repo\n",
    "!git clone https://github.com/anemll/qwen3_apple_style_2bit_qat_lora.git || (cd qwen3_apple_style_2bit_qat_lora && git pull)\n",
    "%cd qwen3_apple_style_2bit_qat_lora\n",
    "!git fetch && git pull && git reset --hard HEAD\n",
    "\n",
    "import sys\n",
    "[sys.modules.pop(k) for k in list(sys.modules) if k.startswith('qat_lora')]\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q transformers accelerate safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# EXTRACT CHECKPOINT + CONFIGURATION\n# ============================================================\n\nimport torch\nimport os\nimport gc\nimport glob\n\nMODEL_ID = 'Qwen/Qwen3-0.6B'\n\n# Paths\nDRIVE_RUNS = '/content/drive/MyDrive/qwen3_runs'\n\n# Extract Q2 checkpoint from Drive (Colab storage is ephemeral!)\nos.makedirs(LOCAL_RUNS, exist_ok=True)\narchive_path = f'{DRIVE_RUNS}/q2_pt_good1.tgz'\n\nif os.path.exists(archive_path):\n    print(f'Extracting {archive_path}...')\n    !tar -xzf {archive_path} -C {LOCAL_RUNS}/\n    print('Extraction complete.')\nelse:\n    print(f'WARNING: Archive not found at {archive_path}')\n    print('Upload q2_pt_good1.tgz to Google Drive qwen3_runs folder')\n\n# Find checkpoint\nV1_CHECKPOINT = None\nsearch_paths = [\n    f'{LOCAL_RUNS}/tmp/backup_mlp_e2e_w_0.3824.pt',\n    f'{LOCAL_RUNS}/q2_pt_good1/backup_mlp_e2e_w_0.3824.pt',\n    f'{LOCAL_RUNS}/backup_mlp_e2e_w_0.3824.pt',\n]\n\nfor p in search_paths:\n    if os.path.exists(p):\n        V1_CHECKPOINT = p\n        break\n\n# Fallback: glob search\nif V1_CHECKPOINT is None:\n    matches = glob.glob(f'{LOCAL_RUNS}/**/backup_mlp_e2e_w_*.pt', recursive=True)\n    if matches:\n        V1_CHECKPOINT = matches[0]\n\nassert V1_CHECKPOINT and os.path.exists(V1_CHECKPOINT), \\\n    f'Checkpoint not found! Found files:\\n' + \\\n    '\\n'.join(glob.glob(f'{LOCAL_RUNS}/**/*.pt', recursive=True)[:10])\n\nprint(f'V1 checkpoint: {V1_CHECKPOINT}')\n\n# ============================================================\n# Q2_A4 Quantization Config (MUST MATCH V1 CHECKPOINT!)\n# ============================================================\n# MLP: 2-bit (lut_size=4), scale_rank=32\nLUT_BITS = 2\nLUT_SIZE = 2**LUT_BITS  # 4\nSCALE_RANK = 32\n\n# Attention: 4-bit (lut_size=16), scale_rank=8\nATTN_LUT_BITS = 4\nATTN_LUT_SIZE = 2**ATTN_LUT_BITS  # 16\nATTN_SCALE_RANK = 8\n\n# Training - SMALLER BATCH SIZE for Q2_A4 (rank=32 uses more memory with STE)\nBATCH_SIZE = 4  # Reduced for high rank + STE memory usage\nDISTILL_TEMP = 2.0\n\nif not torch.cuda.is_available():\n    raise RuntimeError('Training requires CUDA!')\nDEVICE = torch.device('cuda')\n\n# STE-FP16 settings\nV2_DTYPE = torch.float32  # FP32 master weights\nUSE_STE_FP16 = True\nUSE_FP16 = False\n\nQUAL = f'q{LUT_BITS}_a{ATTN_LUT_BITS}_ste_fp16'\nprint(f'\\n=== Q2_A4 Configuration ===')\nprint(f'MLP:  {LUT_SIZE} LUT ({LUT_BITS}-bit), rank={SCALE_RANK}')\nprint(f'Attn: {ATTN_LUT_SIZE} LUT ({ATTN_LUT_BITS}-bit), rank={ATTN_SCALE_RANK}')\nprint(f'Batch size: {BATCH_SIZE} (reduced for high rank + STE)')\nprint(f'STE-FP16: {USE_STE_FP16}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD KD CACHE\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "\n",
    "CACHE_NAME = 'alpaca_chat_think_both_L128_K128_R1024'\n",
    "CACHE_TGZ = f'{CACHE_NAME}.tgz'\n",
    "\n",
    "!mkdir -p {LOCAL_CACHES}\n",
    "\n",
    "cache_local_path = f'{LOCAL_CACHES}/{CACHE_NAME}'\n",
    "if not os.path.exists(cache_local_path):\n",
    "    print(f'Extracting {CACHE_TGZ} from Google Drive...')\n",
    "    !tar -xzf {GD_CACHES}/{CACHE_TGZ} -C {LOCAL_CACHES}/\n",
    "else:\n",
    "    print(f'Cache already exists at {cache_local_path}')\n",
    "\n",
    "!ls -la {cache_local_path}/ | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load V1 Model and Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# LOAD BASE MODEL FOR V1\n# ============================================================\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nprint(f'Loading {MODEL_ID}...')\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n\n# Load V1 model in BF16 (will convert to FP16 during V1→V2 conversion)\nv1_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    torch_dtype=torch.bfloat16,  # V1 in BF16\n    trust_remote_code=True,\n)\nprint(f'Base model loaded: {sum(p.numel() for p in v1_model.parameters()):,} params')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# REPLACE WITH V1 LAYERS AND LOAD CHECKPOINT\n# ============================================================\n\nimport sys\nsys.path.insert(0, '.')\n\nimport importlib\nimport qat_lora\nimportlib.reload(qat_lora)\n\nfrom qat_lora import (\n    AnemllQATLinear,\n    AnemllQuantConfig,\n    replace_linear_with_anemll,\n    load_checkpoint,\n)\n\n# Create V1 configs\nv1_mlp_config = AnemllQuantConfig(\n    lut_size=LUT_SIZE,\n    scale_rank=SCALE_RANK,\n)\nv1_attn_config = AnemllQuantConfig(\n    lut_size=ATTN_LUT_SIZE,\n    scale_rank=ATTN_SCALE_RANK,\n)\n\n# Replace with V1 layers\nprint('Replacing with V1 AnemllQATLinear...')\nreplace_linear_with_anemll(\n    v1_model,\n    mlp_config=v1_mlp_config,\n    attn_config=v1_attn_config,\n    quantize_attn=True,\n    quantize_lm_head=False,\n)\n\n# Load V1 checkpoint\nprint(f'\\nLoading V1 checkpoint from {V1_CHECKPOINT}...')\nv1_model.load_state_dict(torch.load(V1_CHECKPOINT, map_location='cpu'), strict=False)\nv1_model.to(DEVICE)\nprint('V1 checkpoint loaded!')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUATE V1 MODEL\n",
    "# ============================================================\n",
    "\n",
    "from qat_lora import evaluate_kd_loss\n",
    "\n",
    "v1_model.eval()\n",
    "v1_loss = evaluate_kd_loss(v1_model, cache_local_path, DEVICE, num_samples=50, temperature=DISTILL_TEMP)\n",
    "print(f'V1 KD Loss: {v1_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Create V2 Model and Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CREATE V2 MODEL (FP32 master weights + STE-FP16 forward)\n# ============================================================\n\nfrom qat_lora import (\n    AnemllQATLinearV2,\n    AnemllQuantConfigV2,\n    replace_linear_with_anemll_v2,\n    freeze_Q_all,\n)\n\n# Load fresh base model for V2 in FP32 (master weights)\nprint(f'Loading fresh base model for V2 in {V2_DTYPE}...')\nv2_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    torch_dtype=V2_DTYPE,  # FP32 master weights!\n    trust_remote_code=True,\n)\n\n# Create V2 configs with STE-FP16 enabled\n# - force_positive_scales=False: V1 scales can be negative\n# - magnitude_activation='identity': Store raw magnitude  \n# - use_ste_fp16=True: Forward simulates FP16 via STE\nv2_mlp_config = AnemllQuantConfigV2(\n    lut_size=LUT_SIZE,\n    scale_rank=SCALE_RANK,\n    force_positive_scales=False,  # V1 has negative scales!\n    magnitude_activation='identity',  # No softplus - store raw magnitude\n    use_ste_fp16=USE_STE_FP16,  # Enable STE-FP16 forward!\n)\nv2_attn_config = AnemllQuantConfigV2(\n    lut_size=ATTN_LUT_SIZE,\n    scale_rank=ATTN_SCALE_RANK,\n    force_positive_scales=False,  # V1 has negative scales!\n    magnitude_activation='identity',  # No softplus - store raw magnitude\n    use_ste_fp16=USE_STE_FP16,  # Enable STE-FP16 forward!\n)\n\n# Replace with V2 layers\nprint('Replacing with V2 AnemllQATLinearV2...')\nreplace_linear_with_anemll_v2(\n    v2_model,\n    mlp_config=v2_mlp_config,\n    attn_config=v2_attn_config,\n)\n\nv2_model.to(DEVICE)\nprint(f'V2 model created in {V2_DTYPE}!')\nprint(f'STE-FP16 enabled: {USE_STE_FP16}')\nprint('Note: force_positive_scales=False, magnitude_activation=identity for V1 compatibility')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CONVERT V1 → V2 (FP32 master weights)\n# ============================================================\n\ndef convert_v1_layer_to_v2(v1_layer, v2_layer, target_dtype=torch.float32):\n    \"\"\"Convert V1 layer parameters to V2 format.\n    \n    V1: scales = A @ B (arbitrary, can be negative)\n    V2: scales = (g * A_dir) @ B_dir\n    \n    With magnitude_activation='identity', we store raw magnitude directly.\n    With force_positive_scales=False, we preserve signs in A_dir/B_dir.\n    \"\"\"\n    with torch.no_grad():\n        # Copy base parameters\n        v2_layer.weight.data = v1_layer.weight.data.to(target_dtype)\n        if v1_layer.bias is not None and v2_layer.bias is not None:\n            v2_layer.bias.data = v1_layer.bias.data.to(target_dtype)\n        v2_layer.lut.data = v1_layer.lut.data.to(target_dtype)\n        \n        # Get V1 scales (handle potential padding)\n        A = v1_layer.scale_A  # [out, rank]\n        B_full = v1_layer.scale_B  # [rank, padded_in]\n        B = B_full[:, :v1_layer.in_features]  # [rank, in]\n        \n        # Compute norms (keep signs!)\n        A_norms = A.norm(dim=0, keepdim=True).clamp(min=1e-6)  # [1, rank]\n        B_norms = B.norm(dim=1, keepdim=True).clamp(min=1e-6)  # [rank, 1]\n        \n        # Unit-norm directions (preserving signs)\n        A_dir = A / A_norms  # [out, rank]\n        B_dir = B / B_norms  # [rank, in]\n        \n        # Magnitude is product of norms (always positive)\n        magnitude = (A_norms.squeeze() * B_norms.squeeze())  # [rank]\n        \n        # Store directly - no inverse_softplus needed with identity activation\n        v2_layer.scale_A.data = A_dir.to(target_dtype)\n        v2_layer.scale_B.data = B_dir.to(target_dtype)\n        v2_layer.rank_magnitude.data = magnitude.to(target_dtype)\n\n\nprint(f'Converting V1 → V2 in {V2_DTYPE}...')\nconverted = 0\n\n# Collect V1 and V2 layers\nv1_layers = {name: m for name, m in v1_model.named_modules() \n             if type(m).__name__ == 'AnemllQATLinear'}\nv2_layers = {name: m for name, m in v2_model.named_modules() \n             if type(m).__name__ == 'AnemllQATLinearV2'}\n\nprint(f'Found {len(v1_layers)} V1 layers, {len(v2_layers)} V2 layers')\n\n# Convert each layer\nfor name in v1_layers:\n    if name in v2_layers:\n        convert_v1_layer_to_v2(v1_layers[name], v2_layers[name], target_dtype=V2_DTYPE)\n        converted += 1\n        if converted <= 3:\n            v2 = v2_layers[name]\n            print(f'  {name}: mag=[{v2.rank_magnitude.min():.3f}, {v2.rank_magnitude.max():.3f}]')\n\nprint(f'\\nConverted {converted} layers to V2 {V2_DTYPE}')\n\n# Free V1 model memory\ndel v1_model\nimport gc\ngc.collect()\ntorch.cuda.empty_cache()\nprint('V1 model freed from memory')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# FREEZE Q (compute quantization indices)\n# ============================================================\n\n# With STE-FP16, we keep master weights in FP32 but forward simulates FP16.\n# No need to convert to FP16 here - just freeze Q.\n\nprint('Freezing Q (computing indices)...')\nfreeze_Q_all(v2_model, verbose=False)\nprint('Q frozen for all V2 layers.')\n\n# Verify dtype (should be FP32 master weights)\nfor name, m in v2_model.named_modules():\n    if type(m).__name__ == 'AnemllQATLinearV2':\n        print(f'\\nVerified {name}:')\n        print(f'  weight.dtype: {m.weight.dtype} (master weights)')\n        print(f'  lut.dtype: {m.lut.dtype}')\n        print(f'  _Q.dtype: {m._Q.dtype if m._Q is not None else None}')\n        print(f'  STE-FP16: {m.config.use_ste_fp16} (forward simulates FP16)')\n        break"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUATE CONVERTED V2 MODEL\n",
    "# ============================================================\n",
    "\n",
    "v2_model.eval()\n",
    "v2_converted_loss = evaluate_kd_loss(v2_model, cache_local_path, DEVICE, num_samples=50, temperature=DISTILL_TEMP)\n",
    "print(f'\\n=== Conversion Results ===')\n",
    "print(f'V1 Loss: {v1_loss:.4f}')\n",
    "print(f'V2 Loss (after conversion): {v2_converted_loss:.4f}')\n",
    "print(f'Difference: {abs(v2_converted_loss - v1_loss):.4f}')\n",
    "\n",
    "if abs(v2_converted_loss - v1_loss) < 0.1:\n",
    "    print('Conversion successful - losses are close!')\n",
    "else:\n",
    "    print('Note: Some difference expected due to different forward implementations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Step 3: Finetune V2 Model with STE-FP16\n\nTraining with:\n- **FP32 master weights**: Stable gradients, no underflow\n- **STE-FP16 forward**: Each operation rounded to FP16 precision (matches ANE)\n- **No autocast**: STE handles the FP16 simulation internally"
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# MEMORY CLEANUP\n# ============================================================\n\nimport gc\nimport torch\n\n# Clear CUDA cache and collect garbage before training\ntorch.cuda.empty_cache()\ngc.collect()\n\nprint(f'GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f}GB allocated')\nprint(f'GPU Memory: {torch.cuda.memory_reserved()/1e9:.2f}GB reserved')\n!nvidia-smi --query-gpu=memory.used,memory.free --format=csv",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# PASS 1: ALL SCALES (MLP + ATTENTION) - 1000 steps\n# ============================================================\n\nfrom qat_lora import train_e2e, unfreeze_model_for_training_v2\n\n# Clear memory before training\ntorch.cuda.empty_cache()\ngc.collect()\n\n# Unfreeze scales for training\nunfreeze_model_for_training_v2(v2_model)\n\n# Enable all scales\nfor name, module in v2_model.named_modules():\n    if type(module).__name__ == 'AnemllQATLinearV2':\n        if hasattr(module, 'scale_A') and module.scale_A is not None:\n            module.scale_A.requires_grad = True\n            module.scale_B.requires_grad = True\n            module.rank_magnitude.requires_grad = True\n        module.weight.requires_grad = False  # Keep weights frozen\n\ntrainable = sum(p.numel() for p in v2_model.parameters() if p.requires_grad)\nprint(f'Trainable params: {trainable:,}')\n\n# Pass 1: Train all scales\nprint('\\n=== PASS 1: All Scales (MLP + Attention) ===')\ne2e_result = train_e2e(\n    model=v2_model,\n    cache_dir=cache_local_path,\n    device=DEVICE,\n    max_steps=1000,  # First pass: 1000 steps\n    batch_size=BATCH_SIZE,\n    lr=1e-4,\n    use_cosine_schedule=True,\n    warmup_steps=100,\n    min_lr_ratio=0.1,\n    temperature=DISTILL_TEMP,\n    train_weights=False,\n    train_scales=True,\n    hard_top1_weight=0.0,\n    hard_full_weight=0.0,\n    logging_steps=20,\n    eval_steps=100,\n    verbose=True,\n    train_mlp_only=False,  # Train all scales\n    use_fp16=USE_FP16,\n)\nprint(f'\\nPass 1 complete! Final loss: {e2e_result[\"final_loss\"]:.4f}')"
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# PASS 2: MLP-ONLY REFINEMENT - 1000 steps\n# ============================================================\n\nfrom qat_lora import train_e2e, unfreeze_model_for_training_v2\n\n# Clear memory before training\ntorch.cuda.empty_cache()\ngc.collect()\n\n# Unfreeze and set MLP-only\nunfreeze_model_for_training_v2(v2_model)\n\n# Freeze attention, enable MLP scales only\nfor name, module in v2_model.named_modules():\n    if type(module).__name__ == 'AnemllQATLinearV2':\n        is_mlp = '.mlp.' in name\n        if hasattr(module, 'scale_A') and module.scale_A is not None:\n            module.scale_A.requires_grad = is_mlp\n            module.scale_B.requires_grad = is_mlp\n            module.rank_magnitude.requires_grad = is_mlp\n        module.weight.requires_grad = False\n\nmlp_trainable = sum(p.numel() for p in v2_model.parameters() if p.requires_grad)\nprint(f'MLP-only trainable params: {mlp_trainable:,}')\n\n# Pass 2: MLP-only refinement with lower LR\nprint('\\n=== PASS 2: MLP-Only Refinement ===')\nmlp_result = train_e2e(\n    model=v2_model,\n    cache_dir=cache_local_path,\n    device=DEVICE,\n    max_steps=1000,  # Second pass: 1000 steps\n    batch_size=BATCH_SIZE,\n    lr=5e-5,  # Lower LR for refinement\n    use_cosine_schedule=True,\n    warmup_steps=50,\n    min_lr_ratio=0.1,\n    temperature=DISTILL_TEMP,\n    train_weights=False,\n    train_scales=True,\n    hard_top1_weight=0.0,\n    hard_full_weight=0.0,\n    logging_steps=20,\n    eval_steps=100,\n    verbose=True,\n    train_mlp_only=True,  # MLP only!\n    use_fp16=USE_FP16,\n)\n\nprint(f'\\nPass 2 complete! Final loss: {mlp_result[\"final_loss\"]:.4f}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# EXPORT TO FP16 (ANE-READY)\n# ============================================================\n\nimport os\nimport json\n\nV2_RUN_NAME = f'anemll_v2_{QUAL}_from_v1'\nV2_SAVE_DIR = f'{LOCAL_RUNS}/{V2_RUN_NAME}'\nos.makedirs(V2_SAVE_DIR, exist_ok=True)\n\n# Convert to FP16 for ANE export\n# Training was in FP32 (master weights), now we snap to FP16\nprint('Converting model to FP16 for ANE export...')\nv2_model.half()  # Convert all params to FP16\n\n# Disable STE (not needed in FP16)\nfor name, m in v2_model.named_modules():\n    if type(m).__name__ == 'AnemllQATLinearV2':\n        m.config.use_ste_fp16 = False\n\n# Save checkpoint\ntorch.save(v2_model.state_dict(), f'{V2_SAVE_DIR}/model_state_dict.pt')\n\n# Get final loss from whichever pass was run\nfinal_loss_value = mlp_result['final_loss'] if 'mlp_result' in dir() else e2e_result.get('final_loss', 0.0) if 'e2e_result' in dir() else 0.0\n\n# Save config with ALL Q2_A4 parameters\nconfig = {\n    'model_id': MODEL_ID,\n    'version': 'v2',\n    'precision': 'fp16',\n    'training': 'ste_fp16',\n    # MLP config\n    'mlp_lut_bits': LUT_BITS,\n    'mlp_lut_size': LUT_SIZE,\n    'mlp_scale_rank': SCALE_RANK,\n    # Attention config\n    'attn_lut_bits': ATTN_LUT_BITS,\n    'attn_lut_size': ATTN_LUT_SIZE,\n    'attn_scale_rank': ATTN_SCALE_RANK,\n    # Losses\n    'v1_loss': v1_loss if 'v1_loss' in dir() else 0.0,\n    'v2_converted_loss': v2_converted_loss if 'v2_converted_loss' in dir() else 0.0,\n    'final_loss': final_loss_value,\n}\nwith open(f'{V2_SAVE_DIR}/config.json', 'w') as f:\n    json.dump(config, f, indent=2)\n\n# Also save to tmp for quick access\ntorch.save(v2_model.state_dict(), '/tmp/v2_q2a4_ste_fp16.pt')\n\nprint(f'Saved FP16 V2 checkpoint to {V2_SAVE_DIR}')\nprint(f'Also saved to /tmp/v2_q2a4_ste_fp16.pt')\nprint(f'Final loss: {final_loss_value:.4f}')\nprint(f'ANE-ready - trained with STE-FP16, exported to FP16!')\nprint(f'\\nConfig: MLP={LUT_BITS}b/r{SCALE_RANK}, Attn={ATTN_LUT_BITS}b/r{ATTN_SCALE_RANK}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# FINAL EVALUATION\n# ============================================================\n\nv2_model.eval()\nfinal_loss = evaluate_kd_loss(v2_model, cache_local_path, DEVICE, num_samples=50, temperature=DISTILL_TEMP)\n\nprint(f'\\n=== STE-FP16 Training Results ===')\nprint(f'V1 Original:           {v1_loss:.4f}')\nprint(f'V2 After Convert:      {v2_converted_loss:.4f}')\nprint(f'V2 After STE-FP16 Train: {final_loss:.4f}')\nprint(f'Total Improvement:     {v1_loss - final_loss:.4f}')\nprint(f'\\nModel exported to FP16 - ANE-ready!')\nprint(f'Training used STE-FP16: FP32 master weights + FP16 forward simulation')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# UPLOAD TO GOOGLE DRIVE\n",
    "# ============================================================\n",
    "\n",
    "!tar -czvf {V2_RUN_NAME}.tgz -C {LOCAL_RUNS} {V2_RUN_NAME}\n",
    "!cp {V2_RUN_NAME}.tgz {GD_RUNS}/\n",
    "print(f'\\nUploaded to {GD_RUNS}/{V2_RUN_NAME}.tgz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Load V2 Checkpoint (for inference from saved model)\n\nThis section shows how to properly load a V2 checkpoint that includes `_Q` and `_indices`.\nUse `load_v2_checkpoint()` instead of `model.load_state_dict()` to ensure all buffers are loaded correctly."
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# LOAD V2 CHECKPOINT (for inference from saved model)\n# ============================================================\n# Use this cell to load a previously saved V2 checkpoint.\n# The load_v2_checkpoint() function properly handles _Q and _indices buffers.\n\nfrom qat_lora import (\n    AnemllQuantConfigV2,\n    replace_linear_with_anemll_v2,\n    load_v2_checkpoint,\n    freeze_model_for_inference_v2,\n)\n\n# Path to saved V2 checkpoint\nV2_CHECKPOINT = f'{LOCAL_RUNS}/{V2_RUN_NAME}/model_state_dict.pt'\n# Or load from a specific path:\n# V2_CHECKPOINT = '/path/to/anemll_v2_q2_a4_ste_fp16_from_v1/model_state_dict.pt'\n\n# Load fresh base model\nprint(f'Loading base model for inference...')\ninference_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    torch_dtype=torch.float16,\n    trust_remote_code=True,\n)\n\n# Create V2 configs (must match training config - Q2_A4)\nv2_mlp_config = AnemllQuantConfigV2(\n    lut_size=LUT_SIZE,  # 4 (2-bit)\n    scale_rank=SCALE_RANK,  # 32\n    force_positive_scales=False,\n    magnitude_activation='identity',\n    use_ste_fp16=False,\n)\nv2_attn_config = AnemllQuantConfigV2(\n    lut_size=ATTN_LUT_SIZE,  # 16 (4-bit)\n    scale_rank=ATTN_SCALE_RANK,  # 8\n    force_positive_scales=False,\n    magnitude_activation='identity',\n    use_ste_fp16=False,\n)\n\n# Replace with V2 layers\nprint('Replacing with V2 layers...')\nprint(f'  MLP: {LUT_SIZE} LUT, rank={SCALE_RANK}')\nprint(f'  Attn: {ATTN_LUT_SIZE} LUT, rank={ATTN_SCALE_RANK}')\nreplace_linear_with_anemll_v2(\n    inference_model,\n    mlp_config=v2_mlp_config,\n    attn_config=v2_attn_config,\n    quantize_attn=True,\n)\n\n# Load checkpoint with proper _Q and _indices handling\nprint(f'\\nLoading V2 checkpoint from {V2_CHECKPOINT}...')\nstats = load_v2_checkpoint(\n    inference_model,\n    V2_CHECKPOINT,\n    device=DEVICE,\n    verbose=True,\n)\n\n# Freeze for inference (should be no-op if _Q already loaded)\nfreeze_model_for_inference_v2(inference_model, verbose=False)\nprint('\\nModel ready for inference!')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FREEZE FOR INFERENCE\n",
    "# ============================================================\n",
    "\n",
    "from qat_lora import freeze_model_for_inference_v2\n",
    "\n",
    "print('Freezing V2 model for inference...')\n",
    "freeze_model_for_inference_v2(v2_model, verbose=False)\n",
    "print('Ready for inference!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# TEST INFERENCE\n# ============================================================\n\ndef run_inference(model, tokenizer, prompt, max_new_tokens=256):\n    messages = [\n        {'role': 'system', 'content': 'You are a helpful assistant.'},\n        {'role': 'user', 'content': prompt}\n    ]\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = tokenizer(text, return_tensors='pt').to(DEVICE)\n    \n    with torch.no_grad():\n        output = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n    \n    return tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n\nprompts = [\n    'What is the capital of France?',\n    'Explain quantum mechanics briefly.',\n    'What is 2+2?',\n    'What is Apple Neural Engine',\n    'What is History of Alibaba Group',\n]\n\nv2_model.eval()\nfor prompt in prompts:\n    response = run_inference(v2_model, tokenizer, prompt)\n    print(f'Prompt: {prompt}')\n    print(f'Response: {response}')\n    print('-' * 50)"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}