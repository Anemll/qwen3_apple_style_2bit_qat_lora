{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate KD Cache: WikiText-103, L=1024 (Raw Text)\n",
    "\n",
    "**Purpose:** Anti-repetition + long-context coherence training\n",
    "\n",
    "This notebook generates a KD cache using **Qwen3-32B** as teacher on **WikiText-103** raw text.\n",
    "\n",
    "**Key settings:**\n",
    "- `template_mode=none` (raw text, no chat template)\n",
    "- `L=1024` (long sequences for coherence)\n",
    "- Clean Wikipedia text (reduces repetition patterns)\n",
    "\n",
    "**Requirements:**\n",
    "- GPU with 80GB+ VRAM (A100 80GB, H100)\n",
    "- Or 40GB+ with 4-bit quantization (batch_size=1)\n",
    "\n",
    "**Estimated storage:** ~15-25GB for 8K sequences at L=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CHECK GPU\n",
    "# ============================================================\n",
    "\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# SETUP: Clone repo and install dependencies\n# ============================================================\n\n!git clone https://github.com/anemll/qwen3_apple_style_2bit_qat_lora.git 2>/dev/null || echo \"Repo already exists\"\n%cd qwen3_apple_style_2bit_qat_lora\n!git pull origin main\n\n!pip install -q transformers accelerate datasets sentencepiece"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CONFIG - WikiText-103, L=1024, Raw Text\n# ============================================================\n\n# Teacher model - Qwen3-32B (or 0.6B for cheaper runs)\nMODEL_NAME = 'Qwen/Qwen3-32B'\n# MODEL_NAME = 'Qwen/Qwen3-0.6B'  # Cheaper alternative\n\n# Dataset - WikiText-103 for clean long-form text\nDATASET_NAME = 'Salesforce/wikitext'\nDATASET_SUBSET = 'wikitext-103-v1'\nDATASET_SPLIT = 'train'\nDATASET_FORMAT = 'plain'  # Raw text, no chat template\n\n# Cache parameters - L=1024 for long-context coherence\nTOP_K = 128           # Number of top-k logits to cache\nRAND_NEG = 64         # Random negatives\nMAX_LENGTH = 1024     # Long sequences for coherence training\nNUM_SEQUENCES = 8000  # Start with 8K (~8.2M tokens)\nSHARD_SIZE = 100      # Smaller shards for L=1024 (larger tensors)\n\n# Batch size for 80GB A100 with 32B @ L=1024\n# Tested: batch=16 uses ~75/80GB VRAM\nBATCH_SIZE = 16\n\n# No thinking variants - raw text only\nENABLE_THINKING = 'none'\n\n# Output cache name\nCACHE_NAME = f\"wikitext103_32B_L{MAX_LENGTH}_K{TOP_K}_R{RAND_NEG}_N{NUM_SEQUENCES}\"\nCACHE_DIR = f\"caches/{CACHE_NAME}\"\n\nprint(f\"=\"*60)\nprint(f\"KD Cache Configuration: WikiText-103 L=1024\")\nprint(f\"=\"*60)\nprint(f\"Teacher model:    {MODEL_NAME}\")\nprint(f\"Dataset:          {DATASET_NAME} / {DATASET_SUBSET}\")\nprint(f\"Format:           {DATASET_FORMAT} (raw text, no template)\")\nprint(f\"Top-K:            {TOP_K}\")\nprint(f\"Random negatives: {RAND_NEG}\")\nprint(f\"Max length:       {MAX_LENGTH}\")\nprint(f\"Sequences:        {NUM_SEQUENCES} (~{NUM_SEQUENCES * MAX_LENGTH / 1e6:.1f}M tokens)\")\nprint(f\"Batch size:       {BATCH_SIZE} (~75GB VRAM on A100-80GB)\")\nprint(f\"Shard size:       {SHARD_SIZE}\")\nprint(f\"Output:           {CACHE_DIR}\")\nprint(f\"=\"*60)\nprint(f\"\\nEstimated storage: ~{NUM_SEQUENCES * MAX_LENGTH * TOP_K * 4 / 1e9:.1f}GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# OPTIONAL: Mount Google Drive for saving cache\n",
    "# ============================================================\n",
    "\n",
    "SAVE_TO_DRIVE = True  # Set to False if not using Colab/Drive\n",
    "\n",
    "if SAVE_TO_DRIVE:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        DRIVE_CACHE_DIR = '/content/drive/MyDrive/qwen3_caches'\n",
    "        !mkdir -p {DRIVE_CACHE_DIR}\n",
    "        print(f\"Google Drive mounted. Will save to: {DRIVE_CACHE_DIR}\")\n",
    "    except:\n",
    "        print(\"Not running in Colab or Drive mount failed. Saving locally only.\")\n",
    "        SAVE_TO_DRIVE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# GENERATE KD CACHE\n# ============================================================\n# WikiText-103 with L=1024 for long-context coherence\n#\n# Tested on Colab A100 80GB:\n#   - 32B, batch=16: ~60 min for 8K sequences (~42s/shard)\n# ============================================================\n\n%cd /content/qwen3_apple_style_2bit_qat_lora\n\n!python scripts/precompute_teacher_topk.py \\\n    --teacher_model_name_or_path {MODEL_NAME} \\\n    --dataset_name {DATASET_NAME} \\\n    --dataset_config_name {DATASET_SUBSET} \\\n    --dataset_split {DATASET_SPLIT} \\\n    --dataset_format {DATASET_FORMAT} \\\n    --enable_thinking {ENABLE_THINKING} \\\n    --max_length {MAX_LENGTH} \\\n    --topk {TOP_K} \\\n    --rand_neg {RAND_NEG} \\\n    --num_sequences {NUM_SEQUENCES} \\\n    --output_dir {CACHE_DIR} \\\n    --batch_size {BATCH_SIZE} \\\n    --shard_size {SHARD_SIZE} \\\n    --dtype bf16 \\\n    --device auto"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VERIFY CACHE\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "if os.path.isdir(CACHE_DIR):\n",
    "    # Count shards\n",
    "    shards = [f for f in os.listdir(CACHE_DIR) if f.startswith('shard_')]\n",
    "    print(f\"[cache] Generated {len(shards)} shards\")\n",
    "    \n",
    "    # Check meta.json\n",
    "    meta_path = os.path.join(CACHE_DIR, 'meta.json')\n",
    "    if os.path.exists(meta_path):\n",
    "        with open(meta_path) as f:\n",
    "            meta = json.load(f)\n",
    "        print(f\"[cache] Meta info:\")\n",
    "        for k, v in meta.items():\n",
    "            print(f\"  - {k}: {v}\")\n",
    "    \n",
    "    # Calculate size\n",
    "    total_size = sum(os.path.getsize(os.path.join(CACHE_DIR, f)) for f in os.listdir(CACHE_DIR))\n",
    "    print(f\"[cache] Total size: {total_size / (1024**3):.2f} GB\")\n",
    "else:\n",
    "    print(f\"[cache] ERROR: {CACHE_DIR} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INSPECT A SAMPLE SHARD\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "shard_files = sorted([f for f in os.listdir(CACHE_DIR) if f.startswith('shard_')])\n",
    "if shard_files:\n",
    "    shard_path = os.path.join(CACHE_DIR, shard_files[0])\n",
    "    shard = torch.load(shard_path, map_location='cpu')\n",
    "    \n",
    "    print(f\"Shard: {shard_files[0]}\")\n",
    "    print(f\"Keys: {list(shard.keys())}\")\n",
    "    \n",
    "    # Expected shapes for L=1024, K=128:\n",
    "    # input_ids: [SHARD_SIZE, 1024]\n",
    "    # topk_indices: [SHARD_SIZE, 1024, 128]\n",
    "    # topk_probs: [SHARD_SIZE, 1024, 128]\n",
    "    \n",
    "    if 'input_ids' in shard:\n",
    "        print(f\"\\ninput_ids shape: {shard['input_ids'].shape}  (expected: [{SHARD_SIZE}, {MAX_LENGTH}])\")\n",
    "    if 'topk_indices' in shard:\n",
    "        print(f\"topk_indices shape: {shard['topk_indices'].shape}  (expected: [{SHARD_SIZE}, {MAX_LENGTH}, {TOP_K}])\")\n",
    "    if 'topk_probs' in shard:\n",
    "        print(f\"topk_probs shape: {shard['topk_probs'].shape}  (expected: [{SHARD_SIZE}, {MAX_LENGTH}, {TOP_K}])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE TO GOOGLE DRIVE\n",
    "# ============================================================\n",
    "\n",
    "if SAVE_TO_DRIVE and os.path.isdir(CACHE_DIR):\n",
    "    print(f\"[save] Copying {CACHE_NAME} to Google Drive...\")\n",
    "    !rsync -ah --info=progress2 {CACHE_DIR}/ {DRIVE_CACHE_DIR}/{CACHE_NAME}/\n",
    "    \n",
    "    # Verify\n",
    "    gd_path = f\"{DRIVE_CACHE_DIR}/{CACHE_NAME}\"\n",
    "    if os.path.isdir(gd_path):\n",
    "        num_files = len(os.listdir(gd_path))\n",
    "        print(f\"[save] Successfully saved to Google Drive: {num_files} files\")\n",
    "    else:\n",
    "        print(f\"[save] ERROR: Failed to copy to Google Drive\")\n",
    "else:\n",
    "    print(\"[save] Skipping Google Drive save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Batch Size Reference (Colab A100 80GB)\n\n32B @ L=1024 in bf16:\n- **batch_size=16**: ~75/80GB VRAM (tested on Colab A100 High-Memory)\n\n### For different GPUs:\n\n| GPU | VRAM | Recommended batch_size |\n|-----|------|------------------------|\n| Colab A100 80GB | 80GB | 16 |\n| A100 40GB | 40GB | 4-8 |\n| Colab T4 | 16GB | Use 0.6B teacher |\n\n### If you get OOM errors:\n\n```python\nBATCH_SIZE = 8   # Try halving\n# or\nMODEL_NAME = 'Qwen/Qwen3-0.6B'  # Smaller teacher\nBATCH_SIZE = 32\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Cache purpose:** Anti-repetition + long-context coherence\n",
    "\n",
    "**Cache generated:** `wikitext103_32B_L1024_K128_R64_N8000`\n",
    "\n",
    "**Contains:**\n",
    "- 8K sequences x 1024 tokens = ~8.2M tokens of teacher supervision\n",
    "- Top-128 teacher logits from Qwen3-32B\n",
    "- Raw text format (no chat template, no thinking tags)\n",
    "- Clean Wikipedia text for coherence patterns\n",
    "\n",
    "**To use in LoRA recovery training:**\n",
    "```bash\n",
    "python scripts/train_recovery_lora.py \\\n",
    "    --kd-cache-dir caches/wikitext103_32B_L1024_K128_R64_N8000 \\\n",
    "    --seq-len 1024 \\\n",
    "    --lora-mode kd \\\n",
    "    ...\n",
    "```\n",
    "\n",
    "**Load from Google Drive:**\n",
    "```bash\n",
    "rsync -ah --info=progress2 /content/drive/MyDrive/qwen3_caches/wikitext103_32B_L1024_K128_R64_N8000/ caches/wikitext103_32B_L1024_K128_R64_N8000/\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}