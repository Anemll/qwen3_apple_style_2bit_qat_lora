{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchAO Per-Tensor 4-bit Quantization for Qwen3-0.6B\n",
    "\n",
    "This notebook demonstrates **per-tensor 4-bit quantization** using TorchAO's low-level primitives.\n",
    "\n",
    "**Key Challenge**: TorchAO's `Int4WeightOnlyConfig` only supports per-group quantization (group_size=32/64/128/256).\n",
    "For true per-tensor quantization (single scale per weight matrix), we use TorchAO's `quantize_affine`/`dequantize_affine` primitives.\n",
    "\n",
    "**Contents**:\n",
    "- PTQ (Post-Training Quantization): Apply int4 per-tensor quantization to pretrained model\n",
    "- QAT (Quantization-Aware Training): Train with fake quantization + STE\n",
    "- Inference with quantized model\n",
    "\n",
    "**Per-Tensor vs Per-Group**:\n",
    "| Aspect | Per-Tensor | Per-Group (TorchAO default) |\n",
    "|--------|-----------|---------------------------|\n",
    "| Scale count | 1 per weight | out_features Ã— (in_features/group_size) |\n",
    "| Accuracy | Lower | Higher |\n",
    "| Simplicity | Higher | Lower |\n",
    "| Hardware compat | Broader | Specific (tinygemm) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Config (edit these) ----\n",
    "MODEL_NAME = 'Qwen/Qwen3-0.6B'\n",
    "DEVICE = 'auto'  # 'cuda', 'mps', 'cpu', or 'auto'\n",
    "SYMMETRIC = True  # Symmetric quantization (recommended for weights)\n",
    "SKIP_LM_HEAD = True  # Skip quantizing the language model head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TorchAO and dependencies\n",
    "!pip install -q torchao>=0.7.0 transformers>=4.51.0 accelerate datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Imports & Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom tqdm.auto import tqdm\nimport math\n\n# TorchAO primitives (optional - we have manual fallback)\nTORCHAO_AVAILABLE = False\ntry:\n    from torchao.quantization.quant_primitives import (\n        quantize_affine,\n        dequantize_affine,\n        choose_qparams_affine,\n        MappingType,\n        ZeroPointDomain,\n    )\n    TORCHAO_AVAILABLE = True\n    print(\"TorchAO available - can use either TorchAO or manual quantization\")\nexcept ImportError:\n    print(\"TorchAO not available - using manual quantization implementation\")\n\nprint(f'torch: {torch.__version__}')\nprint(f'cuda: {torch.cuda.is_available()}')\nprint(f'mps: {torch.backends.mps.is_available()}')\n\n# Auto device selection\ndef get_device(device_str='auto'):\n    if device_str == 'auto':\n        if torch.cuda.is_available():\n            return torch.device('cuda')\n        elif torch.backends.mps.is_available():\n            return torch.device('mps')\n        return torch.device('cpu')\n    return torch.device(device_str)\n\ndevice = get_device(DEVICE)\nprint(f'Using device: {device}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3) Per-Tensor Int4 Quantization Functions\n\nManual implementation of per-tensor int4 quantization.\n\n**Why manual?** TorchAO's `quantize_affine`/`dequantize_affine` with large `block_size` can have issues. Our manual implementation is simpler and more robust.\n\n**Formula**:\n- Symmetric: `scale = max(|W|) / 7`, `q = round(W / scale)`, `W_dq = q * scale`\n- Asymmetric: `scale = (max - min) / 15`, `zp = round(-min/scale - 8)`, `q = round(W/scale) + zp`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Int4 quantization parameters\nQUANT_MIN = -8  # 4-bit signed min\nQUANT_MAX = 7   # 4-bit signed max\n\ndef quantize_per_tensor_int4_manual(weight: torch.Tensor, symmetric: bool = True):\n    \"\"\"\n    Quantize weight to int4 with per-tensor scale (manual implementation).\n    \n    This is a simpler, more robust implementation that doesn't rely on \n    TorchAO's block_size semantics which may not work for full-tensor sizes.\n    \n    Args:\n        weight: Float weight tensor of shape (out_features, in_features)\n        symmetric: Use symmetric quantization (zero_point = 0)\n    \n    Returns:\n        weight_q: Quantized weight (int8 storage, int4 values)\n        scale: Per-tensor scale factor (scalar)\n        zero_point: Per-tensor zero point (scalar, 0 for symmetric)\n    \"\"\"\n    # Compute scale for per-tensor quantization\n    if symmetric:\n        # Symmetric: scale = max(|w|) / qmax\n        w_abs_max = weight.abs().max().clamp(min=1e-8)\n        scale = w_abs_max / QUANT_MAX\n        zero_point = torch.tensor(0, dtype=torch.int32, device=weight.device)\n    else:\n        # Asymmetric: scale = (max - min) / (qmax - qmin)\n        w_min = weight.min()\n        w_max = weight.max()\n        scale = (w_max - w_min).clamp(min=1e-8) / (QUANT_MAX - QUANT_MIN)\n        zero_point = torch.round(-w_min / scale + QUANT_MIN).to(torch.int32)\n    \n    # Quantize: q = round(w / scale) + zp, clamped to [qmin, qmax]\n    if symmetric:\n        weight_q = torch.round(weight / scale).clamp(QUANT_MIN, QUANT_MAX).to(torch.int8)\n    else:\n        weight_q = (torch.round(weight / scale) + zero_point).clamp(QUANT_MIN, QUANT_MAX).to(torch.int8)\n    \n    return weight_q, scale, zero_point\n\n\ndef dequantize_per_tensor_int4_manual(\n    weight_q: torch.Tensor,\n    scale: torch.Tensor,\n    zero_point: torch.Tensor,\n    output_dtype: torch.dtype = torch.float32,\n):\n    \"\"\"\n    Dequantize int4 weight back to float (manual implementation).\n    \n    Args:\n        weight_q: Quantized weight tensor (int8)\n        scale: Per-tensor scale (scalar)\n        zero_point: Per-tensor zero point (scalar)\n        output_dtype: Output float dtype\n    \n    Returns:\n        Dequantized float weight\n    \"\"\"\n    # Dequantize: w = (q - zp) * scale\n    weight_dq = (weight_q.to(output_dtype) - zero_point.to(output_dtype)) * scale.to(output_dtype)\n    return weight_dq\n\n\n# Also try TorchAO primitives with different approach\ndef quantize_per_tensor_int4_torchao(weight: torch.Tensor, symmetric: bool = True):\n    \"\"\"\n    Quantize using TorchAO primitives with per-tensor granularity.\n    Uses block_size=(1, 1) and broadcasts, then takes mean for single scale.\n    \"\"\"\n    from torchao.quantization.quant_primitives import (\n        choose_qparams_affine,\n        quantize_affine,\n        MappingType,\n    )\n    \n    # For per-tensor, we compute our own scale and use quantize_affine\n    mapping = MappingType.SYMMETRIC if symmetric else MappingType.ASYMMETRIC\n    \n    if symmetric:\n        w_abs_max = weight.abs().max().clamp(min=1e-8)\n        scale = (w_abs_max / QUANT_MAX).reshape(1, 1)\n        zero_point = torch.zeros(1, 1, dtype=torch.int32, device=weight.device)\n    else:\n        w_min = weight.min()\n        w_max = weight.max()\n        scale = ((w_max - w_min).clamp(min=1e-8) / (QUANT_MAX - QUANT_MIN)).reshape(1, 1)\n        zero_point = torch.round(-w_min / scale + QUANT_MIN).to(torch.int32).reshape(1, 1)\n    \n    # Use block_size that matches scale shape for broadcasting\n    block_size = weight.shape  # Full tensor\n    \n    weight_q = quantize_affine(\n        weight,\n        block_size,\n        scale,\n        zero_point,\n        torch.int8,\n        QUANT_MIN,\n        QUANT_MAX,\n    )\n    \n    return weight_q, scale.squeeze(), zero_point.squeeze()\n\n\n# Use manual implementation (more robust)\nquantize_per_tensor_int4 = quantize_per_tensor_int4_manual\ndequantize_per_tensor_int4 = dequantize_per_tensor_int4_manual\n\n\n# Test the functions\nprint(\"Testing quantization functions...\")\ntest_weight = torch.randn(64, 128)\nw_q, scale, zp = quantize_per_tensor_int4(test_weight, symmetric=SYMMETRIC)\nw_dq = dequantize_per_tensor_int4(w_q, scale, zp)\n\nprint(f\"Original shape: {test_weight.shape}, dtype: {test_weight.dtype}\")\nprint(f\"Quantized shape: {w_q.shape}, dtype: {w_q.dtype}\")\nprint(f\"Scale: {scale.item():.6f} (shape: {scale.shape})\")\nprint(f\"Zero point: {zp.item()} (shape: {zp.shape})\")\nprint(f\"Quantized range: [{w_q.min().item()}, {w_q.max().item()}]\")\nprint(f\"Dequantized range: [{w_dq.min().item():.4f}, {w_dq.max().item():.4f}]\")\nprint(f\"Original range: [{test_weight.min().item():.4f}, {test_weight.max().item():.4f}]\")\nprint(f\"Reconstruction MSE: {F.mse_loss(test_weight, w_dq).item():.6f}\")\n\n# Verify the quantization is working correctly\nprint(f\"\\n--- Sanity Check ---\")\nprint(f\"Scale * QUANT_MAX = {(scale * QUANT_MAX).item():.4f}\")\nprint(f\"Original abs max = {test_weight.abs().max().item():.4f}\")\nprint(f\"These should be approximately equal for symmetric quantization\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Fake Quantize for QAT (with STE)\n",
    "\n",
    "For Quantization-Aware Training, we need a differentiable fake quantize operation.\n",
    "The Straight-Through Estimator (STE) passes gradients through the quantization unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class PerTensorInt4FakeQuantize(torch.autograd.Function):\n    \"\"\"\n    Fake quantize with Straight-Through Estimator (STE).\n    \n    Forward: quantize -> dequantize (simulates quantization effect)\n    Backward: pass gradients through unchanged (STE)\n    \"\"\"\n    \n    @staticmethod\n    def forward(ctx, weight, symmetric=True):\n        # Quantize and immediately dequantize using our manual implementation\n        w_q, scale, zp = quantize_per_tensor_int4(weight.detach(), symmetric)\n        w_dq = dequantize_per_tensor_int4(w_q, scale, zp, output_dtype=weight.dtype)\n        \n        # Save for potential gradient clipping (optional)\n        ctx.save_for_backward(weight)\n        \n        return w_dq\n    \n    @staticmethod\n    def backward(ctx, grad_output):\n        # STE: pass gradients through unchanged\n        return grad_output, None\n\n\ndef fake_quant_int4(weight: torch.Tensor, symmetric: bool = True) -> torch.Tensor:\n    \"\"\"\n    Apply fake int4 quantization with STE.\n    \n    Args:\n        weight: Float weight tensor\n        symmetric: Use symmetric quantization\n    \n    Returns:\n        Fake-quantized weight (same dtype as input)\n    \"\"\"\n    return PerTensorInt4FakeQuantize.apply(weight, symmetric)\n\n\n# Test gradient flow\nprint(\"Testing STE gradient flow...\")\ntest_w = torch.randn(32, 64, requires_grad=True)\nw_fq = fake_quant_int4(test_w)\nloss = w_fq.sum()\nloss.backward()\nprint(f\"Gradient exists: {test_w.grad is not None}\")\nprint(f\"Gradient shape: {test_w.grad.shape}\")\nprint(f\"Gradient mean: {test_w.grad.mean().item():.4f} (should be ~1.0 for STE)\")\n\n# Verify the fake quantization output\nprint(f\"\\n--- Fake Quantization Sanity Check ---\")\nprint(f\"Input dtype: {test_w.dtype}\")\nprint(f\"Output dtype: {w_fq.dtype}\")\nprint(f\"Input range: [{test_w.min().item():.4f}, {test_w.max().item():.4f}]\")\nprint(f\"Output range: [{w_fq.min().item():.4f}, {w_fq.max().item():.4f}]\")\nprint(f\"MSE between input and fake-quantized: {F.mse_loss(test_w.detach(), w_fq.detach()).item():.6f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Int4Linear Module (for QAT)\n",
    "\n",
    "A drop-in replacement for `nn.Linear` that applies per-tensor int4 fake quantization during forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Int4Linear(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer with per-tensor int4 fake quantization.\n",
    "    \n",
    "    During training (QAT): applies fake quantization with STE\n",
    "    During inference: uses fake-quantized weights (or can be converted to real int4)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        bias: bool = True,\n",
    "        symmetric: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.symmetric = symmetric\n",
    "        \n",
    "        # Full-precision weights (updated by optimizer)\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.empty(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in = self.in_features\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Apply fake quantization to weights\n",
    "        w_q = fake_quant_int4(self.weight, self.symmetric)\n",
    "        return F.linear(x, w_q, self.bias)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_linear(cls, linear: nn.Linear, symmetric: bool = True) -> 'Int4Linear':\n",
    "        \"\"\"Create Int4Linear from existing nn.Linear.\"\"\"\n",
    "        int4_linear = cls(\n",
    "            linear.in_features,\n",
    "            linear.out_features,\n",
    "            bias=linear.bias is not None,\n",
    "            symmetric=symmetric,\n",
    "        )\n",
    "        int4_linear.weight.data = linear.weight.data.clone()\n",
    "        if linear.bias is not None:\n",
    "            int4_linear.bias.data = linear.bias.data.clone()\n",
    "        return int4_linear\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return f'in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}, symmetric={self.symmetric}'\n",
    "\n",
    "\n",
    "# Test Int4Linear\n",
    "print(\"Testing Int4Linear...\")\n",
    "linear = nn.Linear(128, 64)\n",
    "int4_linear = Int4Linear.from_linear(linear, symmetric=SYMMETRIC)\n",
    "x = torch.randn(2, 128)\n",
    "y = int4_linear(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(int4_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Replace Linear Layers with Int4Linear\n",
    "\n",
    "Utility function to replace all `nn.Linear` layers in a model with `Int4Linear` for QAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_with_int4(\n",
    "    model: nn.Module,\n",
    "    skip_patterns: list = None,\n",
    "    symmetric: bool = True,\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Replace nn.Linear layers with Int4Linear for QAT.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        skip_patterns: List of name patterns to skip (e.g., ['lm_head'])\n",
    "        symmetric: Use symmetric quantization\n",
    "    \n",
    "    Returns:\n",
    "        Modified model (in-place)\n",
    "    \"\"\"\n",
    "    skip_patterns = skip_patterns or []\n",
    "    replaced_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    # Collect modules to replace (can't modify during iteration)\n",
    "    replacements = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Check skip patterns\n",
    "            if any(pattern in name for pattern in skip_patterns):\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            replacements.append((name, module))\n",
    "    \n",
    "    # Perform replacements\n",
    "    for name, module in replacements:\n",
    "        # Navigate to parent module\n",
    "        parts = name.split('.')\n",
    "        parent = model\n",
    "        for part in parts[:-1]:\n",
    "            parent = getattr(parent, part)\n",
    "        \n",
    "        # Replace\n",
    "        int4_linear = Int4Linear.from_linear(module, symmetric=symmetric)\n",
    "        setattr(parent, parts[-1], int4_linear)\n",
    "        replaced_count += 1\n",
    "    \n",
    "    print(f\"Replaced {replaced_count} Linear layers with Int4Linear\")\n",
    "    print(f\"Skipped {skipped_count} layers (patterns: {skip_patterns})\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def count_parameters(model: nn.Module) -> dict:\n",
    "    \"\"\"Count model parameters.\"\"\"\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return {'total': total, 'trainable': trainable}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Load Qwen3-0.6B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"Loading model: {MODEL_NAME}\")\n\n# Load in float32 for quantization\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=torch.float32,\n    trust_remote_code=True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n\n# Ensure pad token\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nparams = count_parameters(model)\nprint(f\"Model loaded: {params['total']:,} parameters\")\nprint(f\"Model dtype: {next(model.parameters()).dtype}\")\n\n# Check baseline perplexity BEFORE quantization\nprint(\"\\n--- Baseline check (before quantization) ---\")\nsample_text_short = \"The quick brown fox jumps over the lazy dog.\"\nmodel.eval()\nmodel.to(device)\nwith torch.no_grad():\n    inputs = tokenizer(sample_text_short, return_tensors='pt').to(device)\n    outputs = model(inputs['input_ids'], labels=inputs['input_ids'])\n    baseline_loss = outputs.loss.item()\n    baseline_ppl = torch.exp(outputs.loss).item()\nprint(f\"Baseline loss: {baseline_loss:.4f}\")\nprint(f\"Baseline perplexity: {baseline_ppl:.2f}\")\nprint(f\"Logits range: [{outputs.logits.min().item():.2f}, {outputs.logits.max().item():.2f}]\")\n\n# Move back to CPU for quantization\nmodel.to('cpu')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) PTQ: Apply Per-Tensor Int4 Quantization\n",
    "\n",
    "Post-Training Quantization: quantize weights and measure reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def apply_ptq_int4(\n    model: nn.Module,\n    skip_patterns: list = None,\n    symmetric: bool = True,\n) -> tuple:\n    \"\"\"\n    Apply PTQ int4 quantization to model weights.\n    \"\"\"\n    skip_patterns = skip_patterns or []\n    quantized_weights = {}\n    total_mse = 0.0\n    num_quantized = 0\n    \n    with torch.no_grad():\n        for name, param in tqdm(list(model.named_parameters()), desc=\"Quantizing\"):\n            # Only quantize 2D weight matrices\n            if 'weight' not in name or param.dim() != 2:\n                continue\n            \n            # Check skip patterns\n            if any(pattern in name for pattern in skip_patterns):\n                continue\n            \n            # Store original for comparison\n            original = param.data.clone()\n            \n            # Quantize\n            w_q, scale, zp = quantize_per_tensor_int4(param.data, symmetric)\n            w_dq = dequantize_per_tensor_int4(w_q, scale, zp, output_dtype=param.dtype)\n            \n            # Debug: Check for NaN/Inf\n            if torch.isnan(w_dq).any() or torch.isinf(w_dq).any():\n                print(f\"WARNING: NaN/Inf in {name}!\")\n                print(f\"  scale: {scale}, zp: {zp}\")\n                print(f\"  w_q range: [{w_q.min()}, {w_q.max()}]\")\n                continue\n            \n            # Calculate MSE\n            mse = F.mse_loss(original, w_dq).item()\n            total_mse += mse\n            num_quantized += 1\n            \n            # Store quantized info\n            quantized_weights[name] = {\n                'weight_int4': w_q.cpu(),\n                'scale': scale.cpu() if hasattr(scale, 'cpu') else scale,\n                'zero_point': zp.cpu() if hasattr(zp, 'cpu') else zp,\n                'mse': mse,\n            }\n            \n            # Replace weight with dequantized version\n            param.data.copy_(w_dq)\n    \n    avg_mse = total_mse / num_quantized if num_quantized > 0 else 0\n    print(f\"\\nQuantized {num_quantized} weight tensors\")\n    print(f\"Average reconstruction MSE: {avg_mse:.6f}\")\n    \n    return model, quantized_weights\n\n\n# Apply PTQ\nskip = ['lm_head'] if SKIP_LM_HEAD else []\nmodel_ptq, quant_weights = apply_ptq_int4(model, skip_patterns=skip, symmetric=SYMMETRIC)\n\n# Show per-layer MSE for first few layers\nprint(\"\\nPer-layer MSE (first 10):\")\nfor i, (name, info) in enumerate(list(quant_weights.items())[:10]):\n    print(f\"  {name}: MSE={info['mse']:.6f}\")\n\n# Debug: Check model weights after PTQ\nprint(\"\\n--- Debug: Checking model weights after PTQ ---\")\nfor name, param in list(model_ptq.named_parameters())[:5]:\n    if param.dim() == 2:\n        print(f\"{name}:\")\n        print(f\"  shape: {param.shape}, dtype: {param.dtype}\")\n        print(f\"  range: [{param.min().item():.4f}, {param.max().item():.4f}]\")\n        print(f\"  has NaN: {torch.isnan(param).any().item()}, has Inf: {torch.isinf(param).any().item()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Evaluate Perplexity\n",
    "\n",
    "Measure model quality after quantization using perplexity on sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@torch.no_grad()\ndef calculate_perplexity(\n    model: nn.Module,\n    tokenizer,\n    text: str,\n    device: torch.device,\n    max_length: int = 512,\n    debug: bool = False,\n) -> float:\n    \"\"\"\n    Calculate perplexity on given text.\n    \"\"\"\n    model.eval()\n    model.to(device)\n    \n    # Tokenize\n    encodings = tokenizer(\n        text,\n        return_tensors='pt',\n        truncation=True,\n        max_length=max_length,\n    ).to(device)\n    \n    input_ids = encodings['input_ids']\n    \n    if debug:\n        print(f\"Input shape: {input_ids.shape}\")\n        print(f\"Model device: {next(model.parameters()).device}\")\n        print(f\"Input device: {input_ids.device}\")\n    \n    # Forward pass\n    outputs = model(input_ids, labels=input_ids)\n    loss = outputs.loss\n    \n    if debug:\n        print(f\"Loss: {loss.item():.4f}\")\n        print(f\"Logits shape: {outputs.logits.shape}\")\n        print(f\"Logits has NaN: {torch.isnan(outputs.logits).any().item()}\")\n        print(f\"Logits has Inf: {torch.isinf(outputs.logits).any().item()}\")\n        print(f\"Logits range: [{outputs.logits.min().item():.4f}, {outputs.logits.max().item():.4f}]\")\n    \n    perplexity = torch.exp(loss).item()\n    return perplexity\n\n\n# Sample text for evaluation\nsample_text = \"\"\"The quick brown fox jumps over the lazy dog. \nMachine learning is a subset of artificial intelligence that enables computers to learn from data. \nDeep neural networks have revolutionized many fields including computer vision and natural language processing.\nQuantization is a technique to reduce model size by using lower precision representations for weights and activations.\"\"\"\n\n# Calculate perplexity with debug\nprint(\"=\" * 60)\nprint(\"Perplexity Evaluation (with debug)\")\nprint(\"=\" * 60)\nppl = calculate_perplexity(model_ptq, tokenizer, sample_text, device, debug=True)\nprint(f\"\\nPerplexity after PTQ: {ppl:.2f}\")\n\n# If perplexity is too high, check a single layer's quantization in detail\nif ppl > 1000:\n    print(\"\\n--- HIGH PERPLEXITY DEBUG ---\")\n    print(\"Checking first quantized layer in detail...\")\n    \n    first_layer_name = list(quant_weights.keys())[0]\n    info = quant_weights[first_layer_name]\n    print(f\"\\nLayer: {first_layer_name}\")\n    print(f\"Quantized weight dtype: {info['weight_int4'].dtype}\")\n    print(f\"Scale: {info['scale']}\")\n    print(f\"Zero point: {info['zero_point']}\")\n    \n    # Re-dequantize and check\n    w_q = info['weight_int4']\n    scale = info['scale']\n    zp = info['zero_point']\n    w_dq = dequantize_per_tensor_int4(w_q, scale, zp)\n    print(f\"Dequantized range: [{w_dq.min().item():.6f}, {w_dq.max().item():.6f}]\")\n    print(f\"Dequantized has NaN: {torch.isnan(w_dq).any().item()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Inference with Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    model: nn.Module,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    device: torch.device,\n",
    "    max_new_tokens: int = 100,\n",
    "    temperature: float = 0.7,\n",
    "    do_sample: bool = True,\n",
    "    use_chat_template: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate text with the model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Prepare input\n",
    "    if use_chat_template and hasattr(tokenizer, 'apply_chat_template'):\n",
    "        messages = [{'role': 'user', 'content': prompt}]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=False,\n",
    "        )\n",
    "        inputs = tokenizer(text, return_tensors='pt').to(device)\n",
    "    else:\n",
    "        inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature if do_sample else 1.0,\n",
    "            do_sample=do_sample,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode (skip input tokens)\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:],\n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "# Test generation\n",
    "prompts = [\n",
    "    \"Explain quantum computing in simple terms:\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Write a short poem about AI:\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Inference with Per-Tensor Int4 Quantized Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    response = generate_text(\n",
    "        model_ptq, tokenizer, prompt, device,\n",
    "        max_new_tokens=50, do_sample=False,\n",
    "    )\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11) QAT: Prepare Model for Training\n",
    "\n",
    "Replace linear layers with Int4Linear for Quantization-Aware Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload fresh model for QAT\n",
    "print(\"Loading fresh model for QAT...\")\n",
    "model_qat = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float32,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Replace linear layers\n",
    "skip = ['lm_head'] if SKIP_LM_HEAD else []\n",
    "model_qat = replace_linear_with_int4(model_qat, skip_patterns=skip, symmetric=SYMMETRIC)\n",
    "\n",
    "# Move to device\n",
    "model_qat = model_qat.to(device)\n",
    "\n",
    "# Count Int4Linear layers\n",
    "int4_count = sum(1 for m in model_qat.modules() if isinstance(m, Int4Linear))\n",
    "print(f\"\\nTotal Int4Linear layers: {int4_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12) QAT Training Loop (Demo)\n",
    "\n",
    "A minimal QAT training example. For full training, use a proper dataset and training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_qat_step(\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    input_ids: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    gradient_accumulation_steps: int = 1,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Single QAT training step.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    outputs = model(input_ids=input_ids, labels=labels)\n",
    "    loss = outputs.loss / gradient_accumulation_steps\n",
    "    loss.backward()\n",
    "    \n",
    "    return loss.item() * gradient_accumulation_steps\n",
    "\n",
    "\n",
    "def train_qat_demo(\n",
    "    model: nn.Module,\n",
    "    tokenizer,\n",
    "    device: torch.device,\n",
    "    num_steps: int = 10,\n",
    "    learning_rate: float = 1e-5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Demo QAT training loop.\n",
    "    \"\"\"\n",
    "    # Sample training texts\n",
    "    train_texts = [\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"Machine learning enables computers to learn from data.\",\n",
    "        \"Neural networks are inspired by biological neurons.\",\n",
    "        \"Deep learning has transformed artificial intelligence.\",\n",
    "        \"Quantization reduces model size while preserving accuracy.\",\n",
    "    ]\n",
    "    \n",
    "    # Setup optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    print(f\"Starting QAT demo training for {num_steps} steps...\")\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Get random training text\n",
    "        text = train_texts[step % len(train_texts)]\n",
    "        \n",
    "        # Tokenize\n",
    "        encodings = tokenizer(\n",
    "            text,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            padding='max_length',\n",
    "        ).to(device)\n",
    "        \n",
    "        input_ids = encodings['input_ids']\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        # Training step\n",
    "        optimizer.zero_grad()\n",
    "        loss = train_qat_step(model, optimizer, input_ids, labels)\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss)\n",
    "        \n",
    "        if (step + 1) % 5 == 0:\n",
    "            print(f\"Step {step + 1}/{num_steps}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    print(f\"\\nQAT demo training complete!\")\n",
    "    print(f\"Initial loss: {losses[0]:.4f}, Final loss: {losses[-1]:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "\n",
    "# Run demo training\n",
    "losses = train_qat_demo(model_qat, tokenizer, device, num_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13) Save/Load Quantized Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_quantized_checkpoint(\n",
    "    model: nn.Module,\n",
    "    quantized_weights: dict,\n",
    "    path: str,\n",
    "    model_name: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Save quantized model checkpoint.\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'format': 'torchao_int4_per_tensor',\n",
    "        'model_name': model_name,\n",
    "        'quantization': {\n",
    "            'bits': 4,\n",
    "            'granularity': 'per_tensor',\n",
    "            'symmetric': SYMMETRIC,\n",
    "            'quant_min': QUANT_MIN,\n",
    "            'quant_max': QUANT_MAX,\n",
    "        },\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'quantized_weights': quantized_weights,\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Saved checkpoint to: {path}\")\n",
    "\n",
    "\n",
    "def load_quantized_checkpoint(path: str, model: nn.Module = None):\n",
    "    \"\"\"\n",
    "    Load quantized model checkpoint.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(path, map_location='cpu')\n",
    "    \n",
    "    print(f\"Loaded checkpoint format: {checkpoint['format']}\")\n",
    "    print(f\"Model name: {checkpoint['model_name']}\")\n",
    "    print(f\"Quantization config: {checkpoint['quantization']}\")\n",
    "    \n",
    "    if model is not None:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"Loaded model state dict\")\n",
    "    \n",
    "    return checkpoint\n",
    "\n",
    "\n",
    "# Example save (uncomment to run)\n",
    "# save_quantized_checkpoint(\n",
    "#     model_ptq,\n",
    "#     quant_weights,\n",
    "#     'qwen3_int4_per_tensor.pt',\n",
    "#     model_name=MODEL_NAME,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14) Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Per-tensor int4 quantization** using TorchAO's low-level primitives (`quantize_affine`, `dequantize_affine`)\n",
    "2. **PTQ (Post-Training Quantization)**: Direct weight quantization with reconstruction error analysis\n",
    "3. **QAT (Quantization-Aware Training)**: Fake quantization with STE for training\n",
    "4. **Inference**: Generation with quantized model\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- TorchAO's `Int4WeightOnlyConfig` only supports per-group quantization (group_size=32/64/128/256)\n",
    "- For **true per-tensor** quantization, use `block_size = tuple(weight.shape)`\n",
    "- Per-tensor has lower accuracy but better hardware compatibility\n",
    "- QAT can help recover accuracy lost from aggressive quantization\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Train with larger dataset for better QAT results\n",
    "- Add LoRA for parameter-efficient fine-tuning\n",
    "- Compare accuracy vs per-group quantization\n",
    "- Export to target deployment format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Notebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}