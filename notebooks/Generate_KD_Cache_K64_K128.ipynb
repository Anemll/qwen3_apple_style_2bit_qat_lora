{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE5FVqXEUsK3"
      },
      "source": [
        "# Generate KD Cache (K=64 or K=128)\n",
        "\n",
        "This notebook generates higher-fidelity KD caches for 2-bit QAT training.\n",
        "Richer teacher signal helps with quality recovery.\n",
        "\n",
        "**Available configurations:**\n",
        "- **K=64**: Good balance of quality vs size (~6 GB)\n",
        "- **K=128**: Maximum fidelity for fine-tuning (~12 GB)\n",
        "\n",
        "**Changes from default (K=32):**\n",
        "- `--top_k 64/128` (up from 32)\n",
        "- `--random_negatives 512/1024` (up from 256)\n",
        "- Optionally: longer context `--max_length 256`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "1WlB-VNXUsK4",
        "outputId": "88785f3d-908d-45d1-de84-31dded9ae0df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'qwen3_apple_style_2bit_qat_lora' already exists and is not an empty directory.\n",
            "/content/qwen3_apple_style_2bit_qat_lora/qwen3_apple_style_2bit_qat_lora\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# SETUP: Clone repo and install dependencies\n",
        "# ============================================================\n",
        "\n",
        "!git clone https://github.com/anemll/qwen3_apple_style_2bit_qat_lora.git\n",
        "%cd qwen3_apple_style_2bit_qat_lora\n",
        "!pip install -q transformers accelerate datasets sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "n3sW64s3UsK5",
        "outputId": "d730f392-02db-4c30-edc9-e56f323b29d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Will generate cache: alpaca_chat_think_both_L128_K128_R1024\n",
            "  - Top-K: 128\n",
            "  - Random negatives: 1024\n",
            "  - Max length: 128\n",
            "  - Sequences: 20000\n",
            "  - Estimated size: ~13 GB\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CONFIG - Choose K=64 or K=128\n",
        "# ============================================================\n",
        "\n",
        "MODEL_NAME = 'Qwen/Qwen3-0.6B'\n",
        "\n",
        "# ============================================================\n",
        "# TOP-K CONFIGURATION (uncomment ONE option)\n",
        "# ============================================================\n",
        "\n",
        "# Option A: K=64 - Good balance of quality vs size (~6 GB)\n",
        "TOP_K = 64\n",
        "RANDOM_NEGATIVES = 512\n",
        "\n",
        "# Option B: K=128 - Maximum fidelity for fine-tuning (~12 GB)\n",
        "TOP_K = 128\n",
        "RANDOM_NEGATIVES = 1024  # Also increase negatives for K=128\n",
        "\n",
        "# ============================================================\n",
        "# Other parameters\n",
        "# ============================================================\n",
        "MAX_LENGTH = 128        # Sequence length (can increase to 256 for longer context)\n",
        "NUM_SEQUENCES = 20000   # Number of sequences to cache\n",
        "\n",
        "# Output cache name (auto-generated from params)\n",
        "CACHE_NAME = f\"alpaca_chat_think_both_L{MAX_LENGTH}_K{TOP_K}_R{RANDOM_NEGATIVES}\"\n",
        "CACHE_DIR = f\"caches/{CACHE_NAME}\"\n",
        "\n",
        "print(f\"Will generate cache: {CACHE_NAME}\")\n",
        "print(f\"  - Top-K: {TOP_K}\")\n",
        "print(f\"  - Random negatives: {RANDOM_NEGATIVES}\")\n",
        "print(f\"  - Max length: {MAX_LENGTH}\")\n",
        "print(f\"  - Sequences: {NUM_SEQUENCES}\")\n",
        "print(f\"  - Estimated size: ~{TOP_K * 0.1:.0f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "lnbJzdW0UsK5",
        "outputId": "bdcf9338-9100-4fef-8753-0beb92164b6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# MOUNT GOOGLE DRIVE (for saving cache)\n",
        "# ============================================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create destination directory\n",
        "!mkdir -p /content/drive/MyDrive/qwen3_caches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Ct7Z2ccHUsK5",
        "outputId": "2c50d463-459c-4b09-82a8-855b76ed70d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qwen3_apple_style_2bit_qat_lora\n",
            "[device] cuda | dtype=torch.bfloat16\n",
            "2025-12-25 18:14:57.241273: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766686497.265968   29544 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766686497.271188   29544 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766686497.284573   29544 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766686497.284594   29544 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766686497.284598   29544 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766686497.284602   29544 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-25 18:14:57.288513: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[write] shard_00000.pt | N=1024\n",
            "[write] shard_00001.pt | N=1024\n",
            "[write] shard_00002.pt | N=1024\n",
            "[write] shard_00003.pt | N=1024\n",
            "[write] shard_00004.pt | N=1024\n",
            "[write] shard_00005.pt | N=1024\n",
            "[write] shard_00006.pt | N=1024\n",
            "[write] shard_00007.pt | N=1024\n",
            "[write] shard_00008.pt | N=1024\n",
            "[write] shard_00009.pt | N=1024\n",
            "[write] shard_00010.pt | N=1024\n",
            "[write] shard_00011.pt | N=1024\n",
            "[write] shard_00012.pt | N=1024\n",
            "[write] shard_00013.pt | N=1024\n",
            "[write] shard_00014.pt | N=1024\n",
            "[write] shard_00015.pt | N=1024\n",
            "[write] shard_00016.pt | N=1024\n",
            "[write] shard_00017.pt | N=1024\n",
            "[write] shard_00018.pt | N=1024\n",
            "[write] shard_00019.pt | N=544\n",
            "Done. Cached sequences=20000 | out_dir=caches/alpaca_chat_think_both_L128_K128_R1024\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# GENERATE KD CACHE (K=64 or K=128 based on config)\n",
        "# ============================================================\n",
        "# K=64:  ~30-60 minutes depending on GPU\n",
        "# K=128: ~60-90 minutes depending on GPU\n",
        "\n",
        "# ============================================================\n",
        "# GENERATE KD CACHE (K=64 or K=128 based on config)\n",
        "# ============================================================\n",
        "# K=64:  ~30-60 minutes depending on GPU\n",
        "# K=128: ~60-90 minutes depending on GPU\n",
        "\n",
        "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
        "\n",
        "!python scripts/precompute_teacher_topk.py \\\n",
        "  --teacher_model_name_or_path {MODEL_NAME} \\\n",
        "  --dataset_name tatsu-lab/alpaca \\\n",
        "  --dataset_split train \\\n",
        "  --dataset_format alpaca_chat \\\n",
        "  --enable_thinking both \\\n",
        "  --max_length {MAX_LENGTH} \\\n",
        "  --topk {TOP_K} \\\n",
        "  --rand_neg {RANDOM_NEGATIVES} \\\n",
        "  --num_sequences {NUM_SEQUENCES} \\\n",
        "  --output_dir {CACHE_DIR} \\\n",
        "  --batch_size 32 \\\n",
        "  --device auto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "DIew-bhPUsK6",
        "outputId": "d446629d-cffd-4767-d128-258fa718455d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[cache] Generated 20 shards\n",
            "[cache] Meta info:\n",
            "  - top_k: None\n",
            "  - random_negatives: None\n",
            "  - max_length: 128\n",
            "  - total_sequences: None\n",
            "[cache] Total size: 16.36 GB\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# VERIFY CACHE\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "if os.path.isdir(CACHE_DIR):\n",
        "    # Count shards\n",
        "    shards = [f for f in os.listdir(CACHE_DIR) if f.startswith('shard_')]\n",
        "    print(f\"[cache] Generated {len(shards)} shards\")\n",
        "\n",
        "    # Check meta.json\n",
        "    meta_path = os.path.join(CACHE_DIR, 'meta.json')\n",
        "    if os.path.exists(meta_path):\n",
        "        with open(meta_path) as f:\n",
        "            meta = json.load(f)\n",
        "        print(f\"[cache] Meta info:\")\n",
        "        print(f\"  - top_k: {meta.get('top_k')}\")\n",
        "        print(f\"  - random_negatives: {meta.get('random_negatives')}\")\n",
        "        print(f\"  - max_length: {meta.get('max_length')}\")\n",
        "        print(f\"  - total_sequences: {meta.get('total_sequences')}\")\n",
        "\n",
        "    # Calculate size\n",
        "    total_size = sum(os.path.getsize(os.path.join(CACHE_DIR, f)) for f in os.listdir(CACHE_DIR))\n",
        "    print(f\"[cache] Total size: {total_size / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(f\"[cache] ERROR: {CACHE_DIR} not found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "0rukuigJUsK6",
        "outputId": "9022db5e-29d3-41ad-cf94-b1eaccecbb0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[save] Copying alpaca_chat_think_both_L128_K128_R1024 to Google Drive...\n",
            "         17.57G 100%   88.52MB/s    0:03:09 (xfr#21, to-chk=0/22)\n",
            "[save] Successfully saved to Google Drive: 21 files\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# COPY CACHE TO GOOGLE DRIVE (folder, no compression)\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "\n",
        "DEST_DIR_GD = \"/content/drive/MyDrive/qwen3_caches\"\n",
        "\n",
        "if os.path.isdir(CACHE_DIR):\n",
        "    print(f\"[save] Copying {CACHE_NAME} to Google Drive...\")\n",
        "    !rsync -ah --info=progress2 {CACHE_DIR}/ {DEST_DIR_GD}/{CACHE_NAME}/\n",
        "\n",
        "    # Verify\n",
        "    gd_path = f\"{DEST_DIR_GD}/{CACHE_NAME}\"\n",
        "    if os.path.isdir(gd_path):\n",
        "        num_files = len(os.listdir(gd_path))\n",
        "        print(f\"[save] Successfully saved to Google Drive: {num_files} files\")\n",
        "    else:\n",
        "        print(f\"[save] ERROR: Failed to copy to Google Drive\")\n",
        "else:\n",
        "    print(f\"[save] ERROR: {CACHE_DIR} not found. Run cache generation first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_a_IuneUsK6",
        "outputId": "c9fcb134-3ddb-42d5-8899-90697493ea6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[archive] Compressing alpaca_chat_think_both_L128_K128_R1024...\n",
            "alpaca_chat_think_both_L128_K128_R1024/\n",
            "alpaca_chat_think_both_L128_K128_R1024/shard_00018.pt\n",
            "alpaca_chat_think_both_L128_K128_R1024/shard_00011.pt\n",
            "alpaca_chat_think_both_L128_K128_R1024/shard_00000.pt\n",
            "alpaca_chat_think_both_L128_K128_R1024/shard_00010.pt\n",
            "alpaca_chat_think_both_L128_K128_R1024/shard_00008.pt\n",
            "alpaca_chat_think_both_L128_K128_R1024/shard_00005.pt\n",
            "alpaca_chat_think_both_L128_K128_R1024/meta.json\n",
            "alpaca_chat_think_both_L128_K128_R1024/shard_00015.pt\n",
            "alpaca_chat_think_both_L128_K128_R1024/shard_00016.pt\n",
            "alpaca_chat_think_both_L128_K128_R1024/shard_00009.pt\n",
            "alpaca_chat_think_both_L128_K128_R1024/shard_00001.pt\n",
            "alpaca_chat_think_both_L128_K128_R1024/shard_00003.pt\n",
            "alpaca_chat_think_both_L128_K128_R1024/shard_00006.pt\n",
            "alpaca_chat_think_both_L128_K128_R1024/shard_00014.pt\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# OPTIONAL: Also create compressed archive\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "\n",
        "CREATE_ARCHIVE = True  # Set to True if you want .tgz as well\n",
        "\n",
        "if CREATE_ARCHIVE and os.path.isdir(CACHE_DIR):\n",
        "    print(f\"[archive] Compressing {CACHE_NAME}...\")\n",
        "    !tar -zcvf {CACHE_NAME}.tgz -C caches {CACHE_NAME}\n",
        "\n",
        "    # Copy archive to Google Drive\n",
        "    print(f\"[save] Copying {CACHE_NAME}.tgz to Google Drive...\")\n",
        "    !rsync -ah --info=progress2 {CACHE_NAME}.tgz {DEST_DIR_GD}/\n",
        "\n",
        "    # Verify\n",
        "    archive_size = os.path.getsize(f\"{DEST_DIR_GD}/{CACHE_NAME}.tgz\")\n",
        "    print(f\"[save] Archive saved: {archive_size / (1024**3):.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfuGIUbvUsK6"
      },
      "source": [
        "## Summary\n",
        "\n",
        "**Cache configurations:**\n",
        "\n",
        "| Config | Top-K | Negatives | Est. Size | Use Case |\n",
        "|--------|-------|-----------|-----------|----------|\n",
        "| K=64   | 64    | 512       | ~6 GB     | Good balance for 2-bit QAT |\n",
        "| K=128  | 128   | 1024      | ~12 GB    | Maximum fidelity for fine-tuning |\n",
        "\n",
        "**To use this cache in training notebooks:**\n",
        "\n",
        "For K=64:\n",
        "```python\n",
        "CACHE_DIR_CHAT = 'caches/alpaca_chat_think_both_L128_K64_R512'\n",
        "```\n",
        "\n",
        "For K=128:\n",
        "```python\n",
        "CACHE_DIR_CHAT = 'caches/alpaca_chat_think_both_L128_K128_R1024'\n",
        "```\n",
        "\n",
        "**Load from Google Drive:**\n",
        "```python\n",
        "CACHE_NAME = \"alpaca_chat_think_both_L128_K64_R512\"  # or K128_R1024\n",
        "!rsync -ah --info=progress2 /content/drive/MyDrive/qwen3_caches/{CACHE_NAME}/ caches/{CACHE_NAME}/\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}