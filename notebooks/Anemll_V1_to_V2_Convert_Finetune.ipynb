{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V1 to V2 Conversion + Finetuning\n",
    "\n",
    "This notebook:\n",
    "1. Loads a trained V1 checkpoint\n",
    "2. Converts V1 scales to V2 format (unit-norm + rank_magnitude)\n",
    "3. Finetunes the V2 model with scale-only training\n",
    "\n",
    "## V1 → V2 Conversion:\n",
    "- V1: `scales = scale_A @ scale_B` (arbitrary magnitudes)\n",
    "- V2: `scales = (g * A_dir) @ B_dir` where A_dir, B_dir are unit-norm\n",
    "\n",
    "The conversion extracts norms from V1 scales and stores them in `rank_magnitude`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GOOGLE DRIVE PATHS\n",
    "# ============================================================\n",
    "\n",
    "GD_RUNS = '/content/drive/MyDrive/qwen3_runs'\n",
    "GD_CACHES = '/content/drive/MyDrive/qwen3_caches'\n",
    "\n",
    "LOCAL_RUNS = 'runs'\n",
    "LOCAL_CACHES = 'caches'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone/update repo\n",
    "!git clone https://github.com/anemll/qwen3_apple_style_2bit_qat_lora.git || (cd qwen3_apple_style_2bit_qat_lora && git pull)\n",
    "%cd qwen3_apple_style_2bit_qat_lora\n",
    "!git fetch && git pull && git reset --hard HEAD\n",
    "\n",
    "import sys\n",
    "[sys.modules.pop(k) for k in list(sys.modules) if k.startswith('qat_lora')]\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q transformers accelerate safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CONFIGURATION\n# ============================================================\n\nimport torch\nimport os\n\nMODEL_ID = 'Qwen/Qwen3-0.6B'\n\n# V1 checkpoint archive name (the .tgz file in GD_RUNS)\nV1_ARCHIVE = 'anemll_q4_a4_e2e_v2_scales_only.tgz'\nV1_FOLDER = 'anemll_q4_a4_e2e_v2_scales_only'\n\n# Extract V1 checkpoint from .tgz\nos.makedirs(LOCAL_RUNS, exist_ok=True)\nv1_extract_path = f'{LOCAL_RUNS}/{V1_FOLDER}'\nif not os.path.exists(v1_extract_path):\n    print(f'Extracting {V1_ARCHIVE} from Google Drive...')\n    !tar -xzf {GD_RUNS}/{V1_ARCHIVE} -C {LOCAL_RUNS}/\nelse:\n    print(f'V1 checkpoint already extracted at {v1_extract_path}')\n\n# Path to the actual checkpoint file\nV1_CHECKPOINT = f'{v1_extract_path}/model_state_dict.pt'\nprint(f'V1 checkpoint: {V1_CHECKPOINT}')\n\n# Verify it exists\nassert os.path.exists(V1_CHECKPOINT), f'V1 checkpoint not found at {V1_CHECKPOINT}'\n\n# Quantization config (must match V1 checkpoint)\nLUT_BITS = 4\nLUT_SIZE = 2**LUT_BITS\nSCALE_RANK = 4\n\nATTN_LUT_BITS = 4\nATTN_LUT_SIZE = 2**ATTN_LUT_BITS\nATTN_SCALE_RANK = 4\n\n# Training\nBATCH_SIZE = 32 if torch.cuda.is_available() else 4\nDISTILL_TEMP = 2.0\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nDTYPE = torch.bfloat16\n\nQUAL = f'q{LUT_BITS}_a{ATTN_LUT_BITS}'\nprint(f'Quality: {QUAL}')\nprint(f'Device: {DEVICE}, dtype: {DTYPE}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD KD CACHE\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "\n",
    "CACHE_NAME = 'alpaca_chat_think_both_L128_K128_R1024'\n",
    "CACHE_TGZ = f'{CACHE_NAME}.tgz'\n",
    "\n",
    "!mkdir -p {LOCAL_CACHES}\n",
    "\n",
    "cache_local_path = f'{LOCAL_CACHES}/{CACHE_NAME}'\n",
    "if not os.path.exists(cache_local_path):\n",
    "    print(f'Extracting {CACHE_TGZ} from Google Drive...')\n",
    "    !tar -xzf {GD_CACHES}/{CACHE_TGZ} -C {LOCAL_CACHES}/\n",
    "else:\n",
    "    print(f'Cache already exists at {cache_local_path}')\n",
    "\n",
    "!ls -la {cache_local_path}/ | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load V1 Model and Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD BASE MODEL\n",
    "# ============================================================\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(f'Loading {MODEL_ID}...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "# Load for V1\n",
    "v1_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=DTYPE,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "print(f'Base model loaded: {sum(p.numel() for p in v1_model.parameters()):,} params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# REPLACE WITH V1 LAYERS AND LOAD CHECKPOINT\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "import importlib\n",
    "import qat_lora\n",
    "importlib.reload(qat_lora)\n",
    "\n",
    "from qat_lora import (\n",
    "    AnemllQATLinear,\n",
    "    AnemllQuantConfig,\n",
    "    replace_linear_with_anemll,\n",
    "    load_checkpoint,\n",
    ")\n",
    "\n",
    "# Create V1 configs\n",
    "v1_mlp_config = AnemllQuantConfig(\n",
    "    lut_size=LUT_SIZE,\n",
    "    scale_rank=SCALE_RANK,\n",
    ")\n",
    "v1_attn_config = AnemllQuantConfig(\n",
    "    lut_size=ATTN_LUT_SIZE,\n",
    "    scale_rank=ATTN_SCALE_RANK,\n",
    ")\n",
    "\n",
    "# Replace with V1 layers\n",
    "print('Replacing with V1 AnemllQATLinear...')\n",
    "replace_linear_with_anemll(\n",
    "    v1_model,\n",
    "    config=v1_mlp_config,\n",
    "    attn_config=v1_attn_config,\n",
    ")\n",
    "\n",
    "# Load V1 checkpoint\n",
    "print(f'\\nLoading V1 checkpoint from {V1_CHECKPOINT}...')\n",
    "v1_model.load_state_dict(torch.load(V1_CHECKPOINT, map_location='cpu'), strict=False)\n",
    "v1_model.to(DEVICE)\n",
    "print('V1 checkpoint loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUATE V1 MODEL\n",
    "# ============================================================\n",
    "\n",
    "from qat_lora import evaluate_kd_loss\n",
    "\n",
    "v1_model.eval()\n",
    "v1_loss = evaluate_kd_loss(v1_model, cache_local_path, DEVICE, num_samples=50, temperature=DISTILL_TEMP)\n",
    "print(f'V1 KD Loss: {v1_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Create V2 Model and Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CREATE V2 MODEL\n",
    "# ============================================================\n",
    "\n",
    "from qat_lora import (\n",
    "    AnemllQATLinearV2,\n",
    "    AnemllQuantConfigV2,\n",
    "    replace_linear_with_anemll_v2,\n",
    "    freeze_Q_all,\n",
    ")\n",
    "\n",
    "# Load fresh base model for V2\n",
    "print('Loading fresh base model for V2...')\n",
    "v2_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=DTYPE,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Create V2 configs\n",
    "v2_mlp_config = AnemllQuantConfigV2(\n",
    "    lut_size=LUT_SIZE,\n",
    "    scale_rank=SCALE_RANK,\n",
    ")\n",
    "v2_attn_config = AnemllQuantConfigV2(\n",
    "    lut_size=ATTN_LUT_SIZE,\n",
    "    scale_rank=ATTN_SCALE_RANK,\n",
    ")\n",
    "\n",
    "# Replace with V2 layers\n",
    "print('Replacing with V2 AnemllQATLinearV2...')\n",
    "replace_linear_with_anemll_v2(\n",
    "    v2_model,\n",
    "    mlp_config=v2_mlp_config,\n",
    "    attn_config=v2_attn_config,\n",
    ")\n",
    "\n",
    "v2_model.to(DEVICE)\n",
    "print('V2 model created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONVERT V1 → V2\n",
    "# ============================================================\n",
    "\n",
    "def convert_v1_layer_to_v2(v1_layer, v2_layer):\n",
    "    \"\"\"Convert V1 layer parameters to V2 format.\n",
    "    \n",
    "    V1: scales = A @ B (arbitrary magnitudes)\n",
    "    V2: scales = (g * A_dir) @ B_dir (unit-norm + magnitude)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Copy base parameters\n",
    "        v2_layer.weight.data = v1_layer.weight.data.clone()\n",
    "        if v1_layer.bias is not None and v2_layer.bias is not None:\n",
    "            v2_layer.bias.data = v1_layer.bias.data.clone()\n",
    "        v2_layer.lut.data = v1_layer.lut.data.clone()\n",
    "        \n",
    "        # Get V1 scales (handle potential padding)\n",
    "        A = v1_layer.scale_A  # [out, rank]\n",
    "        B_full = v1_layer.scale_B  # [rank, padded_in]\n",
    "        B = B_full[:, :v1_layer.in_features]  # [rank, in]\n",
    "        \n",
    "        # Compute norms\n",
    "        A_norms = A.norm(dim=0, keepdim=True).clamp(min=1e-8)  # [1, rank]\n",
    "        B_norms = B.norm(dim=1, keepdim=True).clamp(min=1e-8)  # [rank, 1]\n",
    "        \n",
    "        # V2 stores unit-norm directions + magnitude\n",
    "        A_dir = A / A_norms  # [out, rank] unit-norm columns\n",
    "        B_dir = B / B_norms  # [rank, in] unit-norm rows\n",
    "        \n",
    "        # Magnitude is product of norms\n",
    "        rank_magnitude = (A_norms.squeeze() * B_norms.squeeze())  # [rank]\n",
    "        \n",
    "        # Store in V2 layer\n",
    "        v2_layer.scale_A.data = A_dir.to(v1_layer.weight.dtype)\n",
    "        v2_layer.scale_B.data = B_dir.to(v1_layer.weight.dtype)\n",
    "        v2_layer.rank_magnitude.data = rank_magnitude.to(v1_layer.weight.dtype)\n",
    "\n",
    "\n",
    "print('Converting V1 → V2...')\n",
    "converted = 0\n",
    "\n",
    "# Collect V1 and V2 layers\n",
    "v1_layers = {name: m for name, m in v1_model.named_modules() \n",
    "             if type(m).__name__ == 'AnemllQATLinear'}\n",
    "v2_layers = {name: m for name, m in v2_model.named_modules() \n",
    "             if type(m).__name__ == 'AnemllQATLinearV2'}\n",
    "\n",
    "print(f'Found {len(v1_layers)} V1 layers, {len(v2_layers)} V2 layers')\n",
    "\n",
    "# Convert each layer\n",
    "for name in v1_layers:\n",
    "    if name in v2_layers:\n",
    "        convert_v1_layer_to_v2(v1_layers[name], v2_layers[name])\n",
    "        converted += 1\n",
    "        if converted <= 3:\n",
    "            print(f'  Converted: {name}')\n",
    "\n",
    "print(f'\\nConverted {converted} layers')\n",
    "\n",
    "# Free V1 model memory\n",
    "del v1_model\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FREEZE Q FOR V2\n",
    "# ============================================================\n",
    "\n",
    "print('Freezing Q (computing indices once)...')\n",
    "freeze_Q_all(v2_model, verbose=False)\n",
    "print('Q frozen for all V2 layers.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUATE CONVERTED V2 MODEL\n",
    "# ============================================================\n",
    "\n",
    "v2_model.eval()\n",
    "v2_converted_loss = evaluate_kd_loss(v2_model, cache_local_path, DEVICE, num_samples=50, temperature=DISTILL_TEMP)\n",
    "print(f'\\n=== Conversion Results ===')\n",
    "print(f'V1 Loss: {v1_loss:.4f}')\n",
    "print(f'V2 Loss (after conversion): {v2_converted_loss:.4f}')\n",
    "print(f'Difference: {abs(v2_converted_loss - v1_loss):.4f}')\n",
    "\n",
    "if abs(v2_converted_loss - v1_loss) < 0.1:\n",
    "    print('Conversion successful - losses are close!')\n",
    "else:\n",
    "    print('Note: Some difference expected due to different forward implementations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Finetune V2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# V2 SCALE FINETUNING (MLP + ATTENTION)\n",
    "# ============================================================\n",
    "\n",
    "from qat_lora import train_e2e, unfreeze_model_for_training_v2\n",
    "\n",
    "# Unfreeze scales for training\n",
    "unfreeze_model_for_training_v2(v2_model)\n",
    "\n",
    "# Enable all scales\n",
    "for name, module in v2_model.named_modules():\n",
    "    if type(module).__name__ == 'AnemllQATLinearV2':\n",
    "        if hasattr(module, 'scale_A') and module.scale_A is not None:\n",
    "            module.scale_A.requires_grad = True\n",
    "            module.scale_B.requires_grad = True\n",
    "            module.rank_magnitude.requires_grad = True\n",
    "        module.weight.requires_grad = False  # Keep weights frozen\n",
    "\n",
    "trainable = sum(p.numel() for p in v2_model.parameters() if p.requires_grad)\n",
    "print(f'Trainable params: {trainable:,}')\n",
    "\n",
    "# Train all scales\n",
    "print('\\nStarting V2 scale finetuning...')\n",
    "e2e_result = train_e2e(\n",
    "    model=v2_model,\n",
    "    cache_dir=cache_local_path,\n",
    "    device=DEVICE,\n",
    "    max_steps=2000,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=1e-4,\n",
    "    use_cosine_schedule=True,\n",
    "    warmup_steps=100,\n",
    "    min_lr_ratio=0.1,\n",
    "    temperature=DISTILL_TEMP,\n",
    "    train_weights=False,\n",
    "    train_scales=True,\n",
    "    hard_top1_weight=0.0,\n",
    "    hard_full_weight=0.0,\n",
    "    logging_steps=20,\n",
    "    eval_steps=100,\n",
    "    verbose=True,\n",
    "    train_mlp_only=False,  # Train all scales\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE CONVERTED + FINETUNED V2 CHECKPOINT\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "\n",
    "V2_RUN_NAME = f'anemll_v2_{QUAL}_from_v1_finetuned'\n",
    "V2_SAVE_DIR = f'{LOCAL_RUNS}/{V2_RUN_NAME}'\n",
    "os.makedirs(V2_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Save checkpoint\n",
    "torch.save(v2_model.state_dict(), f'{V2_SAVE_DIR}/model_state_dict.pt')\n",
    "\n",
    "# Also save to tmp for quick access\n",
    "torch.save(v2_model.state_dict(), '/tmp/v2_from_v1_finetuned.pt')\n",
    "\n",
    "print(f'Saved V2 checkpoint to {V2_SAVE_DIR}')\n",
    "print(f'Also saved to /tmp/v2_from_v1_finetuned.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "v2_model.eval()\n",
    "final_loss = evaluate_kd_loss(v2_model, cache_local_path, DEVICE, num_samples=50, temperature=DISTILL_TEMP)\n",
    "\n",
    "print(f'\\n=== Final Results ===')\n",
    "print(f'V1 Original:        {v1_loss:.4f}')\n",
    "print(f'V2 After Convert:   {v2_converted_loss:.4f}')\n",
    "print(f'V2 After Finetune:  {final_loss:.4f}')\n",
    "print(f'Total Improvement:  {v1_loss - final_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# UPLOAD TO GOOGLE DRIVE\n",
    "# ============================================================\n",
    "\n",
    "!tar -czvf {V2_RUN_NAME}.tgz -C {LOCAL_RUNS} {V2_RUN_NAME}\n",
    "!cp {V2_RUN_NAME}.tgz {GD_RUNS}/\n",
    "print(f'\\nUploaded to {GD_RUNS}/{V2_RUN_NAME}.tgz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FREEZE FOR INFERENCE\n",
    "# ============================================================\n",
    "\n",
    "from qat_lora import freeze_model_for_inference_v2\n",
    "\n",
    "print('Freezing V2 model for inference...')\n",
    "freeze_model_for_inference_v2(v2_model, verbose=False)\n",
    "print('Ready for inference!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST INFERENCE\n",
    "# ============================================================\n",
    "\n",
    "def run_inference(model, tokenizer, prompt, max_new_tokens=256):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "        {'role': 'user', 'content': prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text, return_tensors='pt').to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "    \n",
    "    return tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "prompts = [\n",
    "    'What is the capital of France?',\n",
    "    'Explain quantum mechanics briefly.',\n",
    "    'What is 2+2?',\n",
    "]\n",
    "\n",
    "v2_model.eval()\n",
    "for prompt in prompts:\n",
    "    response = run_inference(v2_model, tokenizer, prompt)\n",
    "    print(f'Prompt: {prompt}')\n",
    "    print(f'Response: {response}')\n",
    "    print('-' * 50)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}