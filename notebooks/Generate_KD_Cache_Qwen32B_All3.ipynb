{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate KD Cache with Qwen3-32B Teacher (All 3 Variants)\n",
    "\n",
    "This notebook generates a high-quality KD cache using **Qwen3-32B** as the teacher model.\n",
    "\n",
    "**Three thinking variants:**\n",
    "- `true` - with thinking tags (`<think>...</think>`)\n",
    "- `false` - without thinking tags\n",
    "- `none` - raw text (no chat template)\n",
    "\n",
    "**Requirements:**\n",
    "- GPU with 80GB+ VRAM (A100 80GB, H100) for full precision\n",
    "- Or 40GB+ VRAM with 4-bit quantization\n",
    "\n",
    "**Estimated time:** ~4-8 hours for 50K sequences on A100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CHECK GPU\n",
    "# ============================================================\n",
    "\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SETUP: Clone repo and install dependencies\n",
    "# ============================================================\n",
    "\n",
    "!git clone https://github.com/anemll/qwen3_apple_style_2bit_qat_lora.git 2>/dev/null || echo \"Repo already exists\"\n",
    "%cd qwen3_apple_style_2bit_qat_lora\n",
    "!git pull origin main\n",
    "\n",
    "!pip install -q transformers accelerate datasets sentencepiece\n",
    "!pip install -q bitsandbytes  # For 4-bit quantization if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "# Teacher model - Qwen3-32B\n",
    "MODEL_NAME = 'Qwen/Qwen3-32B'\n",
    "\n",
    "# Dataset\n",
    "DATASET_NAME = 'teknium/OpenHermes-2.5'\n",
    "DATASET_FORMAT = 'openhermes'\n",
    "\n",
    "# Cache parameters\n",
    "TOP_K = 128           # Number of top-k logits to cache\n",
    "RAND_NEG = 64         # Random negatives\n",
    "MAX_LENGTH = 128      # Sequence length\n",
    "NUM_SEQUENCES = 50000 # Total sequences (3x variants = 150K effective samples)\n",
    "SHARD_SIZE = 1000     # Samples per shard\n",
    "\n",
    "# Batch size - adjust based on GPU memory\n",
    "# A100 80GB: batch_size=8-16\n",
    "# A100 40GB with 4-bit: batch_size=16-32\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Enable all 3 thinking variants\n",
    "ENABLE_THINKING = 'all'\n",
    "\n",
    "# Output cache name\n",
    "CACHE_NAME = f\"openhermes_32B_L{MAX_LENGTH}_K{TOP_K}_ALL_N{NUM_SEQUENCES//1000}K\"\n",
    "CACHE_DIR = f\"caches/{CACHE_NAME}\"\n",
    "\n",
    "print(f\"=\"*60)\n",
    "print(f\"KD Cache Configuration\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"Teacher model:    {MODEL_NAME}\")\n",
    "print(f\"Dataset:          {DATASET_NAME}\")\n",
    "print(f\"Thinking mode:    {ENABLE_THINKING} (3 variants)\")\n",
    "print(f\"Top-K:            {TOP_K}\")\n",
    "print(f\"Random negatives: {RAND_NEG}\")\n",
    "print(f\"Max length:       {MAX_LENGTH}\")\n",
    "print(f\"Sequences:        {NUM_SEQUENCES}\")\n",
    "print(f\"Batch size:       {BATCH_SIZE}\")\n",
    "print(f\"Output:           {CACHE_DIR}\")\n",
    "print(f\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# OPTIONAL: Mount Google Drive for saving cache\n",
    "# ============================================================\n",
    "\n",
    "SAVE_TO_DRIVE = True  # Set to False if not using Colab/Drive\n",
    "\n",
    "if SAVE_TO_DRIVE:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        DRIVE_CACHE_DIR = '/content/drive/MyDrive/qwen3_caches'\n",
    "        !mkdir -p {DRIVE_CACHE_DIR}\n",
    "        print(f\"Google Drive mounted. Will save to: {DRIVE_CACHE_DIR}\")\n",
    "    except:\n",
    "        print(\"Not running in Colab or Drive mount failed. Saving locally only.\")\n",
    "        SAVE_TO_DRIVE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GENERATE KD CACHE\n",
    "# ============================================================\n",
    "# Estimated time: ~4-8 hours for 50K sequences on A100\n",
    "#\n",
    "# With --enable_thinking all, each input generates 3 variants:\n",
    "#   1. With thinking tags\n",
    "#   2. Without thinking tags  \n",
    "#   3. Raw text (no template)\n",
    "# ============================================================\n",
    "\n",
    "%cd /content/qwen3_apple_style_2bit_qat_lora\n",
    "\n",
    "!python scripts/precompute_teacher_topk.py \\\n",
    "    --teacher_model_name_or_path {MODEL_NAME} \\\n",
    "    --dataset_name {DATASET_NAME} \\\n",
    "    --dataset_split train \\\n",
    "    --dataset_format {DATASET_FORMAT} \\\n",
    "    --enable_thinking {ENABLE_THINKING} \\\n",
    "    --max_length {MAX_LENGTH} \\\n",
    "    --topk {TOP_K} \\\n",
    "    --rand_neg {RAND_NEG} \\\n",
    "    --num_sequences {NUM_SEQUENCES} \\\n",
    "    --output_dir {CACHE_DIR} \\\n",
    "    --batch_size {BATCH_SIZE} \\\n",
    "    --shard_size {SHARD_SIZE} \\\n",
    "    --dtype bf16 \\\n",
    "    --device auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VERIFY CACHE\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "if os.path.isdir(CACHE_DIR):\n",
    "    # Count shards\n",
    "    shards = [f for f in os.listdir(CACHE_DIR) if f.startswith('shard_')]\n",
    "    print(f\"[cache] Generated {len(shards)} shards\")\n",
    "    \n",
    "    # Check meta.json\n",
    "    meta_path = os.path.join(CACHE_DIR, 'meta.json')\n",
    "    if os.path.exists(meta_path):\n",
    "        with open(meta_path) as f:\n",
    "            meta = json.load(f)\n",
    "        print(f\"[cache] Meta info:\")\n",
    "        for k, v in meta.items():\n",
    "            print(f\"  - {k}: {v}\")\n",
    "    \n",
    "    # Calculate size\n",
    "    total_size = sum(os.path.getsize(os.path.join(CACHE_DIR, f)) for f in os.listdir(CACHE_DIR))\n",
    "    print(f\"[cache] Total size: {total_size / (1024**3):.2f} GB\")\n",
    "else:\n",
    "    print(f\"[cache] ERROR: {CACHE_DIR} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INSPECT A SAMPLE SHARD\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "shard_files = sorted([f for f in os.listdir(CACHE_DIR) if f.startswith('shard_')])\n",
    "if shard_files:\n",
    "    shard_path = os.path.join(CACHE_DIR, shard_files[0])\n",
    "    shard = torch.load(shard_path, map_location='cpu')\n",
    "    \n",
    "    print(f\"Shard: {shard_files[0]}\")\n",
    "    print(f\"Keys: {list(shard.keys())}\")\n",
    "    \n",
    "    if 'input_ids' in shard:\n",
    "        print(f\"\\ninput_ids shape: {shard['input_ids'].shape}\")\n",
    "    if 'topk_indices' in shard:\n",
    "        print(f\"topk_indices shape: {shard['topk_indices'].shape}\")\n",
    "    if 'topk_probs' in shard:\n",
    "        print(f\"topk_probs shape: {shard['topk_probs'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE TO GOOGLE DRIVE\n",
    "# ============================================================\n",
    "\n",
    "if SAVE_TO_DRIVE and os.path.isdir(CACHE_DIR):\n",
    "    print(f\"[save] Copying {CACHE_NAME} to Google Drive...\")\n",
    "    !rsync -ah --info=progress2 {CACHE_DIR}/ {DRIVE_CACHE_DIR}/{CACHE_NAME}/\n",
    "    \n",
    "    # Verify\n",
    "    gd_path = f\"{DRIVE_CACHE_DIR}/{CACHE_NAME}\"\n",
    "    if os.path.isdir(gd_path):\n",
    "        num_files = len(os.listdir(gd_path))\n",
    "        print(f\"[save] Successfully saved to Google Drive: {num_files} files\")\n",
    "    else:\n",
    "        print(f\"[save] ERROR: Failed to copy to Google Drive\")\n",
    "else:\n",
    "    print(\"[save] Skipping Google Drive save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Optimization Options\n",
    "\n",
    "If you run out of memory with Qwen3-32B, try these options:\n",
    "\n",
    "### Option 1: Reduce batch size\n",
    "```python\n",
    "BATCH_SIZE = 4  # or even 2\n",
    "```\n",
    "\n",
    "### Option 2: Use 4-bit quantization (add to script)\n",
    "```bash\n",
    "# Modify precompute_teacher_topk.py to support load_in_4bit\n",
    "# Or use this in a custom cell:\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Option 3: Use Qwen3-14B instead\n",
    "```python\n",
    "MODEL_NAME = 'Qwen/Qwen3-14B'  # Fits in 40GB with bf16\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Cache generated:** `openhermes_32B_L128_K128_ALL_N50K`\n",
    "\n",
    "**Contains:**\n",
    "- 50K sequences Ã— 3 variants = ~150K effective training samples\n",
    "- Top-128 teacher logits from Qwen3-32B\n",
    "- Three thinking modes: think, no-think, no-template\n",
    "\n",
    "**To use in training:**\n",
    "```python\n",
    "CACHE_DIR = 'caches/openhermes_32B_L128_K128_ALL_N50K'\n",
    "```\n",
    "\n",
    "**Load from Google Drive:**\n",
    "```bash\n",
    "rsync -ah --info=progress2 /content/drive/MyDrive/qwen3_caches/openhermes_32B_L128_K128_ALL_N50K/ caches/openhermes_32B_L128_K128_ALL_N50K/\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
