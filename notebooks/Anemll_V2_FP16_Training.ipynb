{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anemll V2 FP16 Training Pipeline\n",
    "\n",
    "Full FP16 training pipeline for V2 QAT - ensures no precision mismatch with ANE.\n",
    "\n",
    "## Key Differences from BF16 Training:\n",
    "- Model loaded in **FP16** (not BF16)\n",
    "- LUT created in **FP16**\n",
    "- Indices computed in **FP16** (same as ANE)\n",
    "- Uses **GradScaler** for stable FP16 training (CUDA)\n",
    "\n",
    "## Benefits:\n",
    "- **No precision mismatch**: Indices computed in FP16 = same as ANE\n",
    "- **No snap needed**: Model is already in FP16 format\n",
    "- **ANE-ready**: Direct export without conversion\n",
    "\n",
    "## Pipeline:\n",
    "1. Load model in FP16\n",
    "2. Replace linears with AnemllQATLinearV2\n",
    "3. Convert V2 layers to FP16 (ensures LUT is FP16)\n",
    "4. Freeze Q (indices computed in FP16)\n",
    "5. Train with use_fp16=True (GradScaler on CUDA)\n",
    "6. Save - no snap needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GOOGLE DRIVE PATHS (STANDARD)\n",
    "# ============================================================\n",
    "\n",
    "# Checkpoints/runs go here\n",
    "GD_RUNS = '/content/drive/MyDrive/qwen3_runs'\n",
    "\n",
    "# KD caches go here\n",
    "GD_CACHES = '/content/drive/MyDrive/qwen3_caches'\n",
    "\n",
    "# Local directories (on Colab VM)\n",
    "LOCAL_RUNS = 'runs'\n",
    "LOCAL_CACHES = 'caches'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo if needed\n",
    "!git clone https://github.com/anemll/qwen3_apple_style_2bit_qat_lora.git || (cd qwen3_apple_style_2bit_qat_lora && git pull)\n",
    "%cd qwen3_apple_style_2bit_qat_lora\n",
    "!git fetch && git pull\n",
    "!git reset --hard HEAD\n",
    "\n",
    "import sys\n",
    "[sys.modules.pop(k) for k in list(sys.modules) if k.startswith('qat_lora')]\n",
    "\n",
    "# Import V2 modules\n",
    "from qat_lora import (\n",
    "    AnemllQATLinearV2,\n",
    "    AnemllQuantConfigV2,\n",
    "    replace_linear_with_anemll_v2,\n",
    "    freeze_Q_all,\n",
    "    freeze_model_for_inference_v2,\n",
    "    unfreeze_model_for_training_v2,\n",
    "    convert_model_to_fp16_v2,  # NEW: FP16 conversion\n",
    "    evaluate_kd_loss,\n",
    "    train_all_layers,\n",
    "    train_e2e,\n",
    "    save_checkpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD KD CACHE FROM GOOGLE DRIVE\n",
    "# ============================================================\n",
    "\n",
    "CACHE_NAME = 'alpaca_chat_think_both_L128_K128_R1024'\n",
    "CACHE_TGZ = f'{CACHE_NAME}.tgz'\n",
    "\n",
    "!mkdir -p {LOCAL_CACHES}\n",
    "\n",
    "import os\n",
    "cache_local_path = f'{LOCAL_CACHES}/{CACHE_NAME}'\n",
    "if not os.path.exists(cache_local_path):\n",
    "    print(f'Extracting {CACHE_TGZ} from Google Drive...')\n",
    "    !tar -xzf {GD_CACHES}/{CACHE_TGZ} -C {LOCAL_CACHES}/\n",
    "else:\n",
    "    print(f'Cache already exists at {cache_local_path}')\n",
    "\n",
    "!ls -la {cache_local_path}/ | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION - FP16 TRAINING\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "\n",
    "# Model\n",
    "MODEL_ID = 'Qwen/Qwen3-0.6B'\n",
    "\n",
    "# Quantization config\n",
    "LUT_BITS = 4\n",
    "LUT_SIZE = 2**LUT_BITS\n",
    "SCALE_RANK = 4\n",
    "\n",
    "# Attention quantization\n",
    "ATTN_LUT_BITS = 4\n",
    "ATTN_LUT_SIZE = 2**ATTN_LUT_BITS\n",
    "ATTN_SCALE_RANK = 4\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 32 if torch.cuda.is_available() else 4\n",
    "GRAD_ACCUM = 1 if torch.cuda.is_available() else 4\n",
    "\n",
    "# KD / Distillation params\n",
    "DISTILL_TEMP = 2.0\n",
    "\n",
    "# Device - CUDA required for GradScaler\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"FP16 training with GradScaler requires CUDA!\")\n",
    "DEVICE = torch.device('cuda')\n",
    "\n",
    "# *** FP16 from the start! ***\n",
    "DTYPE = torch.float16\n",
    "USE_FP16_TRAINING = True\n",
    "\n",
    "QUAL = f'q{LUT_BITS}_a{ATTN_LUT_BITS}_fp16'\n",
    "\n",
    "print(f'=== FP16 Training Pipeline ===')\n",
    "print(f'Quality: {QUAL}')\n",
    "print(f'Device: {DEVICE}, dtype: {DTYPE}')\n",
    "print(f'GradScaler: Enabled (CUDA FP16)')\n",
    "print(f'Quant config: lut={LUT_SIZE}, rank={SCALE_RANK}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD MODEL IN FP16\n",
    "# ============================================================\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(f'Loading {MODEL_ID} in FP16...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,  # FP16 from the start!\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(f'Loaded in FP16. Parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
    "print(f'Model dtype: {next(model.parameters()).dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# REPLACE LINEARS WITH V2\n",
    "# ============================================================\n",
    "\n",
    "mlp_config = AnemllQuantConfigV2(\n",
    "    lut_size=LUT_SIZE,\n",
    "    scale_rank=SCALE_RANK,\n",
    "    learnable_lut=False,\n",
    "    force_positive_scales=True,\n",
    "    positive_scale_method=\"abs\",\n",
    "    magnitude_activation=\"softplus\",\n",
    "    magnitude_eps=1e-6,\n",
    ")\n",
    "\n",
    "attn_config = AnemllQuantConfigV2(\n",
    "    lut_size=ATTN_LUT_SIZE,\n",
    "    scale_rank=ATTN_SCALE_RANK,\n",
    "    learnable_lut=False,\n",
    "    force_positive_scales=True,\n",
    "    positive_scale_method=\"abs\",\n",
    "    magnitude_activation=\"softplus\",\n",
    "    magnitude_eps=1e-6,\n",
    ")\n",
    "\n",
    "print('Replacing linear layers with V2...')\n",
    "count = replace_linear_with_anemll_v2(\n",
    "    model,\n",
    "    mlp_config=mlp_config,\n",
    "    attn_config=attn_config,\n",
    "    quantize_attn=True,\n",
    "    quantize_lm_head=False,\n",
    ")\n",
    "\n",
    "print(f'Replaced {count} layers with AnemllQATLinearV2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONVERT V2 LAYERS TO FP16 (CRITICAL!)\n",
    "# ============================================================\n",
    "# This ensures LUT is created in FP16, so indices will be computed in FP16\n",
    "# This must be done BEFORE freeze_Q_all()\n",
    "\n",
    "print('Converting V2 layers to FP16 (LUT, scales, weights)...')\n",
    "convert_model_to_fp16_v2(model, verbose=True)\n",
    "\n",
    "# Verify FP16 conversion\n",
    "for name, module in model.named_modules():\n",
    "    if type(module).__name__ == 'AnemllQATLinearV2':\n",
    "        print(f'\\nVerifying {name}:')\n",
    "        print(f'  weight.dtype: {module.weight.dtype}')\n",
    "        print(f'  scale_A.dtype: {module.scale_A.dtype}')\n",
    "        print(f'  lut.dtype: {module.lut.dtype}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FREEZE Q IN FP16\n",
    "# ============================================================\n",
    "# Now indices will be computed using FP16 arithmetic - same as ANE!\n",
    "\n",
    "print('Freezing Q (computing indices in FP16)...')\n",
    "freeze_Q_all(model, verbose=False)\n",
    "print('Q frozen for all layers in FP16.')\n",
    "print('Indices computed in FP16 = same precision as ANE inference!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VERIFY FP16 SETUP\n",
    "# ============================================================\n",
    "\n",
    "print('=== FP16 Setup Verification ===')\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if type(module).__name__ == 'AnemllQATLinearV2':\n",
    "        print(f'\\nLayer: {name}')\n",
    "        print(f'  weight.dtype: {module.weight.dtype}')\n",
    "        print(f'  scale_A.dtype: {module.scale_A.dtype}')\n",
    "        print(f'  lut.dtype: {module.lut.dtype}')\n",
    "        print(f'  _Q.dtype: {module._Q.dtype if module._Q is not None else \"None\"}')\n",
    "        print(f'  _indices.dtype: {module._indices.dtype if module._indices is not None else \"None\"}')\n",
    "        break\n",
    "\n",
    "# Initial loss\n",
    "initial_loss = evaluate_kd_loss(model, cache_local_path, DEVICE, num_samples=40, temperature=DISTILL_TEMP)\n",
    "print(f'\\nInitial KD Loss (FP16): {initial_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FP16 Training with GradScaler\n",
    "\n",
    "Training uses:\n",
    "- `torch.amp.autocast` for FP16 forward pass\n",
    "- `torch.cuda.amp.GradScaler` for gradient scaling\n",
    "\n",
    "This prevents gradient underflow in FP16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FP16 E2E TRAINING - MLP SCALES\n",
    "# ============================================================\n",
    "\n",
    "print('=== FP16 Training with GradScaler ===')\n",
    "\n",
    "e2e_mlp_result = train_e2e(\n",
    "    model=model,\n",
    "    cache_dir=cache_local_path,\n",
    "    device=DEVICE,\n",
    "    max_steps=4000,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=5e-4,\n",
    "    use_cosine_schedule=True,\n",
    "    warmup_steps=100,\n",
    "    min_lr_ratio=0.1,\n",
    "    temperature=DISTILL_TEMP,\n",
    "    train_weights=False,\n",
    "    train_scales=True,\n",
    "    hard_top1_weight=0.0,\n",
    "    hard_full_weight=0.0,\n",
    "    logging_steps=20,\n",
    "    eval_steps=100,\n",
    "    verbose=True,\n",
    "    train_mlp_only=True,\n",
    "    use_fp16=True,  # FP16 with GradScaler!\n",
    ")\n",
    "\n",
    "print(f'\\nMLP Training Result:')\n",
    "print(f'  Initial: {e2e_mlp_result[\"initial_loss\"]:.4f}')\n",
    "print(f'  Final: {e2e_mlp_result[\"final_loss\"]:.4f}')\n",
    "print(f'  Best: {e2e_mlp_result[\"best_loss\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FP16 E2E TRAINING - ATTENTION SCALES\n",
    "# ============================================================\n",
    "\n",
    "unfreeze_model_for_training_v2(model)\n",
    "\n",
    "# Freeze MLP scales, only train attention\n",
    "for name, module in model.named_modules():\n",
    "    if type(module).__name__ == 'AnemllQATLinearV2':\n",
    "        is_attn = any(x in name for x in ['q_proj', 'k_proj', 'v_proj', 'o_proj'])\n",
    "        if hasattr(module, 'scale_A') and module.scale_A is not None:\n",
    "            module.scale_A.requires_grad = is_attn\n",
    "            module.scale_B.requires_grad = is_attn\n",
    "            module.rank_magnitude.requires_grad = is_attn\n",
    "        module.weight.requires_grad = False\n",
    "\n",
    "e2e_attn_result = train_e2e(\n",
    "    model=model,\n",
    "    cache_dir=cache_local_path,\n",
    "    device=DEVICE,\n",
    "    max_steps=2000,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=1e-4,\n",
    "    use_cosine_schedule=True,\n",
    "    warmup_steps=100,\n",
    "    min_lr_ratio=0.1,\n",
    "    temperature=DISTILL_TEMP,\n",
    "    train_weights=False,\n",
    "    train_scales=True,\n",
    "    hard_top1_weight=0.0,\n",
    "    hard_full_weight=0.0,\n",
    "    logging_steps=20,\n",
    "    eval_steps=100,\n",
    "    verbose=True,\n",
    "    train_mlp_only=False,\n",
    "    use_fp16=True,\n",
    ")\n",
    "\n",
    "print(f'\\nAttention Training Result:')\n",
    "print(f'  Initial: {e2e_attn_result[\"initial_loss\"]:.4f}')\n",
    "print(f'  Final: {e2e_attn_result[\"final_loss\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FP16 E2E TRAINING - JOINT MLP + ATTENTION\n",
    "# ============================================================\n",
    "\n",
    "unfreeze_model_for_training_v2(model)\n",
    "\n",
    "# Enable ALL scales\n",
    "for name, module in model.named_modules():\n",
    "    if type(module).__name__ == 'AnemllQATLinearV2':\n",
    "        if hasattr(module, 'scale_A') and module.scale_A is not None:\n",
    "            module.scale_A.requires_grad = True\n",
    "            module.scale_B.requires_grad = True\n",
    "            module.rank_magnitude.requires_grad = True\n",
    "        module.weight.requires_grad = False\n",
    "\n",
    "e2e_joint_result = train_e2e(\n",
    "    model=model,\n",
    "    cache_dir=cache_local_path,\n",
    "    device=DEVICE,\n",
    "    max_steps=1000,\n",
    "    batch_size=32,\n",
    "    lr=5e-5,\n",
    "    use_cosine_schedule=True,\n",
    "    warmup_steps=50,\n",
    "    min_lr_ratio=0.1,\n",
    "    temperature=DISTILL_TEMP,\n",
    "    train_weights=False,\n",
    "    train_scales=True,\n",
    "    hard_top1_weight=0.0,\n",
    "    hard_full_weight=0.0,\n",
    "    logging_steps=20,\n",
    "    eval_steps=100,\n",
    "    verbose=True,\n",
    "    train_mlp_only=False,\n",
    "    use_fp16=True,\n",
    ")\n",
    "\n",
    "print(f'\\nJoint Training Result:')\n",
    "print(f'  Initial: {e2e_joint_result[\"initial_loss\"]:.4f}')\n",
    "print(f'  Final: {e2e_joint_result[\"final_loss\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE FP16 CHECKPOINT (NO SNAP NEEDED!)\n",
    "# ============================================================\n",
    "# Model is already in FP16, indices computed in FP16\n",
    "# No snap_for_ane() needed - model is ANE-ready!\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "RUN_NAME = f'anemll_v2_{QUAL}_fp16_trained'\n",
    "SAVE_DIR = f'{LOCAL_RUNS}/{RUN_NAME}'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Save state dict\n",
    "torch.save(model.state_dict(), f'{SAVE_DIR}/model_state_dict.pt')\n",
    "\n",
    "# Save config\n",
    "config = {\n",
    "    'model_id': MODEL_ID,\n",
    "    'version': 'v2',\n",
    "    'precision': 'fp16',  # FP16 trained!\n",
    "    'lut_bits': LUT_BITS,\n",
    "    'attn_lut_bits': ATTN_LUT_BITS,\n",
    "    'scale_rank': SCALE_RANK,\n",
    "    'attn_scale_rank': ATTN_SCALE_RANK,\n",
    "    'initial_loss': initial_loss,\n",
    "    'final_loss': e2e_joint_result['final_loss'],\n",
    "    'training_mode': 'fp16_gradscaler',\n",
    "}\n",
    "with open(f'{SAVE_DIR}/config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f'\\n=== FP16 Checkpoint Saved ===')\n",
    "print(f'Path: {SAVE_DIR}')\n",
    "print(f'Precision: FP16 (ANE-ready, no snap needed)')\n",
    "print(f'Final Loss: {e2e_joint_result[\"final_loss\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to Google Drive\n",
    "!tar -czvf {RUN_NAME}.tgz -C {LOCAL_RUNS} {RUN_NAME}\n",
    "!cp {RUN_NAME}.tgz {GD_RUNS}/\n",
    "print(f'Uploaded to {GD_RUNS}/{RUN_NAME}.tgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST FP16 INFERENCE\n",
    "# ============================================================\n",
    "\n",
    "# Freeze for fast inference\n",
    "freeze_model_for_inference_v2(model, verbose=False)\n",
    "\n",
    "def run_inference(model, tokenizer, prompt, max_new_tokens=256):\n",
    "    messages = [{'role': 'user', 'content': prompt}]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True, enable_thinking=True\n",
    "    )\n",
    "    inputs = tokenizer(text, return_tensors='pt').to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs, max_new_tokens=max_new_tokens,\n",
    "            do_sample=True, temperature=0.6, top_p=0.9,\n",
    "        )\n",
    "    return tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=False)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "prompts = [\n",
    "    'What is the capital of France?',\n",
    "    'What is 2+2?',\n",
    "    'Explain quantum mechanics briefly.',\n",
    "]\n",
    "\n",
    "print('=== FP16 Inference Test ===')\n",
    "for prompt in prompts:\n",
    "    response = run_inference(model, tokenizer, prompt)\n",
    "    print(f'\\nPrompt: {prompt}')\n",
    "    print(f'Response: {response}')\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "## FP16 Training Complete!\n",
    "\n",
    "The model was trained entirely in FP16:\n",
    "- LUT created in FP16\n",
    "- Indices computed in FP16  \n",
    "- Training with GradScaler for stability\n",
    "- No snap_for_ane() needed - model is ANE-ready!\n",
    "\n",
    "## Key Difference from BF16 Training:\n",
    "- **BF16**: Indices computed in BF16, then snapped to FP16 (potential mismatch)\n",
    "- **FP16**: Indices computed in FP16 from start (same as ANE)\n",
    "\n",
    "## Next Steps:\n",
    "1. Convert to CoreML with ANEMLL converter\n",
    "2. Deploy to ANE\n",
    "3. No additional snapping needed!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
