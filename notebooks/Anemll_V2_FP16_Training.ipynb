{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Anemll V2 FP16 Training Pipeline\n\nConverts V1 checkpoint to V2 format and fine-tunes in FP16 for ANE deployment.\n\n## Pipeline:\n1. Load V1 model + checkpoint\n2. Create V2 model\n3. Convert V1 → V2 (extract norms into rank_magnitude)\n4. Convert to FP16 (LUT, scales, weights)\n5. Freeze Q (indices computed in FP16 = same as ANE)\n6. Train in FP16\n7. Save - no snap needed!\n\n## Key Benefits:\n- **No precision mismatch**: Indices computed in FP16 = same as ANE\n- **Proper V1→V2 conversion**: Preserves trained scales\n- **ANE-ready**: Direct export without conversion"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GOOGLE DRIVE PATHS (STANDARD)\n",
    "# ============================================================\n",
    "\n",
    "# Checkpoints/runs go here\n",
    "GD_RUNS = '/content/drive/MyDrive/qwen3_runs'\n",
    "\n",
    "# KD caches go here\n",
    "GD_CACHES = '/content/drive/MyDrive/qwen3_caches'\n",
    "\n",
    "# Local directories (on Colab VM)\n",
    "LOCAL_RUNS = 'runs'\n",
    "LOCAL_CACHES = 'caches'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone repo if needed\n!git clone https://github.com/anemll/qwen3_apple_style_2bit_qat_lora.git || (cd qwen3_apple_style_2bit_qat_lora && git pull)\n%cd qwen3_apple_style_2bit_qat_lora\n!git fetch && git pull\n!git reset --hard HEAD\n\nimport sys\n[sys.modules.pop(k) for k in list(sys.modules) if k.startswith('qat_lora')]\n\n# Import both V1 and V2 modules\nfrom qat_lora import (\n    # V1 (for loading checkpoint)\n    AnemllQATLinear,\n    AnemllQuantConfig,\n    replace_linear_with_anemll,\n    # V2 (for training)\n    AnemllQATLinearV2,\n    AnemllQuantConfigV2,\n    replace_linear_with_anemll_v2,\n    freeze_Q_all,\n    freeze_model_for_inference_v2,\n    unfreeze_model_for_training_v2,\n    convert_model_to_fp16_v2,\n    evaluate_kd_loss,\n    train_e2e,\n    save_checkpoint,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD KD CACHE FROM GOOGLE DRIVE\n",
    "# ============================================================\n",
    "\n",
    "CACHE_NAME = 'alpaca_chat_think_both_L128_K128_R1024'\n",
    "CACHE_TGZ = f'{CACHE_NAME}.tgz'\n",
    "\n",
    "!mkdir -p {LOCAL_CACHES}\n",
    "\n",
    "import os\n",
    "cache_local_path = f'{LOCAL_CACHES}/{CACHE_NAME}'\n",
    "if not os.path.exists(cache_local_path):\n",
    "    print(f'Extracting {CACHE_TGZ} from Google Drive...')\n",
    "    !tar -xzf {GD_CACHES}/{CACHE_TGZ} -C {LOCAL_CACHES}/\n",
    "else:\n",
    "    print(f'Cache already exists at {cache_local_path}')\n",
    "\n",
    "!ls -la {cache_local_path}/ | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CONFIGURATION - V1→V2 FP16 CONVERSION\n# ============================================================\n\nimport torch\nimport os\n\n# Model\nMODEL_ID = 'Qwen/Qwen3-0.6B'\n\n# V1 checkpoint archive (the .tgz file in GD_RUNS)\n# ⚠️ SET YOUR V1 CHECKPOINT HERE:\nV1_ARCHIVE = 'anemll_q4_a4_e2e_v2_scales_only.tgz'\nV1_FOLDER = 'anemll_q4_a4_e2e_v2_scales_only'\n\n# Extract V1 checkpoint from .tgz\nos.makedirs(LOCAL_RUNS, exist_ok=True)\nv1_extract_path = f'{LOCAL_RUNS}/{V1_FOLDER}'\nif not os.path.exists(v1_extract_path):\n    print(f'Extracting {V1_ARCHIVE} from Google Drive...')\n    !tar -xzf {GD_RUNS}/{V1_ARCHIVE} -C {LOCAL_RUNS}/\nelse:\n    print(f'V1 checkpoint already extracted at {v1_extract_path}')\n\nV1_CHECKPOINT = f'{v1_extract_path}/model_state_dict.pt'\nassert os.path.exists(V1_CHECKPOINT), f'V1 checkpoint not found: {V1_CHECKPOINT}'\nprint(f'V1 checkpoint: {V1_CHECKPOINT}')\n\n# Quantization config (must match V1 checkpoint)\nLUT_BITS = 4\nLUT_SIZE = 2**LUT_BITS\nSCALE_RANK = 4\n\nATTN_LUT_BITS = 4\nATTN_LUT_SIZE = 2**ATTN_LUT_BITS\nATTN_SCALE_RANK = 4\n\n# Training\nBATCH_SIZE = 32 if torch.cuda.is_available() else 4\nDISTILL_TEMP = 2.0\n\n# Device - CUDA required for FP16 training\nif not torch.cuda.is_available():\n    raise RuntimeError(\"FP16 training requires CUDA!\")\nDEVICE = torch.device('cuda')\n\nQUAL = f'q{LUT_BITS}_a{ATTN_LUT_BITS}_fp16'\n\nprint(f'\\n=== V1→V2 FP16 Pipeline ===')\nprint(f'Quality: {QUAL}')\nprint(f'Device: {DEVICE}')\nprint(f'Quant config: lut={LUT_SIZE}, rank={SCALE_RANK}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# STEP 1: LOAD V1 MODEL + CHECKPOINT\n# ============================================================\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nprint(f'Loading {MODEL_ID} for V1...')\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n\n# Load base model (BF16 for V1 - will convert to FP16 after V2 conversion)\nv1_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n)\n\n# Create V1 configs\nv1_mlp_config = AnemllQuantConfig(\n    lut_size=LUT_SIZE,\n    scale_rank=SCALE_RANK,\n)\nv1_attn_config = AnemllQuantConfig(\n    lut_size=ATTN_LUT_SIZE,\n    scale_rank=ATTN_SCALE_RANK,\n)\n\n# Replace with V1 layers\nprint('Replacing with V1 AnemllQATLinear...')\nreplace_linear_with_anemll(\n    v1_model,\n    mlp_config=v1_mlp_config,\n    attn_config=v1_attn_config,\n    quantize_attn=True,\n    quantize_lm_head=False,\n)\n\n# Load V1 checkpoint\nprint(f'Loading V1 checkpoint: {V1_CHECKPOINT}')\nv1_state = torch.load(V1_CHECKPOINT, map_location='cpu')\nif 'model_state_dict' in v1_state:\n    v1_state = v1_state['model_state_dict']\nv1_model.load_state_dict(v1_state, strict=False)\nv1_model.to(DEVICE)\nv1_model.eval()\n\nprint('V1 model loaded!')\n\n# Evaluate V1 loss for reference\nv1_loss = evaluate_kd_loss(v1_model, cache_local_path, DEVICE, num_samples=50, temperature=DISTILL_TEMP)\nprint(f'\\nV1 KD Loss: {v1_loss:.4f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# STEP 2: CREATE V2 MODEL\n# ============================================================\n\n# Load fresh base model for V2 (in FP16!)\nprint('Loading fresh base model for V2 in FP16...')\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    torch_dtype=torch.float16,  # FP16 from the start!\n    trust_remote_code=True,\n)\n\n# Create V2 configs\nv2_mlp_config = AnemllQuantConfigV2(\n    lut_size=LUT_SIZE,\n    scale_rank=SCALE_RANK,\n    force_positive_scales=True,\n    positive_scale_method=\"abs\",\n    magnitude_activation=\"softplus\",\n    magnitude_eps=1e-6,\n)\n\nv2_attn_config = AnemllQuantConfigV2(\n    lut_size=ATTN_LUT_SIZE,\n    scale_rank=ATTN_SCALE_RANK,\n    force_positive_scales=True,\n    positive_scale_method=\"abs\",\n    magnitude_activation=\"softplus\",\n    magnitude_eps=1e-6,\n)\n\n# Replace with V2 layers\nprint('Replacing with V2 AnemllQATLinearV2...')\ncount = replace_linear_with_anemll_v2(\n    model,\n    mlp_config=v2_mlp_config,\n    attn_config=v2_attn_config,\n    quantize_attn=True,\n    quantize_lm_head=False,\n)\n\nprint(f'Replaced {count} layers with V2')\nprint(f'Model dtype: {next(model.parameters()).dtype}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# STEP 3: CONVERT V1 → V2\n# ============================================================\n\ndef convert_v1_layer_to_v2(v1_layer, v2_layer):\n    \"\"\"Convert V1 layer parameters to V2 format.\n    \n    V1: scales = A @ B (arbitrary magnitudes)\n    V2: scales = (g * A_dir) @ B_dir (unit-norm + magnitude)\n    \"\"\"\n    with torch.no_grad():\n        # Copy base parameters (convert to FP16)\n        v2_layer.weight.data = v1_layer.weight.data.to(torch.float16)\n        if v1_layer.bias is not None and v2_layer.bias is not None:\n            v2_layer.bias.data = v1_layer.bias.data.to(torch.float16)\n        \n        # Copy LUT (will be overwritten by convert_to_fp16 later)\n        v2_layer.lut.data = v1_layer.lut.data.to(torch.float16)\n        \n        # Get V1 scales (handle potential padding)\n        A = v1_layer.scale_A  # [out, rank]\n        B_full = v1_layer.scale_B  # [rank, padded_in]\n        B = B_full[:, :v1_layer.in_features]  # [rank, in]\n        \n        # Compute norms\n        A_norms = A.norm(dim=0, keepdim=True).clamp(min=1e-8)  # [1, rank]\n        B_norms = B.norm(dim=1, keepdim=True).clamp(min=1e-8)  # [rank, 1]\n        \n        # V2 stores unit-norm directions + magnitude\n        A_dir = A / A_norms  # [out, rank] unit-norm columns\n        B_dir = B / B_norms  # [rank, in] unit-norm rows\n        \n        # Magnitude is product of norms\n        rank_magnitude = (A_norms.squeeze() * B_norms.squeeze())  # [rank]\n        \n        # Store in V2 layer (FP16)\n        v2_layer.scale_A.data = A_dir.to(torch.float16)\n        v2_layer.scale_B.data = B_dir.to(torch.float16)\n        v2_layer.rank_magnitude.data = rank_magnitude.to(torch.float16)\n\n\nprint('Converting V1 → V2 in FP16...')\nconverted = 0\n\n# Collect V1 and V2 layers\nv1_layers = {name: m for name, m in v1_model.named_modules() \n             if type(m).__name__ == 'AnemllQATLinear'}\nv2_layers = {name: m for name, m in model.named_modules() \n             if type(m).__name__ == 'AnemllQATLinearV2'}\n\nprint(f'Found {len(v1_layers)} V1 layers, {len(v2_layers)} V2 layers')\n\n# Convert each layer\nfor name in v1_layers:\n    if name in v2_layers:\n        convert_v1_layer_to_v2(v1_layers[name], v2_layers[name])\n        converted += 1\n        if converted <= 3:\n            print(f'  Converted: {name}')\n\nprint(f'\\nConverted {converted} layers to V2 FP16 format')\n\n# Free V1 model memory\ndel v1_model, v1_layers\nimport gc\ngc.collect()\ntorch.cuda.empty_cache()\nprint('V1 model freed from memory')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# STEP 4: ENSURE ALL V2 LAYERS ARE FP16\n# ============================================================\n# This recomputes LUT in FP16 to ensure indices will be computed in FP16\n\nprint('Ensuring all V2 layers are FP16 (recomputing LUT)...')\nconvert_model_to_fp16_v2(model, verbose=True)\n\n# Verify FP16 conversion\nfor name, module in model.named_modules():\n    if type(module).__name__ == 'AnemllQATLinearV2':\n        print(f'\\nVerifying {name}:')\n        print(f'  weight.dtype: {module.weight.dtype}')\n        print(f'  scale_A.dtype: {module.scale_A.dtype}')\n        print(f'  lut.dtype: {module.lut.dtype}')\n        print(f'  lut range: [{module.lut.min():.4f}, {module.lut.max():.4f}]')\n        break"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# STEP 5: FREEZE Q IN FP16\n# ============================================================\n# Indices computed in FP16 = same precision as ANE!\n\nmodel.to(DEVICE)\n\nprint('Freezing Q (computing indices in FP16)...')\nfreeze_Q_all(model, verbose=False)\nprint('Q frozen for all layers in FP16.')\n\n# Verify Q is frozen\nfor name, module in model.named_modules():\n    if type(module).__name__ == 'AnemllQATLinearV2':\n        print(f'\\nVerifying {name}:')\n        print(f'  _Q.dtype: {module._Q.dtype if module._Q is not None else \"None\"}')\n        print(f'  _Q.shape: {module._Q.shape if module._Q is not None else \"None\"}')\n        print(f'  _indices.dtype: {module._indices.dtype if module._indices is not None else \"None\"}')\n        break\n\n# Evaluate converted V2 model\nmodel.eval()\nv2_converted_loss = evaluate_kd_loss(model, cache_local_path, DEVICE, num_samples=50, temperature=DISTILL_TEMP)\n\nprint(f'\\n=== Conversion Results ===')\nprint(f'V1 Loss: {v1_loss:.4f}')\nprint(f'V2 Loss (after FP16 conversion): {v2_converted_loss:.4f}')\nprint(f'Difference: {abs(v2_converted_loss - v1_loss):.4f}')\n\nif abs(v2_converted_loss - v1_loss) < 0.5:\n    print('Conversion successful - losses are close!')\nelse:\n    print('Note: Some difference expected due to FP16 precision')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# FP16 Training\n\nTraining in pure FP16 (model already in FP16):\n- Uses `torch.amp.autocast` for FP16 forward pass\n- No GradScaler needed (model is already FP16)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FP16 E2E TRAINING - MLP SCALES\n",
    "# ============================================================\n",
    "\n",
    "print('=== FP16 Training with GradScaler ===')\n",
    "\n",
    "e2e_mlp_result = train_e2e(\n",
    "    model=model,\n",
    "    cache_dir=cache_local_path,\n",
    "    device=DEVICE,\n",
    "    max_steps=4000,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=5e-4,\n",
    "    use_cosine_schedule=True,\n",
    "    warmup_steps=100,\n",
    "    min_lr_ratio=0.1,\n",
    "    temperature=DISTILL_TEMP,\n",
    "    train_weights=False,\n",
    "    train_scales=True,\n",
    "    hard_top1_weight=0.0,\n",
    "    hard_full_weight=0.0,\n",
    "    logging_steps=20,\n",
    "    eval_steps=100,\n",
    "    verbose=True,\n",
    "    train_mlp_only=True,\n",
    "    use_fp16=True,  # FP16 with GradScaler!\n",
    ")\n",
    "\n",
    "print(f'\\nMLP Training Result:')\n",
    "print(f'  Initial: {e2e_mlp_result[\"initial_loss\"]:.4f}')\n",
    "print(f'  Final: {e2e_mlp_result[\"final_loss\"]:.4f}')\n",
    "print(f'  Best: {e2e_mlp_result[\"best_loss\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FP16 E2E TRAINING - ATTENTION SCALES\n",
    "# ============================================================\n",
    "\n",
    "unfreeze_model_for_training_v2(model)\n",
    "\n",
    "# Freeze MLP scales, only train attention\n",
    "for name, module in model.named_modules():\n",
    "    if type(module).__name__ == 'AnemllQATLinearV2':\n",
    "        is_attn = any(x in name for x in ['q_proj', 'k_proj', 'v_proj', 'o_proj'])\n",
    "        if hasattr(module, 'scale_A') and module.scale_A is not None:\n",
    "            module.scale_A.requires_grad = is_attn\n",
    "            module.scale_B.requires_grad = is_attn\n",
    "            module.rank_magnitude.requires_grad = is_attn\n",
    "        module.weight.requires_grad = False\n",
    "\n",
    "e2e_attn_result = train_e2e(\n",
    "    model=model,\n",
    "    cache_dir=cache_local_path,\n",
    "    device=DEVICE,\n",
    "    max_steps=2000,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=1e-4,\n",
    "    use_cosine_schedule=True,\n",
    "    warmup_steps=100,\n",
    "    min_lr_ratio=0.1,\n",
    "    temperature=DISTILL_TEMP,\n",
    "    train_weights=False,\n",
    "    train_scales=True,\n",
    "    hard_top1_weight=0.0,\n",
    "    hard_full_weight=0.0,\n",
    "    logging_steps=20,\n",
    "    eval_steps=100,\n",
    "    verbose=True,\n",
    "    train_mlp_only=False,\n",
    "    use_fp16=True,\n",
    ")\n",
    "\n",
    "print(f'\\nAttention Training Result:')\n",
    "print(f'  Initial: {e2e_attn_result[\"initial_loss\"]:.4f}')\n",
    "print(f'  Final: {e2e_attn_result[\"final_loss\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FP16 E2E TRAINING - JOINT MLP + ATTENTION\n",
    "# ============================================================\n",
    "\n",
    "unfreeze_model_for_training_v2(model)\n",
    "\n",
    "# Enable ALL scales\n",
    "for name, module in model.named_modules():\n",
    "    if type(module).__name__ == 'AnemllQATLinearV2':\n",
    "        if hasattr(module, 'scale_A') and module.scale_A is not None:\n",
    "            module.scale_A.requires_grad = True\n",
    "            module.scale_B.requires_grad = True\n",
    "            module.rank_magnitude.requires_grad = True\n",
    "        module.weight.requires_grad = False\n",
    "\n",
    "e2e_joint_result = train_e2e(\n",
    "    model=model,\n",
    "    cache_dir=cache_local_path,\n",
    "    device=DEVICE,\n",
    "    max_steps=1000,\n",
    "    batch_size=32,\n",
    "    lr=5e-5,\n",
    "    use_cosine_schedule=True,\n",
    "    warmup_steps=50,\n",
    "    min_lr_ratio=0.1,\n",
    "    temperature=DISTILL_TEMP,\n",
    "    train_weights=False,\n",
    "    train_scales=True,\n",
    "    hard_top1_weight=0.0,\n",
    "    hard_full_weight=0.0,\n",
    "    logging_steps=20,\n",
    "    eval_steps=100,\n",
    "    verbose=True,\n",
    "    train_mlp_only=False,\n",
    "    use_fp16=True,\n",
    ")\n",
    "\n",
    "print(f'\\nJoint Training Result:')\n",
    "print(f'  Initial: {e2e_joint_result[\"initial_loss\"]:.4f}')\n",
    "print(f'  Final: {e2e_joint_result[\"final_loss\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# SAVE FP16 CHECKPOINT (NO SNAP NEEDED!)\n# ============================================================\n# Model is already in FP16, indices computed in FP16\n# No snap_for_ane() needed - model is ANE-ready!\n\nimport json\nimport os\n\nRUN_NAME = f'anemll_v2_{QUAL}_fp16_trained'\nSAVE_DIR = f'{LOCAL_RUNS}/{RUN_NAME}'\nos.makedirs(SAVE_DIR, exist_ok=True)\n\n# Save state dict\ntorch.save(model.state_dict(), f'{SAVE_DIR}/model_state_dict.pt')\n\n# Save config\nconfig = {\n    'model_id': MODEL_ID,\n    'version': 'v2',\n    'precision': 'fp16',  # FP16 trained!\n    'lut_bits': LUT_BITS,\n    'attn_lut_bits': ATTN_LUT_BITS,\n    'scale_rank': SCALE_RANK,\n    'attn_scale_rank': ATTN_SCALE_RANK,\n    'v1_loss': v1_loss,\n    'v2_converted_loss': v2_converted_loss,\n    'final_loss': e2e_joint_result['final_loss'],\n    'training_mode': 'fp16_pure',\n}\nwith open(f'{SAVE_DIR}/config.json', 'w') as f:\n    json.dump(config, f, indent=2)\n\nprint(f'\\n=== FP16 Checkpoint Saved ===')\nprint(f'Path: {SAVE_DIR}')\nprint(f'Precision: FP16 (ANE-ready, no snap needed)')\nprint(f'V1 Loss: {v1_loss:.4f}')\nprint(f'V2 Converted Loss: {v2_converted_loss:.4f}')\nprint(f'Final Loss: {e2e_joint_result[\"final_loss\"]:.4f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to Google Drive\n",
    "!tar -czvf {RUN_NAME}.tgz -C {LOCAL_RUNS} {RUN_NAME}\n",
    "!cp {RUN_NAME}.tgz {GD_RUNS}/\n",
    "print(f'Uploaded to {GD_RUNS}/{RUN_NAME}.tgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST FP16 INFERENCE\n",
    "# ============================================================\n",
    "\n",
    "# Freeze for fast inference\n",
    "freeze_model_for_inference_v2(model, verbose=False)\n",
    "\n",
    "def run_inference(model, tokenizer, prompt, max_new_tokens=256):\n",
    "    messages = [{'role': 'user', 'content': prompt}]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True, enable_thinking=True\n",
    "    )\n",
    "    inputs = tokenizer(text, return_tensors='pt').to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs, max_new_tokens=max_new_tokens,\n",
    "            do_sample=True, temperature=0.6, top_p=0.9,\n",
    "        )\n",
    "    return tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=False)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "prompts = [\n",
    "    'What is the capital of France?',\n",
    "    'What is 2+2?',\n",
    "    'Explain quantum mechanics briefly.',\n",
    "]\n",
    "\n",
    "print('=== FP16 Inference Test ===')\n",
    "for prompt in prompts:\n",
    "    response = run_inference(model, tokenizer, prompt)\n",
    "    print(f'\\nPrompt: {prompt}')\n",
    "    print(f'Response: {response}')\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Summary\n\n## V1→V2 FP16 Conversion Complete!\n\nPipeline executed:\n1. ✅ Loaded V1 checkpoint\n2. ✅ Converted V1 → V2 (extracted norms into rank_magnitude)\n3. ✅ All tensors in FP16 (LUT, scales, weights)\n4. ✅ Indices computed in FP16 (same as ANE)\n5. ✅ Fine-tuned in FP16\n6. ✅ Saved - no snap needed!\n\n## Key Benefits:\n- **Preserves V1 training**: Converts trained scales to V2 format\n- **No precision mismatch**: Indices computed in FP16 = same as ANE\n- **ANE-ready**: Direct export without conversion\n\n## Next Steps:\n1. Convert to CoreML with ANEMLL converter\n2. Deploy to ANE\n3. No additional snapping needed!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}