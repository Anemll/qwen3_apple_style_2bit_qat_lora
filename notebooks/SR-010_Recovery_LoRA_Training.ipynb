{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# SR-010: Recovery LoRA Training\n\n**Version:** 1.0.0 | **Last Updated:** 2026-01-07 PST\n\nTrain lightweight LoRA adapters on quantized V2 models to recover accuracy.\n\n**Training Modes:**\n- `recover`: CE loss on raw text (default)\n- `sft`: Supervised fine-tuning on instruction/response pairs\n- `kd`: Knowledge distillation from teacher model\n\n**Prerequisites:**\n- Trained V2 QAT checkpoint (e.g., from SR-008)\n- CUDA GPU (recommended: T4, A100, or L4)\n\n**Changelog:**\n- v1.0.0: Initial release with recover/sft/kd modes"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "%cd /content\n",
    "!git clone https://github.com/Anemll/qwen3_apple_style_2bit_qat_lora.git repo\n",
    "%cd repo\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!pip install -q transformers accelerate datasets sentencepiece protobuf wandb\n\n# Login to W&B via Colab secret (optional)\n# Go to: Colab menu -> Secrets (key icon) -> Add \"WANDB_API_KEY\"\ntry:\n    from google.colab import userdata\n    wandb_key = userdata.get('WANDB_API_KEY')\n    if wandb_key:\n        import wandb\n        wandb.login(key=wandb_key)\n        print(\"W&B: Logged in via secret\")\n    else:\n        print(\"W&B: No API key found in secrets (optional)\")\nexcept Exception as e:\n    print(f\"W&B: Secret not configured (optional) - {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "\n",
    "# Base model\n",
    "MODEL_ID = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# V2 QAT checkpoint path (from Google Drive)\n",
    "V2_CHECKPOINT = \"/content/drive/MyDrive/qwen3_runs/SR-008a-revive-hermes/final_v2_q4_r32_fp32_20260107_015153.pt\"\n",
    "\n",
    "# Training mode: \"recover\", \"sft\", or \"kd\"\n",
    "LORA_MODE = \"recover\"\n",
    "\n",
    "# Teacher model (only for kd mode)\n",
    "TEACHER_MODEL = None  # e.g., \"Qwen/Qwen3-4B-Instruct\" for KD mode\n",
    "\n",
    "# LoRA config\n",
    "RECOVERY_R = 8        # LoRA rank (8-32 typical)\n",
    "MLP_ONLY = False      # True = only MLP layers, False = MLP + attention\n",
    "\n",
    "# Training config\n",
    "MAX_STEPS = 500\n",
    "SEQ_LEN = 1024\n",
    "BATCH_SIZE = 4\n",
    "LR = 3e-4\n",
    "LOG_INTERVAL = 10\n",
    "SAVE_STEPS = 100\n",
    "\n",
    "# Data source (HuggingFace dataset)\n",
    "TRAIN_DATA_HF = \"Salesforce/wikitext\"\n",
    "HF_SUBSET = \"wikitext-103-v1\"\n",
    "HF_SPLIT = \"train\"\n",
    "HF_MAX_SAMPLES = 10000\n",
    "\n",
    "# Output\n",
    "OUTPUT_DIR = \"runs/recovery_lora\"\n",
    "GDRIVE_OUTPUT = \"/content/drive/MyDrive/qwen3_runs/SR-010-recovery-lora\"\n",
    "\n",
    "# W&B (optional)\n",
    "USE_WANDB = False\n",
    "WANDB_PROJECT = \"qwen3-recovery-lora\"\n",
    "WANDB_RUN = \"sr010-recover\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify checkpoint exists and sync to local storage\nimport os\nimport shutil\nfrom pathlib import Path\n\n# Check if checkpoint exists on Google Drive\nif not os.path.exists(V2_CHECKPOINT):\n    print(f\"ERROR: Checkpoint not found: {V2_CHECKPOINT}\")\n    print(\"\\nAvailable checkpoints in qwen3_runs:\")\n    runs_dir = Path(\"/content/drive/MyDrive/qwen3_runs\")\n    if runs_dir.exists():\n        for d in sorted(runs_dir.iterdir()):\n            if d.is_dir():\n                pts = list(d.glob(\"*.pt\"))\n                if pts:\n                    print(f\"  {d.name}/\")\n                    for pt in sorted(pts)[:5]:\n                        size_mb = pt.stat().st_size / 1e6\n                        print(f\"    - {pt.name} ({size_mb:.1f} MB)\")\nelse:\n    size_mb = os.path.getsize(V2_CHECKPOINT) / 1e6\n    print(f\"Checkpoint found: {V2_CHECKPOINT}\")\n    print(f\"Size: {size_mb:.1f} MB\")\n    \n    # Sync to local storage for faster training\n    LOCAL_CHECKPOINT_DIR = \"/content/checkpoints\"\n    os.makedirs(LOCAL_CHECKPOINT_DIR, exist_ok=True)\n    \n    local_ckpt = Path(LOCAL_CHECKPOINT_DIR) / Path(V2_CHECKPOINT).name\n    \n    if not local_ckpt.exists():\n        print(f\"\\nSyncing checkpoint to local storage...\")\n        shutil.copy(V2_CHECKPOINT, local_ckpt)\n        print(f\"  Copied to: {local_ckpt}\")\n        \n        # Also copy config.json if it exists\n        config_src = Path(V2_CHECKPOINT).parent / \"config.json\"\n        if config_src.exists():\n            shutil.copy(config_src, Path(LOCAL_CHECKPOINT_DIR) / \"config.json\")\n            print(f\"  Copied config.json\")\n    else:\n        print(f\"\\nLocal checkpoint already exists: {local_ckpt}\")\n    \n    # Update V2_CHECKPOINT to use local path\n    V2_CHECKPOINT_LOCAL = str(local_ckpt)\n    print(f\"\\nUsing local checkpoint: {V2_CHECKPOINT_LOCAL}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Recovery LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build training command (uses local checkpoint for speed)\n# V2_CHECKPOINT_LOCAL was set in the sync cell above\n\ncheckpoint_path = V2_CHECKPOINT_LOCAL if 'V2_CHECKPOINT_LOCAL' in dir() else V2_CHECKPOINT\n\ncmd = f\"\"\"\npython scripts/train_recovery_lora.py \\\n    --model {MODEL_ID} \\\n    --v2-checkpoint \"{checkpoint_path}\" \\\n    --train-data-hf {TRAIN_DATA_HF} \\\n    --hf-subset {HF_SUBSET} \\\n    --hf-split {HF_SPLIT} \\\n    --hf-max-samples {HF_MAX_SAMPLES} \\\n    --lora-mode {LORA_MODE} \\\n    --recovery-r {RECOVERY_R} \\\n    --lr {LR} \\\n    --max-steps {MAX_STEPS} \\\n    --seq-len {SEQ_LEN} \\\n    --batch-size {BATCH_SIZE} \\\n    --log-interval {LOG_INTERVAL} \\\n    --save-steps {SAVE_STEPS} \\\n    --output {OUTPUT_DIR}\n\"\"\"\n\n# Add optional flags\nif MLP_ONLY:\n    cmd += \"    --mlp-only \\\\\\n\"\n\nif LORA_MODE == \"kd\" and TEACHER_MODEL:\n    cmd += f\"    --teacher {TEACHER_MODEL} \\\\\\n\"\n\nif USE_WANDB:\n    cmd += f\"    --wandb --wandb-project {WANDB_PROJECT} --wandb-run {WANDB_RUN} \\\\\\n\"\n\nprint(\"Training command:\")\nprint(cmd)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best checkpoint\n",
    "from pathlib import Path\n",
    "\n",
    "output_path = Path(OUTPUT_DIR)\n",
    "checkpoints = sorted(output_path.glob(\"recovery_*.pt\"))\n",
    "\n",
    "if checkpoints:\n",
    "    latest_ckpt = checkpoints[-1]\n",
    "    print(f\"Found {len(checkpoints)} checkpoints\")\n",
    "    print(f\"Latest: {latest_ckpt}\")\n",
    "else:\n",
    "    print(\"No checkpoints found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference with LoRA\n",
    "# Option A: Full checkpoint with embedded LoRA\n",
    "!python scripts/test_inference.py \"{latest_ckpt}\" \\\n",
    "    --prompt \"What is the capital of France?\" \\\n",
    "    --max-tokens 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive mode\n",
    "# !python scripts/test_inference.py \"{latest_ckpt}\" --interactive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare: Base vs LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts for comparison\n",
    "TEST_PROMPTS = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain quantum mechanics briefly.\",\n",
    "    \"Write a haiku about coding.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test base model (without LoRA)\nbase_ckpt = V2_CHECKPOINT_LOCAL if 'V2_CHECKPOINT_LOCAL' in dir() else V2_CHECKPOINT\n\nprint(\"=\" * 60)\nprint(\"BASE MODEL (no LoRA)\")\nprint(\"=\" * 60)\nfor prompt in TEST_PROMPTS:\n    print(f\"\\nPrompt: {prompt}\")\n    !python scripts/test_inference.py \"{base_ckpt}\" --prompt \"{prompt}\" --max-tokens 128 2>/dev/null | tail -5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with LoRA\n",
    "print(\"=\" * 60)\n",
    "print(\"WITH RECOVERY LoRA\")\n",
    "print(\"=\" * 60)\n",
    "for prompt in TEST_PROMPTS:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    !python scripts/test_inference.py \"{latest_ckpt}\" --prompt \"{prompt}\" --max-tokens 128 2>/dev/null | tail -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory on Google Drive\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "os.makedirs(GDRIVE_OUTPUT, exist_ok=True)\n",
    "\n",
    "# Copy checkpoints\n",
    "output_path = Path(OUTPUT_DIR)\n",
    "for f in output_path.glob(\"*.pt\"):\n",
    "    dest = Path(GDRIVE_OUTPUT) / f.name\n",
    "    print(f\"Copying {f.name} -> {dest}\")\n",
    "    shutil.copy(f, dest)\n",
    "\n",
    "# Copy config.json if exists\n",
    "config_path = output_path / \"config.json\"\n",
    "if config_path.exists():\n",
    "    shutil.copy(config_path, Path(GDRIVE_OUTPUT) / \"config.json\")\n",
    "    print(\"Copied config.json\")\n",
    "\n",
    "# Copy training log if exists\n",
    "log_path = output_path / \"training_log.csv\"\n",
    "if log_path.exists():\n",
    "    shutil.copy(log_path, Path(GDRIVE_OUTPUT) / \"training_log.csv\")\n",
    "    print(\"Copied training_log.csv\")\n",
    "\n",
    "print(f\"\\nSaved to: {GDRIVE_OUTPUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plot Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log_file = Path(OUTPUT_DIR) / \"training_log.csv\"\n",
    "if log_file.exists():\n",
    "    df = pd.read_csv(log_file)\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(df['step'], df['train_loss'], label='Train Loss')\n",
    "    if 'eval_loss' in df.columns:\n",
    "        plt.plot(df['step'], df['eval_loss'], label='Eval Loss', linestyle='--')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Recovery LoRA Training')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nFinal loss: {df['train_loss'].iloc[-1]:.4f}\")\n",
    "    print(f\"Best loss: {df['train_loss'].min():.4f}\")\n",
    "else:\n",
    "    print(\"No training log found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Alternative: Knowledge Distillation Mode\n",
    "\n",
    "Use a larger teacher model to guide the LoRA training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# KD mode training (requires more VRAM)\nkd_ckpt = V2_CHECKPOINT_LOCAL if 'V2_CHECKPOINT_LOCAL' in dir() else V2_CHECKPOINT\n\nKD_CMD = f\"\"\"\npython scripts/train_recovery_lora.py \\\n    --model {MODEL_ID} \\\n    --v2-checkpoint \"{kd_ckpt}\" \\\n    --train-data-hf {TRAIN_DATA_HF} \\\n    --hf-subset {HF_SUBSET} \\\n    --lora-mode kd \\\n    --teacher Qwen/Qwen3-4B-Instruct \\\n    --kd-temperature 2.0 \\\n    --kd-alpha 0.5 \\\n    --recovery-r 8 \\\n    --lr 3e-4 \\\n    --max-steps 500 \\\n    --seq-len 512 \\\n    --batch-size 2 \\\n    --output runs/recovery_kd\n\"\"\"\n\nprint(\"KD mode command (requires A100 or similar):\")\nprint(KD_CMD)\n\n# Uncomment to run:\n# !{KD_CMD}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Alternative: SFT Mode with Alpaca\n",
    "\n",
    "Supervised fine-tuning on instruction data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# SFT mode with Alpaca dataset\nsft_ckpt = V2_CHECKPOINT_LOCAL if 'V2_CHECKPOINT_LOCAL' in dir() else V2_CHECKPOINT\n\nSFT_CMD = f\"\"\"\npython scripts/train_recovery_lora.py \\\n    --model {MODEL_ID} \\\n    --v2-checkpoint \"{sft_ckpt}\" \\\n    --train-data-hf tatsu-lab/alpaca \\\n    --dataset-format alpaca \\\n    --template-mode think \\\n    --lora-mode sft \\\n    --recovery-r 8 \\\n    --lr 3e-4 \\\n    --max-steps 1000 \\\n    --seq-len 1024 \\\n    --batch-size 4 \\\n    --output runs/recovery_sft\n\"\"\"\n\nprint(\"SFT mode command:\")\nprint(SFT_CMD)\n\n# Uncomment to run:\n# !{SFT_CMD}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "**Memory Requirements:**\n",
    "- `recover` mode: ~8-10 GB VRAM (T4 OK)\n",
    "- `sft` mode: ~8-10 GB VRAM (T4 OK)\n",
    "- `kd` mode: ~16-20 GB VRAM (A100/L4 recommended)\n",
    "\n",
    "**Recommended Settings:**\n",
    "- Start with `recovery-r 8`, increase to 16/32 if needed\n",
    "- Use `--mlp-only` first for faster iteration\n",
    "- Sequence length 1024-2048 for general recovery\n",
    "\n",
    "**Checkpoint Types:**\n",
    "- Full checkpoint: Contains base + LoRA weights (~5GB)\n",
    "- LoRA-only (`--save-lora-only`): Just LoRA weights (~17MB)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}